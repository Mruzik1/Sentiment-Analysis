How is Accuracy defined when the loss function is mean square error? Is it mean absolute percentage error? The model I use has output activation linear and is compiled with loss= mean_squared_error and the output looks like this: So what does e.g. val_acc: 0.3250 mean? Mean_squared_error should be a scalar not a percentage - shouldnt it? So is val_acc - mean squared error, or mean percentage error or another function? From definition of MSE on wikipedia:https://en.wikipedia.org/wiki/Mean_squared_error The MSE is a measure of the quality of an estimator—it is always
  non-negative, and values closer to zero are better. Does that mean a value of val_acc: 0.0 is better than val_acc: 0.325? edit: more examples of the output of accuracy metric when I train - where the accuracy is increase as I train more. While the loss function - mse should decrease. Is Accuracy well defined for mse - and how is it defined in Keras? There are at least two separate issues with your question. The first one should be clear by now from the comments by Dr. Snoopy and the other answer: accuracy is meaningless in a regression problem, such as yours; see also the comment by patyork in this Keras thread. For good or bad, the fact is that Keras will not "protect" you or any other user from putting not-meaningful requests in your code, i.e. you will not get any error, or even a warning, that you are attempting something that does not make sense, such as requesting the accuracy in a regression setting. Having clarified that, the other issue is: Since Keras does indeed return an "accuracy", even in a regression setting, what exactly is it and how is it calculated? To shed some light here, let's revert to a public dataset (since you do not provide any details about your data), namely the Boston house price dataset (saved locally as housing.csv), and run a simple experiment as follows: As in your case, the model fitting history (not shown here) shows a decreasing loss, and an accuracy roughly increasing. Let's evaluate now the model performance in the same training set, using the appropriate Keras built-in function: The exact contents of the score array depend on what exactly we have requested during model compilation; in our case here, the first element is the loss (MSE), and the second one is the "accuracy". At this point, let us have a look at the definition of Keras binary_accuracy in the metrics.py file: So, after Keras has generated the predictions y_pred, it first rounds them, and then checks to see how many of them are equal to the true labels y_true, before getting the mean. Let's replicate this operation using plain Python & Numpy code in our case, where the true labels are Y: Well, bingo! This is actually the same value returned by score[1] above... To make a long story short: since you (erroneously) request metrics=['accuracy'] in your model compilation, Keras will do its best to satisfy you, and will return some "accuracy" indeed, calculated as shown above, despite this being completely meaningless in your setting. There are quite a few settings where Keras, under the hood, performs rather meaningless operations without giving any hint or warning to the user; two of them I have happened to encounter are: Giving meaningless results when, in a multi-class setting, one happens to request loss='binary_crossentropy' (instead of categorical_crossentropy) with metrics=['accuracy'] - see my answers in Keras binary_crossentropy vs categorical_crossentropy performance? and Why is binary_crossentropy more accurate than categorical_crossentropy for multiclass classification in Keras? Disabling completely Dropout, in the extreme case when one requests a dropout rate of 1.0 - see my answer in Dropout behavior in Keras with rate=1 (dropping all input units) not as expected The loss function (Mean Square Error in this case) is used to indicate how far your predictions deviate from the target values. In the training phase, the weights are updated based on this quantity. If you are dealing with a classification problem, it is quite common to define an additional metric called accuracy. It monitors in how many cases the correct class was predicted. This is expressed as a percentage value. Consequently, a value of 0.0 means no correct decision and 1.0 only correct decisons.
While your network is training, the loss is decreasing and usually the accuracy increases. Note, that in contrast to loss, the accuracy is usally not used to update the parameters of your network. It helps to monitor the learning progress and the current performane of the network. @desertnaut has said it very clearly. Consider the following two pieces of code compile code binary_accuracy code Your labels should be integer，Because keras does not round y_true, and you get high accuracy.......I have a machine learning classification problem with 80% categorical variables. Must I use one hot encoding if I want to use some classifier for the classification? Can i pass the data to a classifier without the encoding?  I am trying to do the following for feature selection: I read the train file: I change the type of the categorical features to 'category': I use one hot encoding:  The problem is that the 3'rd part often get stuck, although I am using a strong machine. Thus, without the one hot encoding I can't do any feature selection, for determining the importance of the features. What do you recommend? Approach 1: You can use pandas' pd.get_dummies. Example 1: Example 2: The following will transform a given column into one hot. Use prefix to have multiple dummies. Approach 2: Use Scikit-learn Using a OneHotEncoder has the advantage of being able to fit on some training data and then transform on some other data using the same instance. We also have handle_unknown to further control what the encoder does with unseen data. Given a dataset with three features and four samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. Here is the link for this example: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html Much easier to use Pandas for basic one-hot encoding. If you're looking for more options you can use scikit-learn. For basic one-hot encoding with Pandas you pass your data frame into the get_dummies function. For example, if I have a dataframe called imdb_movies:  ...and I want to one-hot encode the Rated column, I do this:  This returns a new dataframe with a column for every "level" of rating that exists, along with either a 1 or 0 specifying the presence of that rating for a given observation. Usually, we want this to be part of the original dataframe. In this case, we attach our new dummy coded frame onto the original frame using "column-binding. We can column-bind by using Pandas concat function:  We can now run an analysis on our full dataframe. SIMPLE UTILITY FUNCTION I would recommend making yourself a utility function to do this quickly: Usage: Result:  Also, as per @pmalbu comment, if you would like the function to remove the original feature_to_encode then use this version: You can encode multiple features at the same time as follows: You can do it with numpy.eye and a using the array element selection mechanism: The the return value of indices_to_one_hot(nb_classes, data) is now The .reshape(-1) is there to make sure you have the right labels format (you might also have [[2], [3], [4], [0]]). One hot encoding with pandas is very easy: EDIT: Another way to one_hot using sklearn's LabelBinarizer : Firstly, easiest way to one hot encode: use Sklearn. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html Secondly, I don't think using pandas to one hot encode is that simple (unconfirmed though) Creating dummy variables in pandas for python Lastly, is it necessary for you to one hot encode? One hot encoding exponentially increases the number of features, drastically increasing the run time of any classifier or anything else you are going to run. Especially when each categorical feature has many levels. Instead you can do dummy coding. Using dummy encoding usually works well, for much less run time and complexity. A wise prof once told me, 'Less is More'.  Here's the code for my custom encoding function if you want. EDIT: Comparison to be clearer: One-hot encoding: convert n levels to n-1 columns. You can see how this will explode your memory if you have many different types (or levels) in your categorical feature. Keep in mind, this is just ONE column. Dummy Coding: Convert to numerical representations instead. Greatly saves feature space, at the cost of a bit of accuracy. You can use numpy.eye function. Result pandas as has inbuilt function "get_dummies" to get one hot encoding of that particular column/s. one line code for one-hot-encoding: Here is a solution using DictVectorizer and the Pandas DataFrame.to_dict('records') method. One-hot encoding requires bit more than converting the values to indicator variables. Typically ML process requires you to apply this coding several times to validation or test data sets and applying the model you construct to real-time observed data. You should store the mapping (transform) that was used to construct the model. A good solution would use the DictVectorizer or LabelEncoder (followed by get_dummies. Here is a function that you can use: This works on a pandas dataframe and for each column of the dataframe it creates and returns a mapping back. So you would call it like this: Then on the test data, the call is made by passing the dictionary returned back from training: An equivalent method is to use DictVectorizer. A related post on the same is on my blog. I mention it here since it provides some reasoning behind this approach over simply using get_dummies post  (disclosure: this is my own blog). You can pass the data to catboost classifier without encoding. Catboost handles categorical variables itself by performing one-hot and target expanding mean encoding. You can do the following as well. Note for the below you don't have to use pd.concat.  You can also change explicit columns to categorical. For example, here I am changing the Color and Group I know I'm late to this party, but the simplest way to hot encode a dataframe in an automated way is to use this function: This works for me: Output: I used this in my acoustic model:
probably this helps in ur model. Here is a function to do one-hot-encoding without using numpy, pandas, or other packages. It takes a list of integers, booleans, or strings (and perhaps other types too). Example: I know there are already a lot of answers to this question, but I noticed two things. First, most of the answers use packages like numpy and/or pandas. And this is a good thing. If you are writing production code, you should probably be using robust, fast algorithms like those provided in the numpy/pandas packages. But, for the sake of education, I think someone should provide an answer which has a transparent algorithm and not just an implementation of someone else's algorithm. Second, I noticed that many of the answers do not provide a robust implementation of one-hot encoding because they do not meet one of the requirements below. Below are some of the requirements (as I see them) for a useful, accurate, and robust one-hot encoding function: A one-hot encoding function must: I tested many of the answers to this question and most of them fail on one of the requirements above. Try this: df_encoded.head() The resulting dataframe df_train_encoded is the same as the original, but the categorical features are now replaced with their one-hot-encoded versions. More information on category_encoders here. To add to other questions, let me provide how I did it with a Python 2.0 function using Numpy:  The line n_values = np.max(y_) + 1 could be hard-coded for you to use the good number of neurons in case you use mini-batches for example.  Demo project/tutorial where this function has been used: 
https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition It can and it should be easy as : Usage : Expanding @Martin Thoma's answer Lets assume out of 10 variables, you have 3 categorical variables in your data frame named as cname1, cname2 and cname3.
Then following code will automatically create one hot encoded variable in the new dataframe. A simple example using vectorize in numpy and apply example in pandas: Here i tried with this approach :I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using train_test_split from sklearn.cross_validation, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data.  I know that a workaround would be to use train_test_split two times and somehow adjust the indices. But is there a more standard / built-in way to split the data into 3 sets instead of 2? Numpy solution. We will shuffle the whole dataset first (df.sample(frac=1, random_state=42)) and then split our data set into the following parts: [int(.6*len(df)), int(.8*len(df))] - is an indices_or_sections  array for numpy.split(). Here is a small demo for np.split() usage - let's split 20-elements array into the following parts: 80%, 10%, 10%: However, one approach to dividing the dataset into train, test, cv with 0.6, 0.2, 0.2 would be to use the train_test_split method twice. Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets.     Here is a Python function that splits a Pandas dataframe into train, validation, and test dataframes with stratified sampling. It performs this split by calling scikit-learn's function train_test_split() twice. Below is a complete working example. Consider a dataset that has a label upon which you want to perform the stratification. This label has its own distribution in the original dataset, say 75% foo, 15% bar and 10% baz. Now let's split the dataset into train, validation, and test into subsets using a 60/20/20 ratio, where each split retains the same distribution of the labels. See the illustration below:  Here is the example dataset: Now, let's call the split_stratified_into_train_val_test() function from above to get train, validation, and test dataframes following a 60/20/20 ratio. The three dataframes df_train, df_val, and df_test contain all the original rows but their sizes will follow the above ratio. Further, each of the three splits will have the same distribution of the label, namely 75% foo, 15% bar and 10% baz. In the case of supervised learning, you may want to split both X and y (where X is your input and y the ground truth output).
You just have to pay attention to shuffle X and y the same way before splitting. Here, either X and y are in the same dataframe, so we shuffle them,  separate them and apply the split for each (just like in chosen answer), or X and y are in two different dataframes, so we shuffle X, reorder y the same way as the shuffled X and apply the split to each. It is very convenient to use train_test_split without performing reindexing after dividing to several sets and not writing some additional code. Best answer above does not mention that by separating two times using train_test_split not changing partition sizes won`t give initially intended partition: Then the portion of validation and test sets in the x_remain change and could be counted as In this occasion all initial partitions are saved. Here we split data 2 times with sklearn's train_test_split Considering that df id your original dataframe: 1 - First you split data between Train and Test (10%): 2 - Then you split the train set between train and validation (20%): 3 - Then, you slice the original dataframe according to the indices generated in the steps above: The result is going to be like this:  Note: This soluctions uses the workaround mentioned in the question. Split the dataset in training and testing set as in the other answers, using Then, if you fit your model, you can add validation_split as a parameter. Then you do not need to create the validation set in advance. For example: The validation set is meant to serve as a representative on-the-run-testing-set during training of the training set, taken entirely from the training set, be it by k-fold cross-validation (recommended) or by validation_split; then you do not need to create a validation set separately and still you split a dataset into the three sets you are asking for. ANSWER FOR ANY AMOUNT OF SUB-SETS: This work for any size of percentage. In your case, you should do percentage = [train_percentage, val_percentage, test_percentage]. The easiest way that I could think of is mapping split fractions to array index as follows: where data = random.shuffle(data)This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I'm aware of the gradient descent and the back-propagation algorithm. What I don't get is: when is using a bias important and how do you use it? For example, when mapping the AND function, when I use two inputs and one output, it does not give the correct weights. However, when I use three inputs (one of which is a bias), it gives the correct weights. I think that biases are almost always helpful.  In effect, a bias value allows you to shift the activation function to the left or right, which may be critical for successful learning. It might help to look at a simple example.  Consider this 1-input, 1-output network that has no bias:  The output of the network is computed by multiplying the input (x) by the weight (w0) and passing the result through some kind of activation function (e.g. a sigmoid function.) Here is the function that this network computes, for various values of w0:  Changing the weight w0 essentially changes the "steepness" of the sigmoid.  That's useful, but what if you wanted the network to output 0 when x is 2?  Just changing the steepness of the sigmoid won't really work -- you want to be able to shift the entire curve to the right. That's exactly what the bias allows you to do.  If we add a bias to that network, like so:  ...then the output of the network becomes sig(w0*x + w1*1.0).  Here is what the output of the network looks like for various values of w1:  Having a weight of -5 for w1 shifts the curve to the right, which allows us to have a network that outputs 0 when x is 2. A simpler way to understand what the bias is: it is somehow similar to the constant b of a linear function y = ax + b It allows you to move the line up and down to fit the prediction with the data better. Without b, the line always goes through the origin (0, 0) and you may get a poorer fit. Here are some further illustrations showing the result of a simple 2-layer feed forward neural network with and without bias units on a two-variable regression problem. Weights are initialized randomly and standard ReLU activation is used. As the answers before me concluded, without the bias the ReLU-network is not able to deviate from zero at (0,0). 

 Two different kinds of parameters can
  be adjusted during the training of an
  ANN, the weights and the value in the
  activation functions. This is
  impractical and it would be easier if
  only one of the parameters should be
  adjusted. To cope with this problem a
  bias neuron is invented. The bias
  neuron lies in one layer, is connected
  to all the neurons in the next layer,
  but none in the previous layer and it
  always emits 1. Since the bias neuron
  emits 1 the weights, connected to the
  bias neuron, are added directly to the
  combined sum of the other weights
  (equation 2.1), just like the t value
  in the activation functions.1 The reason it's impractical is because you're simultaneously adjusting the weight and the value, so any change to the weight can neutralize the change to the value that was useful for a previous data instance... adding a bias neuron without a changing value allows you to control the behavior of the layer. Furthermore the bias allows you to use a single neural net to represent similar cases.  Consider the AND boolean function represented by the following neural network:   
(source: aihorizon.com)  A single perceptron can be used to
  represent many boolean functions.  For example, if we assume boolean values
  of 1 (true) and -1 (false), then one
  way to use a two-input perceptron to
  implement the AND function is to set
  the weights w0 = -3, and w1 = w2 = .5.
  This perceptron can be made to
  represent the OR function instead by
  altering the threshold to w0 = -.3. In
  fact, AND and OR can be viewed as
  special cases of m-of-n functions:
  that is, functions where at least m of
  the n inputs to the perceptron must be
  true. The OR function corresponds to
  m = 1 and the AND function to m = n.
  Any m-of-n function is easily
  represented using a perceptron by
  setting all input weights to the same
  value (e.g., 0.5) and then setting the
  threshold w0 accordingly.  Perceptrons can represent all of the
  primitive boolean functions AND, OR,
  NAND ( 1 AND), and NOR ( 1 OR). Machine Learning- Tom Mitchell) The threshold is the bias and w0 is the weight associated with the bias/threshold neuron. The bias is not an NN term. It's a generic algebra term to consider. Y = M*X + C (straight line equation) Now if C(Bias) = 0 then, the line will always pass through the origin, i.e. (0,0), and depends on only one parameter, i.e. M, which is the slope so we have less things to play with. C, which is the bias takes any number and has the activity to shift the graph, and hence able to represent more complex situations. In a logistic regression, the expected value of the target is transformed by a link function to restrict its value to the unit interval. In this way, model predictions can be viewed as primary outcome probabilities as shown: Sigmoid function on Wikipedia This is the final activation layer in the NN map that turns on and off the neuron. Here also bias has a role to play and it shifts the curve flexibly to help us map the model. A layer in a neural network without a bias is nothing more than the multiplication of an input vector with a matrix. (The output vector might be passed through a sigmoid function for normalisation and for use in multi-layered ANN afterwards, but that’s not important.) This means that you’re using a linear function and thus an input of all zeros will always be mapped to an output of all zeros. This might be a reasonable solution for some systems but in general it is too restrictive. Using a bias, you’re effectively adding another dimension to your input space, which always takes the value one, so you’re avoiding an input vector of all zeros. You don’t lose any generality by this because your trained weight matrix needs not be surjective, so it still can map to all values previously possible. 2D ANN: For a ANN mapping two dimensions to one dimension, as in reproducing the AND or the OR (or XOR) functions, you can think of a neuronal network as doing the following: On the 2D plane mark all positions of input vectors. So, for boolean values, you’d want to mark (-1,-1), (1,1), (-1,1), (1,-1). What your ANN now does is drawing a straight line on the 2d plane, separating the positive output from the negative output values. Without bias, this straight line has to go through zero, whereas with bias, you’re free to put it anywhere.
So, you’ll see that without bias you’re facing a problem with the AND function, since you can’t put both (1,-1) and (-1,1) to the negative side. (They are not allowed to be on the line.) The problem is equal for the OR function. With a bias, however, it’s easy to draw the line. Note that the XOR function in that situation can’t be solved even with bias. When you use ANNs, you rarely know about the internals of the systems you want to learn. Some things cannot be learned without a bias. E.g., have a look at the following data: (0, 1), (1, 1), (2, 1), basically a function that maps any x to 1.  If you have a one layered network (or a linear mapping), you cannot find a solution. However, if you have a bias it's trivial! In an ideal setting, a bias could also map all points to the mean of the target points and let the hidden neurons model the differences from that point. Modification of neuron WEIGHTS alone only serves to manipulate the shape/curvature of your transfer function, and not its equilibrium/zero crossing point. The introduction of bias neurons allows you to shift the transfer function curve horizontally (left/right) along the input axis while leaving the shape/curvature unaltered.
This will allow the network to produce arbitrary outputs different from the defaults and hence you can customize/shift the input-to-output mapping to suit your particular needs. See here for graphical explanation:
http://www.heatonresearch.com/wiki/Bias In a couple of experiments in my masters thesis (e.g. page 59), I found that the bias might be important for the first layer(s), but especially at the fully connected layers at the end it seems not to play a big role. This might be highly dependent on the network architecture / dataset. If you're working with images, you might actually prefer to not use a bias at all. In theory, that way your network will be more independent of data magnitude, as in whether the picture is dark, or bright and vivid. And the net is going to learn to do it's job through studying relativity inside your data. Lots of modern neural networks utilize this. For other data having biases might be critical. It depends on what type of data you're dealing with. If your information is magnitude-invariant --- if inputting [1,0,0.1] should lead to the same result as if inputting [100,0,10], you might be better off without a bias. Bias determines how much angle your weight will rotate. In a two-dimensional chart, weight and bias can help us to find the decision boundary of outputs. Say we need to build a AND function, the input(p)-output(t) pair should be {p=[0,0], t=0},{p=[1,0], t=0},{p=[0,1], t=0},{p=[1,1], t=1}  Now we need to find a decision boundary, and the ideal boundary should be:  See? W is perpendicular to our boundary. Thus, we say W decided the direction of boundary. However, it is hard to find correct W at first time. Mostly, we choose original W value randomly. Thus, the first boundary may be this:
 Now the boundary is parallel to the y axis. We want to rotate the boundary. How? By changing the W. So, we use the learning rule function: W'=W+P:  W'=W+P  is equivalent to W' = W + bP, while b=1. Therefore, by changing the value of b(bias), you can decide the angle between W' and W. That is "the learning rule of ANN". You could also read Neural Network Design by Martin T. Hagan / Howard B. Demuth / Mark H. Beale, chapter 4 "Perceptron Learning Rule" In simpler terms, biases allow for more and more variations of weights to be learnt/stored... (side-note: sometimes given some threshold). Anyway, more variations mean that biases add richer representation of the input space to the model's learnt/stored weights. (Where better weights can enhance the neural net’s guessing power) For example, in learning models, the hypothesis/guess is desirably bounded by y=0 or y=1 given some input, in maybe some classification task... i.e some y=0 for some x=(1,1) and some y=1 for some x=(0,1). (The condition on the hypothesis/outcome is the threshold I talked about above. Note that my examples setup inputs X to be each x=a double or 2 valued-vector, instead of Nate's single valued x inputs of some collection X). If we ignore the bias, many inputs may end up being represented by a lot of the same weights (i.e. the learnt weights mostly occur close to the origin (0,0).
The model would then be limited to poorer quantities of good weights, instead of the many many more good weights it could better learn with bias. (Where poorly learnt weights lead to poorer guesses or a decrease in the neural net’s guessing power) So, it is optimal that the model learns both close to the origin, but also, in as many places as possible inside the threshold/decision boundary. With the bias we can enable degrees of freedom close to the origin, but not limited to origin's immediate region. In neural networks: In absence of bias, the neuron may not be activated by considering only the weighted sum from the input layer. If the neuron is not activated, the information from this neuron is not passed through rest of the neural network. The value of bias is learnable.  Effectively, bias = — threshold. You can think of bias as how easy it is to get the neuron to output a 1 — with a really big bias, it’s very easy for the neuron to output a 1, but if the bias is very negative, then it’s difficult. In summary: bias helps in controlling the value at which the activation function will trigger. Follow this video for more details. Few more useful links: geeksforgeeks towardsdatascience Expanding on zfy's explanation: The equation for one input, one neuron, one output should look: where x is the value from the input node and 1 is the value of the bias node;
y can be directly your output or be passed into a function, often a sigmoid function. Also note that the bias could be any constant, but to make everything simpler we always pick 1 (and probably that's so common that zfy did it without showing & explaining it). Your network is trying to learn coefficients a and b to adapt to your data.
So you can see why adding the element b * 1 allows it to fit better to more data: now you can change both slope and intercept. If you have more than one input your equation will look like: Note that the equation still describes a one neuron, one output network; if you have more neurons you just add one dimension to the coefficient matrix, to multiplex the inputs to all nodes and sum back each node contribution. That you can write in vectorized format as i.e. putting coefficients in one array and (inputs + bias) in another you have your desired solution as the dot product of the two vectors (you need to transpose X for the shape to be correct, I wrote XT a 'X transposed') So in the end you can also see your bias as is just one more input to represent the part of the output that is actually independent of your input. To think in a simple way, if you have y=w1*x where y is your output and w1 is the weight, imagine a condition where x=0 then y=w1*x equals to 0. If you want to update your weight you have to compute how much change by delw=target-y where target is your target output. In this case 'delw' will not change since y is computed as 0. So, suppose if you can add some extra value it will help y = w1x + w01, where bias=1 and weight can be adjusted to get a correct bias. Consider the example below. In terms of line slope, intercept is a specific form of linear equations. y = mx + b Check the image image Here b is (0,2) If you want to increase it to (0,3) how will you do it by changing the value of b the bias. For all the ML books I studied, the W is always defined as the connectivity index between two neurons, which means the higher connectivity between two neurons. The stronger the signals will be transmitted from the firing neuron to the target neuron or Y = w * X as a result to maintain the biological character of neurons, we need to keep the 1 >=W >= -1, but in the real regression, the W will end up with |W| >=1 which contradicts how the neurons are working. As a result, I propose W = cos(theta), while 1 >= |cos(theta)|, and Y= a * X = W * X + b while a = b + W = b + cos(theta), b is an integer. Bias acts as our anchor. It's a way for us to have some kind of baseline where we don't go below that. In terms of a graph, think of like y=mx+b it's like a y-intercept of this function. output = input times the weight value and added a bias value and then apply an activation function. The term bias is used to adjust the final output matrix as the y-intercept does. For instance, in the classic equation, y = mx + c, if c = 0, then the line will always pass through 0. Adding the bias term provides more flexibility and better generalisation to our neural network model. The bias helps to get a better equation. Imagine the input and output like a function y = ax + b and you need to put the right line between the input(x) and output(y) to minimise the global error between each point and the line, if you keep the equation like this y = ax, you will have one parameter for adaptation only, even if you find the best a minimising the global error it will be kind of far from the wanted value. You can say the bias makes the equation more flexible to adapt to the best valuesGiven a 1D array of indices: I want to one-hot encode this as a 2D array: Create a zeroed array b with enough columns, i.e. a.max() + 1.
Then, for each row i, set the a[i]th column to 1. In case you are using keras, there is a built in utility for that: And it does pretty much the same as @YXD's answer (see source-code). Here is what I find useful: Here num_classes stands for number of classes you have. So if you have a vector with shape of (10000,) this function transforms it to (10000,C). Note that a is zero-indexed, i.e. one_hot(np.array([0, 1]), 2) will give [[1, 0], [0, 1]]. Exactly what you wanted to have I believe. PS: the source is Sequence models - deeplearning.ai You can also use eye function of numpy: numpy.eye(number of classes)[vector containing the labels] You can use  sklearn.preprocessing.LabelBinarizer: Example: output: Amongst other things, you may initialize sklearn.preprocessing.LabelBinarizer() so that the output of transform is sparse. For 1-hot-encoding For Example ENJOY CODING You can use the following code for converting into a one-hot vector: let x is the normal class vector having a single column with classes 0 to some number: if 0 is not a class; then remove +1. Here is a function that converts a 1-D vector to a 2-D one-hot array. Below is some example usage: I think the short answer is no. For a more generic case in n dimensions, I came up with this: I am wondering if there is a better solution -- I don't like that I have to create those lists in the last two lines. Anyway, I did some measurements with timeit and it seems that the numpy-based (indices/arange) and the iterative versions perform about the same.  Just to elaborate on the excellent answer from K3---rnc, here is a more generic version: Also, here is a quick-and-dirty benchmark of this method and a method from the currently accepted answer by YXD (slightly changed, so that they offer the same API except that the latter works only with 1D ndarrays): The latter method is ~35% faster (MacBook Pro 13 2015), but the former is more general: If using tensorflow, there is one_hot(): I recently ran into a problem of same kind and found said solution which turned out to be only satisfying if you have numbers that go within a certain formation. For example if you want to one-hot encode following list: go ahead, the posted solutions are already mentioned above. But what if considering this data: If you do it with methods mentioned above, you will likely end up with 90 one-hot columns. This is because all answers include something like n = np.max(a)+1. I found a more generic solution that worked out for me and wanted to share with you: I hope someone encountered same restrictions on above solutions and this might come in handy Such type of encoding are usually part of numpy array. If you are using a numpy array like this : then there is very simple way to convert that to 1-hot encoding That's it. clean and easy solution: I find the easiest solution combines np.take and np.eye works for x of any shape. Here is an example function that I wrote to do this based upon the answers above and my own use case: I am adding for completion a simple function, using only numpy operators:  It takes as input a probability matrix: e.g.:  [[0.03038822 0.65810204 0.16549407 0.3797123 ] 
   ...
  [0.02771272 0.2760752  0.3280924  0.33458805]]   And it will return [[0 1 0 0]  ...  [0 0 0 1]] Here's a dimensionality-independent standalone solution. This will convert any N-dimensional array arr of nonnegative integers to a one-hot N+1-dimensional array one_hot, where one_hot[i_1,...,i_N,c] = 1 means arr[i_1,...,i_N] = c. You can recover the input via np.argmax(one_hot, -1) Use the following code. It works best. Found it here P.S You don't need to go into the link. Link to documentation: neuraxle.steps.numpy.OneHotEncoderSo I've been following Google's official tensorflow guide and trying to build a simple neural network using Keras. But when it comes to training the model, it does not use the entire dataset (with 60000 entries) and instead uses only 1875 entries for training. Any possible fix? Output: Here's the original google colab notebook where I've been working on this: https://colab.research.google.com/drive/1NdtzXHEpiNnelcMaJeEm6zmp34JMcN38 The number 1875 shown during fitting the model is not the training samples; it is the number of batches. model.fit includes an optional argument batch_size, which, according to the documentation: If unspecified, batch_size will default to 32. So, what happens here is - you fit with the default batch size of 32 (since you have not specified anything different), so the total number of batches for your data is It does not train on 1875 samples. 1875 here is the number of steps, not samples. In fit method, there is an argument, batch_size. The default value for it is 32. So 1875*32=60000. The implementation is correct. If you train it with batch_size=16, you will see the number of steps will be 3750 instead of 1875, since 60000/16=3750. Just use batch_size = 1, if you want the entire 60000 data samples to be visible.In the tensorflow API docs they use a keyword called logits. What is it? A lot of methods are written like: If logits is just a generic Tensor input, why is it named logits? Secondly, what is the difference between the following two methods? I know what tf.nn.softmax does, but not the other. An example would be really helpful. The softmax+logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear.  It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities (you might have an input of 5). Internally, it first applies softmax to the unscaled output, and then and then computes the cross entropy of those values vs. what they "should" be as defined by the labels. tf.nn.softmax produces the result of applying the softmax function to an input tensor.  The softmax "squishes" the inputs so that sum(input) = 1, and it does the mapping by interpreting the inputs as log-probabilities (logits) and then converting them back into raw probabilities between 0 and 1.  The shape of output of a softmax is the same as the input: See this answer for more about why softmax is used extensively in DNNs. tf.nn.softmax_cross_entropy_with_logits combines the softmax step with the calculation of the cross-entropy loss after applying the softmax function, but it does it all together in a more mathematically careful way.  It's similar to the result of: The cross entropy is a summary metric: it sums across the elements.  The output of tf.nn.softmax_cross_entropy_with_logits on a shape [2,5] tensor is of shape [2,1] (the first dimension is treated as the batch). If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.  Otherwise, you'll end up hacking it by adding little epsilons here and there. Edited 2016-02-07:
If you have single-class labels, where an object can only belong to one class, you might now  consider using tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.  This function was added after release 0.6.0. Short version: Suppose you have two tensors, where y_hat contains computed scores for each class (for example, from y = W*x +b) and y_true contains one-hot encoded true labels.  If you interpret the scores in y_hat as unnormalized log probabilities, then they are logits. Additionally, the total cross-entropy loss computed in this manner: is essentially equivalent to the total cross-entropy loss computed with the function softmax_cross_entropy_with_logits(): Long version: In the output layer of your neural network, you will probably compute an array that contains the class scores for each of your training instances, such as from a computation y_hat = W*x + b. To serve as an example, below I've created a y_hat as a 2 x 3 array, where the rows correspond to the training instances and the columns correspond to classes. So here there are 2 training instances and 3 classes. Note that the values are not normalized (i.e. the rows don't add up to 1). In order to normalize them, we can apply the softmax function, which interprets the input as unnormalized log probabilities (aka logits) and outputs normalized linear probabilities.  It's important to fully understand what the softmax output is saying. Below I've shown a table that more clearly represents the output above. It can be seen that, for example, the probability of training instance 1 being "Class 2" is 0.619. The class probabilities for each training instance are normalized, so the sum of each row is 1.0. So now we have class probabilities for each training instance, where we can take the argmax() of each row to generate a final classification. From above, we may generate that training instance 1 belongs to "Class 2" and training instance 2 belongs to "Class 1".  Are these classifications correct? We need to measure against the true labels from the training set. You will need a one-hot encoded y_true array, where again the rows are training instances and columns are classes. Below I've created an example y_true one-hot array where the true label for training instance 1 is "Class 2" and the true label for training instance 2 is "Class 3". Is the probability distribution in y_hat_softmax close to the probability distribution in y_true? We can use cross-entropy loss to measure the error.  We can compute the cross-entropy loss on a row-wise basis and see the results. Below we can see that training instance 1 has a loss of 0.479, while training instance 2 has a higher loss of 1.200. This result makes sense because in our example above, y_hat_softmax showed that training instance 1's highest probability was for "Class 2", which matches training instance 1 in y_true; however, the prediction for training instance 2 showed a highest probability for "Class 1", which does not match the true class "Class 3". What we really want is the total loss over all the training instances. So we can compute: Using softmax_cross_entropy_with_logits() We can instead compute the total cross entropy loss using the tf.nn.softmax_cross_entropy_with_logits() function, as shown below.  Note that total_loss_1 and total_loss_2 produce essentially equivalent results with some small differences in the very final digits. However, you might as well use the second approach: it takes one less line of code and accumulates less numerical error because the softmax is done for you inside of softmax_cross_entropy_with_logits(). tf.nn.softmax computes the forward propagation through a softmax layer. You use it during evaluation of the model when you compute the probabilities that the model outputs. tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer. It is only used during training.  The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them). When we wish to constrain an output between 0 and 1, but our model architecture outputs unconstrained values, we can add a normalisation layer to enforce this. A common choice is a sigmoid function.1 In binary classification this is typically the logistic function, and in multi-class tasks the multinomial logistic function (a.k.a softmax).2 If we want to interpret the outputs of our new final layer as 'probabilities', then (by implication) the unconstrained inputs to our sigmoid must be inverse-sigmoid(probabilities). In the logistic case this is equivalent to the log-odds of our probability (i.e. the log of the odds) a.k.a. logit: That is why the arguments to softmax is called logits in Tensorflow - because under the assumption that softmax is the final layer in the model, and the output p is interpreted as a probability, the input x to this layer is interpretable as a logit: In Machine Learning there is a propensity to generalise terminology borrowed from maths/stats/computer science, hence in Tensorflow logit (by analogy) is used as a synonym for the input to many normalisation functions. Above answers have enough description for the asked question. Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions. Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy() You can find prominent difference between them in a resource intensive model. Tensorflow 2.0 Compatible Answer: The explanations of dga and stackoverflowuser2010 are very detailed about Logits and the related Functions.  All those functions, when used in Tensorflow 1.x will work fine, but if you migrate your code from 1.x (1.14, 1.15, etc) to 2.x (2.0, 2.1, etc..), using those functions result in error. Hence, specifying the 2.0 Compatible Calls for all the functions, we discussed above, if we migrate from 1.x to 2.x, for the benefit of the community. Functions in 1.x: Respective Functions when Migrated from 1.x  to 2.x: For more information about migration from 1.x to 2.x, please refer this Migration Guide. One more thing that I would definitely like to highlight as logit is just a raw output, generally the output of last layer. This can be a negative value as well. If we use it as it's for "cross entropy" evaluation as mentioned below: then it wont work. As log of -ve is not defined.
So using o softmax activation, will overcome this problem. This is my understanding, please correct me if Im wrong. Logits are the unnormalized outputs of a neural network. Softmax is a normalization function that squashes the outputs of a neural network so that they are all between 0 and 1 and sum to 1. Softmax_cross_entropy_with_logits is a loss function that takes in the outputs of a neural network (after they have been squashed by softmax) and the true labels for those outputs, and returns a loss value.I'm trying to train a CNN to categorize text by topic. When I use binary cross-entropy I get ~80% accuracy, with categorical cross-entropy I get ~50% accuracy. I don't understand why this is. It's a multiclass problem, doesn't that mean that I have to use categorical cross-entropy and that the results with binary cross-entropy are meaningless? Then I compile it either it like this using categorical_crossentropy as the loss function: or  Intuitively it makes sense why I'd want to use categorical cross-entropy, I don't understand why I get good results with binary, and poor results with categorical. The reason for this apparent performance discrepancy between categorical & binary cross entropy is what user xtof54 has already reported in his answer below, i.e.: the accuracy computed with the Keras method evaluate is just plain
wrong when using binary_crossentropy with more than 2 labels I would like to elaborate more on this, demonstrate the actual underlying issue, explain it, and offer a remedy. This behavior is not a bug; the underlying reason is a rather subtle & undocumented issue at how Keras actually guesses which accuracy to use, depending on the loss function you have selected, when you include simply metrics=['accuracy'] in your model compilation. In other words, while your first compilation option is valid, your second one: will not produce what you expect, but the reason is not the use of binary cross entropy (which, at least in principle, is an absolutely valid loss function). Why is that? If you check the metrics source code, Keras does not define a single accuracy metric, but several different ones, among them binary_accuracy and categorical_accuracy. What happens under the hood is that, since you have selected binary cross entropy as your loss function and have not specified a particular accuracy metric, Keras (wrongly...) infers that you are interested in the binary_accuracy, and this is what it returns - while in fact you are interested in the categorical_accuracy. Let's verify that this is the case, using the MNIST CNN example in Keras, with the following modification: To remedy this, i.e. to use indeed binary cross entropy as your loss function (as I said, nothing wrong with this, at least in principle) while still getting the categorical accuracy required by the problem at hand, you should ask explicitly for categorical_accuracy in the model compilation as follows: In the MNIST example, after training, scoring, and predicting the test set as I show above, the two metrics now are the same, as they should be: System setup: UPDATE: After my post, I discovered that this issue had already been identified in this answer. It all depends on the type of classification problem you are dealing with. There are three main categories In the first case, binary cross-entropy should be used and targets should be encoded as one-hot vectors. In the second case, categorical cross-entropy should be used and targets should be encoded as one-hot vectors. In the last case, binary cross-entropy should be used and targets should be encoded as one-hot vectors. Each output neuron (or unit) is considered as a separate random binary variable, and the loss for the entire vector of outputs is the product of the loss of single binary variables. Therefore it is the product of binary cross-entropy for each single output unit. The binary cross-entropy is defined as  and categorical cross-entropy is defined as  where c is the index running over the number of classes C. I came across an "inverted" issue — I was getting good results with categorical_crossentropy (with 2 classes) and poor with binary_crossentropy. It seems that problem was with wrong activation function. The correct settings were: It's really interesting case. Actually in your setup the following statement is true: This means that up to a constant multiplication factor your losses are equivalent. The weird behaviour that you are observing during a training phase might be an example of a following phenomenon: That's why this constant factor might help in case of binary_crossentropy. After many epochs - the learning rate value is greater than in categorical_crossentropy case. I usually restart training (and learning phase) a few times when I notice such behaviour or/and adjusting a class weights using the following pattern: This makes loss from a less frequent classes balancing the influence of a dominant class loss at the beginning of a training and in a further part of an optimization process. EDIT: Actually - I checked that even though in case of maths: should hold - in case of keras it's not true, because keras is automatically normalizing all outputs to sum up to 1. This is the actual reason behind this weird behaviour as in case of multiclassification such normalization harms a training. After commenting @Marcin answer, I have more carefully checked one of my students code where I found the same weird behavior, even after only 2 epochs ! (So @Marcin's explanation was not very likely in my case). And I found that the answer is actually very simple: the accuracy computed with the Keras method evaluate is just plain wrong when using binary_crossentropy with more than 2 labels. You can check that by recomputing the accuracy yourself (first call the Keras method "predict" and then compute the number of correct answers returned by predict): you get the true accuracy, which is much lower than the Keras "evaluate" one. a simple example under a multi-class setting to illustrate suppose you have 4 classes (onehot encoded) and below is just one prediction true_label = [0,1,0,0]
predicted_label = [0,0,1,0] when using categorical_crossentropy, the accuracy is just 0 , it only cares about if you get the concerned class right. however when using binary_crossentropy, the accuracy is calculated for all classes, it would be 50% for this prediction. and the final result will be the mean of the individual accuracies for both cases. it is recommended to use categorical_crossentropy for multi-class(classes are mutually exclusive) problem but binary_crossentropy for multi-label problem. As it is a multi-class problem, you have to use the categorical_crossentropy, the binary cross entropy will produce bogus results, most likely will only evaluate the first two classes only. 50% for a multi-class problem can be quite good, depending on the number of classes. If you have n classes, then 100/n is the minimum performance you can get by outputting a random class. You are passing a target array of shape (x-dim, y-dim) while using as loss categorical_crossentropy. categorical_crossentropy expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via: Alternatively, you can use the loss function sparse_categorical_crossentropy instead, which does expect integer targets. when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). Take a look at the equation you can find that binary cross entropy not only punish those label = 1, predicted =0, but also label = 0, predicted = 1. However categorical cross entropy only punish those label = 1 but predicted = 1.That's why we make assumption that there is only ONE label positive. The main point is answered satisfactorily with the brilliant piece of sleuthing by desernaut. However there are occasions when BCE (binary cross entropy) could throw different results than CCE (categorical cross entropy) and may be the preferred choice. While the thumb rules shared above (which loss to select) work fine for 99% of the cases, I would like to add a few new dimensions to this discussion. The OP had a softmax activation and this throws a probability distribution as the predicted value. It is a multi-class problem. The preferred loss is categorical CE. Essentially this boils down to -ln(p) where 'p' is the predicted probability of the lone positive class in the sample. This means that the negative predictions dont have a role to play in calculating CE. This is by intention. On a rare occasion, it may be needed to make the -ve voices count. This can be done by treating the above sample as a series of binary predictions. So if expected is [1 0 0 0 0] and predicted is [0.1 0.5 0.1 0.1 0.2], this is further broken down into: Now we proceed to compute 5 different cross entropies - one for each of the above 5 expected/predicted combo and sum them up. Then: The CE has a different scale but continues to be a measure of the difference between the expected and predicted values. The only difference is that in this scheme, the -ve values are also penalized/rewarded along with the +ve values. In case your problem is such that you are going to use the output probabilities (both +ve and -ves) instead of using the max() to predict just the 1 +ve label, then you may want to consider this version of CE. How about a multi-label situation where expected = [1 0 0 0 1]? Conventional approach is to use one sigmoid per output neuron instead of an overall softmax. This ensures that the output probabilities are independent of each other. So we get something like: By definition, CE measures the difference between 2 probability distributions. But the above two lists are not probability distributions. Probability distributions should always add up to 1. So conventional solution is to use same loss approach as before - break the expected and predicted values into 5 individual probability distributions, proceed to calculate 5 cross entropies and sum them up. Then: The challenge happens when the number of classes may be very high - say a 1000 and there may be only couple of them present in each sample. So the expected is something like: [1,0,0,0,0,0,1,0,0,0.....990 zeroes]. The predicted could be something like: [.8, .1, .1, .1, .1, .1, .8, .1, .1, .1.....990 0.1's] In this case the CE = You can see how the -ve classes are beginning to create a nuisance value when calculating the loss. The voice of the +ve samples (which may be all that we care about) is getting drowned out. What do we do? We can't use categorical CE (the version where only +ve samples are considered in calculation). This is because, we are forced to break up the probability distributions into multiple binary probability distributions because otherwise it would not be a probability distribution in the first place. Once we break it into multiple binary probability distributions, we have no choice but to use binary CE and this of course gives weightage to -ve classes. One option is to drown the voice of the -ve classes by a multiplier. So we multiply all -ve losses by a value gamma where gamma < 1. Say in above case, gamma can be .0001. Now the loss comes to: The nuisance value has come down. 2 years back Facebook did that and much more in a paper they came up with where they also multiplied the -ve losses by p to the power of x. 'p' is the probability of the output being a +ve and x is a constant>1. This penalized -ve losses even further especially the ones where the model is pretty confident (where 1-p is close to 1). This combined effect of punishing negative class losses combined with harsher punishment for the easily classified cases (which accounted for majority of the -ve cases) worked beautifully for Facebook and they called it focal loss. So in response to OP's question of whether binary CE makes any sense at all in his case, the answer is - it depends. In 99% of the cases the conventional thumb rules work but there could be occasions when these rules could be bent or even broken to suit the problem at hand. For a more in-depth treatment, you can refer to: https://towardsdatascience.com/cross-entropy-classification-losses-no-math-few-stories-lots-of-intuition-d56f8c7f06b0 The binary_crossentropy(y_target, y_predict) doesn't need to apply to binary classification problem. In the source code of binary_crossentropy(), the nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) of tensorflow was actually used. And, in the documentation, it says that: Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.Can I extract the underlying decision-rules (or 'decision paths') from a trained tree in a decision tree as a textual list? Something like: I believe that this answer is more correct than the other answers here: This prints out a valid Python function. Here's an example output for a tree that is trying to return its input, a number between 0 and 10. Here are some stumbling blocks that I see in other answers: I created my own function to extract the rules from the decision trees created by sklearn: This function first starts with the nodes (identified by -1 in the child arrays) and then recursively finds the parents. I call this a node's 'lineage'.  Along the way, I grab the values I need to create if/then/else SAS logic: The sets of tuples below contain everything I need to create SAS if/then/else statements. I do not like using do blocks in SAS which is why I create logic describing a node's entire path. The single integer after the tuples is the ID of the terminal node in a path. All of the preceding tuples combine to create that node.  Scikit learn introduced a delicious new method called export_text in version 0.21 (May 2019) to extract the rules from a tree. Documentation here. It's no longer necessary to create a custom function. Once you've fit your model, you just need two lines of code. First, import export_text: Second, create an object that will contain your rules. To make the rules look more readable, use the feature_names argument and pass a list of your feature names. For example, if your model is called model and your features are named in a dataframe called X_train, you could create an object called tree_rules: Then just print or save tree_rules. Your output will look like this: I modified the code submitted by Zelazny7 to print some pseudocode: if you call get_code(dt, df.columns) on the same example you will obtain: There is a new DecisionTreeClassifier method, decision_path, in the 0.18.0 release.  The developers provide an extensive (well-documented) walkthrough. The first section of code in the walkthrough that prints the tree structure seems to be OK.  However, I modified the code in the second section to interrogate one sample.  My changes denoted with # <-- Edit The changes marked by # <-- in the code below have since been updated in walkthrough link after the errors were pointed out in pull requests #8653 and #10951. It's much easier to follow along now.  Change the sample_id to see the decision paths for other samples.  I haven't asked the developers about these changes, just seemed more intuitive when working through the example. You can see a digraph Tree. Then, clf.tree_.feature and clf.tree_.value are array of nodes splitting feature and array of nodes values respectively. You can refer to more details from this github source.  I needed a more human-friendly format of rules from the Decision Tree. I'm building open-source AutoML Python package and many times MLJAR users want to see the exact rules from the tree. That's why I implemented a function based on paulkernfeld answer. The rules are sorted by the number of training samples assigned to each rule. For each rule, there is information about the predicted class name and probability of prediction for classification tasks. For the regression task, only information about the predicted value is printed. The printed rules: I've summarized the ways to extract rules from the Decision Tree in my article: Extract Rules from Decision Tree in 3 Ways with Scikit-Learn and Python. This is the code you need I have modified the top liked code to indent in a jupyter notebook python 3 correctly Just because everyone was so helpful I'll just add a modification to Zelazny7 and Daniele's beautiful solutions. This one is for python 2.7, with tabs to make it more readable: I've been going through this, but i needed the rules to be written in this format  So I adapted the answer of @paulkernfeld (thanks) that you can customize to your need Now you can use export_text. A complete example from [sklearn][1] Codes below is my approach under anaconda python 2.7 plus a package name "pydot-ng" to making a PDF file with decision rules. I hope it is helpful. a tree graphy show here Here is a way to translate the whole tree into a single (not necessarily too human-readable) python expression using the SKompiler library: This builds on @paulkernfeld 's answer. If you have a dataframe X with your features and a target dataframe y with your resonses and you you want to get an idea which y value ended in which node (and also ant to plot it accordingly) you can do the following: not the most elegant version but it does the job...   Here is a function, printing rules of a scikit-learn decision tree under python 3 and with offsets for conditional blocks to make the structure more readable: You can also make it more informative by distinguishing it to which class it belongs or even by mentioning its output value.  Here is my approach to extract the decision rules in a form that can be used in directly in sql, so the data can be grouped by node. (Based on the approaches of previous posters.)  The result will be subsequent CASE clauses that can be copied to an sql statement, ex. SELECT COALESCE(*CASE WHEN <conditions> THEN > <NodeA>*, > *CASE WHEN
 <conditions> THEN <NodeB>*, > ....)NodeName,* > FROM <table or view> Modified Zelazny7's code to fetch SQL from the decision tree. Apparently a long time ago somebody already decided to try to add the following function to the official scikit's tree export functions (which basically only supports export_graphviz) Here is his full commit: https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py Not exactly sure what happened to this comment. But you could also try to use that function. I think this warrants a serious documentation request to the good people of scikit-learn to properly document the sklearn.tree.Tree API which is the underlying tree structure that DecisionTreeClassifier exposes as its attribute tree_. Just use the function from sklearn.tree like this And then look in your project folder for the file tree.dot, copy the ALL the content and paste it here http://www.webgraphviz.com/ and generate your graph :) Thank for the wonderful solution of @paulkerfeld. On top of his solution, for all those who want to have a serialized version of trees, just use tree.threshold, tree.children_left, tree.children_right, tree.feature and tree.value. Since the leaves don't have splits and hence no feature names and children, their placeholder in tree.feature and tree.children_*** are _tree.TREE_UNDEFINED and _tree.TREE_LEAF. Every split is assigned a unique index by depth first search. 
 Notice that the tree.value is of shape [n, 1, 1] Here is a function that generates Python code from a decision tree by converting the output of export_text: Sample usage: Sample output: The above example is generated with names = ['f'+str(j+1) for j in range(NUM_FEATURES)]. One handy feature is that it can generate smaller file size with reduced spacing. Just set spacing=2. From this answer, you get a readable and efficient representation: https://stackoverflow.com/a/65939892/3746632 Output looks like this. X is 1d vector to represent a single instance's features.I am relatively new to machine learning/python/ubuntu. I have a set of images in .jpg format where half contain a feature I want caffe to learn and half don't. I'm having trouble in finding a way to convert them to the required lmdb format. I have the necessary text input files.  My question is can anyone provide a step by step guide on how to use convert_imageset.cpp in the ubuntu terminal? Thanks First thing you must do is build caffe and caffe's tools (convert_imageset is one of these tools).
After installing caffe and makeing it make sure you ran make tools as well.
Verify that a binary file convert_imageset is created in $CAFFE_ROOT/build/tools. Images: put all images in a folder (I'll call it here /path/to/jpegs/).
Labels: create a text file (e.g., /path/to/labels/train.txt) with a line per input image  . For example:   img_0000.jpeg 1
  img_0001.jpeg 0
  img_0002.jpeg 0   In this example the first image is labeled 1 while the other two are labeled 0.  Run the binary in shell Command line explained:   Other flags that might be useful: You can check out $CAFFE_ROOT/examples/imagenet/convert_imagenet.sh
for an example how to use convert_imageset.This is my test code: The output is: But What Happend? The documentation says:  Note: if the input to the layer has a rank greater than 2, then it is
  flattened prior to the initial dot product with kernel. While the output is reshaped? Currently, contrary to what has been stated in documentation, the Dense layer is applied on the last axis of input tensor: Contrary to the documentation, we don't actually flatten it. It's
  applied on the last axis independently. In other words, if a Dense layer with m units is applied on an input tensor of shape (n_dim1, n_dim2, ..., n_dimk) it would have an output shape of (n_dim1, n_dim2, ..., m). As a side note: this makes TimeDistributed(Dense(...)) and Dense(...) equivalent to each other. Another side note: be aware that this has the effect of shared weights. For example, consider this toy network: The model summary: As you can see the Dense layer has only 60 parameters. How? Each unit in the Dense layer is connected to the 5 elements of each row in the input with the same weights, therefore 10 * 5 + 10 (bias params per unit) = 60. Update. Here is a visual illustration of the example above:This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I've been developing an internal website for a portfolio management tool.  There is a lot of text data, company names etc.  I've been really impressed with some search engines ability to very quickly respond to queries with "Did you mean: xxxx". I need to be able to intelligently take a user query and respond with not only raw search results but also with a "Did you mean?" response when there is a highly likely alternative answer etc [I'm developing in ASP.NET (VB - don't hold it against me! )] UPDATE:
OK, how can I mimic this without the millions of 'unpaid users'? Here's the explanation directly from the source ( almost )  at min 22:03 Worth watching! Basically and according  to Douglas Merrill former CTO of Google it is like this: 1) You write a  ( misspelled )  word  in google  2) You don't find what you wanted ( don't click on any results ) 3) You realize you misspelled the word  so you rewrite the word in the search box. 4) You find what you want ( you click in the first links )  This pattern multiplied millions of times, shows what are the most common misspells and what are the most "common" corrections.  This way Google can almost instantaneously, offer spell correction in every language. Also this means if overnight everyone start to spell night as "nigth" google would suggest that word instead.   EDIT @ThomasRutter: Douglas describe it as "statistical machine learning".  They know who correct the query, because they know which query comes from which user ( using cookies )  If the users perform a query, and only 10% of the users click on a result and 90% goes back and type another query ( with the corrected word ) and this time that 90% clicks on a result, then they know they have found a correction.  They can also know if those are "related" queries of two different, because they have information of all the links they show.  Furthermore, they are now including the context into the spell check, so they can even suggest different word depending on the context.  See this demo of google wave ( @ 44m 06s )  that shows how the context is taken into account to automatically correct the spelling. Here it is explained how that natural language processing works. And finally here is an awesome demo of what can be done adding automatic machine translation ( @ 1h 12m 47s )  to the mix.  
  I've added anchors of minute and seconds to the videos to skip directly to the content, if they don't work, try reloading the page or scrolling by hand to the mark. 
 I found this article some time ago: How to Write a Spelling Corrector, written by Peter Norvig (Director of Research at Google Inc.). It's an interesting read about the "spelling correction" topic. The examples are in Python but it's clear and simple to understand, and I think that the algorithm can be easily 
translated to other languages. Below follows a short description of the algorithm.
The algorithm consists of two steps, preparation and word checking. Step 1: Preparation - setting up the word database Best is if you can use actual search words and their occurence.
If you don't have that a large set of text can be used instead.
Count the occurrence (popularity) of each word. Step 2. Word checking - finding words that are similar to the one checked Similar means that the edit distance is low (typically 0-1 or 0-2). The edit distance is the minimum number of inserts/deletes/changes/swaps needed to transform one word to another. Choose the most popular word from the previous step and suggest it as a correction (if other than the word itself). For the theory of "did you mean" algorithm you can refer to Chapter 3 of Introduction to Information Retrieval. It is available online for free. Section 3.3 (page 52) exactly answers your question. And to specifically answer your update you only need a dictionary of words and nothing else (including millions of users). Hmm... I thought that google used their vast corpus of data (the internet) to do some serious NLP (Natural Language Processing).  For example, they have so much data from the entire internet that they can count the number of times a three-word sequence occurs (known as a trigram). So if they see a sentence like: "pink frugr concert", they could see it has few hits, then find the most likely "pink * concert" in their corpus. They apparently just do a variation of what Davide Gualano was saying, though, so definitely read that link. Google does of course use all web-pages it knows as a corpus, so that makes its algorithm particularly effective. My guess is that they use a combination of a Levenshtein distance algorithm and the masses of data they collect regarding the searches that are run. They could pull a set of searches that have the shortest Levenshtein distance from the entered search string, then pick the one with the most results. Normally a production spelling corrector utilizes several methodologies to provide a spelling suggestion. Some are: Decide on a way to determine whether spelling correction is required. These may include insufficient results, results which are not specific or accurate enough (according to some measure), etc. Then: Use a large body of text or a dictionary, where all, or most are known to be correctly spelled. These are easily found online, in places such as LingPipe. Then to determine  the best suggestion you look for a word which is the closest match based on several measures. The most intuitive one is similar characters. What has been shown through research and experimentation is that two or three character sequence matches work better. (bigrams and trigrams). To further improve results, weigh a higher score upon a match at the beginning, or end of the word. For performance reasons, index all these words as trigrams or bigrams, so that when you are performing a lookup, you convert to n-gram, and lookup via hashtable or trie. Use heuristics related to potential keyboard mistakes based on character location. So that "hwllo" should be "hello" because 'w' is close to 'e'. Use a phonetic key (Soundex, Metaphone) to index the words and lookup possible corrections. In practice this normally returns worse results than using n-gram indexing, as described above. In each case you must select the best correction from a list. This may be a distance metric such as levenshtein, the keyboard metric, etc. For a multi-word phrase, only one word may be misspelled, in which case you can use the remaining words as context in determining a best match. Use Levenshtein distance, then create a Metric Tree (or Slim tree) to index words.
Then run a 1-Nearest Neighbour query, and you got the result. Google apparently suggests queries with best results, not with those which are spelled correctly. But in this case, probably a spell-corrector would be more feasible, Of course you could store some value for every query, based on some metric of how good results it returns. So, You need a dictionary (english or based on your data) Generate a word trellis and calculate probabilities for the transitions using your dictionary. Add a decoder to calculate minimum error distance using your trellis. Of course you should take care of insertions and deletions when calculating distances. Fun thing is that QWERTY keyboard maximizes the distance if you hit keys close to each other.(cae would turn car, cay would turn cat) Return the word which has the minimum distance.  Then you could compare that to your query database and check if there is better results for other close matches. Here is the best answer I found, Spelling corrector implemented and described by Google's Director of Research Peter Norvig. If you want to read more about the theory behind this, you can read his book chapter. The idea of this algorithm is based on statistical machine learning.  I saw something on this a few years back, so may have changed since, but apparently they started it by analysing their logs for the same users submitting very similar queries in a short space of time, and used machine learning based on how users had corrected themselves. As a guess... it could  Could be something from AI like Hopfield network or back propagation network, or something else "identifying fingerprints", restoring broken data, or spelling corrections as Davide mentioned already ... Simple. They have tons of data. They have statistics for every possible term, based on how often it is queried, and what variations of it usually yield results the users click... so, when they see you typed a frequent misspelling for a search term, they go ahead and propose the more usual answer. Actually, if the misspelling is in effect the most frequent searched term, the algorythm will take it for the right one. regarding your question how to mimic the behavior without having tons of data - why not use tons of data collected by google? Download the google sarch results for the misspelled word and search for "Did you mean:" in the HTML. I guess that's called mashup nowadays :-) Apart from the above answers, in case you want to implement something by yourself quickly, here is a suggestion -  You can find the implementation and detailed documentation of this algorithm on  GitHub. You mean to say spell checker? If it is a spell checker rather than a whole phrase then I've got a link about the spell checking where the algorithm is developed in python. Check this link  Meanwhile, I am also working on project that includes searching databases using text. I guess this would solve your problem This is an old question, and I'm surprised that nobody suggested the OP using Apache Solr. Apache Solr is a full text search engine that besides many other functionality also provides spellchecking or query suggestions. From the documentation: By default, the Lucene Spell checkers sort suggestions first by the
  score from the string distance calculation and second by the frequency
  (if available) of the suggestion in the index. There is a specific data structure - ternary search tree - that naturally supports partial matches and near-neighbor matches. Easiest way to figure it out is to Google dynamic programming. It's an algorithm that's been borrowed from Information Retrieval and is used heavily in modern day bioinformatics to see how similiar two gene sequences are. Optimal solution uses dynamic programming and recursion. This is a very solved problem with lots of solutions.  Just google around until you find some open source code.I am learning neural networks and I built a simple one in Keras for the iris dataset classification from the UCI machine learning repository. I used a one hidden layer network with a 8 hidden nodes. Adam optimizer is used with a learning rate of 0.0005 and is run for 200 Epochs. Softmax is used at the output with loss as catogorical-crossentropy. I am getting the following learning curves.   As you can see, the learning curve for the accuracy has a lot of flat regions and I don't understand why. The error seems to be decreasing constantly but the accuracy doesn't seem to be increasing in the same manner. What does the flat regions in the accuracy learning curve imply? Why is the accuracy not increasing at those regions even though error seems to be decreasing?  Is this normal in training or it is more likely that I am doing something wrong here? A little understanding of the actual meanings (and mechanics) of both loss and accuracy will be of much help here (refer also to this answer of mine, although I will reuse some parts)... For the sake of simplicity, I will limit the discussion to the case of binary classification, but the idea is generally applicable; here is the equation of the (logistic) loss:  Now, let's suppose that we have a true label y[k] = 1, for which, at an early point during training, we make a rather poor prediction of p[k] = 0.1; then, plugging the numbers to the loss equation above: Suppose now that, an the next training step, we are getting better indeed, and we get p[k] = 0.22; now we have: Hopefully you start getting the idea, but let's see one more later snapshot, where we get, say, p[k] = 0.49; then: As you can see, our classifier indeed got better in this particular sample, i.e. it went from a loss of 2.3 to 1.5 to 0.71, but this improvement has still not shown up in the accuracy, which cares only for correct classifications: from an accuracy viewpoint, it doesn't matter that we get better estimates for our p[k], as long as these estimates remain below the threshold of 0.5. The moment our p[k] exceeds the threshold of 0.5, the loss continues to decrease smoothly as it has been so far, but now we have a jump in the accuracy contribution of this sample from 0 to 1/n, where n is the total number of samples. Similarly, you can confirm by yourself that, once our p[k] has exceeded 0.5, hence giving a correct classification (and now contributing positively to the accuracy), further improvements of it (i.e getting closer to 1.0) still continue to decrease the loss, but have no further impact to the accuracy. Similar arguments hold for cases where the true label y[m] = 0 and the corresponding estimates for p[m] start somewhere above the 0.5 threshold; and even if p[m] initial estimates are below 0.5 (hence providing correct classifications and already contributing positively to the accuracy), their convergence towards 0.0 will decrease the loss without improving the accuracy any further. Putting the pieces together, hopefully you can now convince yourself that a smoothly decreasing loss and a more "stepwise" increasing accuracy not only are not incompatible, but they make perfect sense indeed. On a more general level: from the strict perspective of mathematical optimization, there is no such thing called "accuracy" - there is only the loss; accuracy gets into the discussion only from a business perspective (and a different business logic might even call for a threshold different than the default 0.5). Quoting from my own linked answer: Loss and accuracy are different things; roughly speaking, the accuracy is what we are actually interested in from a business perspective, while the loss is the objective function that the learning algorithms (optimizers) are trying to minimize from a mathematical perspective. Even more roughly speaking, you can think of the loss as the "translation" of the business objective (accuracy) to the mathematical domain, a translation which is necessary in classification problems (in regression ones, usually the loss and the business objective are the same, or at least can be the same in principle, e.g. the RMSE)...Hi I have been trying to make a custom loss function in keras for dice_error_coefficient. It has its implementations in tensorboard and I tried using the same function in keras with tensorflow but it keeps returning a NoneType when I used model.train_on_batch or model.fit where as it gives proper values when used in metrics in the model. Can please someone help me out with what should i do? I have tried following libraries like Keras-FCN by ahundt where he has used custom loss functions but none of it seems to work. The target and output in the code are y_true and y_pred respectively as used in the losses.py file in keras. There are two steps in implementing a parameterized custom loss function in Keras. First, writing a method for the coefficient/metric. Second, writing a wrapper function to format things the way Keras needs them to be. It's actually quite a bit cleaner to use the Keras backend instead of tensorflow directly for simple custom loss functions like DICE. Here's an example of the coefficient implemented that way: Now for the tricky part. Keras loss functions must only take (y_true, y_pred) as parameters. So we need a separate function that returns another function.  Finally, you can use it as follows in Keras compile. According to the documentation, you can use a custom loss function like this: Any callable with the signature loss_fn(y_true, y_pred) that returns an array of losses (one of sample in the input batch) can be passed to compile() as a loss. Note that sample weighting is automatically supported for any such loss. As a simple example: Complete example: In addition, you can extend an existing loss function by inheriting from it. For example masking the BinaryCrossEntropy: A good starting point is the custom log guide: https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_lossesHaving this: And running: I get: This is incorrect. The tags for quick brown lazy in the sentence should be: Testing this through their online tool gives the same result; quick, brown and fox should be adjectives not nouns. In short: NLTK is not perfect. In fact, no model is perfect. Note: As of NLTK version 3.1, default pos_tag function is no longer the old MaxEnt English pickle.  It is now the perceptron tagger from @Honnibal's implementation, see nltk.tag.pos_tag Still it's better but not perfect: At some point, if someone wants TL;DR solutions, see https://github.com/alvations/nltk_cli In long: Try using other tagger (see https://github.com/nltk/nltk/tree/develop/nltk/tag) , e.g.: Using default MaxEnt POS tagger from NLTK, i.e. nltk.pos_tag: Using Stanford POS tagger: Using HunPOS (NOTE: the default encoding is ISO-8859-1 not UTF8): Using Senna (Make sure you've the latest version of NLTK, there were some changes made to the API): Or try building a better POS tagger: Complains about pos_tag accuracy on stackoverflow include: Issues about NLTK HunPos include: Issues with NLTK and Stanford POS tagger include: Solutions such as changing to the Stanford or Senna or HunPOS tagger will definitely yield results, but here is a much simpler way to experiment with different taggers that are also included within NLTK. The default POS tagger in NTLK right now is the averaged perceptron tagger.  Here's a function that will opt to use the Maxent Treebank Tagger instead: I have found that the averaged perceptron pre-trained tagger in NLTK is biased to treating some adjectives as nouns, as in your example.  The treebank tagger has gotten more adjectives correct for me.I'm evaluating tools for production ML based applications and one of our options is Spark MLlib , but I have some questions about how to serve a model once its trained?  For example in Azure ML, once trained, the model is exposed as a web service which can be consumed from any application, and it's a similar case with Amazon ML. How do you serve/deploy ML models in Apache Spark ? From one hand, a machine learning model built with spark can't be served the way you serve in Azure ML or Amazon ML in a traditional manner.  Databricks claims to be able to deploy models using it's notebook but I haven't actually tried that yet.  On other hand, you can use a model in three ways : Those are the three possible ways.  Of course, you can think of an architecture in which you have RESTful service behind which you can build using spark-jobserver per example to train and deploy but needs some development. It's not a out-of-the-box solution.  You might also use projects like Oryx 2 to create your full lambda architecture to train, deploy and serve a model. Unfortunately, describing each of the mentioned above solution is quite broad and doesn't fit in the scope of SO.  One option is to use MLeap to serve a Spark PipelineModel online with no dependencies on Spark/SparkContext. Not having to use the SparkContext is important as it will drop scoring time for a single record from ~100ms to single-digit microseconds. In order to use it, you have to: MLeap is well integrated with all the Pipeline Stages available in Spark MLlib (with the exception of LDA at the time of this writing). However, things might get a bit more complicated if you are using custom Estimators/Transformers. Take a look at the MLeap FAQ for more info about custom transformers/estimators, performances, and integration. You are comparing two rather different things. Apache Spark is a computation engine, while mentioned by you Amazon and Microsoft solutions are offering services. These services might as well have Spark with MLlib behind the scene. They save you from the trouble building a web service yourself, but you pay extra. Number of companies, like Domino Data Lab, Cloudera or IBM offer products that you can deploy on your own Spark cluster and easily build service around your models (with various degrees of flexibility). Naturally you build a service yourself with various open source tools. Which specifically? It all depends on what you are after. How user should interact with the model? Should there be some sort of UI or jest a REST API? Do you need to change some parameters on the model or the model itself? Are the jobs more of a batch or real-time nature? You can naturally build all-in-one solution, but that's going to be a huge effort. My personal recommendation would be to take advantage, if you can, of one of the available services from Amazon, Google, Microsoft or whatever. Need on-premises deployment? Check Domino Data Lab, their product is mature and allows easy working with models (from building till deployment). Cloudera is more focused on cluster computing (including Spark), but it will take a while before they have something mature. [EDIT] I'd recommend to have a look at Apache PredictionIO, open source machine learning server  - amazing project with lot's of potential.  I have been able to just get this to work. Caveats: Python 3.6 + using Spark ML API (not MLLIB, but sure it should work the same way) Basically, follow this example provided on MSFT's AzureML github. Word of warning: the code as-is will provision but there is an error in the example run() method at the end: Should be: Also, completely agree with MLeap assessment answer, this can make the process run way faster but thought I would answer the question specificallyI am developing a Bi-LSTM model and want to add a attention layer to it. But I am not getting how to add it. My current code for the model is And the model summary is This can be a possible custom solution with a custom layer that computes attention on the positional/temporal dimension it's build to receive 3D tensors and output 3D tensors (return_sequences=True) or 2D tensors (return_sequences=False). below a dummy example with return_sequences=True with return_sequences=False You can integrate it into your networks easily here the running notebook In case, someone is using only Tensorflow and not keras externally, this is the way to do it.I'm new to neural networks/machine learning/genetic algorithms, and for my first implementation I am writing a network that learns to play snake (An example in case you haven't played it before) I have a few questions that I don't fully understand: Before my questions I just want to make sure I understand the general idea correctly. There is a population of snakes, each with randomly generated DNA. The DNA is the weights used in the neural network. Each time the snake moves, it uses the neural net to decide where to go (using a bias). When the population dies, select some parents (maybe highest fitness), and crossover their DNA with a slight mutation chance.  1) If given the whole board as an input (about 400 spots) enough hidden layers (no idea how many, maybe 256-64-32-2?), and enough time, would it learn to not box itself in? 2) What would be good inputs? Here are some of my ideas: 3) Given the input method, what would be a good starting place for hidden layer sizes (of course plan on tweaking this, just don't know what a good starting place) 4) Finally, the fitness of the snake. Besides time to get the apple, it's length, and it's lifetime, should anything else be factored in? In order to get the snake to learn to not block itself in, is there anything else I could add to the fitness to help that? Thank you! In this post, I will advise you of: General opinion of your idea: I can see what you're trying to do, and I believe that your game idea (of using randomly generated identities of adversaries that control their behavior in a way that randomly alters the way they're using artificial intelligence to behave intelligently) has a lot of potential.  Mapping navigational instructions to action sequences with a neural network For processing your game board, because it involves dense (as opposed to sparse) data, you could find a Convolutional Neural Network (CNN) to be useful. However, because you need to translate the map to an action sequence, sequence-optimized neural networks (such as Recurrent Neural Networks) will likely be the most useful for you. I did find some studies that use neural networks to map navigational instructions to action sequences, construct the game map, and move a character through a game with many types of inputs: General opinion of what will help you It sounds like you're missing some basic understanding of how neural networks work, so my primary recommendation to you is to study more of the underlying mechanics behind neural networks in general. It's important to keep in mind that a neural network is a type of machine learning model. So, it doesn't really make sense to just construct a neural network with random parameters. A neural network is a machine learning model that is trained from sample data, and once it is trained, it can be evaluated on test data (e.g. to perform predictions).  The root of machine learning is largely influenced by Bayesian statistics, so you might benefit from getting a textbook on Bayesian statistics to gain a deeper understanding of how machine-based classification works in general.  It will also be valuable for you to learn the differences between different types of neural networks, such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs).  If you want to tinker with how neural networks can be used for classification tasks, try this:  To learn the math:
My professional opinion is that learning the underlying math of neural networks is very important. If it's intimidating, I give you my testimony that I was able to learn all of it on my own. But if you prefer learning in a classroom environment, then I recommend that you try that. A great resource and textbook for learning the mechanics and mathematics of neural networks is:  Tutorials for neural network libraries I recommend that you try working through the tutorials for a neural network library, such as:  I saw similar application. Inputs usually were snake coordinates, apple coordinates and some sensory data(is wall next to snake head or no in your case). Using genetic algorithm is a good idea in this case. You doing only parametric learning(finding set of weights), but structure will be based on your estimation. GA can be also used for structure learning(finding topology of ANN). But using GA for both will be very computational hard. Professor Floreano did something similar. He use GA for finding weights for neural network controller of robot. Robot was in labyrinth and perform some task. Neural network hidden layer was one neuron with recurrent joints on inputs and one lateral connection on himself. There was two outputs. Outputs were connected on input layer and hidden layer(mentioned one neuron). But Floreano did something more interesting. He say, We don't born with determined synapses, our synapses change in our lifetime. So he use GA for finding rules for change of synapses. These rules was based on Hebbian learning. He perform node encoding(for all weights connected to neuron will apply same rule). On beginning, he initialized weights on small random values. Finding rules instead of numerical value of synapse leads to better results.
One from Floreno's articles. And on the and my own experience. In last semester I and my schoolmate get a task finding the rules for synapse with GA but for Spiking neural network. Our SNN was controller for kinematic model of mobile robot and task was lead robot in to the chosen point. We obtained some results but not expected. You can see results here. So I recommend you use "ordinary" ANN instead off SNN because SNN brings new phenomens.I'm using linear_model.LinearRegression from scikit-learn as a predictive model. It works and it's perfect. I have a problem to evaluate the predicted results using the accuracy_score metric. This is my true Data : My predicted Data: My code: Error message: Despite the plethora of wrong answers here that attempt to circumvent the error by numerically manipulating the predictions, the root cause of your error is a theoretical and not computational issue: you are trying to use a classification metric (accuracy) in a regression (i.e. numeric prediction) model (LinearRegression), which is meaningless. Just like the majority of performance metrics, accuracy compares apples to apples (i.e true labels of 0/1 with predictions again of 0/1); so, when you ask the function to compare binary true labels (apples) with continuous predictions (oranges), you get an expected error, where the message tells you exactly what the problem is from a computational point of view: Despite that the message doesn't tell you directly that you are trying to compute a metric that is invalid for your problem (and we shouldn't actually expect it to go that far), it is certainly a good thing that scikit-learn at least gives you a direct and explicit warning that you are attempting something wrong; this is not necessarily the case with other frameworks - see for example the behavior of Keras in a very similar situation, where you get no warning at all, and one just ends up complaining for low "accuracy" in a regression setting... I am super-surprised with all the other answers here (including the accepted & highly upvoted one) effectively suggesting to manipulate the predictions in order to simply get rid of the error; it's true that, once we end up with a set of numbers, we can certainly start mingling with them in various ways (rounding, thresholding etc) in order to make our code behave, but this of course does not mean that our numeric manipulations are meaningful in the specific context of the ML problem we are trying to solve. So, to wrap up: the problem is that you are applying a metric (accuracy) that is inappropriate for your model (LinearRegression): if you are in a classification setting, you should change your model (e.g. use LogisticRegression instead); if you are in a regression (i.e. numeric prediction) setting, you should change the metric. Check the list of metrics available in scikit-learn, where you can confirm that accuracy is used only in classification. Compare also the situation with a recent SO question, where the OP is trying to get the accuracy of a list of models: where the first 6 models work OK, while all the rest (commented-out) ones give the same error. By now, you should be able to convince yourself that all the commented-out models are regression (and not classification) ones, hence the justified error. A last important note: it may sound legitimate for someone to claim: OK, but I want to use linear regression and then just
round/threshold the outputs, effectively treating the predictions as
"probabilities" and thus converting the model into a classifier Actually, this has already been suggested in several other answers here, implicitly or not; again, this is an invalid approach (and the fact that you have negative predictions should have already alerted you that they cannot be interpreted as probabilities). Andrew Ng, in his popular Machine Learning course at Coursera, explains why this is a bad idea - see his Lecture 6.1 - Logistic Regression | Classification at Youtube (explanation starts at ~ 3:00), as well as section 4.2 Why Not Linear Regression [for classification]? of the (highly recommended and freely available) textbook An Introduction to Statistical Learning by Hastie, Tibshirani and coworkers... accuracy_score is a classification metric, you cannot use it for a regression problem. You can see the available regression metrics in the docs. The problem is that the true y is binary (zeros and ones), while your predictions are not. You probably generated probabilities and not predictions, hence the result :)
Try instead to generate class membership, and it should work! The sklearn.metrics.accuracy_score(y_true, y_pred) method defines y_pred as:  y_pred : 1d array-like, or label indicator array / sparse matrix. 
Predicted labels, as returned by a classifier. Which means y_pred has to be an array of 1's or 0's (predicated labels). They should not be probabilities. The predicated labels (1's and 0's) and/or predicted probabilites can be generated using the LinearRegression() model's methods predict() and predict_proba() respectively. 1. Generate predicted labels: output:  y_preds can now be used for the accuracy_score() method: accuracy_score(y_true, y_pred) 2. Generate probabilities for labels: Some metrics such as 'precision_recall_curve(y_true, probas_pred)' require probabilities, which can be generated as follows: output: This resolve same problem for me,
use .round() for preditions, I was facing the same issue.The dtypes of y_test and y_pred were different. Make sure that the dtypes are same for both.
 The error is because difference in datatypes of y_pred and y_true. y_true might be dataframe and y_pred is arraylist. If you convert both to arrays, then issue will get resolved. Just use accuracy_score is a classification metric, you cannot use it for a regression problem. Use this way:How do I save a trained Naive Bayes classifier to disk and use it to predict data? I have the following sample program from the scikit-learn website: Classifiers are just objects that can be pickled and dumped like any other. To continue your example: Edit: if you are using a sklearn Pipeline in which you have custom transformers that cannot be serialized by pickle (nor by joblib), then using Neuraxle's custom ML Pipeline saving is a solution where you can define your own custom step savers on a per-step basis. The savers are called for each step if defined upon saving, and otherwise joblib is used as default for steps without a saver. You can also use joblib.dump and joblib.load which is much more efficient at handling numerical arrays than the default python pickler. Joblib is included in scikit-learn: Edit: in Python 3.8+ it's now possible to use pickle for efficient pickling of object with large numerical arrays as attributes if you use pickle protocol 5 (which is not the default). What you are looking for is called Model persistence in sklearn words and it is documented in introduction and in model persistence sections. So you have initialized your classifier and trained it for a long time with After this you have two options: 1) Using Pickle 2) Using Joblib One more time it is helpful to read the above-mentioned links  In many cases, particularly with text classification it is not enough just to store the classifier but you'll need to store the vectorizer as well so that you can vectorize your input in future. future use case: Before dumping the vectorizer, one can delete the stop_words_ property of vectorizer by: to make dumping more efficient.
Also if your classifier parameters is sparse (as in most text classification examples) you can convert the parameters from dense to sparse which will make a huge difference in terms of memory consumption, loading and dumping. Sparsify the model by: Which will automatically work for SGDClassifier but in case you know your model is sparse (lots of zeros in clf.coef_) then you can manually convert clf.coef_ into a csr scipy sparse matrix by: and then you can store it more efficiently. sklearn estimators implement methods to make it easy for you to save relevant trained properties of an estimator. Some estimators implement __getstate__ methods themselves, but others, like the GMM just use the base implementation which simply saves the objects inner dictionary: The recommended method to save your model to disc is to use the pickle module: However, you should save additional data so you can retrain your model in the future, or suffer dire consequences (such as being locked into an old version of sklearn). From the documentation: In order to rebuild a similar model with future versions of
  scikit-learn, additional metadata should be saved along the pickled
  model:  The training data, e.g. a reference to a immutable snapshot  The python source code used to generate the model  The versions of scikit-learn and its dependencies  The cross validation score obtained on the training data This is especially true for Ensemble estimators that rely on the tree.pyx module written in Cython(such as IsolationForest), since it creates a coupling to the implementation, which is not guaranteed to be stable between versions of sklearn. It has seen backwards incompatible changes in the past. If your models become very large and loading becomes a nuisance, you can also use the more efficient joblib. From the documentation: In the specific case of the scikit, it may be more interesting to use
  joblib’s replacement of pickle (joblib.dump & joblib.load), which is
  more efficient on objects that carry large numpy arrays internally as
  is often the case for fitted scikit-learn estimators, but can only
  pickle to the disk and not to a string: sklearn.externals.joblib has been deprecated since 0.21 and will be removed in v0.23:  /usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/init.py:15:
  FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will
  be removed in 0.23. Please import this functionality directly from
  joblib, which can be installed with: pip install joblib. If this
  warning is raised when loading pickled models, you may need to
  re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning) Therefore, you need to install joblib:  and finally write the model to disk:  Now in order to read the dumped file all you need to run is:Caffe has a layer type "Python".   For instance, this layer type can be used as a loss layer.
On other occasions it is used as an input layer. What is this layer type?
How can this layer be used? Prune's and Bharat's answers gives the overall purpose of a "Python" layer: a general purpose layer which is implemented in python rather than c++. I intend this answer to serve as a tutorial for using "Python" layer. Please see the excellent answers of Prune and Bharat. In order to use 'Python" layer you need to compile caffe with flag set in 'Makefile.config'. A "Python" layer should be implemented as a python class derived from caffe.Layer base class. This class must have the following four methods: What are these methods? def setup(self, bottom, top): This method is called once when caffe builds the net. This function should check that number of inputs (len(bottom)) and number of outputs (len(top)) is as expected.
You should also allocate internal parameters of the net here (i.e., self.add_blobs()), see this thread for more information.
This method has access to self.param_str - a string passed from the prototxt to the layer. See this thread for more information. def reshape(self, bottom, top): This method is called whenever caffe reshapes the net. This function should allocate the outputs (each of the top blobs). The outputs' shape is usually related to the bottoms' shape. def forward(self, bottom, top): Implementing the forward pass from bottom to top. def backward(self, top, propagate_down, bottom): This method implements the backpropagation, it propagates the gradients from top to bottom. propagate_down is a Boolean vector of len(bottom) indicating to which of the bottoms the gradient should be propagated. Some more information about bottom and top inputs you can find in this post.   Examples
You can see some examples of simplified python layers here, here and here.
Example of "moving average" output layer can be found here. Trainable parameters
"Python" layer can have trainable parameters (like "Conv", "InnerProduct", etc.).
You can find more information on adding trainable parameters in this thread and this one. There's also a very simplified example in caffe git. See Bharat's answer for details.
You need to add the following to your prototxt: It's very simple: Invoking python code from caffe is nothing you need to worry about. Caffe uses boost API to call python code from compiled c++.
What do you do need to do?
Make sure the python module implementing your layer is in $PYTHONPATH so that when caffe imports it - it can be found.
For instance, if your module my_python_layer.py is in /path/to/my_python_layer.py then should work just fine. You should always test your layer before putting it to use.
Testing the forward function is entirely up to you, as each layer has a different functionality.
Testing the backward method is easy, as this method only implements a gradient of forward it can be numerically tested automatically!
Check out test_gradient_for_python_layer testing utility: It is worth while noting that python code runs on CPU only. Thus, if you plan to have a Python layer in the middle of your net you will see a significant degradation in performance if you plan on using GPU. This happens because caffe needs to copy blobs from GPU to CPU before calling python layer and then copy back to GPU to proceed with the forward/backward pass.
This degradation is far less significant if the python layer is either an input layer or the topmost loss layer.
Update: On Sep 19th, 2017 PR #5904 was merged into master. This PR exposes GPU pointers of blobs via the python interface.
You may access blob._gpu_data_ptr and blob._gpu_diff_ptr directly from python at your own risk. Very simply, it's a layer in which you provide the implementation code, rather than using one of the pre-defined types -- which are all backed by efficient functions. If you want to define a custom loss function, go ahead: write it yourself, and create the layer with type Python.  If you have non-standard input needs, perhaps some data-specific pre-processing, no problem: write it yourself, and create the layer with type Python. Python layers are different from C++ layers which need to be compiled, their parameters need to be added to the proto file and finally you need to register the layer in layer_factory. If you write a python layer, you don't need to worry about any of these things. Layer parameters can be defined as a string, which are accessible as a string in python. For example: if you have a parameter in a layer, you can access it using 'self.param_str', if param_str was defined in your prototxt file. Like other layers, you need to define a class with the following functions: Prototxt example: Here, name of the layer is rpn-data, bottom and top are input and output details of the layer respectively. python_param defines what are the parameters of the Python layer. 'module' specifies what is the file name of your layer. If the file called 'anchor_target_layer.py' is located inside a folder called 'rpn', the parameter would be 'rpn.anchor_target_layer'. The 'layer' parameter is the name of your class, in this case it is 'AnchorTargetLayer'. 'param_str' is a parameter for the layer, which contains a value 16 for the key 'feat_stride'. Unlike C++/CUDA layers, Python layers do not work in a multi-GPU setting in caffe as of now, so that is a disadvantage of using them.From the Udacity's deep learning class, the softmax of y_i is simply the exponential divided by the sum of exponential of the whole Y vector:  Where S(y_i) is the softmax function of y_i and e is the exponential and j is the no. of columns in the input vector Y. I've tried the following: which returns: But the suggested solution was: which produces the same output as the first implementation, even though the first implementation explicitly takes the difference of each column and the max and then divides by the sum. Can someone show mathematically why? Is one correct and the other one wrong? Are the implementation similar in terms of code and time complexity? Which is more efficient? They're both correct, but yours is preferred from the point of view of numerical stability. You start with By using the fact that a^(b - c) = (a^b)/(a^c) we have Which is what the other answer says. You could replace max(x) with any variable and it would cancel out. (Well... much confusion here, both in the question and in the answers...) To start with, the two solutions (i.e. yours and the suggested one) are not equivalent; they happen to be equivalent only for the special case of 1-D score arrays. You would have discovered it if you had tried also the 2-D score array in the Udacity quiz provided example. Results-wise, the only actual difference between the two solutions is the axis=0 argument. To see that this is the case, let's try your solution (your_softmax) and one where the only difference is the axis argument: As I said, for a 1-D score array, the results are indeed identical: Nevertheless, here are the results for the 2-D score array given in the Udacity quiz as a test example: The results are different - the second one is indeed identical with the one expected in the Udacity quiz, where all columns indeed sum to 1, which is not the case with the first (wrong) result. So, all the fuss was actually for an implementation detail - the axis argument. According to the numpy.sum documentation: The default, axis=None, will sum all of the elements of the input array while here we want to sum row-wise, hence axis=0. For a 1-D array, the sum of the (only) row and the sum of all the elements happen to be identical, hence your identical results in that case... The axis issue aside, your implementation (i.e. your choice to subtract the max first) is actually better than the suggested solution! In fact, it is the recommended way of implementing the softmax function - see here for the justification (numeric stability, also pointed out by some other answers here). So, this is really a comment to desertnaut's answer but I can't comment on it yet due to my reputation. As he pointed out, your version is only correct if your input consists of a single sample. If your input consists of several samples, it is wrong. However, desertnaut's solution is also wrong. The problem is that once he takes a 1-dimensional input and then he takes a 2-dimensional input. Let me show this to you. Lets take desertnauts example: This is the output: You can see that desernauts version would fail in this situation. (It would not if the input was just one dimensional like np.array([1, 2, 3, 6]). Lets now use 3 samples since thats the reason why we use a 2 dimensional input. The following x2 is not the same as the one from desernauts example.  This input consists of a batch with 3 samples. But sample one and three are essentially the same. We now expect 3 rows of softmax activations where the first should be the same as the third and also the same as our activation of x1! I hope you can see that this is only the case with my solution. Additionally, here is the results of TensorFlows softmax implementation: And the result: I would say that while both are correct mathematically, implementation-wise, first one is better. When computing softmax, the intermediate values may become very large. Dividing two large numbers can be numerically unstable. These notes (from Stanford) mention a normalization trick which is essentially what you are doing.  sklearn also offers implementation of softmax From mathematical point of view both sides are equal.  And you can easily prove this. Let's m=max(x). Now your function softmax returns a vector, whose i-th coordinate is equal to  notice that this works for any m, because for all (even complex) numbers e^m != 0 from computational complexity point of view they are also equivalent and both run in O(n) time, where n is the size of a vector.  from numerical stability point of view, the first solution is preferred, because e^x grows very fast and even for pretty small values of x it will overflow. Subtracting the maximum value allows to get rid of this overflow. To practically experience the stuff I was talking about try to feed x = np.array([1000, 5]) into both of your functions. One will return correct probability, the second will overflow with nan your solution works only for vectors (Udacity quiz wants you to calculate it for matrices as well). In order to fix it you need to use sum(axis=0) EDIT. As of version 1.2.0, scipy includes softmax as a special function:  https://scipy.github.io/devdocs/generated/scipy.special.softmax.html I wrote a function applying the softmax over any axis: Subtracting the max, as other users described, is good practice. I wrote a detailed post about it here. Here you can find out why they used - max.  From there: "When you’re writing code for computing the Softmax function in practice, the intermediate terms may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick." I was curious to see the performance difference between these Using Increasing the values inside x (+100 +200 +500...) I get consistently better results with the original numpy version (here is just one test) Until.... the values inside x reach ~800, then I get As some said, your version is more numerically stable 'for large numbers'. For small numbers could be the other way around. A more concise version is: To offer an alternative solution, consider the cases where your arguments are extremely large in magnitude such that exp(x) would underflow (in the negative case) or overflow (in the positive case). Here you want to remain in log space as long as possible, exponentiating only at the end where you can trust the result will be well-behaved. I needed something compatible with the output of a dense layer from Tensorflow.  The solution from @desertnaut does not work in this case because I have batches of data. Therefore, I came with another solution that should work in both cases: Results:
 Ref: Tensorflow softmax I would suggest this: It will work for stochastic as well as the batch.
For more detail see :
https://medium.com/@ravish1729/analysis-of-softmax-function-ad058d6a564d In order to maintain for numerical stability, max(x) should be subtracted. The following is the code for softmax function; def softmax(x): Already answered in much detail in above answers. max is subtracted to avoid overflow. I am adding here one more implementation in python3. Everybody seems to post their solution so I'll post mine: I get the exact same results as the imported from sklearn: Based on all the responses and CS231n notes, allow me to summarise: Usage: Output: The softmax function is an activation function that turns numbers into probabilities which sum to one. The softmax function outputs a vector that represents the probability distributions of a list of outcomes. It is also a core element used in deep learning classification tasks. Softmax function is used when we have multiple classes. It is useful for finding out the class which has the max. Probability. The Softmax function is ideally used in the output layer, where we are actually trying to attain the probabilities to define the class of each input. It ranges from 0 to 1. Softmax function turns logits [2.0, 1.0, 0.1] into probabilities [0.7, 0.2, 0.1], and the probabilities sum to 1. Logits are the raw scores output by the last layer of a neural network. Before activation takes place. To understand the softmax function, we must look at the output of the (n-1)th layer. The softmax function is, in fact, an arg max function. That means that it does not return the largest value from the input, but the position of the largest values. For example: Before softmax After softmax Code: This also works with np.reshape. I would like to supplement a little bit more understanding of the problem. Here it is correct of subtracting max of the array. But if you run the code in the other post, you would find it is not giving you right answer when the array is 2D  or higher dimensions. Here I give you some suggestions: Follow the result you will get the correct answer by doing vectorization. Since it is related to the college homework, I cannot post the exact code here, but I would like to give more suggestions if you don't understand. Goal was to achieve similar results using Numpy and Tensorflow. The only change from original answer is axis parameter for np.sum api. Initial approach : axis=0 - This however does not provide intended results when dimensions are N. Modified approach: axis=len(e_x.shape)-1 - Always sum on the last dimension. This provides similar results as tensorflow's softmax function. Here is generalized solution using numpy and comparision for correctness with tensorflow ans scipy: Data preparation: Output: Softmax using tensorflow: Output: Softmax using scipy: Output: Softmax using numpy (https://nolanbconaway.github.io/blog/2017/softmax-numpy) : Output: The purpose of the softmax function is to preserve the ratio of the vectors as opposed to squashing the end-points with a sigmoid as the values saturate (i.e. tend to +/- 1 (tanh) or from 0 to 1 (logistical)). This is because it preserves more information about the rate of change at the end-points and thus is more applicable to neural nets with 1-of-N Output Encoding (i.e. if we squashed the end-points it would be harder to differentiate the 1-of-N output class because we can't tell which one is the "biggest" or "smallest" because they got squished.); also it makes the total output sum to 1, and the clear winner will be closer to 1 while other numbers that are close to each other will sum to 1/p, where p is the number of output neurons with similar values. The purpose of subtracting the max value from the vector is that when you do e^y exponents you may get very high value that clips the float at the max value leading to a tie, which is not the case in this example. This becomes a BIG problem if you subtract the max value to make a negative number, then you have a negative exponent that rapidly shrinks the values altering the ratio, which is what occurred in poster's question and yielded the incorrect answer. The answer supplied by Udacity is HORRIBLY inefficient. The first thing we need to do is calculate e^y_j for all vector components, KEEP THOSE VALUES, then sum them up, and divide. Where Udacity messed up is they calculate e^y_j TWICE!!! Here is the correct answer: This generalizes and assumes you are normalizing the trailing dimension. I used these three simple lines:Is it possible to specify your own distance function using scikit-learn K-Means Clustering? Here's a small kmeans that uses any of the 20-odd distances in
scipy.spatial.distance, or a user function.
Comments would be welcome (this has had only one user so far, not enough);
in particular, what are your N, dim, k, metric ? Some notes added 26mar 2012: 1) for cosine distance, first normalize all the data vectors to |X| = 1; then is fast. For bit vectors, keep the norms separately from the vectors
instead of expanding out to floats
(although some programs may expand for you).
For sparse vectors, say 1 % of N, X . Y should take time O( 2 % N ),
space O(N); but I don't know which programs do that. 2)
Scikit-learn clustering
gives an excellent overview of k-means, mini-batch-k-means ...
with code that works on scipy.sparse matrices. 3) Always check cluster sizes after k-means.
If you're expecting roughly equal-sized clusters, but they come out
[44 37  9  5  5] % ... (sound of head-scratching). Unfortunately no: scikit-learn current implementation of k-means only uses Euclidean distances. It is not trivial to extend k-means to other distances and denis' answer above is not the correct way to implement k-means for other metrics. Just use nltk instead where you can do this, e.g. Yes you can use a difference metric function; however, by definition, the k-means clustering algorithm relies on the eucldiean distance from the mean of each cluster.  You could use a different metric, so even though you are still calculating the mean you could use something like the mahalnobis distance.  There is pyclustering which is python/C++ (so its fast!) and lets you specify a custom metric function Actually, i haven't tested this code but cobbled it together from a ticket and example code. k-means of Spectral Python allows the use of L1 (Manhattan) distance. Sklearn Kmeans uses the Euclidean distance. It has no metric parameter. This said, if you're clustering time series, you can use the tslearn python package, when you can specify a metric (dtw, softdtw, euclidean). The Affinity propagation algorithm from the sklearn library allows you to pass the similarity matrix instead of the samples. So, you can use your metric to compute the similarity matrix (not the dissimilarity matrix) and pass it to the function by setting the "affinity" term to "precomputed".https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation.fit
In terms of the K-Mean, I think it is also possible but I have not tried it.
However, as the other answers stated, finding the mean using a different metric will be the issue. Instead, you can use PAM (K-Medoids) algorthim as it calculates the change in Total Deviation (TD), thus it does not rely on the distance metric. https://python-kmedoids.readthedocs.io/en/latest/#fasterpam Yes, in the current stable version of sklearn (scikit-learn 1.1.3), you can easily use your own distance metric. All you have to do is create a class that inherits from sklearn.cluster.KMeans and overwrites its _transform method. The below example is for the IOU distance from the Yolov2 paper.I have asked a question a few days back on how to find the nearest neighbors for a given vector. My vector is now 21 dimensions and before I proceed further, because I am not from the domain of Machine Learning nor Math, I am beginning to ask myself some fundamental questions: Can someone please clarify the some (or all) of the above questions? I currently study such problems -- classification, nearest neighbor searching -- for music information retrieval. You may be interested in Approximate Nearest Neighbor (ANN) algorithms. The idea is that you allow the algorithm to return sufficiently near neighbors (perhaps not the nearest neighbor); in doing so, you reduce complexity. You mentioned the kd-tree; that is one example. But as you said, kd-tree works poorly in high dimensions. In fact, all current indexing techniques (based on space partitioning) degrade to linear search for sufficiently high dimensions [1][2][3]. Among ANN algorithms proposed recently, perhaps the most popular is Locality-Sensitive Hashing (LSH), which maps a set of points in a high-dimensional space into a set of bins, i.e., a hash table [1][3]. But unlike traditional hashes, a locality-sensitive hash places nearby points into the same bin. LSH has some huge advantages. First, it is simple. You just compute the hash for all points in your database, then make a hash table from them. To query, just compute the hash of the query point, then retrieve all points in the same bin from the hash table. Second, there is a rigorous theory that supports its performance. It can be shown that the query time is sublinear in the size of the database, i.e., faster than linear search. How much faster depends upon how much approximation we can tolerate. Finally, LSH is compatible with any Lp norm for 0 < p <= 2. Therefore, to answer your first question, you can use LSH with the Euclidean distance metric, or you can use it with the Manhattan (L1) distance metric. There are also variants for Hamming distance and cosine similarity. A decent overview was written by Malcolm Slaney and Michael Casey for IEEE Signal Processing Magazine in 2008 [4]. LSH has been applied seemingly everywhere. You may want to give it a try. [1] Datar, Indyk, Immorlica, Mirrokni, "Locality-Sensitive Hashing Scheme Based on p-Stable Distributions," 2004. [2] Weber, Schek, Blott, "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces," 1998. [3] Gionis, Indyk, Motwani, "Similarity search in high dimensions via hashing," 1999. [4] Slaney, Casey, "Locality-sensitive hashing for finding nearest neighbors", 2008. I. The Distance Metric First, the number of features (columns) in a data set is not a factor in selecting a distance metric for use in kNN. There are quite a few published studies directed to precisely this question, and the usual bases for comparison are:  the underlying statistical
distribution of your data; the relationship among the features
that comprise your data (are they
independent--i.e., what does the
covariance matrix look like); and the coordinate space from which your
data was obtained. If you have no prior knowledge of the distribution(s) from which your data was sampled, at least one (well documented and thorough) study concludes that Euclidean distance is the best choice. YEuclidean metric used in mega-scale Web Recommendation Engines as well as in current academic research. Distances calculated by Euclidean have intuitive meaning and the computation scales--i.e., Euclidean distance is calculated the same way, whether the two points are in two dimension or in twenty-two dimension space. It has only failed for me a few times, each of those cases Euclidean distance failed because the underlying (cartesian) coordinate system was a poor choice. And you'll usually recognize this because for instance path lengths (distances) are no longer additive--e.g., when the metric space is a chessboard, Manhattan distance is better than Euclidean, likewise when the metric space is Earth and your distances are trans-continental flights, a distance metric suitable for a polar coordinate system is a good idea (e.g., London to Vienna is is 2.5 hours, Vienna to St. Petersburg is another 3 hrs, more or less in the same direction, yet London to St. Petersburg isn't 5.5 hours, instead, is a little over 3 hrs.) But apart from those cases in which your data belongs in a non-cartesian coordinate system, the choice of distance metric is usually not material. (See this blog post from a CS student, comparing several distance metrics by examining their effect on kNN classifier--chi square give the best results, but the differences are not large; A more comprehensive study is in the academic paper, Comparative Study of Distance Functions for Nearest Neighbors--Mahalanobis (essentially Euclidean normalized by to account for dimension covariance) was the best in this study. One important proviso: for distance metric calculations to be meaningful, you must re-scale your data--rarely is it possible to build a kNN model to generate accurate predictions without doing this. For instance, if you are building a kNN model to predict athletic performance, and your expectation variables are height (cm), weight (kg), bodyfat (%), and resting pulse (beats per minute), then a typical data point might look something like this: [ 180.4, 66.1, 11.3, 71 ]. Clearly the distance calculation will be dominated by height, while the contribution by bodyfat % will be almost negligible. Put another way, if instead, the data were reported differently, so that bodyweight was in grams rather than kilograms, then the original value of 86.1, would be 86,100, which would have a large effect on your results, which is exactly what you don't want. Probably the most common scaling technique is subtracting the mean and dividing by the standard deviation (mean and sd refer calculated separately for each column, or feature in that data set; X refers to an individual entry/cell within a data row): 
II. The Data Structure If you are concerned about performance of the kd-tree structure, A Voronoi Tessellation is a conceptually simple container but that will drastically improve performance and scales better than kd-Trees. This is not the most common way to persist kNN training data, though the application of VT for this purpose, as well as the consequent performance advantages, are well-documented (see e.g. this Microsoft Research report). The practical significance of this is that, provided you are using a 'mainstream' language (e.g., in the TIOBE Index) then you ought to find a library to perform VT. I know in Python and R, there are multiple options for each language (e.g., the voronoi package for R available on CRAN) Using a VT for kNN works like this:: From your data, randomly select w points--these are your Voronoi centers. A Voronoi cell encapsulates all neighboring points that are nearest to each center. Imagine if you assign a different color to each of Voronoi centers, so that each point assigned to a given center is painted that color. As long as you have a sufficient density, doing this will nicely show the boundaries of each Voronoi center (as the boundary that separates two colors. How to select the Voronoi Centers? I use two orthogonal guidelines. After random selecting the w points, calculate the VT for your training data. Next check the number of data points assigned to each Voronoi center--these values should be about the same (given uniform point density across your data space). In two dimensions, this would cause a VT with tiles of the same size.That's the first rule, here's the second. Select w by iteration--run your kNN algorithm with w as a variable parameter, and measure performance (time required to return a prediction by querying the VT). So imagine you have one million data points..... If the points were persisted in an ordinary 2D data structure, or in a kd-tree, you would perform on average a couple million distance calculations for each new data points whose response variable you wish to predict. Of course, those calculations are performed on a single data set. With a V/T, the nearest-neighbor search is performed in two steps one after the other, against two different populations of data--first against the Voronoi centers, then once the nearest center is found, the points inside the cell corresponding to that center are searched to find the actual nearest neighbor (by successive distance calculations) Combined, these two look-ups are much faster than a single brute-force look-up. That's easy to see: for 1M data points, suppose you select 250 Voronoi centers to tesselate your data space. On average, each Voronoi cell will have 4,000 data points. So instead of performing on average 500,000 distance calculations (brute force), you perform far lesss, on average just 125 + 2,000. III. Calculating the Result (the predicted response variable) There are two steps to calculating the predicted value from a set of kNN training data. The first is identifying n, or the number of nearest neighbors to use for this calculation. The second is how to weight their contribution to the predicted value. W/r/t the first component, you can determine the best value of n by solving an optimization problem (very similar to least squares optimization). That's the theory; in practice, most people just use n=3. In any event, it's simple to run your kNN algorithm over a set of test instances (to calculate predicted values) for n=1, n=2, n=3, etc. and plot the error as a function of n. If you just want a plausible value for n to get started, again, just use n = 3. The second component is how to weight the contribution of each of the neighbors (assuming n > 1). The simplest weighting technique is just multiplying each neighbor by a weighting coefficient, which is just the 1/(dist * K), or the inverse of the distance from that neighbor to the test instance often multiplied by some empirically derived constant, K. I am not a fan of this technique because it often over-weights the closest neighbors (and concomitantly under-weights the more distant ones); the significance of this is that a given prediction can be almost entirely dependent on a single neighbor, which in turn increases the algorithm's sensitivity to noise. A must better weighting function, which substantially avoids this limitation is the gaussian function, which in python, looks like this: To calculate a predicted value using your kNN code, you would identify the n nearest neighbors to the data point whose response variable you wish to predict ('test instance'), then call the weight_gauss function, once for each of the n neighbors, passing in the distance between each neighbor the the test point.This function will return the weight for each neighbor, which is then used as that neighbor's coefficient in the weighted average calculation. What you are facing is known as the curse of dimensionality. It is sometimes useful to run an algorithm like PCA or ICA to make sure that you really need all 21 dimensions and possibly find a linear transformation which would allow you to use less than 21 with approximately the same result quality. Update:
I encountered them in a book called Biomedical Signal Processing by Rangayyan (I hope I remember it correctly). ICA is not a trivial technique, but it was developed by researchers in Finland and I think Matlab code for it is publicly available for download. PCA is a more widely used technique and I believe you should be able to find its R or other software implementation. PCA is performed by solving linear equations iteratively. I've done it too long ago to remember how. = ) The idea is that you break up your signals into independent eigenvectors (discrete eigenfunctions, really) and their eigenvalues, 21 in your case. Each eigenvalue shows the amount of contribution each eigenfunction provides to each of your measurements. If an eigenvalue is tiny, you can very closely represent the signals without using its corresponding eigenfunction at all, and that's how you get rid of a dimension. Top answers are good but old, so I'd like to add up a 2016 answer. As said, in a high dimensional space, the curse of dimensionality lurks around the corner, making the traditional approaches, such as the popular k-d tree, to be as slow as a brute force approach. As a result, we turn our interest in Approximate Nearest Neighbor Search (ANNS), which in favor of some accuracy, speedups the process. You get a good approximation of the exact NN, with a good propability. Hot topics that might be worthy: You can also check my relevant answers: To answer your questions one by one: Here is a nice paper to get you started in the right direction. "When in Nearest Neighbour meaningful?" by Beyer et all. I work with text data of dimensions 20K and above. If you want some text related advice, I might be able to help you out. Cosine similarity is a common way to compare high-dimension vectors. Note that since it's a similarity not a distance, you'd want to maximize it not minimize it.  You can also use a domain-specific way to compare the data, for example if your data was DNA sequences, you could use a sequence similarity that takes into account probabilities of mutations, etc. The number of nearest neighbors to use varies depending on the type of data, how much noise there is, etc.  There are no general rules, you just have to find what works best for your specific data and problem by trying all values within a range.  People have an intuitive understanding that the more data there is, the fewer neighbors you need.  In a hypothetical situation where you have all possible data, you only need to look for the single nearest neighbor to classify. The k Nearest Neighbor method is known to be computationally expensive.  It's one of the main reasons people turn to other algorithms like support vector machines. kd-trees indeed won't work very well on high-dimensional data. Because the pruning step no longer helps a lot, as the closest edge - a 1 dimensional deviation - will almost always be smaller than the full-dimensional deviation to the known nearest neighbors. But furthermore, kd-trees only work well with Lp norms for all I know, and there is the distance concentration effect that makes distance based algorithms degrade with increasing dimensionality. For further information, you may want to read up on the curse of dimensionality, and the various variants of it (there is more than one side to it!) I'm not convinced there is a lot use to just blindly approximating Euclidean nearest neighbors e.g. using LSH or random projections. It may be necessary to use a much more fine tuned distance function in the first place! A lot depends on why you want to know the nearest neighbors. You might look into the mean shift  algorithm http://en.wikipedia.org/wiki/Mean-shift if what you really want is to find the modes of your data set.  I think cosine on tf-idf of boolean features would work well for most problems. That's because its time-proven heuristic used in many search engines like Lucene. Euclidean distance in my experience shows bad results for any text-like data. Selecting different weights and k-examples can be done with training data and brute-force parameter selection. iDistance is probably the best for exact knn retrieval in high-dimensional data.  You can view it as an approximate Voronoi tessalation. I've experienced the same problem and can say the following.  Euclidean distance is a good distance metric, however it's computationally more expensive than the Manhattan distance, and sometimes yields slightly poorer results, thus, I'd choose the later. The value of k can be found empirically. You can try different values and check the resulting ROC curves or some other precision/recall measure in order to find an acceptable value. Both Euclidean and Manhattan distances respect the Triangle inequality, thus you can use them in metric trees. Indeed, KD-trees have their performance severely degraded when the data have more than 10 dimensions (I've experienced that problem myself). I found VP-trees to be a better option. KD Trees work fine for 21 dimensions, if you quit early,
after looking at say 5 % of all the points.
FLANN does this (and other speedups)
to match 128-dim SIFT vectors. (Unfortunately FLANN does only the Euclidean metric,
and the fast and solid 
scipy.spatial.cKDTree
does only Lp metrics;
these may or may not be adequate for your data.)
There is of course a speed-accuracy tradeoff here. (If you could describe your Ndata, Nquery, data distribution,
that might help people to try similar data.) Added 26 April, run times for cKDTree with cutoff on my old mac ppc, to give a very rough idea of feasibility: You could try a z order curve. It's easy for 3 dimension. I had a similar question a while back. For fast Approximate Nearest Neighbor Search you can use the annoy library from spotify: https://github.com/spotify/annoy This is some example code for the Python API, which is optimized in C++. They provide different distance measurements. Which distance measurement you want to apply depends highly on your individual problem. Also consider prescaling (meaning weighting) certain dimensions for importance first. Those dimension or feature importance weights might be calculated by something like entropy loss or if you have a supervised learning problem gini impurity gain or mean average loss, where you check how much worse your machine learning model performs, if you scramble this dimensions values. Often the direction of the vector is more important than it's absolute value. For example in the semantic analysis of text documents, where we want document vectors to be close when their semantics are similar, not their lengths. Thus we can either normalize those vectors to unit length or use angular distance (i.e. cosine similarity) as a distance measurement. Hope this is helpful. Is Euclidean distance a good metric for finding the nearest neighbors in the first place? If not, what are my options? I would suggest soft subspace clustering, a pretty common approach nowadays, where feature weights are calculated to find the most relevant dimensions. You can use these weights when using euclidean distance, for example. See curse of dimensionality for common problems and also this article can enlighten you somehow: A k-means type clustering algorithm for subspace clustering of mixed numeric and
categorical datasetsThe classifiers in machine learning packages like liblinear and nltk offer a method show_most_informative_features(), which is really helpful for debugging features: My question is if something similar is implemented for the classifiers in scikit-learn. I searched the documentation, but couldn't find anything the like. If there is no such function yet, does somebody know a workaround how to get to those values? The classifiers themselves do not record feature names, they just see numeric arrays. However, if you extracted your features using a Vectorizer/CountVectorizer/TfidfVectorizer/DictVectorizer, and you are using a linear model (e.g. LinearSVC or Naive Bayes) then you can apply the same trick that the document classification example uses. Example (untested, may contain a bug or two): This is for multiclass classification; for the binary case, I think you should use clf.coef_[0] only. You may have to sort the class_labels. With the help of larsmans code I came up with this code for the binary case: To add an update, RandomForestClassifier now supports the .feature_importances_ attribute. This attribute tells you how much of the observed variance is explained by that feature. Obviously, the sum of all these values must be <= 1.  I find this attribute very useful when performing feature engineering.  Thanks to the scikit-learn team and contributors for implementing this! edit: This works for both RandomForest and GradientBoosting. So RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier and GradientBoostingRegressor all support this.  We've recently released a library (https://github.com/TeamHG-Memex/eli5) which allows to do that: it handles variuos classifiers from scikit-learn, binary / multiclass cases, allows to highlight text according to feature values, integrates with IPython, etc. I actually had to find out Feature Importance on my NaiveBayes classifier and although I used the above functions, I was not able to get feature importance based on classes. I went through the scikit-learn's documentation and tweaked the above functions a bit to find it working for my problem. Hope it helps you too! Note that your classifier(in my case it's NaiveBayes) must have attribute feature_count_ for this to work. You can also do something like this to create a graph of importance features by order: RandomForestClassifier does not yet have a coef_ attrubute, but it will in the 0.17 release, I think. However, see the RandomForestClassifierWithCoef class in Recursive feature elimination on Random Forest using scikit-learn. This may give you some ideas to work around the limitation above. Not exactly what you are looking for, but a quick way to get the largest magnitude coefficients (assuming a pandas dataframe columns are your feature names):  You trained the model like:  Get the 10 largest negative coefficient values (or change to reverse=True for largest positive) like:  First make a list, I give this list the name label.  After that extracting all features name and column name I add in label list. Here I use naive bayes model. In naive bayes model, feature_log_prob_ give probability of features.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Why do we have to normalize the input for a neural network? I understand that sometimes, when for example the input values are non-numerical a certain transformation must be performed, but when we have a numerical input? Why the numbers must be in a certain interval? What will happen if the data is not normalized? It's explained well here. If the input variables are combined linearly, as in an MLP [multilayer perceptron], then it is
  rarely strictly necessary to standardize the inputs, at least in theory. The
  reason is that any rescaling of an input vector can be effectively undone by
  changing the corresponding weights and biases, leaving you with the exact
  same outputs as you had before. However, there are a variety of practical
  reasons why standardizing the inputs can make training faster and reduce the
  chances of getting stuck in local optima. Also, weight decay and Bayesian
  estimation can be done more conveniently with standardized inputs.  In neural networks, it is good idea not just to normalize data but also to scale them. This is intended for faster approaching to global minima at error surface. See the following pictures:
  Pictures are taken from the coursera course about neural networks. Author of the course is Geoffrey Hinton.  Some inputs to NN might not have a 'naturally defined' range of values. For example, the average value might be slowly, but continuously increasing over time (for example a number of records in the database).  In such case feeding this raw value into your network will not work very well. You will teach your network on values from lower part of range, while the actual inputs will be from the higher part of this range (and quite possibly above range, that the network has learned to work with).  You should normalize this value. You could for example tell the network by how much the value has changed since the previous input. This increment usually can be defined with high probability in a specific range, which makes it a good input for network. There are 2 Reasons why we have to Normalize Input Features before Feeding them to Neural Network: Reason 1: If a Feature in the Dataset is big in scale compared to others then this big scaled feature becomes dominating and as a result of that, Predictions of the Neural Network will not be Accurate. Example: In case of Employee Data, if we consider Age and Salary, Age will be a Two Digit Number while Salary can be 7 or 8 Digit (1 Million, etc..). In that Case, Salary will Dominate the Prediction of the Neural Network. But if we Normalize those Features, Values of both the Features will lie in the Range from (0 to 1). Reason 2: Front Propagation of Neural Networks involves the Dot Product of Weights with Input Features. So, if the Values are very high (for Image and Non-Image Data), Calculation of Output takes a lot of Computation Time as well as Memory. Same is the case during Back Propagation. Consequently, Model Converges slowly, if the Inputs are not Normalized. Example: If we perform Image Classification, Size of Image will be very huge, as the Value of each Pixel ranges from 0 to 255. Normalization in this case is very important. Mentioned below are the instances where Normalization is very important: When you use unnormalized input features, the loss function is likely to have very elongated valleys. When optimizing with gradient descent, this becomes an issue because the gradient will be steep with respect some of the parameters. That leads to large oscillations in the search space, as you are bouncing between steep slopes. To compensate, you have to stabilize optimization with small learning rates. Consider features x1 and x2, where range from 0 to 1 and 0 to 1 million, respectively. It turns out the ratios for the corresponding parameters (say, w1 and w2) will also be large. Normalizing tends to make the loss function more symmetrical/spherical. These are easier to optimize because the gradients tend to point towards the global minimum and you can take larger steps. Looking at the neural network from the outside, it is just a function that takes some  arguments and produces a result. As with all functions, it has a domain (i.e. a set of legal arguments). You have to normalize the values that you want to pass to the neural net in order to make sure it is in the domain. As with all functions, if the arguments are not in the domain, the result is not guaranteed to be appropriate.  The exact behavior of the neural net on arguments outside of the domain depends on the implementation of the neural net. But overall, the result is useless if the arguments are not within the domain. I believe the answer is dependent on the scenario. Consider NN (neural network) as an operator F, so that F(input) = output. In the case where this relation is linear so that F(A * input) = A * output, then you might choose to either leave the input/output unnormalised in their raw forms, or normalise both to eliminate A. Obviously this linearity assumption is violated in classification tasks, or nearly any task that outputs a probability, where F(A * input) = 1 * output In practice, normalisation allows non-fittable networks to be fittable, which is crucial to experimenters/programmers. Nevertheless, the precise impact of normalisation will depend not only on the network architecture/algorithm, but also on the statistical prior for the input and output.  What's more, NN is often implemented to solve very difficult problems in a black-box fashion, which means the underlying problem may have a very poor statistical formulation, making it hard to evaluate the impact of normalisation, causing the technical advantage (becoming fittable) to dominate over its impact on the statistics. In statistical sense, normalisation removes variation that is believed to be non-causal in predicting the output, so as to prevent NN from learning this variation as a predictor (NN does not see this variation, hence cannot use it).  The reason normalization is needed is because if you look at how an adaptive step proceeds in one place in the domain of the function, and you just simply transport the problem to the equivalent of the same step translated by some large value in some direction in the domain, then you get different results. It boils down to the question of adapting a linear piece to a data point.  How much should the piece move without turning and how much should it turn in response to that one training point?  It makes no sense to have a changed adaptation procedure in different parts of the domain! So normalization is required to reduce the difference in the training result.  I haven't got this written up, but you can just look at the math for a simple linear function and how it is trained by one training point in two different places.  This problem may have been corrected in some places, but I am not familiar with them.  In ALNs, the problem has been corrected and I can send you a paper if you write to wwarmstrong AT shaw.ca On a high level, if you observe as to where normalization/standardization is mostly used, you will notice that, anytime there is a use of magnitude difference in model building process, it becomes necessary to standardize the inputs so as to ensure that important inputs with small magnitude don't loose their significance midway the model building process. √(3-1)^2+(1000-900)^2 ≈ √(1000-900)^2 

Here, (3-1) contributes hardly a thing to the result and hence the input corresponding to these values is considered futile by the model. Consider the following: Both distance measure(Clustering) and cost function(NNs) use magnitude difference in some way and hence standardization ensures that magnitude difference doesn't command over important input parameters and the algorithm works as expected. Hidden layers are used in accordance with the complexity of our data. If we have input data which is linearly separable then we need not to use hidden layer e.g. OR gate but if we have a non linearly seperable data then we need to use hidden layer for example ExOR logical gate.
Number of nodes taken at any layer depends upon the degree of cross validation of our output.I've noticed that a frequent occurrence during training is NANs being introduced. Often times it seems to be introduced by weights in inner-product/fully-connected or convolution layers blowing up. Is this occurring because the gradient computation is blowing up? Or is it because of weight initialization (if so, why does weight initialization have this effect)? Or is it likely caused by the nature of the input data? The overarching question here is simply: What is the most common reason for NANs to occurring during training? And secondly, what are some methods for combatting this (and why do they work)? I came across this phenomenon several times. Here are my observations: Reason: large gradients throw the learning process off-track. What you should expect: Looking at the runtime log, you should look at the loss values per-iteration. You'll notice that the loss starts to grow significantly from iteration to iteration, eventually the loss will be too large to be represented by a floating point variable and it will become nan. What can you do: Decrease the base_lr (in the solver.prototxt) by an order of magnitude (at least). If you have several loss layers, you should inspect the log to see which layer is responsible for the gradient blow up and decrease the loss_weight (in train_val.prototxt) for that specific layer, instead of the general base_lr. Reason: caffe fails to compute a valid learning rate and gets 'inf' or 'nan' instead, this invalid rate multiplies all updates and thus invalidating all parameters. What you should expect: Looking at the runtime log, you should see that the learning rate itself becomes 'nan', for example: What can you do: fix all parameters affecting the learning rate in your 'solver.prototxt' file.
For instance, if you use lr_policy: "poly" and you forget to define max_iter parameter, you'll end up with  lr = nan...
For more information about learning rate in caffe, see this thread. Reason: Sometimes the computations of the loss in the loss layers causes nans to appear. For example, Feeding InfogainLoss layer with non-normalized values, using custom loss layer with bugs, etc. What you should expect: Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a nan appears. What can you do: See if you can reproduce the error, add printout to the loss layer and debug the error. For example: Once I used a loss that normalized the penalty by the frequency of label occurrence in a batch. It just so happened that if one of the training labels did not appear in the batch at all - the loss computed produced nans. In that case, working with large enough batches (with respect to the number of labels in the set) was enough to avoid this error. Reason: you have an input with nan in it! What you should expect: once the learning process "hits" this faulty input - output becomes nan. Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a nan appears. What can you do: re-build your input datasets (lmdb/leveldn/hdf5...) make sure you do not have bad image files in your training/validation set. For debug you can build a simple net that read the input layer, has a dummy loss on top of it and runs through all the inputs: if one of them is faulty, this dummy net should also produce nan. For some reason, choosing stride > kernel_size for pooling may results with nans. For example: results with nans in y. It was reported that under some settings "BatchNorm" layer may output nans due to numerical instabilities.
This issue was raised in bvlc/caffe and PR #5136 is attempting to fix it. Recently, I became aware of debug_info flag: setting debug_info: true in 'solver.prototxt' will make caffe print to log more debug information (including gradient magnitudes and activation values) during training: This information can help in spotting gradient blowups and other problems in the training process. In my case, not setting the bias in the convolution/deconvolution layers was the cause. Solution: add the following to the convolution layer parameters. This answer is not about a cause for nans, but rather proposes a way to help debug it. 
You can have this python layer: Adding this layer into your train_val.prototxt at certain points you suspect may cause trouble: learning_rate is high and should be decreased
The accuracy in the RNN code was nan, with select the low value for learning rate it fixes I was trying to build a sparse autoencoder and had several layers in it to induce sparsity. While running my net, I encountered the NaN's. On removing some of the layers (in my case, I actually had to remove 1), I found that the NaN's disappeared. So, I guess too much sparsity may lead to NaN's as well (some 0/0 computations may have been invoked!?) One more solution for anyone stuck like I just was- I was receiving nan or inf losses on a network I setup with float16 dtype across the layers and input data.  After all else failed, it occurred to me to switch back to float32, and the nan losses were solved! So bottom line, if you switched dtype to float16, change it back to float32.I have a set of dataframes where one of the columns contains a categorical variable. I'd like to convert it to several dummy variables, in which case I'd normally use get_dummies. What happens is that get_dummies looks at the data available in each dataframe to find out how many categories there are, and thus create the appropriate number of dummy variables. However, in the problem I'm working right now, I actually know in advance what the possible categories are. But when looking at each dataframe individually, not all categories necessarily appear. My question is: is there a way to pass to get_dummies (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s? Something that would make this: Become this: TL;DR: is there a way to pass to get_dummies (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s? Yes, there is! Pandas has a special type of Series just for categorical data. One of the attributes of this series is the possible categories, which get_dummies takes into account. Here's an example: Then, get_dummies will do exactly what you want! There are a bunch of other ways to create a categorical Series or DataFrame, this is just the one I find most convenient. You can read about all of them in the pandas documentation. EDIT: I haven't followed the exact versioning, but there was a bug in how pandas treats sparse matrices, at least until version 0.17.0. It was corrected by version 0.18.1 (released May 2016). For version 0.17.0, if you try to do this with the sparse=True option with a DataFrame, the column of zeros for the missing dummy variable will be a column of NaN, and it will be converted to dense. It looks like pandas 0.21.0 added a CategoricalDType, and creating categoricals which explicitly include the categories as in the original answer was deprecated, I'm not quite sure when. Using transpose and reindex Try this: I did ask this on the pandas github. Turns out it is really easy to get around it when you define the column as a Categorical where you define all the possible categories. get_dummies() will do the rest then as expected. I don't think get_dummies provides this out of the box, it only allows for creating an extra column that highlights NaN values.  To add the missing columns yourself, you could use pd.concat along axis=0 to vertically 'stack' the DataFrames (the dummy columns plus a DataFrame id)  and automatically create any missing columns, use fillna(0) to replace missing values, and then use .groupby('id') to separate the various DataFrame again. Adding the missing category in the test set: Notice that this code also remove column resulting from category in the test dataset but not present in the training dataset As suggested by others - Converting your Categorical features to 'category' data type should resolve the unseen label issue using 'get_dummies'. The shorter the better: Result: Notes: Here's a shorter-shorter version (changed the Index values): Result: Bonus track!  I imagine you have the categories because you did a previous dummy/one hot using training data. You can save the original encoding (.columns), and then apply during production time: Result: If you know your categories you can first apply pd.get_dummies() as you suggested and add the missing category columns afterwards. This will create your example with the missing cat_c: Now simply add the missing category columns with a union operation (as suggested here). I was recently looking to solve this same issue, but working with a multi-column dataframe and with two datasets (a train set and test set for a machine learning task). The test dataframe had the same categorical columns as the train dataframe, but some of these columns had missing categories that were present in the train dataframe. I did not want to manually define all the possible categories for every column. Instead, I combined the train and test dataframes into one, called get_dummies, and then split that back into two.I am working on classifying simple data using KNN with Euclidean distance. I have seen an example on what I would like to do that is done with the MATLAB knnsearch function as shown below: The above code takes a new point i.e. [5 1.45] and finds the 10 closest values to the new point. Can anyone please show me a MATLAB algorithm with a detailed explanation of what the knnsearch function does? Is there any other way to do this? The basis of the K-Nearest Neighbour (KNN) algorithm is that you have a data matrix that consists of N rows and M columns where N is the number of data points that we have, while M is the dimensionality of each data point.   For example, if we placed Cartesian co-ordinates inside a data matrix, this is usually a N x 2 or a N x 3 matrix.  With this data matrix, you provide a query point and you search for the closest k points within this data matrix that are the closest to this query point. We usually use the Euclidean distance between the query and the rest of your points in your data matrix to calculate our distances.  However, other distances like the L1 or the City-Block / Manhattan distance are also used.  After this operation, you will have N Euclidean or Manhattan distances which symbolize the distances between the query with each corresponding point in the data set.  Once you find these, you simply search for the k nearest points to the query by sorting the distances in ascending order and retrieving those k points that have the smallest distance between your data set and the query. Supposing your data matrix was stored in x, and newpoint is a sample point where it has M columns (i.e. 1 x M), this is the general procedure you would follow in point form: Let's do each step slowly. One way that someone may do this is perhaps in a for loop like so: If you wanted to implement the Manhattan distance, this would simply be: dists would be a N element vector that contains the distances between each data point in x and newpoint.  We do an element-by-element subtraction between newpoint and a data point in x, square the differences, then sum them all together.  This sum is then square rooted, which completes the Euclidean distance.  For the Manhattan distance, you would perform an element by element subtraction, take the absolute values, then sum all of the components together.  This is probably the most simplest of the implementations to understand, but it could possibly be the most inefficient... especially for larger sized data sets and larger dimensionality of your data. Another possible solution would be to replicate newpoint and make this matrix the same size as x, then doing an element-by-element subtraction of this matrix, then summing over all of the columns for each row and doing the square root.  Therefore, we can do something like this: For the Manhattan distance, you would do: repmat takes a matrix or vector and repeats them a certain amount of times in a given direction.  In our case, we want to take our newpoint vector, and stack this N times on top of each other to create a N x M matrix, where each row is M elements long.  We subtract these two matrices together, then square each component.  Once we do this, we sum over all of the columns for each row and finally take the square root of all result.  For the Manhattan distance, we do the subtraction, take the absolute value and then sum. However, the most efficient way to do this in my opinion would be to use bsxfun. This essentially does the replication that we talked about under the hood with a single function call.  Therefore, the code would simply be this: To me this looks much cleaner and to the point.  For the Manhattan distance, you would do: Now that we have our distances, we simply sort them.  We can use sort to sort our distances: d would contain the distances sorted in ascending order, while ind tells you for each value in the unsorted array where it appears in the sorted result.  We need to use ind, extract the first k elements of this vector, then use ind to index into our x data matrix to return those points that were the closest to newpoint. The final step is to now return those k data points that are closest to newpoint.  We can do this very simply by: ind_closest should contain the indices in the original data matrix x that are the closest to newpoint.  Specifically, ind_closest contains which rows you need to sample from in x to obtain the closest points to newpoint.  x_closest will contain those actual data points. For your copying and pasting pleasure, this is what the code looks like: Running through your example, let's see our code in action: By inspecting ind_closest and x_closest, this is what we get: If you ran knnsearch, you will see that your variable n matches up with ind_closest.  However, the  variable d returns the distances from newpoint to each point x, not the actual data points themselves.  If you want the actual distances, simply do the following after the code I wrote: Note that the above answer uses only one query point in a batch of N examples.  Very frequently KNN is used on multiple examples simultaneously.  Supposing that we have Q query points that we want to test in the KNN.  This would result in a k x M x Q matrix where for each example or each slice, we return the k closest points with a dimensionality of M.  Alternatively, we can return the IDs of the k closest points thus resulting in a Q x k matrix.  Let's compute both. A naive way to do this would be to apply the above code in a loop and loop over every example. Something like this would work where we allocate a Q x k matrix and apply the bsxfun based approach to set each row of the output matrix to the k closest points in the dataset, where we will use the Fisher Iris dataset just like what we had before.  We'll also keep the same dimensionality as we did in the previous example and I'll use four examples, so Q = 4 and M = 2: Though this is very nice, we can do even better.  There is a way to efficiently compute the squared Euclidean distance between two sets of vectors.  I'll leave it as an exercise if you want to do this with the Manhattan.  Consulting this blog, given that A is a Q1 x M matrix where each row is a point of dimensionality M with Q1 points and B is a Q2 x M matrix where each row is also a point of dimensionality M with Q2 points, we can efficiently compute a distance matrix D(i, j) where the element at row i and column j denotes the distance between row i of A and row j of B using the following matrix formulation: Therefore, if we let A be a matrix of query points and B be the dataset consisting of your original data, we can determine the k closest points by sorting each row individually and determining the k locations of each row that were the smallest.  We can also additionally use this to retrieve the actual points themselves. Therefore: We see that we used the logic for computing the distance matrix is the same but some variables have changed to suit the example.  We also sort each row independently using the two input version of sort and so ind will contain the IDs per row and d will contain the corresponding distances.  We then figure out which indices are the closest to each query point by simply truncating this matrix to k columns.  We then use permute and reshape to determine what the associated closest points are.  We first use all of the closest indices and create a point matrix that stacks all of the IDs on top of each other so we get a Q * k x M matrix.  Using reshape and permute allows us to create our 3D matrix so that it becomes a k x M x Q matrix like we have specified.  If you wanted to get the actual distances themselves, we can index into d and grab what we need.  To do this, you will need to use sub2ind to obtain the linear indices so we can index into d in one shot.  The values of ind_closest already give us which columns we need to access.  The rows we need to access are simply 1, k times, 2, k times, etc. up to Q.  k is for the number of points we wanted to return: When we run the above code for the above query points, these are the indices, points and distances we get: To compare this with knnsearch, you would instead specify a matrix of points for the second parameter where each row is a query point and you will see that the indices and sorted distances match between this implementation and knnsearch. Hope this helps you.  Good luck!I'd like to choose the best algorithm for future. I found some solutions, but I didn't understand which R-Squared value is correct. For this, I divided my data into two as test and training, and I printed two different R squared values ​​below. First R-Squared result is -4.28.
Second R-Squared result is 0.84 But I didn't understand which value is correct. Arguably, the real challenge in such cases is to be sure that you compare apples to apples. And in your case, it seems that you don't. Our best friend is always the relevant documentation, combined with simple experiments. So... Although scikit-learn's LinearRegression() (i.e. your 1st R-squared) is fitted by default  with fit_intercept=True (docs), this is not the case with statsmodels' OLS (your 2nd R-squared); quoting from the docs: An intercept is not included by default and should be added by the user. See statsmodels.tools.add_constant. Keeping this important detail in mind, let's run some simple experiments with dummy data: For all practical purposes, these two values of R-squared produced by scikit-learn and statsmodels are identical. Let's go a step further, and try a scikit-learn model without intercept, but where we use the artificially "intercepted" data X_ we have already built for use with statsmodels: Again, the R-squared is identical with the previous values. So, what happens when we "accidentally" forget to account for the fact that statsmodels OLS is fitted without an intercept? Let's see: Well, an R-squared of 0.80 is indeed very far from the one of 0.16 returned by a model with an intercept, and arguably this is exactly what has happened in your case. So far so good, and I could easily finish the answer here; but there is indeed a point where this harmonious world breaks down: let's see what happens when we fit both models without intercept and with the initial data X where we have not artificially added any interception. We have already fitted the OLS model above, and got an R-squared of 0.80; what about a similar model from scikit-learn? Ooops...! What the heck?? It seems that scikit-earn, when computes the r2_score, always assumes an intercept, either explicitly in the model (fit_intercept=True) or implicitly in the data (the way we have produced X_ from X above, using statsmodels' add_constant); digging a little online reveals a Github thread (closed without a remedy) where it is confirmed that the situation is indeed like that. [UPDATE Dec 2021: for a more detailed & in-depth investigation and explanation of why the two scores are different in this particular case (i.e. both models fitted without an intercept), see this great answer by Flavia] Let me clarify that the discrepancy I have described above has nothing to do with your issue: in your case, the real issue is that you are actually comparing apples (a model with intercept) with oranges (a model without intercept). So, why scikit-learn not only fails in such an (admittedly edge) case, but even when the fact emerges in a Github issue it is actually treated with indifference? (Notice also that the scikit-learn core developer who replies in the above thread casually admits that "I'm not super familiar with stats"...). The answer goes a little beyond coding issues, such as the ones SO is mainly about, but it may be worth elaborating a little here. Arguably, the reason is that the whole R-squared concept comes in fact directly from the world of statistics, where the emphasis is on interpretative models, and it has little use in machine learning contexts, where the emphasis is clearly on predictive models; at least AFAIK, and beyond some very introductory courses, I have never (I mean never...) seen a predictive modeling problem where the R-squared is used for any kind of performance assessment; neither it's an accident that popular machine learning introductions, such as Andrew Ng's Machine Learning at Coursera, do not even bother to mention it. And, as noted in the Github thread above (emphasis added): In particular when using a test set, it's a bit unclear to me what the R^2 means. with which I certainly concur. As for the edge case discussed above (to include or not an intercept term?), I suspect it would sound really irrelevant to modern deep learning practitioners, where the equivalent of an intercept (bias parameters) is always included by default in neural network models... See the accepted (and highly upvoted) answer in the Cross Validated question Difference between statsmodel OLS and scikit linear regression for a more detailed discussion along these last lines. The discussion (and links) in Is R-squared Useless?, triggered by some relevant (negative) remarks by the great statistician Cosma Shalizi, is also enlightening and highly recommended. You seem to be using sklearn.metrics_r2_score. The documentation states that Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse) The Wikipedia article which the documentation leads to points out that values of R2 outside the range 0 to 1 can occur when the model fits the data worse than a horizontal hyperplane. This would occur when the wrong model was chosen, or nonsensical constraints were applied by mistake. For this reason, the fact that you had such a negative r2_score is probably far more significant than that you had a relatively good (but not great) R^2 statistic computed in the other way. If the first score indicates that your model choice is poor then the second statistic is likely to be just an artifact of overfitting. As you note, and as the Wikipedia article notes, there are multiple definitions of "r squared" or "R squared." However, the common ones all have the property that they range from 0 to 1. They are usually positive, as is clear from the "squared" part of the name. (For exceptions to this general rule, see the Wikipedia article.) Your "First R-Squared result" is -4.28, which is not between 0 and 1 and is not even positive. Thus it is not really an "R squared" at all. So use the "Second R-Squared result" which is in the correct range.I am trying to build a model to predict house prices. I have some features X (no. of bathrooms , etc.) and target Y (ranging around $300,000 to $800,000) I have used sklearn's Standard Scaler to standardize Y before fitting it to the model. Here is my Keras model: I am having trouble trying to interpret the results -- what does a MSE of 0.617454319755 mean? Do I have to inverse transform this number, and square root the results, getting an error rate of 741.55 in dollars? I apologise for sounding silly as I am starting out! I apologise for sounding silly as I am starting out! Do not; this is a subtle issue of great importance, which is usually (and regrettably) omitted in tutorials and introductory expositions. Unfortunately, it is not as simple as taking the square root of the inverse-transformed MSE, but it is not that complicated either; essentially what you have to do is: in order to get a performance indicator of your model that will be meaningful in the business context of your problem (e.g. US dollars here). Let's see a quick example with toy data, omitting the model itself (which is irrelevant here, and in fact can be any regression model - not only a Keras one): Now, let's say that we fit our Keras model (not shown here) using the scaled sets X_train and Y_train, and get predictions on the training set: The MSE reported by Keras is actually the scaled MSE, i.e.: while the 3 steps I have described above are simply: So, in our case, if our initial Y were US dollars, the actual error in the same units (dollars) would be 0.32 (dollars). Notice how the naive approach of inverse-transforming the scaled MSE would give a very different (and incorrect) result: MSE is mean square error, here is the formula.
 Basically it is a mean of square of different of expected output and prediction. Making square root of this will not give you the difference between error and output. This is useful for training.  Currently  you have build a model. 
If you want to train the model use these function. If you want to do prediction of the output you should use following code. You can find more details here. https://keras.io/models/about-keras-models/ https://keras.io/models/sequential/I try to understand LSTMs and how to build them with Keras. I found out, that there are principally the 4 modes to run a RNN (the 4 right ones in the picture) 
Image source: Andrej Karpathy Now I wonder how a minimalistic code snippet for each of them would look like in Keras.
So something like for each of the 4 tasks, maybe with a little bit of explanation. So: One-to-one: you could use a Dense layer as you are not processing sequences: One-to-many: this option is not supported well as chaining models is not very easy in Keras, so the following version is the easiest one: Many-to-one: actually, your code snippet is (almost) an example of this approach: Many-to-many: This is the easiest snippet when the length of the input and output matches the number of recurrent steps: Many-to-many when number of steps differ from input/output length: this is freaky hard in Keras. There are no easy code snippets to code that. EDIT: Ad 5 In one of my recent applications, we implemented something which might be similar to many-to-many from the 4th image. In case you want to have a network with the following architecture (when an input is longer than the output): You could achieve this in the following manner: Where N is the number of last steps you want to cover (on image N = 3). From this point getting to: is as simple as artificial padding sequence of length N using e.g. with 0 vectors, in order to adjust it to an appropriate size. Great Answer by @Marcin Możejko I would add the following to NR.5 (many to many with different in/out length): A) as Vanilla LSTM B) as Encoder-Decoder LSTMPerhaps too general a question, but can anyone explain what would cause a Convolutional Neural Network to diverge? Specifics: I am using Tensorflow's iris_training model with some of my own data and keep getting ERROR:tensorflow:Model diverged with loss = NaN. Traceback... tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError: NaN loss during training. Traceback originated with line: I've tried adjusting the optimizer, using a zero for learning rate, and using no optimizer. Any insights into network layers, data size, etc is appreciated. There are lots of things I have seen make a model diverge. Too high of a learning rate.  You can often tell if this is the case if the loss begins to increase and then diverges to infinity. I am not to familiar with the DNNClassifier but I am guessing it uses the categorical cross entropy cost function.  This involves taking the log of the prediction which diverges as the prediction approaches zero.  That is why people usually add a small epsilon value to the prediction to prevent this divergence. I am guessing the DNNClassifier probably does this or uses the tensorflow opp for it.  Probably not the issue. Other numerical stability issues can exist such as division by zero where adding the epsilon can help.  Another less obvious one if the square root whose derivative can diverge if not properly simplified when dealing with finite precision numbers. Yet again I doubt this is the issue in the case of the DNNClassifier. You may have an issue with the input data.  Try calling assert not np.any(np.isnan(x)) on the input data to make sure you are not introducing the nan.  Also make sure all of the target values are valid.  Finally, make sure the data is properly normalized. You probably want to have the pixels in the range [-1, 1] and not [0, 255]. The labels must be in the domain of the loss function, so if using a logarithmic-based loss function all labels must be non-negative (as noted by evan pu and the comments below). If you're training for cross entropy, you want to add a small number like 1e-8 to your output probability. Because log(0) is negative infinity, when your model trained enough the output distribution will be very skewed, for instance say I'm doing a 4 class output, in the beginning my probability looks like but toward the end the probability will probably look like And you take a cross entropy of this distribution everything will explode. The fix is to artifitially add a small number to all the terms to prevent this. In my case I got NAN when setting distant integer LABELs. ie: So, not use a very distant Label. EDIT
You can see the effect in the following simple code: The result shows the NANs after adding the label 8000: If using integers as targets, makes sure they aren't symmetrical at 0.  I.e., don't use classes -1, 0, 1. Use instead 0, 1, 2. If you'd like to gather more information on the error and if the error occurs in the first few iterations, I suggest you run the experiment in CPU-only mode (no GPUs).  The error message will be much more specific.   Source: https://github.com/tensorflow/tensor2tensor/issues/574 Although most of the points are already discussed. But I would like to highlight again one more reason for NaN which is missing. By default activation function is "Relu". It could be possible that intermediate layer's generating a negative value and "Relu" convert it into the 0. Which gradually stops training. I observed the "LeakyRelu" able to solve such problems. Regularization can help. For a classifier, there is a good case for activity regularization, whether it is binary or a multi-class classifier. For a regressor, kernel regularization might be more appropriate.  The reason for nan, inf or -inf often comes from the fact that division by 0.0 in TensorFlow doesn't result in a division by zero exception. It could result in a nan, inf or -inf "value". In your training data you might have 0.0 and thus in your loss function it could happen that you perform a division by 0.0. Output is the following tensor: Adding a small eplison (e.g., 1e-5) often does the trick. Additionally, since TensorFlow 2 the opteration tf.math.division_no_nan is defined. I'd like to plug in some (shallow) reasons I have experienced as follows:      Hope that helps.   I found some interesting thing when battling whit this problem,in addition to the above answers when your data labels are arranged like below  applying shuffle to data may help: I had the same problem. My labels were enjoyment ratings [1, 3, 5]. I read all the answers and they didn't make much sense to the problem I was facing. I changed the labels to [0 1 2] and it worked. Don't know how this happened. TensorFlow uses the labels as positions in a tensor in some contexts so they have to be 0, 1, ..., L-1.  Negative numbers, non-integers, etc. can instead cause the loss to be NaN. The reason could also be using very small values (like 1e9).
Try replacing them with: Or (If you manually changed tf.keras.backend.floatx):Considering the example code. I would like to know How to apply gradient clipping on this network on the RNN where there is a possibility of exploding gradients. This is an example that could be used but where do I introduce this ?
In the def of RNN  But this doesn't make sense as the tensor _X is the input and not the grad what is to be clipped?  Do I have to define my own Optimizer for this or is there a simpler option? Gradient clipping needs to happen after computing the gradients, but before applying them to update the model's parameters. In your example, both of those things are handled by the AdamOptimizer.minimize() method. In order to clip your gradients you'll need to explicitly compute, clip, and apply them as described in this section in TensorFlow's API documentation. Specifically you'll need to substitute the call to the minimize() method with something like the following: Despite what seems to be popular, you probably want to clip the whole gradient by its global norm: Clipping each gradient matrix individually changes their relative scale but is also possible: In TensorFlow 2, a tape computes the gradients, the optimizers come from Keras, and we don't need to store the update op because it runs automatically without passing it to a session: It's easy for tf.keras! This optimizer will clip all gradients to values between [-1.0, 1.0]. See the docs. This is actually properly explained in the documentation.: Calling minimize() takes care of both computing the gradients and
  applying them to the variables. If you want to process the gradients
  before applying them you can instead use the optimizer in three steps: And in the example they provide they use these 3 steps: Here MyCapper is any function that caps your gradient. The list of useful functions (other than tf.clip_by_value()) is here. For those who would like to understand the idea of gradient clipping (by norm): Whenever the gradient norm is greater than a particular threshold, we clip the gradient norm so that it stays within the threshold. This threshold is sometimes set to 5. Let the gradient be g and the max_norm_threshold be j.  Now, if ||g|| > j , we do: g = (  j * g ) /  ||g|| This is the implementation done in tf.clip_by_norm IMO the best solution is wrapping your optimizer with TF's estimator decorator tf.contrib.estimator.clip_gradients_by_norm: This way you only have to define this once, and not run it after every gradients calculation. Documentation:
https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/clip_gradients_by_norm Gradient Clipping basically helps in case of exploding or vanishing gradients.Say your loss is too high which will result in exponential gradients to flow through the network which may result in Nan values . To overcome this we clip gradients within a specific range (-1 to 1 or any range as per condition) . clipped_value=tf.clip_by_value(grad, -range, +range), var) for grad, var in grads_and_vars
 where grads _and_vars are the pairs of gradients (which you calculate via tf.compute_gradients) and their variables they will be applied to. After clipping we simply apply its value using an optimizer.
optimizer.apply_gradients(clipped_value) if  you are training your model using your custom training loop then the one update step will look like Or you could also simply just replace the first line in above code as below second method will also work if you are using model.compile -> model.fit pipeline.How do I initialize weights and biases of a network (via e.g. He or Xavier initialization)? To initialize the weights of a single layer, use a function from torch.nn.init. For instance: Alternatively, you can modify the parameters by writing to conv1.weight.data (which is a torch.Tensor). Example: The same applies for biases: Pass an initialization function to torch.nn.Module.apply. It will initialize the weights in the entire nn.Module recursively. apply(fn): Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init). Example: If you follow the principle of Occam's razor, you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case. With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.  A uniform distribution has the equal probability of picking any number from a set of numbers.  Let's see how well the neural network trains using a uniform weight initialization, where low=0.0 and high=1.0. Below, we'll see another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, we can:  The general rule for setting the weights in a neural network is to set them to be close to zero without being too small.  Good practice is to start your weights in the range of [-y, y] where y=1/sqrt(n)
  (n is the number of inputs to a given neuron). below we compare performance of NN, weights initialized with uniform distribution [-0.5,0.5) versus the one whose weight is initialized using general rule  The normal distribution should have a mean of 0 and a standard deviation of y=1/sqrt(n), where n is the number of inputs to NN below we show the performance of two NN one initialized using uniform-distribution and the other using normal-distribution   To initialize layers, you typically don't need to do anything. PyTorch will do it for you. If you think about it, this makes a lot of sense. Why should we initialize layers, when PyTorch can do that following the latest trends? For instance, the Linear layer's __init__ method will do Kaiming He initialization: Similarly, this holds for other layers types. For e.g., Conv2d, check here. NOTE: The advantage of proper initialization is faster training speed. If your problem requires special initialization, you can still do it afterwards. If you want some extra flexibility, you can also set the weights manually.  Say you have input of all ones: And you want to make a dense layer with no bias (so we can visualize): Set all the weights to 0.5 (or anything else): The weights: All your weights are now 0.5. Pass the data through: Remember that each neuron receives 8 inputs, all of which have weight 0.5 and value of 1 (and no bias), so it sums up to 4 for each.  Sorry for being so late, I hope my answer will help. To initialise weights with a normal distribution use: Or to use a constant distribution write: Or to use an uniform distribution: You can check other methods to initialise tensors here If you cannot use apply for instance if the model does not implement Sequential directly: You can try with torch.nn.init.constant_(x, len(x.shape)) to check that they are appropriately initialized: Here is the better way, just pass your whole model Cuz I haven't had the enough reputation so far, I can't add a comment under  the answer posted by prosti in Jun 26 '19 at 13:16. But I wanna point out that actually we know some assumptions in the paper of Kaiming He, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, are not appropriate, though it looks like the deliberately designed initialization method makes a hit in practice. E.g., within the subsection of Backward Propagation Case, they assume that $w_l$ and $\delta y_l$ are independent of each other. But as we all known, take the score map $\delta y^L_i$ as an instance, it often is $y_i-softmax(y^L_i)=y_i-softmax(w^L_ix^L_i)$ if we use a typical cross entropy loss function objective. So I think the true underlying reason why He's Initialization works well remains to unravel. Cuz everyone has witnessed its power on boosting deep learning training. If you see a deprecation warning (@Fábio Perez)...We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 2 years ago. I noticed that LSH seems a good way to find similar items with high-dimension properties. After reading the paper http://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf, I'm still confused with those formulas. Does anyone know a blog or article that explains that the easy way? The best tutorial I have seen for LSH is in the book: Mining of Massive Datasets.
Check Chapter 3 - Finding Similar Items
http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf Also I recommend the below slide:
http://www.cs.jhu.edu/%7Evandurme/papers/VanDurmeLallACL10-slides.pdf .
The example in the slide helps me a lot in understanding the hashing for cosine similarity. I borrow two slides from Benjamin Van Durme & Ashwin Lall, ACL2010 and try to explain the intuitions of LSH Families for Cosine Distance a bit.
  I have some sample code (just 50 lines) in python here which is using cosine similarity. 
https://gist.github.com/94a3d425009be0f94751 Tweets in vector space can be a great example of high dimensional data. Check out my blog post on applying Locality Sensitive Hashing to tweets to find similar ones.  http://micvog.com/2013/09/08/storm-first-story-detection/ And because one picture is a thousand words check the picture below: 
http://micvog.files.wordpress.com/2013/08/lsh1.png Hope it helps.
@mvogiatzis Here's a presentation from Stanford that explains it. It made a big difference for me. Part two is more about LSH, but part one covers it as well. A picture of the overview (There are much more in the slides):  Near Neighbor Search in High Dimensional Data - Part1:
http://www.stanford.edu/class/cs345a/slides/04-highdim.pdf Near Neighbor Search in High Dimensional Data - Part2:
http://www.stanford.edu/class/cs345a/slides/05-LSH.pdf  It is important to underline that different similarity measures have different implementations of LSH.   In my blog, I tried to thoroughly explain LSH for the cases of minHashing( jaccard similarity measure) and simHashing (cosine distance measure). I hope you find it useful:
https://aerodatablog.wordpress.com/2017/11/29/locality-sensitive-hashing-lsh/ I am a visual person. Here is what works for me as an intuition. Say each of the things you want to search for approximately are physical objects such as an apple, a cube, a chair. My intuition for an LSH is that it is similar to take the shadows of these objects. Like if you take the shadow of a 3D cube you get a 2D square-like on a piece of paper, or a 3D sphere will get you a circle-like shadow on a piece of paper. Eventually, there are many more than three dimensions in a search problem (where each word in a text could be one dimension) but the shadow analogy is still very useful to me. Now we can efficiently compare strings of bits in software. A fixed length bit string is kinda, more or less, like a line in a single dimension.  So with an LSH, I project the shadows of objects eventually as points (0 or 1) on a single fixed length line/bit string. The whole trick is to take the shadows such that they still make sense in the lower dimension  e.g. they resemble the original object in a good enough way that can be recognized.  A 2D drawing of a cube in perspective tells me this is a cube. But I cannot distinguish easily a 2D square from a 3D cube shadow without perspective: they both looks like a square to me.  How I present my object to the light will determine if I get a good recognizable shadow or not. So I think of a "good" LSH as the one that will turn my objects in front of a light such that their shadow is best recognizable as representing my object. So to recap: I think of things to index with an LSH as physical objects like a cube, a table, or chair, and I project their shadows in 2D and eventually along a line (a bit string). And a "good" LSH "function" is how I present my objects in front of a light to get an approximately distinguishable shape in the 2D flatland and later my bit string. Finally when I want to search if an object I have is similar to some objects that I indexed, I take the shadows of this "query" object using the same way to present my object in front of the light (eventually ending up with a bit string too). And now I can compare how similar is that bit string with all my other indexed bit strings which is a proxy for searching for my whole objects if I found a good and recognizable way to present my objects to my light. As a very short, tldr answer:  An example of locality sensitive hashing could be to first set planes randomly (with a rotation and offset) in your space of inputs to hash, and then to drop your points to hash in the space, and for each plane you measure if the point is above or below it (e.g.: 0 or 1), and the answer is the hash. So points similar in space will have a similar hash if measured with the cosine distance before or after.  You could read this example using scikit-learn: https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-LayerMy theta from the above code is 100.2 100.2, but it should be 100.2 61.09 in matlab which is correct. I think your code is a bit too complicated and it needs more structure, because otherwise you'll be lost in all equations and operations. In the end this regression boils down to four operations: In your case, I guess you have confused m with n. Here m denotes the number of examples in your training set, not the number of features. Let's have a look at my variation of your code: At first I create a small random dataset which should look like this:  As you can see I also added the generated regression line and formula that was calculated by excel. You need to take care about the intuition of the regression using gradient descent. As you do a complete batch pass over your data X, you need to reduce the m-losses of every example to a single weight update. In this case, this is the average of the sum over the gradients, thus the division by m.  The next thing you need to take care about is to track the convergence and adjust the learning rate. For that matter you should always track your cost every iteration, maybe even plot it. If you run my example, the theta returned will look like this: Which is actually quite close to the equation that was calculated by excel (y = x + 30). Note that as we passed the bias into the first column, the first theta value denotes the bias weight. Below you can find my implementation of gradient descent for linear regression problem.  At first, you calculate gradient like X.T * (X * w - y) / N and update your current theta with this gradient simultaneously.  Here is the python code: 

 Most of these answers are missing out some explanation on linear regression, as well as having code that is a little convoluted IMO. The thing is, if you have a dataset of "m" samples, each sample called "x^i" (n-dimensional vector), and a vector of outcomes y (m-dimensional vector), you can construct the following matrices:  Now, the goal is to find "w" (n+1 dimensional vector), which describes the line for your linear regression, "w_0" is the constant term, "w_1" and so on are your coefficients of each dimension (feature) in an input sample. So in essence, you want to find "w" such that "X*w" is as close to "y" as possible, i.e. your line predictions will be as close to the original outcomes as possible. Note also that we added an extra component/dimension at the start of each "x^i", which is just "1", to account for the constant term. In addition, "X" is just the matrix you get by "stacking" each outcome as a row, so it's an (m by n+1) matrix. Once you construct that, the Python & Numpy code for gradient descent is actually very straight forward: And voila! That returns the vector "w", or description of your prediction line. But how does it work?
In the code above, I am finding the gradient vector of the cost function (squared differences, in this case), then we are going "against the flow", to find the minimum cost given by the best "w". The actual formula used is in the line For the full maths explanation, and code including the creation of the matrices, see this post on how to implement gradient descent in Python. Edit: For illustration, the above code estimates a line which you can use to make predictions. The image below shows an example of the "learned" gradient descent line (in red), and the original data samples (in blue scatter) from the "fish market" dataset from Kaggle.  I know this question already have been answer but I have made some update to the GD function : This function reduce the alpha over the iteration making the function too converge faster see Estimating linear regression with Gradient Descent (Steepest Descent) for an example in R. I apply the same logic but in Python. Following @thomas-jungblut implementation in python, i did the same for Octave. If you find something wrong please let me know and i will fix+update. Data comes from a txt file with the following rows: think about it as a very rough sample for features [number of bedrooms] [mts2] and last column [rent price] which is what we want to predict. Here is the Octave implementation:When creating a Sequential model in Keras, I understand you provide the input shape in the first layer. Does this input shape then make an implicit input layer? For example, the model below explicitly specifies 2 Dense layers, but is this actually a model with 3 layers consisting of one input layer implied by the input shape, one hidden dense layer with 32 neurons, and then one output layer with 10 possible outputs? Well, it actually is an implicit input layer indeed, i.e. your model is an example of a "good old" neural net with three layers - input, hidden, and output. This is more explicitly visible in the Keras Functional API (check the example in the docs), in which your model would be written as: Actually, this implicit input layer is the reason why you have to include an input_shape argument only in the first (explicit) layer of the model in the Sequential API - in subsequent layers, the input shape is inferred from the output of the previous ones (see the comments in the source code of core.py). You may also find the documentation on tf.contrib.keras.layers.Input enlightening. It depends on your perspective :-) Rewriting your code in line with more recent Keras tutorial examples, you would probably use: ...which makes it much more explicit that you only have 2 Keras layers. And this is exactly what you do have (in Keras, at least) because the "input layer" is not really a (Keras) layer at all: it's only a place to store a tensor, so it may as well be a tensor itself. Each Keras layer is a transformation that outputs a tensor, possibly of a different size/shape to the input. So while there are 3 identifiable tensors here (input, outputs of the two layers), there are only 2 transformations involved corresponding to the 2 Keras layers. On the other hand, graphically, you might represent this network with 3 (graphical) layers of nodes, and two sets of lines connecting the layers of nodes. Graphically, it's a 3-layer network. But "layers" in this graphical notation are bunches of circles that sit on a page doing nothing, whereas a layers in Keras transform tensors and do actual work for you. Personally, I would get used to the Keras perspective :-) Note finally that for fun and/or simplicity, I substituted input_dim=784 for input_shape=(784,) to avoid the syntax that Python uses to both confuse newcomers and create a 1-D tuple: (<value>,).If there's a binary classification problem, the labels are 0 and 1.
I know the prediction is a floating-point number because p is the probability of belonging to that class.  The following is the cross-entropy loss function.  However, p is not necessarily 0 or 1, so how does Keras calculate the accuracy? Will Keras automatically round our predictions to 0 or 1? For example, in the following code, the accuracy is 0.749, but the targets are 0 and 1 and the predictions are floating-point numbers that are not necessarily 0.0 or 1.0. You are a little confused here; you speak about accuracy, while showing the formula for the loss. The equation you show is indeed the cross-entropy loss formula for binary classification (or simply logistic loss). y[i] are the labels, which are indeed either 0 or 1. p[i] are the predictions, usually interpreted as probabilities, which are real numbers in [0,1] (without any rounding). Now for each i, only one term in the sum will survive - the first term vanishes when y[i] = 0, and similarly the second term vanishes when y[i] = 1. Let's see some examples: Suppose that y[0] = 1, while we have predicted p[0] = 0.99 (i.e. a rather good prediction). The second term of the sum vanishes (since 1 - y[0] = 0), while the first one becomes log(0.99) = -0.01; so, the contribution of this sample prediction (i=0) to the overall loss is 0.01 (due to the - sign in front of the sum). Suppose now that the true label of the next sample is again 1, i.e. y[1] = 1, but here we have made a rather poor prediction of p[1] = 0.1; again, the second term vanishes, and the contribution of this prediction to the overall loss is now -log(0.1) = 2.3, which is indeed greater than our first, good prediction, as we should expect intuitively. As a final example, let's suppose that y[2] = 0, and we have made a perfectly good prediction here of p[2] = 0; hence, the first term vanishes, and the second term becomes  i.e. we have no loss contributed, again as we intuitively expected, since we have made a perfectly good prediction here for i=2. The logistic loss formula simply computes all these errors of the individual predictions, sums them, and divides by their number n. Nevertheless, this is the loss (i.e. scores[0] in your snippet), and not the accuracy. Loss and accuracy are different things; roughly speaking, the accuracy is what we are actually interested in from a business perspective, while the loss is the objective function that the learning algorithms (optimizers) are trying to minimize from a mathematical perspective. Even more roughly speaking, you can think of the loss as the "translation" of the business objective (accuracy) to the mathematical domain, a translation which is necessary in classification problems (in regression ones, usually the loss and the business objective are the same, or at least can be the same in principle, e.g. the RMSE)... Will Keras automatically round our predictions to 0 or 1? Actually yes: to compute the accuracy, we implicitly set a threshold in the predicted probabilities (usually 0.5 in binary classification, but this may differ in the case of highly imbalanced data); so, in model.evaluate, Keras actually converts our predictions to 1 if p[i] > 0.5 and to 0 otherwise. Then, the accuracy is computed by simply counting the cases where y_true==y_pred (correct predictions) and dividing by the total number of samples, to give a number in [0,1]. So, to summarize:In the following TensorFlow function, we must feed the activation of artificial neurons in the final layer. That I understand. But I don't understand why it is called logits? Isn't that a mathematical function?  Logits is an overloaded term which can mean many different things: In Math, Logit is a function that maps probabilities ([0, 1]) to R ((-inf, inf))  Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to > 0.5. In ML, it can be  the vector of raw (non-normalized) predictions that a classification
  model generates, which is ordinarily then passed to a normalization
  function. If the model is solving a multi-class classification
  problem, logits typically become an input to the softmax function. The
  softmax function then generates a vector of (normalized) probabilities
  with one value for each possible class. Logits also sometimes refer to the element-wise inverse of the sigmoid function. Just adding this clarification so that anyone who scrolls down this much can at least gets it right, since there are so many wrong answers upvoted. Diansheng's answer and JakeJ's answer get it right.
A new answer posted by Shital Shah is an even better and more complete answer. Yes, logit  as a mathematical function in statistics, but the logit used in context of neural networks is different. Statistical logit doesn't even make any sense here. I couldn't find a formal definition anywhere, but logit basically means: The raw predictions which come out of the last layer of the neural network.
  1. This is the very tensor on which you apply the argmax function to get the predicted class.
  2. This is the very tensor which you feed into the softmax function to get the probabilities for the predicted classes. Also, from a tutorial on official tensorflow website: The final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0–9), with linear activation (the default): If you are still confused, the situation is like this: where, predicted_class_index_by_raw and predicted_class_index_by_prob will be equal. Another name for raw_predictions in the above code is logit. As for the why logit... I have no idea. Sorry.
[Edit: See this answer for  the historical motivations behind the term.] Although, if you want to, you can apply statistical logit to probabilities that come out of the softmax function.  If the probability of a certain class is p,
Then the log-odds of that class is L = logit(p). Also, the probability of that class can be recovered as p = sigmoid(L), using the sigmoid function. Not very useful to calculate log-odds though. Summary In context of deep learning the logits layer means the layer that feeds in to softmax (or other such normalization). The output of the softmax are the probabilities for the classification task and its input is logits layer. The logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1. Historical Context Where does this term comes from? In 1930s and 40s, several people were trying to adapt linear regression to the problem of predicting probabilities. However linear regression produces output from -infinity to +infinity while for probabilities our desired output is 0 to 1. One way to do this is by somehow mapping the probabilities 0 to 1 to -infinity to +infinity and then use linear regression as usual. One such mapping is cumulative normal distribution that was used by Chester Ittner Bliss in 1934 and he called this "probit" model, short for "probability unit". However this function is computationally expensive while lacking some of the desirable properties for multi-class classification. In 1944 Joseph Berkson used the function log(p/(1-p)) to do this mapping and called it logit, short for "logistic unit". The term logistic regression derived from this as well. The Confusion Unfortunately the term logits is abused in deep learning. From pure mathematical perspective logit is a function that performs above mapping. In deep learning people started calling the layer "logits layer" that feeds in to logit function. Then people started calling the output values of this layer "logit" creating the confusion with logit the function. TensorFlow Code Unfortunately TensorFlow code further adds in to confusion by names like tf.nn.softmax_cross_entropy_with_logits. What does logits mean here? It just means the input of the function is supposed to be the output of last neuron layer as described above. The _with_logits suffix is redundant, confusing and pointless. Functions should be named without regards to such very specific contexts because they are simply mathematical operations that can be performed on values derived from many other domains. In fact TensorFlow has another similar function sparse_softmax_cross_entropy where they fortunately forgot to add _with_logits suffix creating inconsistency and adding in to confusion. PyTorch on the other hand simply names its function without these kind of suffixes. Reference The Logit/Probit lecture slides is one of the best resource to understand logit. I have also updated Wikipedia article with some of above information. Logit is a function that maps probabilities [0, 1] to [-inf, +inf].  Softmax is a function that maps [-inf, +inf] to [0, 1] similar as Sigmoid. But Softmax also normalizes the sum of the values(output vector) to be 1. Tensorflow "with logit": It means that you are applying a softmax function to logit numbers to normalize it. The input_vector/logit is not normalized and can scale from [-inf, inf].  This normalization is used for multiclass classification problems. And for multilabel classification problems sigmoid normalization is used i.e. tf.nn.sigmoid_cross_entropy_with_logits Personal understanding, in TensorFlow domain, logits are the values to be used as input to softmax. I came to this understanding based on this tensorflow tutorial. https://www.tensorflow.org/tutorials/layers Although it is true that logit is a function in maths(especially in statistics), I don't think that's the same 'logit' you are looking at. In the book Deep Learning by Ian Goodfellow, he mentioned, The function σ−1(x) is called the logit in statistics, but this term
  is more rarely used in machine learning. σ−1(x) stands for the
  inverse function of logistic sigmoid function. In TensorFlow, it is frequently seen as the name of last layer. In Chapter 10 of the book Hands-on Machine Learning with Scikit-learn and TensorFLow by Aurélien Géron, I came across this paragraph, which stated logits layer clearly. note that logits is the output of the neural network before going
  through the softmax activation function: for optimization reasons, we
  will handle the softmax computation later. That is to say, although we use softmax as the activation function in the last layer in our design, for ease of computation, we take out logits separately. This is because it is more efficient to calculate softmax and cross-entropy loss together. Remember that cross-entropy is a cost function, not used in forward propagation.  (FOMOsapiens). If you check math Logit function, it converts real space from [0,1] interval to infinity [-inf, inf]. Sigmoid and softmax will do exactly the opposite thing. They will convert the [-inf, inf] real space to [0, 1] real space. This is why, in machine learning we may use logit before sigmoid and softmax function (since they match). And this is why "we may call" anything in machine learning that goes in front of sigmoid or softmax function the logit. Here is G. Hinton video using this term. Here is a concise answer for future readers. Tensorflow's logit is defined as the output of a neuron without applying activation function: x: input, w: weight, b: bias. That's it. The following is irrelevant to this question. For historical lectures, read other answers. Hats off to Tensorflow's "creatively" confusing naming convention. In PyTorch, there is only one CrossEntropyLoss and it accepts un-activated outputs. Convolutions, matrix multiplications and activations are same level operations. The design is much more modular and less confusing. This is one of the reasons why I switched from Tensorflow to PyTorch.  The vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, logits typically become an input to the softmax function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class. In addition, logits sometimes refer to the element-wise inverse of the sigmoid function. For more information, see tf.nn.sigmoid_cross_entropy_with_logits. official tensorflow documentation They are basically the fullest learned model you can get from the network, before it's been squashed down to apply to only the number of classes we are interested in.  Check out how some researchers use them to train a shallow neural net based on what a deep network has learned:  https://arxiv.org/pdf/1312.6184.pdf It's kind of like how when learning a subject in detail, you will learn a great many minor points, but then when teaching a student, you will try to compress it to the simplest case.  If the student now tried to teach, it'd be quite difficult, but would be able to describe it just well enough to use the language. The logit (/ˈloʊdʒɪt/ LOH-jit) function is the inverse of the sigmoidal "logistic" function or logistic transform used in mathematics, especially in statistics. When the function's variable represents a probability p, the logit function gives the log-odds, or the logarithm of the odds p/(1 − p). See here: https://en.wikipedia.org/wiki/LogitI have an example of a neural network with two layers. The first layer takes two arguments and has one output. The second should take one argument as result of the first layer and one additional argument. It should looks like this: So, I'd created a model with two layers and tried to merge them but it returns an error: The first layer in a Sequential model must get an "input_shape" or "batch_input_shape" argument. on the line result.add(merged). Model: You're getting the error because result defined as Sequential() is just a container for the model and you have not defined an input for it. Given what you're trying to build set result to take the third input x3. However, my preferred way of building a model that has this type of input structure would be to use the functional api. Here is an implementation of your requirements to get you started: To answer the question in the comments: Concatenation works like this: i.e rows are just joined. Adding to the above-accepted answer so that it helps those who are using tensorflow 2.0  Result: You can experiment with model.summary() (notice the concatenate_XX (Concatenate) layer size) You can view notebook here for detail:
https://nbviewer.jupyter.org/github/anhhh11/DeepLearning/blob/master/Concanate_two_layer_keras.ipynbI apply the
decision tree classifier and the random forest classifier to my data with the following code: Why the result are so much better for the random forest classifier (for 100 runs, with randomly sampling 2/3 of data for the training and 1/3 for the test)? The random forest estimators with one estimator isn't just a decision tree?
Have i done something wrong or misunderstood the concept? The random forest estimators with one estimator isn't just a decision tree? Well, this is a good question, and the answer turns out to be no; the Random Forest algorithm is more than a simple bag of individually-grown decision trees. Apart from the randomness induced from ensembling many trees, the Random Forest (RF) algorithm also incorporates randomness when building individual trees in two distinct ways, none of which is present in the simple Decision Tree (DT) algorithm. The first is the number of features to consider when looking for the best split at each tree node: while DT considers all the features, RF considers a random subset of them, of size equal to the parameter max_features (see the docs). The second is that, while DT considers the whole training set, a single RF tree considers only a bootstrapped sub-sample of it; from the docs again: The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default). The RF algorihm is essentially the combination of two independent ideas: bagging, and random selection of features (see the Wikipedia entry for a nice overview). Bagging is essentially my second point above, but applied to an ensemble; random selection of features is my first point above, and it seems that it had been independently proposed by Tin Kam Ho before Breiman's RF (again, see the Wikipedia entry). Ho had already suggested that random feature selection alone improves performance. This is not exactly what you have done here (you still use the bootstrap sampling idea from bagging, too), but you could easily replicate Ho's idea by setting bootstrap=False in your RandomForestClassifier() arguments. The fact is that, given this research, the difference in performance is not unexpected... To replicate exactly the behaviour of a single tree in RandomForestClassifier(), you should use both bootstrap=False and max_features=None arguments, i.e. in which case neither bootstrap sampling nor random feature selection will take place, and the performance should be roughly equal to that of a single decision tree.When facing difficulties during training (nans, loss does not converge, etc.) it is sometimes useful to look at more verbose training log by setting debug_info: true in the 'solver.prototxt' file. The training log then looks something like: What does it mean? At first glance you can see this log section divided into two: [Forward] and [Backward]. Recall that neural network training is done via forward-backward propagation:
A training example (batch) is fed to the net and a forward pass outputs the current prediction.
Based on this prediction a loss is computed.
The loss is then derived, and a gradient is estimated and propagated backward using the chain rule. Caffe Blob data structure
Just a quick re-cap. Caffe uses Blob data structure to store data/weights/parameters etc. For this discussion it is important to note that Blob has two "parts": data and diff. The values of the Blob are stored in the data part. The diff part is used to store element-wise gradients for the backpropagation step. Forward pass You will see all the layers from bottom to top listed in this part of the log. For each layer you'll see: Layer "conv1" is a convolution layer that has 2 param blobs: the filters and the bias. Consequently, the log has three lines. The filter blob (param blob 0) has data That is the current L2 norm of the convolution filter weights is 0.00899.
The current bias (param blob 1): meaning that currently the bias is set to 0. Last but not least, "conv1" layer has an output, "top" named "conv1" (how original...). The L2 norm of the output is Note that all L2 values for the [Forward] pass are reported on the data part of the  Blobs in question. Loss and gradient
At the end of the [Forward] pass comes the loss layer: In this example the batch loss is 2031.85, the gradient of the loss w.r.t. fc1 is computed and passed to diff part of fc1 Blob. The L2 magnitude of the gradient is 0.1245. Backward pass
All the rest of the layers are listed in this part top to bottom. You can see that the L2 magnitudes reported now are of the diff part of the Blobs (params and layers' inputs). Finally
The last log line of this iteration: reports the total L1 and L2 magnitudes of both data and gradients. What should I look for? If you have nans in your loss, see at what point your data or diff turns into nan: at which layer? at which iteration? Look at the gradient magnitude, they should be reasonable. IF you are starting to see values with e+8 your data/gradients are starting to blow up. Decrease your learning rate! See that the diffs are not zero. Zero diffs mean no gradients = no updates = no learning. If you started from random weights, consider generating random weights with higher variance. Look for activations (rather than gradients) going to zero. If you are using "ReLU" this means your inputs/weights lead you to regions where the ReLU gates are "not active" leading to "dead neurons". Consider normalizing your inputs to have zero mean, add "BatchNorm" layers, setting negative_slope in ReLU.I'm using Python and have some confusion matrixes. I'd like to calculate precisions and recalls and f-measure by confusion matrixes in multiclass classification. My result logs don't contain y_true and y_pred, just contain confusion matrix. Could you tell me how to get these scores from confusion matrix in multiclass classification? Let's consider the case of MNIST data classification (10 classes), where for a test set of 10,000 samples we get the following confusion matrix cm (Numpy array): In order to get the precision & recall (per class), we need to compute the TP, FP, and FN per class. We don't need TN, but we will compute it, too, as it will help us for our sanity check. The True Positives are simply the diagonal elements: The False Positives are the sum of the respective column, minus the diagonal element (i.e. the TP element): Similarly, the False Negatives are the sum of the respective row, minus the diagonal (i.e. TP) element: Now, the True Negatives are a little trickier; let's first think what exactly a True Negative means, with respect to, say class 0: it means all the samples that have been correctly identified as not being 0. So, essentially what we should do is remove the corresponding row & column from the confusion matrix, and then sum up all the remaining elements: Let's make a sanity check: for each class, the sum of TP, FP, FN, and TN must be equal to the size of our test set (here 10,000): let's confirm that this is indeed the case: The result is Having calculated these quantities, it is now straightforward to get the precision & recall per class: which for this example are Similarly we can compute related quantities, like specificity (recall that sensitivity is the same thing with recall): Results for our example: You should now be able to compute these quantities virtually for any size of your confusion matrix. If you have confusion matrix in the form of:  Following simple function can be made:  Testing: Output: Above function can also be extended to produce other scores, the formulae for which are mentioned on https://en.wikipedia.org/wiki/Confusion_matrix There is a package called  'disarray'. So, if I have four classes : I can use disarray to calculate 13 matrices : which gives :This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Is there a rule-of-thumb for how to best divide data into training and validation sets? Is an even 50/50 split advisable? Or are there clear advantages of having more training data relative to validation data (or vice versa)? Or is this choice pretty much application dependent? I have been mostly using an 80% / 20% of training and validation data, respectively, but I chose this division without any principled reason. Can someone who is more experienced in machine learning advise me? There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage. If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive). Assuming you have enough data to do proper held-out test data (rather than cross-validation), the following is an instructive way to get a handle on variances: You'd be surprised to find out that 80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle. It's usually a safe bet if you use that ratio. However, depending on the training/validation methodology you employ, the ratio may change. For example: if you use 10-fold cross validation, then you would end up with a validation set of 10% at each fold. There has been some research into what is the proper ratio between the training set and the validation set: The fraction of patterns reserved for the validation set should be
  inversely proportional to the square root of the number of free
  adjustable parameters. In their conclusion they specify a formula: Validation set (v) to training set (t) size ratio, v/t, scales like
  ln(N/h-max), where N is the number of families of recognizers and
  h-max is the largest complexity of those families. What they mean by complexity is:  Each family of recognizer is characterized by its complexity, which
  may or may not be related to the VC-dimension, the description
  length, the number of adjustable parameters, or other measures of
  complexity. Taking the first rule of thumb (i.e.validation set should be inversely proportional to the square root of the number of free adjustable parameters), you can conclude that if you have 32 adjustable parameters, the square root of 32 is ~5.65, the fraction should be 1/5.65 or 0.177 (v/t). Roughly 17.7% should be reserved for validation and 82.3% for training. Last year, I took Prof: Andrew Ng’s online machine learning course. His recommendation was: Training: 60% Cross-validation: 20% Testing: 20% Well, you should think about one more thing. If you have a really big dataset, like 1,000,000 examples, split 80/10/10 may be unnecessary, because 10% = 100,000 examples may be just too much for just saying that model works fine. Maybe 99/0.5/0.5 is enough because 5,000 examples can represent most of the variance in your data and you can easily tell that model works good based on these 5,000 examples in test and dev. Don't use 80/20 just because you've heard it's ok. Think about the purpose of the test set. Perhaps a 63.2% / 36.8% is a reasonable choice. The reason would be that if you had a total sample size n and wanted to randomly sample with replacement (a.k.a. re-sample, as in the statistical bootstrap) n cases out of the initial n, the probability of an individual case being selected in the re-sample would be approximately 0.632, provided that n is not too small, as explained here: https://stats.stackexchange.com/a/88993/16263 For a sample of n=250, the probability of an individual case being selected for a re-sample to 4 digits is 0.6329.
For a sample of n=20000, the probability is 0.6321. It all depends on the data at hand. If you have considerable amount of data then 80/20 is a good choice as mentioned above. But if you do not Cross-Validation with a 50/50 split might help you a lot more and prevent you from creating a model over-fitting your training data. Suppose you have less data, I suggest to try 70%, 80% and 90% and test which is giving better result. In case of 90% there are chances that for 10% test you get poor accuracy.I was following a tutorial which was available at Part 1 & Part 2. Unfortunately the author didn't have the time for the final section which involved using cosine similarity to actually find the distance between two documents. I followed the examples in the article with the help of the following link from stackoverflow, included is the code mentioned in the above link (just so as to make life easier) as a result of the above code I have the following matrix I am not sure how to use this output in order to calculate cosine similarity, I know how to implement cosine similarity with respect to two vectors of similar length but here I am not sure how to identify the two vectors. First off, if you want to extract count features and apply TF-IDF normalization and row-wise euclidean normalization you can do it in one operation with TfidfVectorizer: Now to find the cosine distances of one document (e.g. the first in the dataset) and all of the others you just need to compute the dot products of the first vector with all of the others as the tfidf vectors are already row-normalized.   As explained by Chris Clark in comments and here Cosine Similarity does not take into account the magnitude of the vectors. Row-normalised have a magnitude of 1 and so the Linear Kernel is sufficient to calculate the similarity values. The scipy sparse matrix API is a bit weird (not as flexible as dense N-dimensional numpy arrays). To get the first vector you need to slice the matrix row-wise to get a submatrix with a single row: scikit-learn already provides pairwise metrics (a.k.a. kernels in machine learning parlance) that work for both dense and sparse representations of vector collections. In this case we need a dot product that is also known as the linear kernel: Hence to find the top 5 related documents, we can use argsort and some negative array slicing (most related documents have highest cosine similarity values, hence at the end of the sorted indices array): The first result is a sanity check: we find the query document as the most similar document with a cosine similarity score of 1 which has the following text: The second most similar document is a reply that quotes the original message hence has many common words: WIth the Help of @excray's comment, I manage to figure it out the answer, What we need to do is actually write a simple for loop to iterate over the two arrays that represent the train data and test data.  First implement a simple lambda function to hold formula for the cosine calculation: And then just write a simple for loop to iterate over the to vector, logic is for every "For each vector in trainVectorizerArray, you have to find the cosine similarity with the vector in testVectorizerArray." Here is the output: I know its an old post. but I tried the http://scikit-learn.sourceforge.net/stable/ package. here is my code to find the cosine similarity. The question was how will you calculate the cosine similarity with this package and here is my code for that Here suppose the query is the first element of train_set and doc1,doc2 and doc3 are the documents which I want to rank with the help of cosine similarity. then I can use this code.  Also the tutorials provided in the question was very useful. Here are all the parts for it 
part-I,part-II,part-III the output will be as follows : here 1 represents that query is matched with itself and the other three are the scores for matching the query with the respective documents. Let me give you another tutorial written by me. It answers your question, but also makes an explanation why we are doing some of the things. I also tried to make it concise.  So you have a list_of_documents which is just an array of strings and another document which is just a string. You need to find such document from the list_of_documents that is the most similar to document. Let's combine them together: documents = list_of_documents + [document] Let's start with dependencies. It will become clear why we use each of them. One of the approaches that can be uses is a bag-of-words approach, where we treat each word in the document independent of others and just throw all of them together in the big bag. From one point of view, it looses a lot of information (like how the words are connected), but from another point of view it makes the model simple. In English and in any other human language there are a lot of "useless" words like 'a', 'the', 'in' which are so common that they do not possess a lot of meaning. They are called stop words and it is a good idea to remove them. Another thing that one can notice is that words like 'analyze', 'analyzer', 'analysis' are really similar. They have a common root and all can be converted to just one word. This process is called stemming and there exist different stemmers which differ in speed, aggressiveness and so on. So we transform each of the documents to list of stems of words without stop words. Also we discard all the punctuation. So how will this bag of words help us? Imagine we have 3 bags: [a, b, c], [a, c, a] and [b, c, d]. We can convert them to vectors in the basis [a, b, c, d]. So we end up with vectors: [1, 1, 1, 0], [2, 0, 1, 0] and [0, 1, 1, 1]. The similar thing is with our documents (only the vectors will be way to longer). Now we see that we removed a lot of words and stemmed other also to decrease the dimensions of the vectors. Here there is just interesting observation. Longer documents will have way more positive elements than shorter, that's why it is nice to normalize the vector. This is called term frequency TF, people also used additional information about how often the word is used in other documents - inverse document frequency IDF. Together we have a metric TF-IDF which have a couple of flavors. This can be achieved with one line in sklearn :-)   Actually vectorizer allows to do a lot of things like removing stop words and lowercasing. I have done them in a separate step only because sklearn does not have non-english stopwords, but nltk has. So we have all the vectors calculated. The last step is to find which one is the most similar to the last one. There are various ways to achieve that, one of them is Euclidean distance which is not so great for the reason discussed here. Another approach is cosine similarity. We iterate all the documents and calculating cosine similarity between the document and the last one: Now minimum will have information about the best document and its score. This should help you.   and output will be: Here is a function that compares your test data against the training data, with the Tf-Idf transformer fitted with the training data. Advantage is that you can quickly pivot or group by to find the n closest elements, and that the calculations are down matrix-wise.I am currently using H2O for a classification problem dataset. I am testing it out with H2ORandomForestEstimator in a python 3.6 environment. I noticed the results of the predict method was giving values between 0 to 1(I am assuming this is the probability).  In my data set, the target attribute is numeric i.e. True values are 1 and False values are 0. I made sure I converted the type to category for the target attribute, I was still getting the same result.  Then I modified to the code to convert the target column to factor using asfactor() method on the H2OFrame still, there wasn't any change on the result.  But when I changed the values in the target attribute to True and False for 1 and 0 respectively, I was getting the expected result(i.e) the output was the classification rather than the probability. In principle & in theory, hard & soft classification (i.e. returning classes & probabilities respectively) are different approaches, each one with its own merits & downsides. Consider for example the following, from the paper Hard or Soft Classification? Large-margin Unified Machines: Margin-based classifiers have been popular in both machine learning and statistics for classification problems. Among numerous classifiers, some are hard classifiers while some are soft ones. Soft classifiers explicitly estimate the class conditional probabilities and then perform classification based on estimated probabilities. In contrast, hard classifiers directly target on the classification decision boundary without producing the probability estimation. These two types of classifiers are based on different philosophies and each has its own merits. That said, in practice, most of the classifiers used today, including Random Forest (the only exception I can think of is the SVM family) are in fact soft classifiers: what they actually produce underneath is a probability-like measure, which subsequently, combined with an implicit threshold (usually 0.5 by default in the binary case), gives a hard class membership like 0/1 or True/False. What is the right way to get the classified prediction result? For starters, it is always possible to go from probabilities to hard classes, but the opposite is not true. Generally speaking, and given the fact that your classifier is in fact a soft one, getting just the end hard classifications (True/False) gives a "black box" flavor to the process, which in principle should be undesirable; handling directly the produced probabilities, and (important!) controlling explicitly the decision threshold should be the preferable way here. According to my experience, these are subtleties that are often lost to new practitioners; consider for example the following, from the Cross Validated thread Reduce Classification probability threshold: the statistical component of your exercise ends when you output a probability for each class of your new sample. Choosing a threshold beyond which you classify a new observation as 1 vs. 0 is not part of the statistics any more. It is part of the decision component. Apart from "soft" arguments (pun unintended) like the above, there are cases where you need to handle directly the underlying probabilities and thresholds, i.e. cases where the default threshold of 0.5 in binary classification will lead you astray, most notably when your classes are imbalanced; see my answer in High AUC but bad predictions with imbalanced data (and the links therein) for a concrete example of such a case. To be honest, I am rather surprised by the behavior of H2O you report (I haven't use it personally), i.e. that the kind of the output is affected by the representation of the input; this should not be the case, and if it is indeed, we may have an issue of bad design. Compare for example the Random Forest classifier in scikit-learn, which includes two different methods, predict and predict_proba, to get the hard classifications and the underlying probabilities respectively (and checking the docs, it is apparent that the output of predict is based on the probability estimates, which have been computed already before). If probabilities are the outcomes for numerical target values, then how do I handle it in case of a multiclass classification? There is nothing new here in principle, apart from the fact that a simple threshold is no longer meaningful; again, from the Random Forest predict docs in scikit-learn: the predicted class is the one with highest mean probability estimate That is, for 3 classes (0, 1, 2), you get an estimate of [p0, p1, p2] (with elements summing up to one, as per the rules of probability), and the predicted class is the one with the highest probability, e.g. class #1 for the case of [0.12, 0.60, 0.28]. Here is a reproducible example with the 3-class iris dataset (it's for the GBM algorithm and in R, but the rationale is the same). Adding to @desertnaut's answer, and since you tagged this question as Python, here is how you handle the last part of your question: If probabilities are the outcomes for numerical target values, then how do I handle it in case of a multiclass classification? This will convert a (num_examples, n_classes) array of probability values to a (num_examples, ) array of predicted classes.When should I use .eval()? I understand it is supposed to allow me to "evaluate my model". How do I turn it back off for training? Example training code using .eval(). model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation: BUT, don't forget to turn back to training mode after eval step: You can turn off evaluation mode by running model.train(). You should use it when running your model as an inference engine - i.e. when testing, validating, and predicting (though practically it will make no difference if your model does not include any of the differently behaving layers). model.eval is a method of torch.nn.Module: Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False). The opposite method is model.train explained nicely by Umang Gupta. An extra addition to the above answers: I recently started working with Pytorch-lightning, which wraps much of the boilerplate in the training-validation-testing pipelines. Among other things, it makes model.eval() and model.train() near redundant by allowing the train_step and validation_step callbacks which wrap the eval and train so you never forget to.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Can anyone please clearly explain the difference between 1D, 2D, and 3D convolutions in convolutional neural networks (in deep learning) with the use of examples? I want to explain with picture from C3D. In a nutshell, convolutional direction & output shape is important!  ↑↑↑↑↑ 1D Convolutions - Basic ↑↑↑↑↑  ↑↑↑↑↑ 2D Convolutions - Basic ↑↑↑↑↑  ↑↑↑↑↑ 3D Convolutions - Basic ↑↑↑↑↑  ↑↑↑↑↑ 2D Convolutions with 3D input  - LeNet, VGG, ..., ↑↑↑↑↑ 
↑↑↑↑↑ Bonus 1x1 conv in CNN  - GoogLeNet, ..., ↑↑↑↑↑  
↑↑↑↑↑ 1D Convolutions with 1D input   ↑↑↑↑↑ 
↑↑↑↑↑ 1D Convolutions with 2D input   ↑↑↑↑↑    Following the answer from @runhani I am adding a few more details to make the explanation a bit more clear and will try to explain this a bit more (and of course with exmaples from TF1 and TF2). One of the main additional bits I'm including are,  Here's how you might do 1D convolution using TF 1 and TF 2. And to be specific my data has following shapes, It's way less work with TF2 as TF2 does not need Session and variable_initializer for example. So let's understand what this is doing using a signal smoothing example. On the left you got the original and on the right you got output of a Convolution 1D which has 3 output channels.  Multiple channels are basically multiple feature representations of an input. In this example you have three representations obtained by three different filters. The first channel is the equally-weighted smoothing filter. The second is a filter that weights the middle of the filter more than the boundaries. The final filter does the opposite of the second. So you can see how these different filters bring about different effects. 1D convolution has been successful used for the sentence classification task.  Off to 2D convolution. If you are a deep learning person, chances that you haven't come across 2D convolution is … well about zero. It is used in CNNs for image classification, object detection, etc. as well as in NLP problems that involve images (e.g. image caption generation). Let's try an example, I got a convolution kernel with the following filters here, And to be specific my data has following shapes, Here you can see the output produced by above code. The first image is the original and going clock-wise you have outputs of the 1st filter, 2nd filter and 3 filter.
 In the context if 2D convolution, it is much easier to understand what these multiple channels mean. Say you are doing face recognition. You can think of (this is a very unrealistic simplification but gets the point across) each filter represents an eye, mouth, nose, etc. So that each feature map would be a binary representation of whether that feature is there in the image you provided. I don't think I need to stress that for a face recognition model those are very valuable features. More information in this article. This is an illustration of what I'm trying to articulate.  2D convolution is very prevalent in the realm of deep learning.  CNNs (Convolution Neural Networks) use 2D convolution operation for almost all computer vision tasks (e.g. Image classification, object detection, video classification).  Now it becomes increasingly difficult to illustrate what's going as the number of dimensions increase. But with good understanding of how 1D and 2D convolution works, it's very straight-forward to generalize that understanding to 3D convolution. So here goes. And to be specific my data has following shapes, 3D convolution has been used when developing machine learning applications involving LIDAR (Light Detection and Ranging) data which is 3 dimensional in nature. Alright you're nearly there. So hold on. Let's see what is stride and padding is. They are quite intuitive if you think about them. If you stride across a corridor, you get there faster in fewer steps. But it also means that you observed lesser surrounding than if you walked across the room. Let's now reinforce our understanding with a pretty picture too! Let's understand these via 2D convolution.  When you use tf.nn.conv2d for example, you need to set it as a vector of 4 elements. There's no reason to get intimidated by this. It just contain the strides in the following order. 2D Convolution - [batch stride, height stride, width stride, channel stride]. Here, batch stride and channel stride you just set to one (I've been implementing deep learning models for 5 years and never had to set them to anything except one). So that leaves you only with 2 strides to set. 3D Convolution - [batch stride, height stride, width stride, depth stride, channel stride]. Here you worry about height/width/depth strides only. Now, you notice that no matter how small your stride is (i.e. 1) there is an unavoidable dimension reduction happening during convolution (e.g. width is 3 after convolving a 4 unit wide image). This is undesirable especially when building deep convolution neural networks. This is where padding comes to the rescue. There are two most commonly used padding types.  Below you can see the difference.  Final word: If you are very curious, you might be wondering. We just dropped a bomb on whole automatic dimension reduction and now talking about having different strides. But the best thing about stride is that you control when where and how the dimensions get reduced. In summary, In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data. In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional. Mostly used on Image data. In 3D CNN, kernel moves in 3 directions. Input and output data of 3D CNN is 4 dimensional. Mostly used on 3D Image data (MRI, CT Scans). You can find more details here: https://medium.com/@xzz201920/conv1d-conv2d-and-conv3d-8a59182c4d6 CNN 1D,2D, or 3D refers to convolution direction, rather than input or filter dimension. For 1 channel input, CNN2D equals to CNN1D is kernel length = input length. (1 conv direction)This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I am trying to build a neural network from scratch.
Across all AI literature there is a consensus that weights should be initialized to random numbers in order for the network to converge faster. But why are neural networks initial weights initialized as random numbers?  I had read somewhere that this is done to "break the symmetry" and this makes the neural network learn faster. How does breaking the symmetry make it learn faster? Wouldn't initializing the weights to 0 be a better idea? That way the weights would be able to find their values (whether positive or negative) faster? Is there some other underlying philosophy behind randomizing the weights apart from hoping that they would be near their optimum values when initialized? Breaking symmetry is essential here, and not for the reason of performance. Imagine first 2 layers of multilayer perceptron (input and hidden layers):   During forward propagation each unit in hidden layer gets signal:  That is, each hidden unit gets sum of inputs multiplied by the corresponding weight.  Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, each hidden unit will get exactly the same signal. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs sigmoid(sum(inputs))). If all weights are zeros, which is even worse, every hidden unit will get zero signal. No matter what was the input - if all weights are the same, all units in hidden layer will be the same too.  This is the main issue with symmetry and reason why you should initialize weights randomly (or, at least, with different values). Note, that this issue affects all architectures that use each-to-each connections.  Analogy: Imagine that someone has dropped you from a helicopter to an unknown mountain top, and you're trapped there. Fog everywhere. You only know that you should get down to the sea level somehow. Which direction should you take to get down to the lowest possible point? If you couldn't reach sea level, the helicopter would take you again and drop you at the same mountain top. You would have to take the same directions again because you're "initializing" yourself to the same starting positions. However, each time the helicopter drops you somewhere randomly on the mountain, you would take different directions and steps. So, you would have a better chance of reaching the lowest possible point. That is what is meant by breaking the symmetry. The initialization is asymmetric (which is different), so you can find different solutions to the same problem. In this analogy, where you land is the weight. So, with different weights, there's a better chance of reaching the lowest (or lower) point. Also, it increases the entropy in the system so the system can create more information to help you find the lower points (local or global minimums).  The answer is pretty simple. The basic training algorithms are greedy in nature - they do not find the global optimum, but rather - "nearest" local solution. As the result, starting from any fixed initialization biases your solution towards some one particular set of weights. If you do it randomly (and possibly many times) then there is much less probable that you will get stuck in some weird part of the error surface. The same argument applies to other algorithms, which are not able to find a global optimum (k-means, EM, etc.) and does not apply to the global optimization techniques (like SMO algorithm for SVM). As you mentioned, the key point is breaking the symmetry. Because if you initialize all  weights to zero then all of the hidden neurons(units) in your neural network will be doing the exact same calculations. This is not something we desire because we want different hidden units to compute different functions. However, this is not possible if you initialize all to the same value. Wouldn't initializing the weights to 0 be a better idea? That way the weights would be able to find their values (whether positive or negative) faster? How does breaking the symmetry make it learn faster? If you initialize all the weights to be zero, then all the the neurons of all the layers performs the same calculation, giving the same output and there by making the whole deep net useless.  If the weights are zero, complexity of the whole deep net would be the same as that of a single neuron and the predictions would be nothing better than random. Nodes that are side-by-side in a hidden layer connected to the same inputs must have different weights for the learning algorithm to update the weights. By making weights as non zero ( but close to 0 like 0.1 etc), the algorithm will learn the weights in next iterations and won't be stuck. In this way, breaking the symmetry happens. Stochastic optimization algorithms such as stochastic gradient descent use randomness in selecting a starting point for the search and in the progression of the search. The progression of the search or learning of a neural network is known as convergence. Discovering a sub-optimal solution or local optima result into premature convergence. Instead of relying on one local optima, if you run your algorithm multiple times with different random weights, there is a best possibility of finding global optima without getting stuck at local optima. Post 2015, due to advancements in machine learning research, He-et-al Initialization is introduced to replace random initialization The weights are still random but differ in range depending on the size of the previous layer of neurons. In summary, non-zero random weights help us Let be more mathematical. In fact, the reason I answer is that I found this bit lacking in the other answers.
Assume you have 2 layers. If we look at the back-propagation algorithm, the computation of  dZ2 = A2 - Y dW2 = (1/m) * dZ2 * A2.T Let's ignore db2. (Sorry not sorry ;) ) dZ1 = W2.T * dZ2 .* g1'(Z1) ... The problem you see is in bold. Computing dZ1 (which is required to compute dW1) has W2 in it which is 0. We never got a chance to change the weights to anything beyond 0 and we never will. So essentially, the neural network does not learn anything. I think it is worse than logistic regression (single unit). In the case of logistic regression, you learn with more iterations since you get different input thanks to X. In this case, the other layers are always giving the same output so you don't learn at all. In addition to initialization with random values, initial weights should not start with large values. This is because we often use the tanh and sigmoid functions in hidden layers and output layers. If you look at the graphs of the two functions, after forward propagation at the first iteration results in higher values, and these values correspond to the places in the sigmoid and tanh functions that converge the derivative to zero. This leads to a cold start of the learning process and an increase in learning time. As a result, if you start weights at random, you can avoid these problems by multiplying these values by values such as "0.01" or "0.001". I learned one thing: if you initialize the weight to zeros, it's obvious that the activation units in the same layer will be the same, that means they'll have the same values. When you backbrop, you will find that all the rows of the gradient dW are the same also, hence all the rows of the weight matrix W are the same after gradient descent updates. In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with n[l]=1n[l]=1 for every layer, and the network is no more powerful than a linear classifier such as logistic regression. 
Andrew Ng course: First of all, some algorithms converge even with zero initial weightings. A simple example is a Linear Perceptron Network. Of course, many learning networks require random initial weighting (although this is not a guarantee of getting the fastest and best answer). Neural networks use Back-propagation to learn and to update weights, and the problem is that in this method, weights converge to the local optimal (local minimum cost/loss), not the global optimal. Random weighting helps the network to take chances for each direction in the available space and gradually improve them to arrive at a better answer and not be limited to one direction or answer. [The image below shows a one-dimensional example of how convergence. Given the initial location, local optimization is achieved but not a global optimization. At higher dimensions, random weighting can increase the chances of being in the right place or starting better, resulting in converging weights to better values.][1] [1]: https://i.stack.imgur.com/2dioT.png [Kalhor, A. (2020). Classification and Regression NNs. Lecture.] In the simplest case, the new weight is as follows: Here the cost function gradient is added to the previous weight to get a new weight. If all the previous weights are the same, then in the next step all the weights may be equal. As a result, in this case, from a geometric point of view, the neural network is inclined in one direction and all weights are the same. But if the weights are different, it is possible to update the weights by different amounts. (depending on the impact factor that each weight has on the result, it affects the cost and the updates of the weights. So even a small error in the initial random weighting can be solved). This was a very simple example, but it shows the effect of random weighting initialization on learning. This enables the neural network to go to different spaces instead of going to one side. As a result, in the process of learning, go to the best of these spacesI have Keras installed with the Tensorflow backend and CUDA.  I'd like to sometimes on demand force Keras to use CPU.  Can this be done without say installing a separate CPU-only Tensorflow in a virtual environment?  If so how?  If the backend were Theano, the flags could be set, but I have not heard of Tensorflow flags accessible via Keras.   If you want to force Keras to use CPU before Keras / Tensorflow is imported. Run your script as See also  This worked for me (win10), place before you import keras: A rather separable way of doing this is to use  Here, with booleans GPU and CPU, we indicate whether we would like to run our code with the GPU or CPU by rigidly defining the number of GPUs and CPUs the Tensorflow session is allowed to access. The variables num_GPU and num_CPU define this value. num_cores then sets the number of CPU cores available for usage via intra_op_parallelism_threads and inter_op_parallelism_threads. The intra_op_parallelism_threads variable dictates the number of threads a parallel operation in a single node in the computation graph is allowed to use (intra). While the inter_ops_parallelism_threads variable defines the number of threads accessible for parallel operations across the nodes of the computation graph (inter). allow_soft_placement allows for operations to be run on the CPU if any of the following criterion are met: there is no GPU implementation for the operation there are no GPU devices known or registered there is a need to co-locate with other inputs from the CPU All of this is executed in the constructor of my class before any other operations, and is completely separable from any model or other code I use.  Note: This requires tensorflow-gpu and cuda/cudnn to be installed because the option is given to use a GPU. Refs: What do the options in ConfigProto like allow_soft_placement and log_device_placement mean? Meaning of inter_op_parallelism_threads and intra_op_parallelism_threads  Just import tensortflow and use keras, it's that easy. As per keras tutorial, you can simply use the same tf.device scope as in regular tensorflow: I just spent some time figure it out.
Thoma's answer is not complete.
Say your program is test.py, you want to use gpu0 to run this program, and keep other gpus free. You should write CUDA_VISIBLE_DEVICES=0 python test.py Notice it's DEVICES not DEVICE For people working on PyCharm, and for forcing CPU, you can add the following line in the Run/Debug configuration, under Environment variables: To disable running on the GPU (tensor flow 2.9) use tf.config.set_visible_devices([], 'GPU'). The empty list argument is to say that there will be no GPUs visible for this run. Do this early in your code, e.g. before Keras initializes the tf configuration. See docs https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/config/set_visible_devicesI've gone through the official doc. I'm having a hard time understanding what this function is used for and how it works. Can someone explain this in layman's terms? unfold imagines a tensor as a longer tensor with repeated columns/rows of values 'folded' on top of each other, which is then "unfolded": E.g. for a 2x5 tensor, unfolding it with step=1, and patch size=2 across dim=1:  fold is roughly the opposite of this operation, but "overlapping" values are summed in the output. The unfold and fold are used to facilitate "sliding window" operations (like convolutions). Suppose you want to apply a function foo to every 5x5 window in a feature map/image: Now windows has size of batch-(55x.size(1))-num_windows, you can apply foo on windows: Now you need to "fold" processed back to the original size of x: You need to take care of padding, and kernel_size that may affect your ability to "fold" back processed to the size of x. Moreover, fold sums over overlapping elements, so you might want to divide the output of fold by patch size. Please note that torch.unfold performs a different operation than nn.Unfold. See this thread for details. Out:  Since there are no answers with 4-D tensors and nn.functional.unfold() only accepts 4-D tensor, I will would to explain this. Assuming the input tensor is of shape (batch_size, channels, height, width), and I have taken an example where batch_size = 1, channels = 2, height = 3, width = 3.  kernel_size = 2 which is nothing but a 2x2 kernelThis might be a beginner question but I have seen a lot of people using LabelEncoder() to replace categorical variables with ordinality. A lot of people using this feature by passing multiple columns at a time, however I have some doubt about having wrong ordinality in some of my features and how it will be effecting my model. Here is an example: Input Output As you can see, the ordinal values are not mapped correctly since my LabelEncoder only cares about the order in the column/array (it should be High=1, Med=2, Low=3 or vice versa). How drastically wrong mapping can effect the models and is there an easy way other than OrdinalEncoder() to map these values properly? TL;DR: Using a LabelEncoder to encode ordinal any kind of features is a bad idea! This is in fact clearly stated in the docs, where it is mentioned that as its name suggests this encoding method is aimed at encoding the label: This transformer should be used to encode target values, i.e. y, and not the input X. As you rightly point out in the question, mapping the inherent ordinality of an ordinal feature to a wrong scale will have a very negative impact on the performance of the model (that is, proportional to the relevance of the feature). And the same applies to a categorical feature, just that the original feature has no ordinality. An intuitive way to think about it, is in the way a decision tree sets its boundaries. During training, a decision tree will learn the optimal features to set at each node, as well as an optimal threshold whereby unseen samples will follow a branch or another depending on these values. If we encode an ordinal feature using a simple LabelEncoder, that could lead to a feature having say 1 represent warm, 2 which maybe would translate to hot, and a 0 representing boiling. In such case, the result will end up being a tree with an unnecessarily high amount of splits, and hence a much higher complexity for what should be simpler to model. Instead, the right approach would be to use an OrdinalEncoder, and define the appropriate mapping schemes for the ordinal features. Or in the case of having a categorical feature, we should be looking at OneHotEncoder or the various encoders available in Category Encoders. Though actually seeing why this is a bad idea will be more intuitive than just words. Let's use a simple example to illustrate the above, consisting on two ordinal features containing a range with the amount of hours spend by a student preparing for an exam and the average grade of all previous assignments, and a target variable indicating whether the exam was past or not. I've defined the dataframe's columns as pd.Categorical: The advantage of defining a categorical column as a pandas' categorical, is that we get to establish an order among its categories, as mentioned earlier. This allows for much faster sorting based on the established order rather than lexical sorting. And it can also be used as a simple way to get codes for the different categories according to their order. So the dataframe we'll be using looks as follows: The corresponding category codes can be obtained with: Now let's fit a DecisionTreeClassifier, and see what is how the tree has defined the splits: We can visualise the tree structure using plot_tree:  Is that all?? Well… yes! I've actually set the features in such a way that there is this simple and obvious relation between the Hours of dedication feature, and whether the exam is passed or not, making it clear that the problem should be very easy to model. Now let's try to do the same by directly encoding all features with an encoding scheme we could have obtained for instance through a LabelEncoder, so disregarding the actual ordinality of the features, and just assigning a value at random:  As expected the tree structure is way more complex than necessary for the simple problem we're trying to model. In order for the tree to correctly predict all training samples it has expanded until a depth of 4, when a single node should suffice. This will imply that the classifier is likely to overfit, since we’re drastically increasing the complexity. And by pruning the tree and tuning the necessary parameters to prevent overfitting we are not solving the problem either, since we’ve added too much noise by wrongly encoding the features. So to summarize, preserving the ordinality of the features once encoding them is crucial, otherwise as made clear with this example we'll lose all their predictable power and just add noise to our model.I have an image of size as RGB uint8(576,720,3) where I want to classify each pixel to a set of colors. I have transformed using rgb2lab from RGB to LAB space, and then removed the L layer so it is now a double(576,720,2) consisting of AB. Now, I want to classify this to some colors that I have trained on another image, and calculated their respective AB-representations as: Now, in order to classify/label each pixel to a cluster 1-7, I currently do the following (pseudo-code): However, this is terribly slow (52 seconds) because of the image resolution and that I manually loop through each x and y. Are there some built-in functions I can use that performs the same job? There must be.  To summarize: I need a classification method that classifies pixel images to an already defined set of clusters. For a N x 2 sized points/pixels array, you can avoid permute as suggested in the other solution by Luis, which could slow down things a bit, to have a kind of "permute-unrolled" version of it and also let's bsxfun work towards a 2D array instead of a 3D array, which must be better with performance. Thus, assuming clusters to be ordered as a N x 2 sized array, you may try this other bsxfun based approach - You can try out another approach that leverages  fast matrix multiplication in MATLAB and is based on this smart solution - Let us consider two matrices A and B between whom we want to calculate the distance matrix. For the sake of an easier explanation that follows next, let us consider A as 3 x 2 and B as 4 x 2 sized arrays, thus indicating that we are working with X-Y points. If we had A as N x 3 and B as M x 3 sized arrays, then those would be X-Y-Z points. Now, if we have to manually calculate the first element of the square of distance matrix, it would look like this – which would be – Now, according to our proposed matrix multiplication, if you check the output of A_ext and B_ext after the loop in the earlier code ends, they would look like the following –   So, if you perform matrix multiplication between A_ext and transpose of B_ext, the first element of the product would be the sum of elementwise multiplication between the first rows of A_ext and B_ext, i.e. sum of these –  The result would be identical to the result obtained from Equation (1) earlier. This would continue for all the elements of A against all the elements of B that are in the same column as in A.  Thus, we would end up with the complete squared distance matrix. That’s all there is!! Vectorized variations of the matrix multiplication based distance matrix calculations are possible, though there weren't any big performance improvements seen with them. Two such variations are listed next. Variation #1 Variation #2 So, these could be considered as experimental versions too. Use pdist2 (Statistics Toolbox) to compute the distances in a vectorized manner:  If you don't have the Statistics Toolbox, you can replace the third line by This gives squared distance instead of distance, but for the purposes of minimizing it doesn't matter.In most of the models, there is a steps parameter indicating the number of steps to run over data. But yet I see in most practical usage, we also execute the fit function N epochs.  What is the difference between running 1000 steps with 1 epoch and running 100 steps with 10 epoch? Which one is better in practice? Any logic changes between consecutive epochs? Data shuffling? A training step is one gradient update. In one step batch_size examples are processed. An epoch consists of one full cycle through the training data. This is usually many steps. As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of: If you choose your training image randomly (and independently) in each step, you normally do not call it epoch. [This is where my answer differs from the previous one. Also see my comment.] An epoch usually means one iteration over all of the training data.  For instance if you have 20,000 images and a batch size of 100 then the epoch should contain 20,000 / 100 = 200 steps.  However I usually just set a fixed number of steps like 1000 per epoch even though I have a much larger data set.  At the end of the epoch I check the average cost and if it improved I save a checkpoint.  There is no difference between steps from one epoch to another.  I just treat them as checkpoints. People often shuffle around the data set between epochs.  I prefer to use the random.sample function to choose the data to process in my epochs. So say I want to do 1000 steps with a batch size of 32.  I will just randomly pick 32,000 samples from the pool of training data. As I am currently experimenting with the tf.estimator API I would like to add my dewy findings here, too. I don't know yet if the usage of steps and epochs parameters is consistent throughout TensorFlow and therefore I am just relating to tf.estimator (specifically tf.estimator.LinearRegressor) for now.  Training steps defined by num_epochs: steps not explicitly defined Comment: I have set num_epochs=1 for the training input and the doc entry for numpy_input_fn tells me "num_epochs: Integer, number of epochs to iterate over data. If None will run forever.". With num_epochs=1 in the above example the training runs exactly x_train.size/batch_size times/steps (in my case this was 175000 steps as x_train had a size of 700000 and batch_size was 4). Training steps defined by num_epochs: steps explicitly defined higher than number of steps implicitly defined by num_epochs=1 Comment: num_epochs=1 in my case would mean 175000 steps (x_train.size/batch_size with x_train.size=700,000 and batch_size=4) and this is exactly the number of steps estimator.train albeit the steps parameter was set to 200,000 estimator.train(input_fn=train_input, steps=200000).  Training steps defined by steps Comment: Although I have set num_epochs=1 when calling numpy_input_fnthe training stops after 1000 steps. This is because steps=1000 in estimator.train(input_fn=train_input, steps=1000) overwrites the num_epochs=1 in tf.estimator.inputs.numpy_input_fn({'x':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True). Conclusion:
Whatever the parameters num_epochs for tf.estimator.inputs.numpy_input_fn and steps for estimator.train define, the lower bound determines the number of steps which will be run through. In easy words 
Epoch: Epoch is considered as number of one pass from entire dataset
Steps: In tensorflow one steps is considered as number of epochs multiplied by examples divided by batch size
 Epoch: A training epoch represents a complete use of all training data for gradients calculation and optimizations(train the model). Step: A training step means using one batch size of training data to train the model. Number of training steps per epoch: total_number_of_training_examples / batch_size. Total number of training steps: number_of_epochs x Number of training steps per epoch. According to Google's Machine Learning Glossary, an epoch is defined as "A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch_size training iterations, where N is the total number of examples." If  you are training model for 10 epochs with batch size 6, given total 12 samples that means: the model will be able to see whole dataset in 2 iterations ( 12 / 6  = 2) i.e. single epoch. overall, the model will have 2 X 10 = 20 iterations (iterations-per-epoch X no-of-epochs) re-evaluation of loss and model parameters will be performed after each iteration! Since there’re no accepted answer yet : 
By default an epoch run over all your training data. In this case you have n steps, with n = Training_lenght / batch_size. If your training data is too big you can decide to limit the number of steps during an epoch.[https://www.tensorflow.org/tutorials/structured_data/time_series?_sm_byp=iVVF1rD6n2Q68VSN] When the number of steps reaches the limit that you’ve set the process will start over, beginning the next epoch.
When working in TF, your data is usually transformed first into a list of batches that will be fed to the model for training. At each step you process one batch. As to whether it’s better to set 1000 steps for 1 epoch or 100 steps with 10 epochs I don’t know if there’s a straight answer. 
But here are results on training a CNN with both approaches using TensorFlow timeseries data tutorials : In this case, both approaches lead to very similar prediction, only the training profiles differ. steps = 20 / epochs = 100
  steps = 200 / epochs = 10   Divide the length of x_train by the batch size with We split the training set into many batches. When we run the algorithm, it requires one epoch to analyze the full training set. An epoch is composed of many iterations (or batches). Iterations: the number of batches needed to complete one Epoch. Batch Size: The number of training samples used in one iteration. Epoch: one full cycle through the training dataset. A cycle is composed of many iterations. Number of Steps per Epoch = (Total Number of Training Samples) / (Batch Size) Example
Training Set = 2,000 images
Batch Size = 10 Number of Steps per Epoch = 2,000 / 10 = 200 steps Hope this helps for better understanding.I'm following this tutorial to make this ML prediction: I'm using Python 3.6 and I get error "Expected 2D array, got 1D array instead:"
I think the script is for older versions, but I don't know how to convert it to the 3.6 version. Already try with the: You are just supposed to provide the predict method with the same 2D array, but with one value that you want to process (or more). In short, you can just replace With And it should work. EDIT: This answer became popular so I thought I'd add a little more explanation about ML. The short version: we can only use predict on data that is of the same dimensionality as the training data (X) was.  In the example in question, we give the computer a bunch of rows in X (with 2 values each) and we show it the correct responses in y. When we want to predict using new values, our program expects the same - a bunch of rows. Even if we want to do it to just one row (with two values), that row has to be part of another array.  The problem is occurring when you run prediction on the array [0.58,0.76]. Fix the problem by reshaping it before you call predict(): I use the below approach. I faced the same issue except that the data type of the instance I wanted to predict was a panda.Series object. Well I just needed to predict one input instance. I took it from a slice of my data. In this case, you'll need to convert it into a 1-D array  and then reshape it. From the docs, values will convert Series into a numpy array. I faced the same problem. You just have to make it an array and moreover you have to put double squared brackets to make it a single element of the 2D array as first bracket initializes the array and the second makes it an element of that array.  So simply replace the last statement by: Just insert the argument between a double square bracket: that worked for me I was facing the same issue earlier but I have somehow found the solution,
You can try reg.predict([[3300]]).   The API used to allow scalar value but now you need to give a 2D array. With one feature my Dataframe list converts to a Series. I had to convert it back to a Dataframe list and it worked. Just enclose your numpy object with two square brackets or vice versa. For example: If initially your x = [8,9,12,7,5] change it to x = [ [8,9,12,7,5] ]. That should fix the dimension issue You can do it like this: np.array(x)[:, None] The X and Y matrix of Independent Variable and Dependent Variable respectively to DataFrame from int64 Type so that it gets converted from 1D array to 2D array.. 
i.e X=pd.DataFrame(X) and Y=pd.dataFrame(Y) where pd is of pandas class in python. and thus feature scaling in-turn doesn't lead to any error!I have a matrix A and I want 2 matrices U and L such that U contains the upper triangular elements of A (all elements above and not including diagonal) and similarly for L(all elements below and not including diagonal). Is there a numpy method to do this? e.g To extract the upper triangle values to a flat vector,
you can do something like the following: Similarly, for the lower triangle, use np.tril. If you want to extract the values that are above the diagonal (or below) then use the k argument. This is usually used when the matrix is symmetric. To put back the extracted vector into a 2D symmetric array, one can follow my answer here: https://stackoverflow.com/a/58806626/5025009 Try numpy.triu (triangle-upper) and numpy.tril (triangle-lower). Code example: Use the Array Creation Routines of numpy.triu and numpy.tril to return a copy of a matrix with the elements above or below the k-th diagonal zeroed.After Training, I saved Both Keras whole Model and Only Weights using  Models and Weights were saved successfully and there was no error.
I can successfully load the weights simply using model.load_weights and they are good to go, but when i try to load the save model via load_model, i am getting an error. I never received this error and i used to load any models successfully. I am using Keras 2.2.4 with tensorflow backend. Python 3.6.
My Code for training is : For me the solution was downgrading the h5py package (in my case to 2.10.0), apparently putting back only Keras and Tensorflow to the correct versions was not enough. I downgraded my h5py package with the following command, Restarted my ipython kernel and it worked. For me it was the version of h5py that was superior to my previous build.
Fixed it by setting to 2.10.0. Downgrade h5py package with the following command to resolve the issue, I had the same problem, solved putting compile=False in load_model: saved using TF format file and not h5py: save_format='tf'. In my case: This is probably due to a model saved from a different version of keras. I got the same problem when loading a model generated by tensorflow.keras (which is similar to keras 2.1.6 for tf 1.12 I think) from keras 2.2.6. You can load the weights with model.load_weights and resave the complete model from the keras version you want to use. The solution than works for me was: I still kept having this error after having tensorflow==2.4.1, h5py==2.1.0, and python 3.8 in my environment.
what fixed it was downgrading the python version to 3.6.9 Downgrading python, tensorflow, keras and h5py resolved the issue.I use KerasClassifier to train the classifier. The code is below: But How to save the final model for future prediction? I usually use below code to save model: But I don't know how to insert the saving model's code into KerasClassifier's code. Thank you. The model has a save method, which saves all the details necessary to reconstitute the model. An example from the keras documentation: you can save the model in json and weights in a hdf5 file format.  files "model_num.h5" and "model_num.json" are created which contain our model and weights  To use the same trained model for further testing you can simply load the hdf5 file and use it for the prediction of different data. 
here's how to load the model from saved files. To predict for different data you can use this You can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain: In your Python code probable the last line should be: This allows you to save the entirety of the state of a model in a single file.
Saved models can be reinstantiated via keras.models.load_model(). The model returned by load_model() is a compiled model ready to be used (unless the saved model was never compiled in the first place). model.save() arguments: you can save the model and load in this way. Generally, we save the model and weights in the same file by calling the save() function. For saving, For Loading the model, In this case, we can simply save and load the model without re-compiling our model again.
Note - This is the preferred way for saving and loading your Keras model. Saving a Keras model: Loading the model back: For more information, read Documentation You can save the best model using keras.callbacks.ModelCheckpoint() Example: This will save the best model in your working directory. Since the syntax of keras, how to save a model, changed over the years I will post a fresh answer. In principle the earliest answer of bogatron, posted Mar 13 '17 at 12:10 is still good, if you want to save your model including the weights into one file. model.save("my_model.h5") This will save the model in the older Keras H5 format. However, there is a new format, the TensorFlow SavedModel format, which will be used if you do not specify the extension .h5, .hdf5 or .keras after the filename. The syntax in this case is model.save("path/to/folder") If the given folder name does not yet exist, it will be created. Two files and two folders will be created within this folder: keras_metadata.pb, saved_model.pb, assets, variables So far you can still decide whether you want to store your model into one single file or into a folder containing files and folders. (See keras documentation at www.tensorflow.org.)I'm trying to train a network with an unbalanced data. I have A (198 samples), B (436 samples), C (710 samples), D (272 samples) and I have read about the "weighted_cross_entropy_with_logits" but all the examples I found are for binary classification so I'm not very confident in how to set those weights. Total samples: 1616 A_weight: 198/1616 = 0.12? The idea behind, if I understood, is to penalize the errors of the majority class and value more positively the hits in the minority one, right? My piece of code: I have read this one and others examples with binary classification but still not very clear. Note that weighted_cross_entropy_with_logits is the weighted variant of sigmoid_cross_entropy_with_logits. Sigmoid cross entropy is typically used for binary classification. Yes, it can handle multiple labels, but sigmoid cross entropy basically makes a (binary) decision on each of them -- for example, for a face recognition net, those (not mutually exclusive) labels could be "Does the subject wear glasses?", "Is the subject female?", etc. In binary classification(s), each output channel corresponds to a binary (soft) decision. Therefore, the weighting needs to happen within the computation of the loss. This is what weighted_cross_entropy_with_logits does, by weighting one term of the cross-entropy over the other. In mutually exclusive multilabel classification, we use softmax_cross_entropy_with_logits, which behaves differently: each output channel corresponds to the score of a class candidate. The decision comes after, by comparing the respective outputs of each channel. Weighting in before the final decision is therefore a simple matter of modifying the scores before comparing them, typically by multiplication with weights. For example, for a ternary classification task, You could also rely on tf.losses.softmax_cross_entropy to handle the last three steps. In your case, where you need to tackle data imbalance, the class weights could indeed be inversely proportional to their frequency in your train data. Normalizing them so that they sum up to one or to the number of classes also makes sense. Note that in the above, we penalized the loss based on the true label of the samples. We could also have penalized the loss based on the estimated labels by simply defining and the rest of the code need not change thanks to broadcasting magic. In the general case, you would want weights that depend on the kind of error you make. In other words, for each pair of labels X and Y, you could choose how to penalize choosing label X when the true label is Y. You end up with a whole prior weight matrix, which results in weights above being a full (num_samples, num_classes) tensor. This goes a bit beyond what you want, but it might be useful to know nonetheless that only your definition of the weight tensor need to change in the code above. See this answer for an alternate solution which works with sparse_softmax_cross_entropy: Tensorflow 2.0 Compatible Answer: Migrating the Code specified in P-Gn's Answer to 2.0, for the benefit of the community. For more information about migration of code from Tensorflow Version 1.x to 2.x, please refer this Migration Guide.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 7 years ago. Does anyone know of recent academic work which has been done on logo recognition in images?
Please answer only if you are familiar with this specific subject (I can search Google for "logo recognition" myself, thank you very much).
Anyone who is knowledgeable in computer vision and has done work on object recognition is welcome to comment as well.  Update:
Please refer to the algorithmic aspects (what approach you think is appropriate, papers in the field, whether it should work(and has been tested) for real world data, efficiency considerations) and not the technical sides (the programming language used or whether it was with OpenCV...)
Work on image indexing and content based image retrieval can also help. You could try to use local features like SIFT here:
http://en.wikipedia.org/wiki/Scale-invariant_feature_transform It should work because logo shape is usually constant, so extracted features shall match well. The workflow will be like this: Detect corners (e.g. Harris corner detector) - for Nike logo they are two sharp ends. Compute descriptors (like SIFT - 128D integer vector) On training stage remember them; on matching stage find nearest neighbours for every feature in the database obtained during training. Finally, you have a set of matches (some of them are probably wrong). Seed out wrong matches using RANSAC. Thus you'll get the matrix that describes transform from ideal logo image to one where you find the logo. Depending on the settings, you could allow different kinds of transforms (just translation; translation and rotation; affine transform). Szeliski's book has a chapter (4.1) on local features.
http://research.microsoft.com/en-us/um/people/szeliski/Book/ P.S.  I assumed you wanna find logos in photos, for example find all Pepsi billboards, so they could be distorted. If you need to find a TV channel logo on the screen (so that it is not rotated and scaled), you could do it easier (pattern matching or something). Conventional SIFT does not consider color information. Since logos usually have constant colors (though the exact color depends on lightning and camera) you might want to consider color information somehow. We worked on logo detection/recognition in real-world images. We also created a dataset FlickrLogos-32 and made it publicly available, including data, ground truth and evaluation scripts. In our work we treated logo recognition as retrieval problem to simplify multi-class recognition and to allow such systems to be easily scalable to many (e.g. thousands) logo classes. Recently, we developed a bundling technique called Bundle min-Hashing that aggregates spatial configurations of multiple local features into highly distinctive feature bundles. The bundle representation is usable for both retrieval and recognition. See the following example heatmaps for logo detections: 
 You will find more details on the internal operations, potential applications of the approach, experiments on its performance and of course also many references to related work in the papers [1][2]. Worked on that: Trademark matching and retrieval in sports video databases
get a PDF of the paper: http://scholar.google.it/scholar?cluster=9926471658203167449&hl=en&as_sdt=2000 We used SIFT as trademark and image descriptors, and a normalized threshold matching to compute the distance between models and images. In our latest work we have been able to greatly reduce computation using meta-models, created evaluating the relevance of the SIFT points that are present in different versions of the same trademark. I'd say that in general working with videos is harder than working on photos due to the very bad visual quality of the TV standards currently used. Marco I worked on a project where we had to do something very similar.  At first I tried using Haar Training techniques using this software OpenCV It worked, but was not an optimal solution for our needs. Our source images (where we were looking for the logo) were a fixed size and only contained the logo.  Because of this we were able to use cvMatchShapes with a known good match and compare the value returned to deem a good match.http://xgboost.readthedocs.org/en/latest/python/python_intro.html On the homepage of xgboost(above link), it says:
To install XGBoost, do the following steps: You need to run make in the root directory of the project In the python-package directory run python setup.py install However, when I did it, for step 1 the following error appear:
make : The term 'make' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the
 spelling of the name, or if a path was included, verify that the path is correct and try again. then I skip step1 and did step 2 directly, another error appear: Does anyone know how to install xgboost for python on Windows10 platform? Thanks for your help! In case anyone's looking for a simpler solution that doesn't require compiling it yourself: If you find it won't install because of a missing dependency, download and install the dependency first and retry.  If it complains about access permissions, try opening your command prompt as Administrator and retry. This gives you xgboost and the scikit-learn wrapper, and saves you from having to go through the pain of compiling it yourself. :) Note that as of the most recent release the Microsoft Visual Studio instructions no longer seem to apply as this link returns a 404 error: https://github.com/dmlc/xgboost/tree/master/windows You can read more about the removal of the MSVC build from Tianqi Chen's comment here. So here's what I did to finish a 64-bit build on Windows: These should be all the tools you need to build the xgboost project.  To get the source code run these lines: Note that I ran this part from a Cygwin shell.  If you are using the Windows command prompt you should be able to change cp to copy and arrive at the same result.  However, if the build fails on you for any reason I would recommend trying again using cygwin. If the build finishes successfully, you should have a file called xgboost.exe located in the project root.  To install the Python package, do the following:   Now you should be good to go.  Open up Python, and you can import the package with: To test the installation, I went ahead and ran the basic_walkthrough.py file that was included in the demo/guide-python folder of the project and didn't get any errors.   I installed XGBoost successfully in Windows 8 64bit, Python 2.7 with Visual Studio 2013 (don't need mingw64) Updated 15/02/2017 With newer version of XGBoost, here are my steps Step 1. Install cmake https://cmake.org/download/ Verify cmake have been installed successfully Step 2. Clone xgboost source Step 3. Create Visual Studio Project Step 4. Build Visual Studio 2013 project After build solution, two new files libxgboost.dll and xgboost.exe are created in folder xgboost_dir/lib Step 5. Build python package Verify xgboost have been installed successfully Old Answer Here are my steps: I just installed xgboost both for my python 2.7 and python 3.5, anaconda, 64bit machine and 64 bit python. both VERY simple, NO VS2013 or git required. I think it works for normal python, too. If you use python 3.5: 1: download the package here, the version depends on your python version, python3.5 or python 3.6, 32bit or 64bit.  2: use the command window, use cd to make the download folder as your pwd, then use OK, finished.
For more detailed steps, see this answer  if you use python 2.7, you do NOT need to download the VS2013 to build it yourself, because I have built it, you can download the file I built and install it directly 1: Download it here by google drive 2: Download it, decompress it, paste it here: "your python path\Lib\site-packages" Then you should have something look like this:  3: In python-package folder showed above, use cmd window, cd there and run use this code  in your python to check whether you have installed mingw-64 or not, No error information means you have installed the mingw-64 and you are finished. If there are error information  "WindowsError: [Error 126] " That means you have not installed mingw-64, and you have one more step to go. Download the mingw-64 here: http://sourceforge.net/projects/mingw-w64/ Choose x86_64 instead of the default "i686" when you installed the mingw-64,
then add "your install path\x86_64-6.2.0-posix-seh-rt_v5-rev1\mingw64\bin;" to your PATH, it should be something like this: "C:\Program Files\mingw-w64\x86_64-6.2.0-posix-seh-rt_v5-rev1\mingw64\bin;" (this is mine). Don't forget the ";" in the PATH. Then you are finished,you can use  in your python to check that, Yeah! PS: if you don't know how to add path, just google it to get solutions. Don't worry, it's very simple. If You are installing XGBoost for a particular Project and You are using Pycahrm then you need to follow the procedures given below: Download xgboost‑0.72‑cp36‑cp36m‑win_amd64.whl from Here (as I am using Python 3.6 if you use different version of Python like 2.7 then you need to install xgboost‑0.72‑cp27‑cp27m‑win_amd64.whl). Copy the to your Project Interpreter directory. You can find the directory of Project Interpreter by clicking File -> Settings -> Project Interpreter from Pycharm. Open Command Prompt. Go to directory to you Project Interpreter from cmd. Write the following command: pip install xgboost-0.72-cp36-cp36m-win_amd64.whl On windows 10 , with python 3.6, below command worked. From Anaconda Prompt, below command can be used directly. The screenshot is attached as proof.
pip install xgboost
 After build the c++ version, copy the release dll and lib files in ../windows/x64/Release/..(if you build x64 version) to ../wrapper/ then run python setup.py install  I followed the steps listed in https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13043/run-xgboost-from-windows-and-python. I will summarize what I did below. 1) Download Visual Basic Studio. You can download the community edition at visual studio website. There is a "free visual studio button on the upper right corner" 2) Copy all content from the git hub repository of xgboost/tree/master/windows and Open Visual studio existing project on Visual studio 3) There are a couple of drop down menus you need to select ( "Release" and "X64" and then select build --> build all from the upper menu. It should look something like the attached screenshot. 4) if you see the message ========== Build: 3 succeeded, 0 failed, 0 up-to-date, 0 skipped ==========, it is all good 5) Browse to python-packages folder where the setup file for XGB resides and run the install command 'python setup.py install'. You can find a similar thread at Install xgboost under python with 32-bit msys failing Hope this helps. To add to the solution by Disco4ever for those attempting to build on 32bit Windows machines. After doing step 6 and creating a config.mk file you need to go into this file and edit the following lines to remove the -m64 flag Adding "git checkout 9a48a40" to Disco4Ever's solution above worked for me: This was originally posted by Cortajarena here:
https://github.com/dmlc/xgboost/issues/1267 Also, for what it's worth, I originally had 32 bit Python running on my 64 bit machine and I had to upload 64 bit Python for XGBoost to work. Thanks to disco4ever answer. I was trying to build xgboost for Python Anaconda environment in my windows 10 64 bit machine. Used Git, mingw64 and basic windows cmd. Everthing worked for me till the copy step: cp make/mingw64.mk config.mk, as I was using windows cmd I modified it to copy c:\xgboost\make\mingw64.mk c:\xgboost\config.mk when I proceeded to the next step : make -j4, I got error that build failed. At this stage after so much frustration just tired something different by clicking on build.sh (shell script). It started executing and auto finished.  Then I executed the same step make -j4, to my awe build was successful. I have seen the most awaited xgboost.exe file in my xgboost folder.  I then proceeded with further steps and executed python setup.py install. finally everything installed perfectly. Then I went to my spyder and checked whether it is working or not. But I was one step away to my happiness because I was still seeing the import error.  Closed all command prompts (Anaconda, Git bash, Windows CMD, cygwin terminal) then again opened spyder and typed 'import xgboost'. SUCCESS, No ERROR.  Once again thank you for everyone. You can install xGBoost using either Visual Studio or minGW. Since, the official xgboost website says that MSVC build is not yet updated, I tried using mingw64.
I am running xgboost (python package) on my win7 x64. Steps I followed were: 1) Follow Disco4Ever's steps for ming64 installation (mentioned above in the answers). 2) Install Git for windows. windows download link. This will also install Git Bash. Add git-installation-directory\cmd to your system environment variable PATH list. 3) Now clone xGBoost in desired location. Type the following in cmd: 4) In xgboost's root directory there should be a shell script named "build". Open it. It'll open up a Git Bash and start building. After building, xgboost.exe file will be created. 5) Now install python package : You can test by importing xgboost in python.   It took a whole day, but I successfully installed xgboost on windows 7 64-bit box using TDM-GCC with OpenMP enabled, instead of MingW following this link - http://dnc1994.com/2016/03/installing-xgboost-on-windows/ Here's a very helpful link with important points to pay attention to during installation. It's very important to install "openmp". Otherwise you'll get error message. The link provides a step by step instruction for installing. Here's some quote: Building Xgboost To be fair, there is nothing wrong about the official guide for
  installing xgboost on Windows. But still, I’d love to stress several
  points here to save your time. Makefile_win is a modified version (thanks to Zhou Xiyou) of the
  original Makefile to suit the building process on Windows. You can
  wget it or download it here. Be sure to use a UNIX shell for thi
  because Windows CMD has issue with mkdir -p command. Git Bash is
  recommended. Be sure to use --recursive option with git clone. Be sure
  to use a proper MinGW. TDM-GCC is recommended. Note that by default it
  wouldn’t install OpenMP for you. You need to specifiy it otherwise the
  building would fail. Another helpful link is the official procedure: official guide Good luck! I would like to add a small workaround to Disco4ever 's solution. For me I was unable to perform cloning in cygwin. So the workaround is perform it in command prompt in windows and do the rest of the task in cygwin. Use cd c:\xgboost in the 3rd line to make it work in cygwin. So the updated last part is like this. And after installation is complete you can uninstall git and cygwin but xgboost and mingw64 must be kept as it is. Note that: before "make -j4" use gcc -v to check your gcc version. As to me, My environment is win10 + anaconda(python 2.7), when I run make -j4. It shows std::mutex error. After I use gcc- v It echo gcc4.7(anaconda's default gcc).After I choose my gcc to mingw64's 6.2 gcc ,then it works. Finally, I use "/d/Anaconda2/python.exe setup.py install" install xgboost python packet. You can install XGBoost using following 3 steps:  Gather information of your system (python version and system architecture - 32 bit or 64 bit) download related .whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/ e.g. if your python version is 3.7 and windows is 32 bit, then suitable file is:  xgboost‑0.72‑cp37‑cp37m‑win32.whl You can also find detailed steps here.  I use Jupyter notebook and I found a really simple way to install XGBoost within Anaconda: Done(An update to this question has been added.) I am a graduate student at the university of Ghent, Belgium; my research is about emotion recognition with deep convolutional neural networks. I'm using the Caffe framework to implement the CNNs. Recently I've run into a problem concerning class imbalance. I'm using 9216 training samples, approx. 5% are labeled positively (1), the remaining samples are labeled negatively (0). I'm using the SigmoidCrossEntropyLoss layer to calculate the loss. When training, the loss decreases and the accuracy is extremely high after even a few epochs. This is due to the imbalance: the network simply always predicts negative (0). (Precision and recall are both zero, backing this claim) To solve this problem, I would like to scale the contribution to the loss depending on the prediction-truth combination (punish false negatives severely). My mentor/coach has also advised me to use a scale factor when backpropagating through stochastic gradient descent (sgd): the factor would be correlated to the imbalance in the batch. A batch containing only negative samples would not update the weights at all. I have only added one custom-made layer to Caffe: to report other metrics such as precision and recall. My experience with Caffe code is limited but I have a lot of expertise writing C++ code. Could anyone help me or point me in the right direction on how to adjust the SigmoidCrossEntropyLoss and Sigmoid layers to accomodate the following changes: Thanks in advance! I have incorporated the InfogainLossLayer as suggested by Shai. I've also added another custom layer that builds the infogain matrix H based on the imbalance in the current batch. Currently, the matrix is configured as follows: I'm planning on experimenting with different configurations for the matrix in the future. I have tested this on a 10:1 imbalance. The results have shown that the network is learning useful things now: (results after 30 epochs) These numbers were reached at around 20 epochs and didn't change significantly after that. !! The results stated above are merely a proof of concept, they were obtained by training a simple network on a 10:1 imbalanced dataset. !! Why don't you use the InfogainLoss layer to compensate for the imbalance in your training set? The Infogain loss is defined using a weight matrix H (in your case 2-by-2) The meaning of its entries are So, you can set the entries of H to reflect the difference between errors in predicting 0 or 1. You can find how to define matrix H for caffe in this thread. Regarding sample weights, you may find this post interesting: it shows how to modify the SoftmaxWithLoss layer to take into account sample weights. Recently, a modification to cross-entropy loss was proposed by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár Focal Loss for Dense Object Detection, (ICCV 2017).
The idea behind focal-loss is to assign different weight for each example based on the relative difficulty of predicting this example (rather based on class size etc.). From the brief time I got to experiment with this loss, it feels superior to "InfogainLoss" with class-size weights. I have also come across this class imbalance problem in my classification task. Right now I am using CrossEntropyLoss with weight (documentation here) and it works fine. The idea is to give more loss to samples in classes with smaller number of images. weight for each class in inversely proportional to the image number in this class. Here is a snippet to calculate weight for all class using numpy,Let's suppose I have a sequence of integers: 0,1,2, .. and want to predict the next integer given the last 3 integers, e.g.: [0,1,2]->5, [3,4,5]->6, etc Suppose I setup my model like so: It is my understanding that model has the following structure (please excuse the crude drawing):  First Question: is my understanding correct? Note I have drawn the previous states C_{t-1}, h_{t-1} entering the picture as this is exposed when specifying stateful=True.  In this simple "next integer prediction" problem, the performance should improve by providing this extra information (as long as the previous state results from the previous 3 integers). This brings me to my main question:  It seems the standard practice (for example see this blog post and the TimeseriesGenerator keras preprocessing utility), is to feed a staggered set of inputs to the model during training. For example: This has me confused because it seems this is requires the output of the 1st Lstm Cell (corresponding to the 1st time step).  See this figure:  From the tensorflow docs: stateful: Boolean (default False). If True, the last state for each
  sample at index i in a batch will be used as initial state for the
  sample of index i in the following batch. it seems this "internal" state isn't available and all that is available is the final state.  See this figure:  So, if my understanding is correct (which it's clearly not), shouldn't we be feeding non-overlapped windows of samples to the model when using stateful=True?  E.g.: The answer is: depends on problem at hand. For your case of one-step prediction - yes, you can, but you don't have to. But whether you do or not will significantly impact learning.  Batch vs. sample mechanism ("see AI" = see "additional info" section) All models treat samples as independent examples; a batch of 32 samples is like feeding 1 sample at a time, 32 times (with differences - see AI). From model's perspective, data is split into the batch dimension, batch_shape[0], and the features dimensions, batch_shape[1:] - the two "don't talk." The only relation between the two is via the gradient (see AI). Overlap vs no-overlap batch Perhaps the best approach to understand it is information-based. I'll begin with timeseries binary classification, then tie it to prediction: suppose you have 10-minute EEG recordings, 240000 timesteps each. Task: seizure or non-seizure? Take 10 samples, shape (240000, 1). How to feed?  Which of the two above do you take? If (2), your neural net will never confuse a seizure for a non-seizure for those 10 samples. But it'll also be clueless about any other sample. I.e., it will massively overfit, because the information it sees per iteration barely differs (1/54000 = 0.0019%) - so you're basically feeding it the same batch several times in a row. Now suppose (3): A lot more reasonable; now our windows have a 50% overlap, rather than 99.998%. Prediction: overlap bad? If you are doing a one-step prediction, the information landscape is now changed: This dramatically changes your loss function, and what is 'good practice' for minimizing it: What should I do? First, make sure you understand this entire post, as nothing here's really "optional." Then, here's the key about overlap vs no-overlap, per batch: Your goal: balance the two; 1's main edge over 2 is: Should I ever use (2) in prediction?  LSTM stateful: may actually be entirely useless for your problem. Stateful is used when LSTM can't process the entire sequence at once, so it's "split up" - or when different gradients are desired from backpropagation. With former, the idea is - LSTM considers former sequence in its assessment of latter: In other words: do not overlap in stateful in separate batches. Same batch is OK, as again, independence - no "state" between the samples. When to use stateful: when LSTM benefits from considering previous batch in its assessment of the next. This can include one-step predictions, but only if you can't feed the entire seq at once: When and how does LSTM "pass states" in stateful? Per above, you cannot do this: This implies 21 causally follows 10 - and will wreck training. Instead do: Batch vs. sample: additional info A "batch" is a set of samples - 1 or greater (assume always latter for this answer)
. Three approaches to iterate over data: Batch Gradient Descent (entire dataset at once), Stochastic GD (one sample at a time), and Minibatch GD (in-between). (In practice, however, we call the last SGD also and only distinguish vs BGD - assume it so for this answer.) Differences: BONUS DIAGRAMS:I have a set of fairly complicated models that I am training and I am looking for a way to save and load the model optimizer states. The "trainer models" consist of different combinations of several other "weight models", of which some have shared weights, some have frozen weights depending on the trainer, etc. It is a bit too complicated of an example to share, but in short, I am not able to use model.save('model_file.h5') and keras.models.load_model('model_file.h5') when stopping and starting my training.  Using model.load_weights('weight_file.h5') works fine for testing my model if the training has finished, but if I attempt to continue training the model using this method, the loss does not come even close to returning to its last location. I have read that this is because the optimizer state is not saved using this method which makes sense. However, I need a method for saving and loading the states of the optimizers of my trainer models. It seems as though keras once had a model.optimizer.get_sate() and model.optimizer.set_sate() that would accomplish what I am after, but that does not seem to be the case anymore (at least for the Adam optimizer). Are there any other solutions with the current Keras? You can extract the important lines from the load_model and save_model functions. For saving optimizer states, in save_model: For loading optimizer states, in load_model: Combining the lines above, here's an example: For those who are not using model.compile and instead performing automatic differentiation to apply the gradients manually with optimizer.apply_gradients, I think I have a solution. First, save the optimizer weights: np.save(path, optimizer.get_weights()) Then, when you are ready to reload the optimizer, show the newly instantiated optimizer the size of the weights it will update by calling optimizer.apply_gradients on a list of tensors of the size of the variables for which you calculate gradients. It is extremely important to then set the weights of the model AFTER you set the weights of the optimizer because momentum-based optimizers like Adam will update the weights of the model even if we give it gradients which are zero. Note that if we try to set the weights before calling apply_gradients for the first time, an error is thrown that the optimizer expects a weight list of length zero. Completing Alex Trevithick answer, it is possible to avoid re calling model.set_weights, simply by saving the state of the variables before applying the gradient and then reloading. This can useful when loading a model from an h5 file, and looks cleaner (imo). The saving/loading functions are the following (thanks Alex again): upgrading Keras to 2.2.4 and using pickle solved this issue for me. with keras release 2.2.3 Keras models can now be safely pickled. Anyone trying to use @Yu-Yang's solution in a distributed setting might run in the following error: or similar. To solve this problem, you simply need to run the model's optimizer weights setting on each replica using the following: For some reason, this isn't needed for setting the model weights, but make sure that you create (via the call here) and load the weights of the model within the strategy scope or you might get an error along the lines of ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x14ffdce82c50>), which is different from the scope used for the original variable. If you want the full-on example, I created a colab showcasing this solution. The code below works for me (Tensorflow 2.5).
I'm using the universal sentence encoder as model, together with an Adam optimizer. Basically what I do is: I make use of a dummy input which sets the optimizer correctly.
Afterwards I set the weights. Save the weights of the optimizer load the optimizer From version 2.11 optimizer.get_weights() is no longer accessible. You can eventually switch to tf.optimizers.legacy classes but it is not recommended. Instead, The class tf.train.Checkpoint is specially designed for saving both model and optimizer weights: Finally, then class tf.train.CheckpointManager manages multiple checkpoint versions and make it very easy:Using a LogisticRegression class in scikit-learn on a version of the flight delay dataset. I use pandas to select some columns: I fill in NaN values with 0: Make sure the categorical columns are marked with the 'category' data type: Then call get_dummies() from pandas: Now I train and test my data set: Once I call the score method I get around 0.867. However, when I call the roc_auc_score method I get a much lower number of around 0.583 Is there any reason why the ROC AUC is much lower than what the score method provides? To start with, saying that an AUC of 0.583 is "lower" than a score* of 0.867 is exactly like comparing apples with oranges. [* I assume your score is mean accuracy, but this is not critical for this discussion - it could be anything else in principle] According to my experience at least, most ML practitioners think that the AUC score measures something different from what it actually does: the common (and unfortunate) use is just like any other the-higher-the-better metric, like accuracy, which may naturally lead to puzzles like the one you express yourself. The truth is that, roughly speaking, the AUC measures the performance of a binary classifier averaged across all possible decision thresholds. The (decision) threshold in binary classification is the value above which we decide to label a sample as 1 (recall that probabilistic classifiers actually return a value p in [0, 1], usually interpreted as a probability - in scikit-learn it is what predict_proba returns). Now, this threshold, in methods like scikit-learn predict which return labels (1/0), is set to 0.5 by default, but this is not the only possibility, and it may not even be desirable in come cases (imbalanced data, for example). The point to take home is that: Given these clarifications, your particular example provides a very interesting case in point: I get a good-enough accuracy ~ 87% with my model; should I care that, according to an AUC of 0.58, my classifier does only slightly better than mere random guessing? Provided that the class representation in your data is reasonably balanced, the answer by now should hopefully be obvious: no, you should not care; for all practical cases, what you care for is a classifier deployed with a specific threshold, and what this classifier does in a purely theoretical and abstract situation when averaged across all possible thresholds should pose very little interest for a practitioner (it does pose interest for a researcher coming up with a new algorithm, but I assume that this is not your case). (For imbalanced data, the argument changes; accuracy here is practically useless, and you should consider precision, recall, and the confusion matrix instead). For this reason, AUC has started receiving serious criticism in the literature (don't misread this - the analysis of the ROC curve itself is highly informative and useful); the Wikipedia entry and the references provided therein are highly recommended reading: Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. [...] One recent explanation of the problem with ROC AUC is that reducing the ROC Curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance points plotted and not the performance of an individual system Emphasis mine - see also On the dangers of AUC... I don't know what exactly AIR_DEL15 is, which you use as your label (it is not in the original data). My guess is that it is an imbalanced feature, i.e there are much more 0's than 1's; in such a case, accuracy as a metric is not meaningful, and you should use precision, recall, and the confusion matrix instead - see also this thread). Just as an extreme example, if 87% of your labels are 0's, you can have a 87% accuracy "classifier" simply (and naively) by classifying all samples as 0; in such a case, you would also have a low AUC (fairly close to 0.5, as in your case). For a more general (and much needed, in my opinion) discussion of what exactly AUC is, see my other answer.On Caffe, I am trying to implement a Fully Convolution Network for semantic segmentation. I was wondering is there a specific strategy to set up your 'solver.prototxt' values for the following hyper-parameters: Does it depend on the number of images you have for your training set? If so, how?  In order to set these values in a meaningful manner, you need to have a few more bits of information regarding your data: 1. Training set size the total number of training examples you have, let's call this quantity T.
2. Training batch size the number of training examples processed together in a single batch, this is usually set by the input data layer in the 'train_val.prototxt'. For example, in this file the train batch size is set to 256. Let's denote this quantity by tb.
3. Validation set size the total number of examples you set aside for validating your model, let's denote this by V.
4. Validation batch size value set in batch_size for the TEST phase. In this example it is set to 50. Let's call this vb. Now, during training, you would like to get an un-biased estimate of the performance of your net every once in a while. To do so you run your net on the validation set for test_iter iterations. To cover the entire validation set you need to have test_iter = V/vb.
How often would you like to get this estimation? It's really up to you. If you have a very large validation set and a slow net, validating too often will make the training process too long. On the other hand, not validating often enough may prevent you from noting if and when your training process failed to converge. test_interval determines how often you validate: usually for large nets you set test_interval in the order of 5K, for smaller and faster nets you may choose lower values. Again, all up to you.  In order to cover the entire training set (completing an "epoch") you need to run T/tb iterations. Usually one trains for several epochs, thus max_iter=#epochs*T/tb. Regarding iter_size: this allows to average gradients over several training mini batches, see this thread fro more information.I am trying to understand the process of model evaluation and validation in machine learning. Specifically, in which order and how the training, validation and test sets must be used.   Let's say I have a dataset and I want to use linear regression. I am hesitating among various polynomial degrees (hyper-parameters). In this wikipedia article, it seems to imply that the sequence should be:  However, this seems strange to me: how can you fit your model with the training set if you haven't chosen yet your hyper-parameters (polynomial degree in this case)?  I see three alternative approachs, I am not sure if they would be correct. So the question is: The Wikipedia article is not wrong; according to my own experience, this is a frequent point of confusion among newcomers to ML. There are two separate ways of approaching the problem: So, the standard point is that you always put aside a portion of your data as test set; this is used for no other reason than assessing the performance of your model in the end (i.e. not back-and-forth and multiple assessments, because in that case you are using your test set as a validation set, which is bad practice). After you have done that, you choose if you will cut another portion of your remaining data to use as a separate validation set, or if you will proceed with cross-validation (in which case, no separate and fixed validation set is required). So, essentially, both your first and third approaches are valid (and mutually exclusive, i.e. you should choose which one you will go with). The second one, as you describe it (CV only in the validation set?), is certainly not (as said, when you choose to go with CV you don't assign a separate validation set). Apart from a brief mention of cross-validation, what the Wikipedia article actually describes is your first approach. Questions of which approach is "better" cannot of course be answered at that level of generality; both approaches are indeed valid, and are used depending on the circumstances. Very loosely speaking, I would say that in most "traditional" (i.e. non deep learning) ML settings, most people choose to go with cross-validation; but there are cases where this is not practical (most deep learning settings, again loosely speaking), and people are going with a separate validation set instead. What Wikipedia means is actually your first approach. 1 Split data into training set, validation set and test set  2 Use the
  training set to fit the model (find the best parameters: coefficients
  of the polynomial). That just means that you use your training data to fit a model. 3 Afterwards, use the validation set to find the best hyper-parameters
  (in this case, polynomial degree) (wikipedia article says:
  "Successively, the fitted model is used to predict the responses for
  the observations in a second dataset called the validation dataset") That means that you use your validation dataset to predict its values with the previously (on the training set) trained model to get a score of how good your model performs on unseen data. You repeat step 2 and 3 for all hyperparameter combinations you want to look at (in your case the different polynomial degrees you want to try) to get a score (e.g. accuracy) for every hyperparmeter combination. Finally, use the test set to score the model fitted with the training
  set. Why you need the validation set is pretty well explained in this stackexchange question 
https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set  In the end you can use any of your three aproaches. approach: is the fastest because you only train one model for every hyperparameter.
also you don't need as much data as for the other two. approach: is slowest because you train for k folds k classifiers plus the final one with all your training data to validate it for every hyperparameter combination. You also need a lot of data because you split your data three times and that first  part again in k folds. But here you have the least variance in your results. Its pretty unlikely to get k good classifiers and a good validation result by coincidence. That could happen more likely in the first approach. Cross Validation is also way more unlikely to overfit. approach: is in its pros and cons in between of the other two. Here you also have less likely overfitting. In the end it will depend on how much data you have and if you get into more complex models like neural networks, how much time/calculationpower you have and are willing to spend. Edit As @desertnaut mentioned: Keep in mind that you should use training- and validationset as training data for your evaluation with the test set. Also you confused training with validation set in your second approach.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. What is the difference between epoch and iteration when training a multi-layer perceptron? In the neural network terminology: For example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch. FYI: Tradeoff batch size vs. number of iterations to train a neural network The term "batch" is ambiguous: some people use it to designate the entire training set, and some people use it to refer to the number of training examples in one forward/backward pass (as I did in this answer). To avoid that ambiguity and make clear that batch corresponds to the number of training examples in one forward/backward pass, one can use the term mini-batch. Epoch and iteration describe different things. An epoch describes the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, an epoch has been completed. An iteration describes the number of times a batch of data passed through the algorithm. In the case of neural networks, that means the forward pass and backward pass. So, every time you pass a batch of data through the NN, you completed an iteration. An example might make it clearer. Say you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs. Therefore, in each epoch, you have 5 batches (10/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.
Since you've specified 3 epochs, you have a total of 15 iterations (5*3 = 15) for training. Many neural network training algorithms involve making multiple presentations of the entire data set to the neural network.  Often, a single presentation of the entire data set is referred to as an "epoch".  In contrast, some algorithms present data to the neural network a single case at a time. "Iteration" is a much more general term, but since you asked about it together with "epoch", I assume that your source is referring to the presentation of a single case to a neural network. To understand the difference between these you must understand the Gradient Descent Algorithm and its Variants. Before I start with the actual answer, I would like to build some background. A batch is the complete dataset. Its size is the total number of training examples in the available dataset. mini-batch size is the number of examples the learning algorithm processes in a single pass (forward and backward). A Mini-batch is a small part of the dataset of given mini-batch size. Iterations is the number of batches of data the algorithm has seen (or simply the number of passes the algorithm has done on the dataset). Epochs is the number of times a learning algorithm sees the complete dataset. Now, this may not be equal to the number of iterations, as the dataset can also be processed in mini-batches, in essence, a single pass may process only a part of the dataset. In such cases, the number of iterations is not equal to the number of epochs. In the case of Batch gradient descent, the whole batch is processed on each training pass. Therefore, the gradient descent optimizer results in smoother convergence than Mini-batch gradient descent, but it takes more time. The batch gradient descent is guaranteed to find an optimum if it exists. Stochastic gradient descent is a special case of mini-batch gradient descent in which the mini-batch size is 1.   I guess in the context of neural network terminology: In order to define iteration (a.k.a steps), you first need to know about batch size: Batch Size: You probably wouldn't like to process the entire training instances all at one forward pass as it is inefficient and needs a huge deal of memory. So what is commonly done is splitting up training instances into subsets (i.e., batches), performing one pass over the selected subset (i.e., batch), and then optimizing the network through backpropagation. The number of training instances within a subset (i.e., batch) is called batch_size. Iteration: (a.k.a training steps) You know that your network has to go over all training instances in one pass in order to complete one epoch. But wait! when you are splitting up your training instances into batches, that means you can only process one batch (a subset of training instances) in one forward pass, so what about the other batches? This is where the term Iteration comes into play: Definition: The number of forwarding passes (The number of batches that you have created) that your network has to do in order to complete one epoch (i.e., going over all training instances) is called Iteration. For example, when you have 10,000 training instances and you want to do batching with the size of 10; you have to do 10,000/10 = 1,000 iterations to complete 1 epoch. Hope this could answer your question! You have training data which you shuffle and pick mini-batches from it. When you adjust your weights and biases using one mini-batch, you have completed one iteration. Once you run out of your mini-batches, you have completed an epoch. Then you shuffle your training data again, pick your mini-batches again, and iterate through all of them again. That would be your second epoch. Typically, you'll split your test set into small batches for the network to learn from, and make the training go step by step through your number of layers, applying gradient-descent all the way down. All these small steps can be called iterations. An epoch corresponds to the entire training set going through the entire network once. It can be useful to limit this, e.g. to fight to overfit. To my understanding, when you need to train a NN, you need a large dataset that involves many data items. when NN is being trained, data items go into NN one by one, that is called an iteration; When the whole dataset goes through, it is called an epoch. I believe iteration is equivalent to a single batch forward+backprop in batch SGD. Epoch is going through the entire dataset once (as someone else mentioned). An epoch contains a few iterations. That's actually what this epoch is. Let's define epoch as the number of iterations over the data set in order to train the neural network. Epoch is 1 complete cycle where the Neural network has seen all the data. One might have said 100,000 images to train the model, however, memory space might not be sufficient to process all the images at once, hence we split training the model on smaller chunks of data called batches. e.g. batch size is 100. We need to cover all the images using multiple batches. So we will need 1000 iterations to cover all the 100,000 images. (100 batch size * 1000 iterations) Once Neural Network looks at the entire data it is called 1 Epoch (Point 1). One might need multiple epochs to train the model. (let us say 10 epochs). An epoch is an iteration of a subset of the samples for training, for example, the gradient descent algorithm in a neural network. A good reference is: http://neuralnetworksanddeeplearning.com/chap1.html Note that the page has a code for the gradient descent algorithm which uses epoch Look at the code. For each epoch, we randomly generate a subset of the inputs for the gradient descent algorithm. Why epoch is effective is also explained on the page. Please take a look. According to Google's Machine Learning Glossary, an epoch is defined as "A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch_size training iterations, where N is the total number of examples." If  you are training model for 10 epochs with batch size 6, given total 12 samples that means: the model will be able to see the whole dataset in 2 iterations ( 12 / 6  = 2) i.e. single epoch. overall, the model will have 2 X 10 = 20 iterations (iterations-per-epoch X no-of-epochs) re-evaluation of loss and model parameters will be performed after each iteration! epoch A full training pass over the entire dataset such that each
example has been seen once. Thus, an epoch represents N/batch
size training iterations, where N is the total number of
examples. iteration A single update of a model's weights during training.
An iteration consists of computing the gradients of the parameters
with respect to the loss on a single batch of data. as bonus: batch The set of examples used in one iteration (that is, one gradient
update) of model training. See also batch size. source: https://developers.google.com/machine-learning/glossary/I have a large set of vectors in 3 dimensions. I need to cluster these based on Euclidean distance such that all the vectors in any particular cluster have a Euclidean distance between each other less than a threshold "T". I do not know how many clusters exist. At the end, there may be individual vectors existing that are not part of any cluster because its euclidean distance is not less than "T" with any of the vectors in the space. What existing algorithms / approach should be used here? You can use hierarchical clustering. It is a rather basic approach, so there are lots of implementations available. It is for example included in Python's scipy.  See for example the following script: Which produces a result similar to the following image. 
 The threshold given as a parameter is a distance value on which basis the decision is made whether points/clusters will be merged into another cluster. The distance metric being used can also be specified. Note that there are various methods for how to compute the intra-/inter-cluster similarity, e.g. distance between the closest points, distance between the furthest points, distance to the cluster centers and so on. Some of these methods are also supported by scipys hierarchical clustering module (single/complete/average... linkage). According to your post I think you would want to use complete linkage.  Note that this approach also allows small (single point) clusters if they don't meet the similarity criterion of the other clusters, i.e. the distance threshold. There are other algorithms that will perform better, which will become relevant in situations with lots of data points. As other answers/comments suggest you might also want to have a look at the DBSCAN algorithm: For a nice overview on these and other clustering algorithms, also have a look at this demo page (of Python's scikit-learn library): Image copied from that place:   As you can see, each algorithm makes some assumptions about the number and shape of the clusters that need to be taken into account. Be it implicit assumptions imposed by the algorithm or explicit assumptions specified by parameterization.  The answer by moooeeeep recommended using hierarchical clustering.  I wanted to elaborate on how to choose the treshold of the clustering. One way is to compute clusterings based on different thresholds t1, t2, t3,... and then compute a metric for the "quality" of the clustering.  The premise is that the quality of a clustering with the optimal number of clusters will have the maximum value of the quality metric. An example of a good quality metric I've used in the past is Calinski-Harabasz.  Briefly: you  compute the average inter-cluster distances and divide them by the within-cluster distances.  The optimal clustering assignment will have clusters that are separated from each other the most, and clusters that are "tightest". By the way, you don't have to use hierarchical clustering.  You can also use something like k-means, precompute it for each k, and then pick the k that has the highest Calinski-Harabasz score. Let me know if you need more references, and I'll scour my hard disk for some papers. Check out the DBSCAN algorithm. It clusters based on local density of vectors, i.e. they must not be more than some ε distance apart, and can determine the number of clusters automatically. It also considers outliers, i.e. points with an unsufficient number of ε-neighbors, to not be part of a cluster. The Wikipedia page links to a few implementations. Use OPTICS, which works well with large data sets. OPTICS: Ordering Points To Identify the Clustering Structure Closely related to DBSCAN, finds core sample of high density and expands clusters from them 1. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN Fine tune eps, min_samples as per your requirement. I want to add to moooeeeep's answer by using hierarchical clustering.
This solution work for me, though it quite "random" to pick threshold value.
By referrence to other source and test by myself, I got better method and threshold could be easily picked by dendrogram: You will see the plot like this
click here.
Then by drawing the horizontal line, let say at distance = 1, the number of conjunctions will be your desire number of clusters. So here I choose threshold = 1 for 4 clusters. Now each value in cluster_list will be an assigned cluster-id of the corresponding point in ori_array. You may have no solution: it is the case when the distance between any two distinct input data points is always greater than T. If you want to compute the number of clusters only from the input data, you may look at MCG, a hierarchical clustering method with an automatic stop criterion: see the free seminar paper at https://hal.archives-ouvertes.fr/hal-02124947/document (contains bibliographic references). I needed a way to "fuzzy sort" lines from OCR output, when the output is sometimes out of order, but within blocks, the lines are usually in order. In this case, the items to sort are dictionaries, which describe words at a location 'x','y' and with size 'w','h'. The general clustering algorithms seemed like overkill and I needed to maintain the order of the items during the sort. Here, I can set the tolerance tol to be about 1/4 the line spacing, and this is called with the field being 'y'. The trouble is that the 'y' coordinate of the OCR output are based on the outline around the word and a later word in the same line might have a 'y' coordinate that is lower than an earlier word. So a full sort by 'y' does not work. This is much like the clustering algorithm, but the intention is a bit different. I am not interested in the statistics of the data points, but I am interested in exactly which cluster each is placed and also it is important to maintain the original order. Maybe there is some way to fuzzy sort using the sorting built-ins, and it might be an alternative to the clustering options for 1-D problems.I'm trying to recover from a PCA done with scikit-learn, which features are selected as relevant. A classic example with IRIS dataset. This returns How can I recover which two features allow these two explained variance among the dataset ?
Said diferently, how can i get the index of this features in iris.feature_names ? This information is included in the pca attribute: components_. As described in the documentation, pca.components_ outputs an array of [n_components, n_features], so to get how components are linearly related with the different features you have to: Note: each coefficient represents the correlation between a particular pair of component and feature IMPORTANT: As a side comment, note the PCA sign does not affect its interpretation since the sign does not affect the variance contained in each component. Only the relative signs of features forming the PCA dimension are important. In fact, if you run the PCA code again, you might get the PCA dimensions with the signs inverted. For an intuition about this, think about a vector and its negative in 3-D space - both are essentially representing the same direction in space.
Check this post for further reference. Edit: as others have commented, you may get same values from .components_ attribute. Each principal component is a linear combination of the original variables:  where X_is are the original variables, and Beta_is are the corresponding weights or so called coefficients. To obtain the weights, you may simply pass identity matrix to the transform method: Each column of the coef matrix above shows the weights in the linear combination which obtains corresponding principal component: For example, above shows that the second principal component (PC-2) is mostly aligned with sepal width, which has the highest weight of 0.926 in absolute value; Since the data were normalized, you can confirm that the principal components have variance 1.0 which is equivalent to each coefficient vector having norm 1.0: One may also confirm that the principal components can be calculated as the dot product of the above coefficients and the original variables: Note that we need to use numpy.allclose instead of regular equality operator, because of floating point precision error. The way this question is phrased reminds me of a misunderstanding of Principle Component Analysis when I was first trying to figure it out. I’d like to go through it here in the hope that others won’t spend as much time on a road-to-nowhere as I did before the penny finally dropped. The notion of “recovering” feature names suggests that PCA identifies those features that are most important in a dataset. That’s not strictly true. PCA, as I understand it, identifies the features with the greatest variance in a dataset, and can then use this quality of the dataset to create a smaller dataset with a minimal loss of descriptive power. The advantages of a smaller dataset is that it requires less processing power and should have less noise in the data. But the features of greatest variance are not the "best" or "most important" features of a dataset, insofar as such concepts can be said to exist at all. To bring that theory into the practicalities of @Rafa’s sample code above:
 consider the following:
 In this case, post_pca_array has the same 150 rows of data as data_scaled, but data_scaled’s four columns have been reduced from four to two. The critical point here is that the two columns – or components, to be terminologically consistent – of post_pca_array are not the two “best” columns of data_scaled. They are two new columns, determined by the algorithm behind sklearn.decomposition’s PCA module. The second column, PC-2 in @Rafa’s example, is informed by sepal_width more than any other column, but the values in PC-2 and data_scaled['sepal_width'] are not the same. As such, while it’s interesting to find out how much each column in original data contributed to the components of a post-PCA dataset, the notion of “recovering” column names is a little misleading, and certainly misled me for a long time. The only situation where there would be a match between post-PCA and original columns would be if the number of principle components were set at the same number as columns in the original. However, there would be no point in using the same number of columns because the data would not have changed. You would only have gone there to come back again, as it were. This prints: So on the PC1 the feature named e is the most important and on PC2 the d. Given your fitted estimator pca, the components are to be found in pca.components_, which represent the directions of highest variance in the dataset.I have performed a PCA analysis over my original dataset and from the compressed dataset transformed by the PCA I have also selected the number of PC I want to keep (they explain almost the 94% of the variance). Now I am struggling with the identification of the original features that are important in the reduced dataset. 
How do I find out which feature is important and which is not among the remaining Principal Components after the dimension reduction?
Here is my code: Furthermore, I tried also to perform a clustering algorithm on the reduced dataset but surprisingly for me, the score is lower than on the original dataset. How is it possible?  First of all, I assume that you call features the variables and not the samples/observations. In this case, you could do something like the following by creating a biplot function that shows everything in one plot. In this example, I am using the iris data. Before the example, please note that the basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings). See my last paragraph after the plot for more details. Overview: PART1: I explain how to check the importance of the features and how to plot a biplot. PART2: I explain how to check the importance of the features and how to save them into a pandas dataframe using the feature names. Visualize what's going on using the biplot  Now, the importance of each feature is reflected by the magnitude of the corresponding values in the eigenvectors (higher magnitude - higher importance) Let's see first what amount of variance does each PC explain. PC1 explains 72% and PC2 23%. Together, if we keep PC1 and PC2 only, they explain 95%. Now, let's find the most important features. Here, pca.components_ has shape [n_components, n_features]. Thus, by looking at the PC1 (First Principal Component) which is the first row: [0.52237162 0.26335492 0.58125401 0.56561105]] we can conclude that feature 1, 3 and 4 (or Var 1, 3 and 4 in the biplot) are the most important. This is also clearly visible from the biplot (that's why we often use this plot to summarize the information in a visual way). To sum up, look at the absolute values of the Eigenvectors' components corresponding to the k largest Eigenvalues. In sklearn the components are sorted by explained_variance_. The larger they are these absolute values, the more a specific feature contributes to that principal component. The important features are the ones that influence more the components and thus, have a large absolute value/score on the component. To  get the most important features on the PCs with names and save them into a pandas dataframe use this: This prints: So on the PC1 the feature named e is the most important and on PC2 the d. Nice article as well here: https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=friends_link&sk=65bf5440e444c24aff192fedf9f8b64f the pca library contains this functionality. A demonstration to extract the feature importance is as following: Plot the explained variance  Make the biplot. It can be nicely seen that the first feature with most variance (f1), is almost horizontal in the plot, whereas the second most variance (f2) is almost vertical. This is expected because most of the variance is in f1, followed by f2 etc.  Biplot in 3d. Here we see the nice addition of the expected f3 in the plot in the z-direction.I've just started to experiment with AWS SageMaker and would like to load data from an S3 bucket into a pandas dataframe in my SageMaker python jupyter notebook for analysis. I could use boto to grab the data from S3, but I'm wondering whether there is a more elegant method as part of the SageMaker framework to do this in my python code? In the simplest case you don't need boto3, because you just read resources.
Then it's even simpler: But as Prateek stated make sure to configure your SageMaker notebook instance to have access to s3. This is done at configuration step in Permissions > IAM role If you have a look here it seems you can specify this in the InputDataConfig. Search for "S3DataSource" (ref) in the document. The first hit is even in Python, on page 25/26. You could also access your bucket as your file system using s3fs  Do make sure the Amazon SageMaker role has policy attached to it to have access to S3. It can be done in IAM. You can also use AWS Data Wrangler https://github.com/awslabs/aws-data-wrangler: A similar answer with the f-string. This code sample to import csv file from S3, tested at SageMaker notebook. Use pip or conda to install s3fs. !pip install s3fs There are multiple ways to read data into Sagemaker. To make the response more comprehensive i am adding details to read the data into Sagemaker Studio Notebook in memory as well as S3 mounting options. Though Notebooks are not recommend for data intensive modeling and are more used for prototyping based on my experience, there are multiple ways the data can be read into it. Both Boto3 and S3FS can also be used in conjunction with python libraries like Pandas to read the data in memory as well as can also be used to
copy the data to local instance EFS. These two options provide a mount like behaviour where the data appears to be in as if the local directory for higher IO operations. Both of these options have their pros and cons.When trying to get cross-entropy with sigmoid activation function, there is a difference between  But they are the same when with softmax activation function. Following is the sample code: You're confusing the cross-entropy for binary and multi-class problems. The formula that you use is correct and it directly corresponds to tf.nn.softmax_cross_entropy_with_logits: p and q are expected to be probability distributions over N classes. In particular, N can be 2, as in the following example: Note that q is computing tf.nn.softmax, i.e. outputs a probability distribution. So it's still multi-class cross-entropy formula, only for N = 2. This time the correct formula is Though mathematically it's a partial case of the multi-class case, the meaning of p and q is different. In the simplest case, each p and q is a number, corresponding to a probability of the class A.  Important: Don't get confused by the common p * -tf.log(q) part and the sum. Previous p was a one-hot vector, now it's a number, zero or one. Same for q - it was a probability distribution, now's it's a number (probability). If p is a vector, each individual component is considered an independent binary classification. See this answer that outlines the difference between softmax and sigmoid functions in tensorflow. So the definition p = [0, 0, 0, 1, 0] doesn't mean a one-hot vector, but 5 different features, 4 of which are off and 1 is on. The definition q = [0.2, 0.2, 0.2, 0.2, 0.2] means that each of 5 features is on with 20% probability. This explains the use of sigmoid function before the cross-entropy: its goal is to squash the logit to [0, 1] interval. The formula above still holds for multiple independent features, and that's exactly what tf.nn.sigmoid_cross_entropy_with_logits computes: You should see that the last three tensors are equal, while the prob1 is only a part of cross-entropy, so it contains correct value only when p is 1: Now it should be clear that taking a sum of -p * tf.log(q) along axis=1 doesn't make sense in this setting, though it'd be a valid formula in multi-class case. you can understand differences between softmax and sigmoid cross entropy in following way: so anyway the cross entropy is: for softmax cross entropy it looks exactly as above formula， but for sigmoid, it looks a little different for it has multi binary probability distribution
for each binary probability distribution, it is p and (1-p) you can treat as two class probability within each binary probability distributionWhat is the meaning of the (None, 100) in Output Shape?
Is this("None") the Sample number or the hidden dimension? None means this dimension is variable.  The first dimension in a keras model is always the batch size. You don't need fixed batch sizes, unless in very specific cases (for instance, when working with stateful=True LSTM layers).     That's why this dimension is often ignored when you define your model. For instance, when you define input_shape=(100,200), actually you're ignoring the batch size and defining the shape of "each sample". Internally the shape will be (None, 100, 200), allowing a variable batch size, each sample in the batch having the shape (100,200).      The batch size will be then automatically defined in the fit or predict methods. Other None dimensions: Not only the batch dimension can be None, but many others as well.    For instance, in a 2D convolutional network, where the expected input is (batchSize, height, width, channels), you can have shapes like (None, None, None, 3), allowing variable image sizes.  In recurrent networks and in 1D convolutions, you can also make the length/timesteps dimension variable, with shapes like (None, None, featuresOrChannels) Yes, None in summary means a dynamic dimension of a batch (mini batch). 
This is why you can set any batch size to your model. The summary() method is part of TF that incorporates Keras method print_summary().I am trying to create a simple deep-learning based model to predict y=x**2
But looks like deep learning is not able to learn the general function outside the scope of its training set. Intuitively I can think that neural network might not be able to fit y=x**2 as there is no multiplication involved between the inputs. Please note I am not asking how to create a model to fit x**2. I have already achieved that. I want to know the answers to following questions: Path to complete notebook:
https://github.com/krishansubudhi/MyPracticeProjects/blob/master/KerasBasic-nonlinear.ipynb training input:  training code Evaluation on random test set  Deep learning in this example is not good at predicting a simple non linear function. But good at predicting values in the sample space of training data. Given my remarks in the comments that your network is certainly not deep, let's accept that your analysis is indeed correct (after all, your model does seem to do a good job inside its training scope), in order to get to your 2nd question, which is the interesting one. Well, this is the kind of questions not exactly suitable for SO, since the exact meaning of "very limited" is arguably unclear... So, let's try to rephrase it: should we expect DL models to predict such numerical functions outside the numeric domain on which they have been trained? An example from a different domain may be enlightening here: suppose we have built a model able to detect & recognize animals in photos with very high accuracy (it is not hypothetical; such models do exist indeed); should we complain when the very same model cannot detect and recognize airplanes (or trees, refrigerators etc - you name it) in these same photos? Put like that, the answer is a clear & obvious no - we should not complain, and in fact we are certainly not even surprised by such a behavior in the first place. It is tempting for us humans to think that such models should be able to extrapolate, especially in the numeric domain, since this is something we do very "easily" ourselves; but ML models, while exceptionally good at interpolating, they fail miserably in extrapolation tasks, such as the one you present here. Trying to make it more intuitive, think that the whole "world" of such models is confined in the domain of their training sets: my example model above would be able to generalize and recognize animals in unseen photos as long as these animals are "between" (mind the quotes) the ones it has seen during training; in a similar manner, your model does a good job predicting the function value for arguments between the sample you have used for training. But in neither case these models are expected to go beyond their training domain (i.e. extrapolate). There is no "world" for my example model beyond animals, and similarly for your model beyond [-500, 500]... For corroboration, consider the very recent paper Neural Arithmetic Logic Units, by DeepMind; quoting from the abstract: Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. See also a relevant tweet of a prominent practitioner:  On to your third question: As it should be clear by now, this is a (hot) area of current research; see the above paper for starters... So, are DL models limited? Definitely - forget the scary tales about AGI for the foreseeable future. Are they very limited, as you put it? Well, I don't know... But, given their limitation in extrapolating, are they useful? This is arguably the real question of interest, and the answer is obviously - hell, yeah!This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. When I trained my neural network with Theano or Tensorflow, they will report a variable called "loss" per epoch. How should I interpret this variable? Higher loss is better or worse, or what does it mean for the final performance (accuracy) of my neural network? The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets. In the case of neural networks, the loss is usually negative log-likelihood and residual sum of squares for classification and regression respectively. Then naturally, the main objective in a learning model is to reduce (minimize) the loss function's value with respect to the model's parameters by changing the weight vector values through different optimization methods, such as backpropagation in neural networks. Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s). The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated. For example, if the number of test samples is 1000 and model classifies 952 of those correctly, then the model's accuracy is 95.2%.  There are also some subtleties while reducing the loss value. For instance, you may run into the problem of over-fitting in which the model "memorizes" the training examples and becomes kind of ineffective for the test set. Over-fitting also occurs in cases where you do not employ a regularization, you have a very complex model (the number of free parameters W is large) or the number of data points N is very low. They are two different metrics to evaluate your model's performance usually being used in different phases. Loss is often used in the training process to find the "best" parameter values for your model (e.g. weights in neural network). It is what you try to optimize in the training by updating weights. Accuracy is more from an applied perspective. Once you find the optimized parameters above, you use this metrics to evaluate how accurate your model's prediction is compared to the true data. Let us use a toy classification example. You want to predict gender from one's weight and height. You have 3 data, they are as follows:(0 stands for male, 1 stands for female) y1 = 0, x1_w = 50kg, x2_h = 160cm; y2 = 0, x2_w = 60kg, x2_h = 170cm; y3 = 1, x3_w = 55kg, x3_h = 175cm; You use a simple logistic regression model that is y = 1/(1+exp-(b1*x_w+b2*x_h)) How do you find b1 and b2? you define a loss first and use optimization method to minimize the loss in an iterative way by updating b1 and b2. In our example, a typical loss for this binary classification problem can be:
(a minus sign should be added in front of the summation sign)  We don't know what b1 and b2 should be. Let us make a random guess say b1 = 0.1 and b2 = -0.03. Then what is our loss now?    so the loss is
  Then you learning algorithm (e.g. gradient descent) will find a way to update b1 and b2 to decrease the loss. What if b1=0.1 and b2=-0.03 is the final b1 and b2 (output from gradient descent), what is the accuracy now? Let's assume if y_hat >= 0.5, we decide our prediction is female(1). otherwise it would be 0. Therefore, our algorithm predict y1 = 1, y2 = 1 and y3 = 1. What is our accuracy? We make wrong prediction on y1 and y2 and make correct one on y3. So now our accuracy is 1/3 = 33.33%  PS: In Amir's answer, back-propagation is said to be an optimization method in NN. I think it would be treated as a way to find gradient for weights in NN. Common optimization method in NN are GradientDescent and Adam. Just to clarify the Training/Validation/Test data sets:
The training set is used to perform the initial training of the model, initializing the weights of the neural network. The validation set is used after the neural network has been trained. It is used for tuning the network's hyperparameters, and comparing how changes to them affect the predictive accuracy of the model. Whereas the training set can be thought of as being used to build the neural network's gate weights, the validation set allows fine tuning of the parameters or architecture of the neural network model. It's useful as it allows repeatable comparison of these different parameters/architectures against the same data and networks weights, to observe how parameter/architecture changes affect the predictive power of the network. Then the test set is used only to test the predictive accuracy of the trained neural network on previously unseen data, after training and parameter/architecture selection with the training and validation data sets.Is there a way to plot both the training losses and validation losses on the same graph? It's easy to have two separate scalar summaries for each of them individually, but this puts them on separate graphs. If both are displayed in the same graph it's much easier to see the gap between them and whether or not they have begin to diverge due to overfitting. Is there a built in way to do this? If not, a work around way? Thank you much! The work-around I have been doing is to use two SummaryWriter with different log dir for training set and cross-validation set respectively. And you will see something like this:  Rather than displaying the two lines separately, you can instead plot the difference between validation and training losses as its own scalar summary to track the divergence. This doesn't give as much information on a single plot (compared with adding two summaries), but it helps with being able to compare multiple runs (and not adding multiple summaries per run). Just for anyone coming accross this via a search: The current best practice to achieve this goal is to just use the SummaryWriter.add_scalars method from torch.utils.tensorboard. From the docs: Expected result:  Many thanks to niko for the tip on Custom Scalars. I was confused by the official custom_scalar_demo.py because there's so much going on, and I had to study it for quite a while before I figured out how it worked. To show exactly what needs to be done to create a custom scalar graph for an existing model, I put together the following complete example: The above consists of an "original model" augmented by three blocks of code indicated by
 My "original model" has these scalars:  and this graph:  My modified model has the same scalars and graph, together with the following custom scalar:  This custom scalar chart is simply a layout which combines the original two scalar charts. Unfortunately the resulting graph is hard to read because both values have the same color.  (They are distinguished only by marker.)  This is however consistent with TensorBoard's convention of having one color per log.   The idea is as follows.  You have some group of variables which you want to plot inside a single chart.  As a prerequisite, TensorBoard should be plotting each variable individually under the "SCALARS" heading.  (This is accomplished by creating a scalar summary for each variable, and then writing those summaries to the log. Nothing new here.)   To plot multiple variables in the same chart, we tell TensorBoard which of these summaries to group together.  The specified summaries are then combined into a single chart under the "CUSTOM SCALARS" heading.  We accomplish this by writing a "Layout" once at the beginning of the log.  Once TensorBoard receives the layout, it automatically produces a combined chart under "CUSTOM SCALARS" as the ordinary "SCALARS" are updated.   Assuming that your "original model" is already sending your variables (as scalar summaries) to TensorBoard, the only modification necessary is to inject the layout before your main iteration loop starts.  Each custom scalar chart selects which summaries to plot by means of a regular expression.  Thus for each group of variables to be plotted together, it can be useful to place the variables' respective summaries into a separate name scope.  (That way your regex can simply select all summaries under that name scope.) Important Note: The op which generates the summary of a variable is distinct from the variable itself.  For example, if I have a variable ns1/my_var, I can create a summary ns2/summary_op_for_myvar.  The custom scalars chart layout cares only about the summary op, not the name or scope of the original variable.  Here is an example, creating two tf.summary.FileWriters which share the same root directory. Creating a tf.summary.scalar shared by the two tf.summary.FileWriters. At every time step, get the summary and update each tf.summary.FileWriter.  Here is the result:  The orange line shows the result of the evaluation stage, and correspondingly, the blue line illustrates the data of the training stage. Also, there is a very useful post by TF team to which you can refer. For completeness, since tensorboard 1.5.0 this is now possible. You can use the custom scalars plugin. For this, you need to first make tensorboard layout configuration and write it to the event file. From the tensorboard example: A Category is group of Charts. Each Chart corresponds to a single plot which displays several scalars together. The Chart can plot simple scalars (MultilineChartContent) or filled areas (MarginChartContent, e.g. when you want to plot the deviation of some value). The tag member of MultilineChartContent must be a list of regex-es which match the tags of the scalars that you want to group in the Chart. For more details check the proto definitions of the objects in https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/custom_scalar/layout.proto. Note that if you have several FileWriters writing to the same directory, you need to write the layout in only one of the files. Writing it to a separate file also works. To view the data in TensorBoard, you need to open the Custom Scalars tab. Here is an example image of what to expect https://user-images.githubusercontent.com/4221553/32865784-840edf52-ca19-11e7-88bc-1806b1243e0d.png The solution in PyTorch 1.5 with the approach of two writers: Keys in the train_losses dict have to match those in the val_losses to be grouped on the same graph. Tensorboard is really nice tool but by its declarative nature can make it difficult to get it to do exactly what you want.   I recommend you checkout Losswise (https://losswise.com) for plotting and keeping track of loss functions as an alternative to Tensorboard.  With Losswise you specify exactly what should be graphed together: And then you get something that looks like:   Notice how the data is fed to a particular graph explicitly via the loss_graph.append call, the data for which then appears in your project's dashboard. In addition, for the above example Losswise would automatically generate a table with columns for min(training_loss) and min(validation_loss) so you can easily compare summary statistics across your experiments.  Very useful for comparing results across a large number of experiments. Please let me contribute with some code sample in the answer given by @Lifu Huang. First download the loger.py from here and then: Finally you run tensorboard --logdir=summaries/ --port=6006and you get:In machine learning task. We should get a group of random w.r.t normal distribution with bound. We can get a normal distribution number with np.random.normal() but it does't offer any bound parameter. I want to know how to do that? The parametrization of truncnorm is complicated, so here is a function that translates the parametrization to something more intuitive: Instance the generator with the parameters: mean, standard deviation, and truncation range: Then, you can use X to generate a value: Or, a numpy array with N generated values: Here is the plot of three different truncated normal distributions:  If you're looking for the Truncated normal distribution, SciPy has a function for it called truncnorm The standard form of this distribution is a standard normal truncated
  to the range [a, b] — notice that a and b are defined over the domain
  of the standard normal. To convert clip values for a specific mean and
  standard deviation, use: a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std truncnorm takes a and b as shape parameters. The above example is bounded by -2 and 2 and returns 10 random variates (using the .rvs() method) Here's a histogram plot for -6, 6:  Besides @bakkal suggestion (+1) you might also want to take a look into Vincent Mazet recipe for achieving this, rewritten as py-rtnorm module by Christoph Lassner. You can subdivide your targeted range (by convention) to equal partitions and then calculate the integration of each and all area, then call uniform method on each partition according to the surface.
It's implemented in python: quad_vec(eval('scipy.stats.norm.pdf'), 1, 4,points=[0.5,2.5,3,4],full_output=True)I was working on a sign language detection project on jupyter notebook. While running the code for live detection I encountered an error as shown below: OpenCV(4.5.1) C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-1drr4hl0\opencv\modules\highgui\src\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage' The code that caused this error is: NB: I installed OpenCV using using pip install. Edit: This solution seems to work for a majority of users, but not
all. If you are in this case, see the proposed answer by
Sachin Mohan I had the exact same error using yolov5, on windows 10. Rebuilding the library by typing then worked for me. Few frustration hours later, saw this solution under the comment of the first answer by Karthik Thilakan This worked for me in the conda environment. Thanks Karthik! :) I installed another GPU and finally upgraded to Tensorflow 2 this week and suddenly, the same issue arose. I finally found my mistake and why uninstalling and reinstalling opencv works for some people. The issue is stated clearly in a text file in your opencv-python dist-packages named METADATA. It states; There are four different packages (see options 1, 2, 3 and 4 below) and you should SELECT ONLY ONE OF THEM. Do not install multiple different packages in the same environment. Further, the file says that; You should always use these packages if you do not use cv2.imshow et al. or you are using some other package (such as PyQt) than OpenCV to create your GUI. referring to Packages for server (headless) environments ... (with) no GUI library dependencies So, if you run; and the result is more than one opencv version, you've likely found your problem. While an uninstall & reinstall of opencv might solve your problem, a more masterful solution is to simply uninstall the headless version as that is the one that does not care about GUIs, as it should be used in server environments. I had the same problem when I wrote a similar program, but issue was with different versions of opencv packages. You can check them with the command: My output was: opencv-contrib-python         4.5.5.62 opencv-python                 4.5.5.62 opencv-python-headless        4.5.4.60 And it turned out that opencv-python-headless must be version 4.5.4 for the program to run properly. So the solution was to change the opencv-python version to be the same as opencv-python-headless. So in that case you can run: worked for me. I had this exact same issue a few weeks back and I'd like to perhaps complement some of the answers touching the headless elephant in the room. My complex project incorporates a few in-house subprojects by other colleagues. These tend to be developed and tested independently, so no cross-contamination occurs. However, since one of them used opencv-python and another went with opencv-python-headless, the final build installed both. THIS IS THE PROBLEM! Whenever I had both, a number of functions, particularly those pertaining visualisation, now failed. Worse: pip list revealed both opencv- versions installed! To make matters worse, whenever I uninstalled and installed again opencv-python (a simple --upgrade never worked, as it claimed the latest version was there and nothing needed upgrading), then it started working. We all hate witchcraft, so... I went down the compilation rabbit hole and obviously nothing good was there to be found. if you check into your .venv\Lib\site-packages, you'll find the following two folders: or whatever your version might be. These are the folders where pip gets its metadata from, but not where the actual code is. In fact, you don't do import opencv-..., but rather import cv2. You do import cv2 in both cases! In fact, -headless is a crippled drop-in for the real thing. So, if you look up in your list, you'll find a cv2 folder. Both libraries deposit their code in this folder. As we know, when it comes to saving files, the last on the scene wins. (Ok, I miss John Bercow.) Now, both libraries saving to the same folder, what is the order? Since they don't depend on one another, and in my case where poetry is being used to manage dependencies, alphabetical order is the default, and (drumroll) -headless comes last. At some point, I just decided to go nuts and remove -headless altogether. I am not the CV dev in the team, so I was just grasping for straws , but... it worked! That's when I looked int the whole drop-in thing. My colleagues were developing with a simple requirements.txt file, so when it came to gathering requirements in a nice proper pyproject.toml file, I just left the -headless option out. You can't have both. Whenever you have multi-part projects, I highly advise to run through the pip list after the environment is built and check for the couple. If you find both, always remove the -headless, as it is a subset of the main one. Achtung: check your .venv\pyvenv.cfg for a line with: This line means your project will be importing any libraries (other than the standard ones) from your global Python install and if you happen to have the -headless in the global environment, you're still in trouble. This error is mostly with Pycharm Ide , I resolved it by changing the project interpreter None of the given solution in the internet worked for me. This solved the issue for me: I was trying to move a set of files to my Windows10 from Ubuntu 18.04 LTD,  and running a cli for inference and the same error as mentioned in the opening post cropped up......I was checking on the versions of Open-CV and Open-CV Headless in both Ubuntu and Windows and they were exactly the same......While it was executing on Ubuntu, it threw the error in Windows......I removed Open-CV Headless and upgraded the Open-CV, and used the same set of commands and Windows started to execute the CLI for inferencing....... for streamlit cloud use opencv-python-headless
but in other platforms use opencv-python try: and then reinstall the OpenCV: you can also save image with single command and then open it from drive.
cv2.imwrite("TestImage.jpg",img) No need to waste time on cv2.imshow()I have constructed a CLDNN (Convolutional, LSTM, Deep Neural Network) structure for raw signal classification task. Each training epoch runs for about 90 seconds and the hyperparameters seems to be very difficult to optimize. I have been research various ways to optimize the hyperparameters (e.g. random or grid search) and found out about Bayesian Optimization. Although I am still not fully understanding the optimization algorithm, I feed like it will help me greatly. I would like to ask few questions regarding the optimization task. I would greatly appreciate any insights into this problem. Although I am still not fully understanding the optimization
  algorithm, I feed like it will help me greatly. First up, let me briefly explain this part.
Bayesian Optimization methods aim to deal with exploration-exploitation trade off in the multi-armed bandit problem. In this problem, there is an unknown function, which we can evaluate in any point, but each evaluation costs (direct penalty or opportunity cost), and the goal is to find its maximum using as few trials as possible. Basically, the trade off is this: you know the function in a finite set of points (of which some are good and some are bad), so you can try an area around the current local maximum, hoping to improve it (exploitation), or you can try a completely new area of space, that can potentially be much better or much worse (exploration), or somewhere in between. Bayesian Optimization methods (e.g. PI, EI, UCB), build a model of the target function using a Gaussian Process (GP) and at each step choose the most "promising" point based on their GP model (note that "promising" can be defined differently by different particular methods). Here's an example:  The true function is f(x) = x * sin(x) (black curve) on [-10, 10] interval. Red dots represent each trial, red curve is the GP mean, blue curve is the mean plus or minus one standard deviation. 
As you can see, the GP model doesn't match the true function everywhere, but the optimizer fairly quickly identified the "hot" area around -8 and started to exploit it. How do I set up the Bayesian Optimization with regards to a deep
  network? In this case, the space is defined by (possibly transformed) hyperparameters, usually a multidimensional unit hypercube.  For example, suppose you have three hyperparameters: a learning rate α in [0.001, 0.01], the regularizer λ in [0.1, 1] (both continuous) and the hidden layer size N in [50..100] (integer). The space for optimization is a 3-dimensional cube [0, 1]*[0, 1]*[0, 1]. Each point (p0, p1, p2) in this cube corresponds to a trinity (α, λ, N) by the following transformation: What is the function I am trying to optimize? Is it the cost of the
  validation set after N epochs? Correct, the target function is neural network validation accuracy. Clearly, each evaluation is expensive, because it requires at least several epochs for training. Also note that the target function is stochastic, i.e. two evaluations on the same point may slightly differ, but it's not a blocker for Bayesian Optimization, though it obviously increases the uncertainty. Is spearmint a good starting point for this task? Any other
  suggestions for this task? spearmint is a good library, you can definitely work with that. I can also recommend hyperopt. In my own research, I ended up writing my own tiny library, basically for two reasons: I wanted to code exact Bayesian method to use (in particular, I found a portfolio strategy of UCB and PI converged faster than anything else, in my case); plus there is another technique that can save up to 50% of training time called learning curve prediction (the idea is to skip full learning cycle when the optimizer is confident the model doesn't learn as fast as in other areas). I'm not aware of any library that implements this, so I coded it myself, and in the end it paid off. If you're interested, the code is on GitHub.I am trying to get Apple's sample Core ML Models that were demoed at the 2017 WWDC to function correctly. I am using the GoogLeNet to try and classify images (see the Apple Machine Learning Page). The model takes a CVPixelBuffer as an input. I have an image called imageSample.jpg that I'm using for this demo. My code is below: I am always getting the unexpected runtime error in the output rather than an image classification. My code to convert the image is below: I got this code from a previous StackOverflow post (last answer here). I recognize that the code may not be correct, but I have no idea of how to do this myself. I believe that this is the section that contains the error. The model calls for the following type of input: Image<RGB,224,224> You don't need to do a bunch of image mangling yourself to use a Core ML model with an image — the new Vision framework can do that for you. The WWDC17 session on Vision should have a bit more info — it's tomorrow afternoon. You can use a pure CoreML, but you should resize an image to (224,224) The expected image size for inputs you can find in the mimodel file:
 A demo project that uses both pure CoreML and Vision variants you can find here: https://github.com/handsomecode/iOS11-Demos/tree/coreml_vision/CoreML/CoreMLDemo If the input is UIImage, rather than an URL, and you want to use VNImageRequestHandler, you can use CIImage. From Classifying Images with Vision and Core MLI have an algorithm that is running on a set of objects. This algorithm produces a score value that dictates the differences between the elements in the set. The sorted output is something like this: [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230] If you lay these values down on a spreadsheet you see that they make up groups [1,1,5,6,1,5] [10,22,23,23] [50,51,51,52] [100,112,130] [500,512,600] [12000,12230] Is there a way to programatically get those groupings? Maybe some clustering algorithm using a machine learning library? Or am I overthinking this? I've looked at scikit but their examples are way too advanced for my problem... Clustering algorithms are designed for multivariate data. When you have 1-dimensional data, sort it, and look for the largest gaps. This is trivial and fast in 1d, and not possible in 2d. If you want something more advanced, use Kernel Density Estimation (KDE) and look for local minima to split the data set. There are a number of duplicates of this question: A good option if you don't know the number of clusters is MeanShift: Output for this algorithm: Modifying quantilevariable you can change the clustering number selection criteria You can use clustering to group these. The trick is to understand that there are two dimensions to your data: the dimension you can see, and the "spatial" dimension that looks like [1, 2, 3... 22]. You can create this matrix in numpy like so: Then you can perform clustering on the matrix, with: kclust's output will look like this: For you, the most interesting part is the first column of the matrix, which says what the centers are along that x dimension: You can then assign your points to a cluster based on which of the five centers they are closest to:Looking at an example 'solver.prototxt', posted on BVLC/caffe git, there is a training meta parameter What does this meta parameter mean? And what value should I assign to it? The weight_decay meta parameter govern the regularization term of the neural net. During training a regularization term is added to the network's loss to compute the backprop gradient. The weight_decay value determines how dominant this regularization term will be in the gradient computation.   As a rule of thumb, the more training examples you have, the weaker this term should be. The more parameters you have (i.e., deeper net, larger filters, larger InnerProduct layers etc.) the higher this term should be. Caffe also allows you to choose between L2 regularization (default) and L1 regularization, by setting However, since in most cases weights are small numbers (i.e., -1<w<1), the L2 norm of the weights is significantly smaller than their L1 norm. Thus, if you choose to use regularization_type: "L1" you might need to tune weight_decay to a significantly smaller value. While learning rate may (and usually does) change during training, the regularization weight is fixed throughout. Weight decay is a regularization term that penalizes big weights.
When the weight decay coefficient is big the penalty for big weights is also big, when it is small weights can freely grow. Look at this answer (not specific to caffe) for a better explanation:
Difference between neural net "weight decay" and "learning rate".I read this thread about the difference between SVC() and LinearSVC() in scikit-learn.  Now I have a data set of binary classification problem(For such a problem, the one-to-one/one-to-rest strategy difference between both functions could be ignore.) I want to try under what parameters would these 2 functions give me the same result. First of all, of course, we should set kernel='linear' for SVC()
However, I just could not get the same result from both functions. I could not find the answer from the documents, could anybody help me to find the equivalent parameter set I am looking for? Updated:
I modified the following code from an example of the scikit-learn website, and apparently they are not the same: Result:
Output Figure from previous code In mathematical sense you need to set: and Another element, which cannot be easily fixed is increasing intercept_scaling in LinearSVC, as in this implementation bias is regularized (which is not true in SVC nor should be true in SVM - thus this is not SVM) - consequently they will never be exactly equal (unless bias=0 for your problem), as they assume two different models Personally I consider LinearSVC one of the mistakes of sklearn developers - this class is simply not a linear SVM. After increasing intercept scaling (to 10.0)  However, if you scale it up too much - it will also fail, as now tolerance and number of iterations are crucial. To sum up: LinearSVC is not linear SVM, do not use it if do not have to.I have trained xor neural network in MATLAB and got these weights: Just from curiosity I have tried to write MATLAB code which computes the output of this network (two neurons in the hidden layer, and one in the output, TANSIG activation function). Code that I got: The problem is when input is lets say [1,1], it outputs -0.9989, when [0,1] 0.4902. While simulating network generated with MATLAB outputs adequately are 0.00055875 and 0.99943. What am I doing wrong? I wrote a simple example of an XOR network. I used newpr, which defaults  to tansig transfer function for both hidden and output layers. then we check the result by computing the output ourselves. The important thing to remember is that by default, inputs/outputs are scaled to the [-1,1] range: or more efficiently expressed as matrix product in one line: You usually don't use a sigmoid on your output layer--are you sure you should have the tansig on out3?  And are you sure you are looking at the weights of the appropriately trained network?  It looks like you've got a network trained to do XOR on [1,1] [1,-1] [-1,1] and [-1,-1], with +1 meaning "xor" and -1 meaning "same".I am building an image processing classifier and this code is an API to predict the image class of the image the whole code is running except this line (pred = model.predict_classes(test_image)) this API is made in Django framework and am using python 2.7 here is a point if I am running this code like normally ( without making an API) it's running perfectly Your test_image and input of tensorflow model is not match. The above is just assumption. If you want to debug, i guess you should print your image size and compare with first layout of your model definition. And check whe the size (width, height, depth) is matchhi I am building a image classifier for one-class classification in which i've used autoencoder while running this model I am getting this error by this line (autoencoder_model.fit) (ValueError: Error when checking target: expected model_2 to have shape (None, 252, 252, 1) but got array with shape (300, 128, 128, 3).) It's a simple incompatibility between the output shape of the decoder and the shape of your training data. (Target means output). I see you've got 2 MaxPoolings (dividing your image size by 4), and three upsamplings (multiplying the decoder's input by 8).  The final output of the autoencoder is too big and doesn't match your data. You must simply work in the model to make the output shape match your training data.  You're using wrong API Take a look at .fit method source code
from https://github.com/keras-team/keras/blob/master/keras/models.py So the x should be data, and the y should be label of the data.
Hope that helpWhen we train neural networks, we typically use gradient descent, which relies on a continuous, differentiable real-valued cost function. The final cost function might, for example, take the mean squared error. Or put another way, gradient descent implicitly assumes the end goal is regression - to minimize a real-valued error measure. Sometimes what we want a neural network to do is perform classification - given an input, classify it into two or more discrete categories. In this case, the end goal the user cares about is classification accuracy - the percentage of cases classified correctly. But when we are using a neural network for classification, though our goal is classification accuracy, that is not what the neural network is trying to optimize. The neural network is still trying to optimize the real-valued cost function. Sometimes these point in the same direction, but sometimes they don't. In particular, I've been running into cases where a neural network trained to correctly minimize the cost function, has a classification accuracy worse than a simple hand-coded threshold comparison. I've boiled this down to a minimal test case using TensorFlow. It sets up a perceptron (neural network with no hidden layers), trains it on an absolutely minimal dataset (one input variable, one binary output variable) assesses the classification accuracy of the result, then compares it to the classification accuracy of a simple hand-coded threshold comparison; the results are 60% and 80% respectively. Intuitively, this is because a single outlier with a large input value, generates a correspondingly large output value, so the way to minimize the cost function is to try extra hard to accommodate that one case, in the process misclassifying two more ordinary cases. The perceptron is correctly doing what it was told to do; it's just that this does not match what we actually want of a classifier. But the classification accuracy is not a continuous differentiable function, so we can't use it as the target for gradient descent. How can we train a neural network so that it ends up maximizing classification accuracy? How can we train a neural network so that it ends up maximizing classification accuracy? I'm asking for a way to get a continuous proxy function that's closer to the accuracy To start with, the loss function used today for classification tasks in (deep) neural nets was not invented with them, but it goes back several decades, and it actually comes from the early days of logistic regression. Here is the equation for the simple case of binary classification:  The idea behind it was exactly to come up with a continuous & differentiable function, so that we would be able to exploit the (vast, and still expanding) arsenal of convex optimization for classification problems. It is safe to say that the above loss function is the best we have so far, given the desired mathematical constraints mentioned above. Should we consider this problem (i.e. better approximating the accuracy) solved and finished? At least in principle, no. I am old enough to remember an era when the only activation functions practically available were tanh and sigmoid; then came ReLU and gave a real boost to the field. Similarly, someone may eventually come up with a better loss function, but arguably this is going to happen in a research paper, and not as an answer to a SO question... That said, the very fact that the current loss function comes from very elementary considerations of probability and information theory (fields that, in sharp contrast with the current field of deep learning, stand upon firm theoretical foundations) creates at least some doubt as to if a better proposal for the loss may be just around the corner. There is another subtle point on the relation between loss and accuracy, which makes the latter something qualitatively different than the former, and is frequently lost in such discussions. Let me elaborate a little... All the classifiers related to this discussion (i.e. neural nets, logistic regression etc) are probabilistic ones; that is, they do not return hard class memberships (0/1) but class probabilities (continuous real numbers in [0, 1]). Limiting the discussion for simplicity to the binary case, when converting a class probability to a (hard) class membership, we are implicitly involving a threshold, usually equal to 0.5, such as if p[i] > 0.5, then class[i] = "1". Now, we can find many cases whet this naive default choice of threshold will not work (heavily imbalanced datasets are the first to come to mind), and we'll have to choose a different one. But the important point for our discussion here is that this threshold selection, while being of central importance to the accuracy, is completely external to the mathematical optimization problem of minimizing the loss, and serves as a further "insulation layer" between them, compromising the simplistic view that loss is just a proxy for accuracy (it is not). As nicely put in the answer of this Cross Validated thread: the statistical component of your exercise ends when you output a probability for each class of your new sample. Choosing a threshold beyond which you classify a new observation as 1 vs. 0 is not part of the statistics any more. It is part of the decision component. Enlarging somewhat an already broad discussion: Can we possibly move completely away from the (very) limiting constraint of mathematical optimization of continuous & differentiable functions? In other words, can we do away with back-propagation and gradient descend? Well, we are actually doing so already, at least in the sub-field of reinforcement learning: 2017 was the year when new research from OpenAI on something called Evolution Strategies made headlines. And as an extra bonus, here is an ultra-fresh (Dec 2017) paper by Uber on the subject, again generating much enthusiasm in the community. I think you are forgetting to pass your output through a simgoid. Fixed below: The output:This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I have noticed that when One Hot encoding is used on a particular data set (a matrix) and used as training data for learning algorithms, it gives significantly better results with respect to prediction accuracy, compared to using the original matrix itself as training data. How does this performance increase happen?  Many learning algorithms either learn a single weight per feature, or they use distances between samples. The former is the case for linear models such as logistic regression, which are easy to explain. Suppose you have a dataset having only a single categorical feature "nationality", with values "UK", "French" and "US". Assume, without loss of generality, that these are encoded as 0, 1 and 2. You then have a weight w for this feature in a linear classifier, which will make some kind of decision based on the constraint w×x + b > 0, or equivalently w×x < b. The problem now is that the weight w cannot encode a three-way choice. The three possible values of w×x are 0, w and 2×w. Either these three all lead to the same decision (they're all < b or ≥b) or "UK" and "French" lead to the same decision, or "French" and "US" give the same decision. There's no possibility for the model to learn that "UK" and "US" should be given the same label, with "French" the odd one out. By one-hot encoding, you effectively blow up the feature space to three features, which will each get their own weights, so the decision function is now w[UK]x[UK] + w[FR]x[FR] + w[US]x[US] < b, where all the x's are booleans. In this space, such a linear function can express any sum/disjunction of the possibilities (e.g. "UK or US", which might be a predictor for someone speaking English). Similarly, any learner based on standard distance metrics (such as k-nearest neighbors) between samples will get confused without one-hot encoding. With the naive encoding and Euclidean distance, the distance between French and US is 1. The distance between US and UK is 2. But with the one-hot encoding, the pairwise distances between [1, 0, 0], [0, 1, 0] and [0, 0, 1] are all equal to √2. This is not true for all learning algorithms; decision trees and derived models such as random forests, if deep enough, can handle categorical variables without one-hot encoding. Regarding the increase of the features by doing one-hot-encoding one can use feature hashing. When you do hashing, you can specify the number of buckets to be much less than the number of the newly introduced features.   When you want to predict categories, you want to predict items of a set. Not using one-hot encoding is akin to letting the categories have neighbour categories (e.g.: if you did a regression with the integers of the categories instead) organized in a certain way and in a certain order.  Now, what happens if you assign category 0 to 0, category 1 to 1, and category 2 to 2 without one-hot encoding, and that your algorithm's prediction isn't sure if it should choose 0 or 2: should he predict 1 despite he thinks it's either 0 or 2? You see where it goes. The same goes for your data inputs: if they shouldn't be supposed to be neighbours, then don't show them to your algorithm as neighbours.Want to improve this question? Update the question so it can be answered with facts and citations by editing this post. Closed 2 years ago. If we have 10 eigenvectors then we can have 10 neural nodes in input layer.If we have 5 output classes then we can have 5 nodes in output layer.But what is the criteria for choosing number of hidden layer in a MLP and how many neural nodes in 1 hidden layer? how many hidden layers?  a model with zero hidden layers will resolve linearly separable data. So unless you already know your data isn't linearly separable, it doesn't hurt to verify this--why use a more complex model than the task requires? If it is linearly separable then a simpler technique will work, but a Perceptron will do the job as well. Assuming your data does require separation by a non-linear technique, then always start with one hidden layer. Almost certainly that's all you will need. If your data is separable using a MLP, then that MLP probably only needs a single hidden layer. There is theoretical justification for this, but my reason is purely empirical:  Many difficult classification/regression problems are solved using single-hidden-layer MLPs, yet I don't recall encountering any multiple-hidden-layer MLPs used to successfully model data--whether on ML bulletin boards, ML Textbooks, academic papers, etc. They exist, certainly, but the circumstances that justify their use is empirically quite rare. 
How many nodes in the hidden layer?  From the MLP academic literature. my own experience, etc., I have gathered and often rely upon several rules of thumb (RoT), and which I have also found to be reliable guides (ie., the guidance was accurate, and even when it wasn't, it was usually clear what to do next): RoT based on improving convergence: When you begin the model building, err on the side of more nodes
  in the hidden layer. Why? First, a few extra nodes in the hidden layer isn't likely do any any harm--your MLP will still converge. On the other hand, too few nodes in the hidden layer can prevent convergence. Think of it this way, additional nodes provides some excess capacity--additional weights to store/release signal to the network during iteration (training, or model building). Second, if you begin with additional nodes in your hidden layer, then it's easy to prune them later (during iteration progress). This is common and there are diagnostic techniques to assist you (e.g., Hinton Diagram, which is just a visual depiction of the weight matrices, a 'heat map' of the weight values,).  RoTs based on size of input layer and size of output layer: A rule of thumb is for the size of this [hidden] layer to be somewhere
  between the input layer size ... and the output layer size.... To calculate the number of hidden nodes we use a general rule of:
  (Number of inputs + outputs) x 2/3 RoT based on principal components: Typically, we specify as many hidden nodes as dimensions [principal
  components] needed to capture 70-90% of the variance of the input data
  set. And yet the NN FAQ author calls these Rules "nonsense" (literally) because they: ignore the number of training instances, the noise in the targets (values of the response variables), and the complexity of the feature space. In his view (and it always seemed to me that he knows what he's talking about), choose the number of neurons in the hidden layer based on whether your MLP includes some form of regularization, or early stopping.
 The only valid technique for optimizing the number of neurons in the Hidden Layer: During your model building, test obsessively; testing will reveal the signatures of "incorrect" network architecture. For instance, if you begin with an MLP having a hidden layer comprised of a small number of nodes (which you will gradually increase as needed, based on test results) your training and generalization error will both be high caused by bias and underfitting. Then increase the number of nodes in the hidden layer, one at a time, until the generalization error begins to increase, this time due to overfitting and high variance. In practice, I do it this way: input layer: the size of my data vactor (the number of features in my model) + 1 for the bias node and not including the response variable, of course output layer: soley determined by my model: regression (one node) versus classification (number of nodes equivalent to the number of classes, assuming softmax) hidden layer: to start, one hidden layer with a number of nodes equal to the size of the input layer. The "ideal" size is more likely to be smaller (i.e, some number of nodes between the number in the input layer and the number in the output layer) rather than larger--again, this is just an empirical observation, and the bulk of this observation is my own experience. If the project justified the additional time required, then I start with a single hidden layer comprised of a small number of nodes, then (as i explained just above) I add nodes to the Hidden Layer, one at a time, while calculating the generalization error, training error, bias, and variance. When generalization error has dipped and just before it begins to increase again, the number of nodes at that point is my choice. See figure below.  To automate the selection of the best number of layers and best number of neurons for each of the layers, you can use genetic optimization. The key pieces would be: You can also consider: It is very difficult to choose the number of neurons in a hidden layer, and to choose the number of hidden layers in your neural network. Usually, for most applications, one hidden layer is enough. Also, the number of neurons in that hidden layer should be between the number of inputs (10 in your example) and the number of outputs (5 in your example). But the best way to choose the number of neurons and hidden layers is experimentation. Train several neural networks with different numbers of hidden layers and hidden neurons, and measure the performance of those networks using cross-validation. You can stick with the number that yields the best performing network. Recently there is theoretical work on this https://arxiv.org/abs/1809.09953.  Assuming you use a RELU MLP, all hidden layers have the same number of nodes and your loss function and true function that you're approximating with a neural network obey some technical properties (in the paper), you can choose your depth to be of order $\log(n)$ and your width of hidden layers to be of order $n^{d/(2(\beta+d))}\log^2(n)$.  Here $n$ is your sample size, $d$ is the dimension of your input vector, and $\beta$ is a smoothness parameter for your true function.  Since $\beta$ is unknown, you will probably want to treat it as a hyperparameter. Doing this you can guarantee that with probability that converges to $1$ as function of sample size your approximation error converges to $0$ as a function of sample size.  They give the rate.  Note that this isn't guaranteed to be the 'best' architecture, but it can at least give you a good place to start with.  Further, my own experience suggests that things like dropout can still help in practice.It is not clear for  me the difference between loss function and metrics in Keras. The documentation was not helpful for me. The loss function is used to optimize your model. This is the function that will get minimized by the optimizer. A metric is used to judge the performance of your model. This is only for you to look at and has nothing to do with the optimization process. The loss function is that parameter one passes to Keras model.compile which is actually optimized while training the model . This loss function is generally minimized by the model. Unlike the loss function , the metric is another list of parameters passed to Keras model.compile which is actually used for judging the performance of the model. For example :   In classification problems, we want to minimize the cross-entropy loss, while also want to assess the model performance with the AUC. In this case, cross-entropy is the loss function and AUC is the metric. Metric is the model performance parameter that one can see while the model is judging itself on the validation set after each epoch of training. It is important to note that the metric is important for few Keras callbacks like EarlyStopping when one wants to stop training the model in case the metric isn't improving for a certaining no. of epochs. I have a contrived example in mind: Let's think about linear regression on a 2D-plane. In this case, loss function would be the mean squared error, the fitted line would minimize this error.  However, for some reason we are very very interested in the area under the curve from 0 to 1 of our fitted line, and thus this can be one of the metrics. And we monitor this metric while the model minimizes the mean squared error loss function.Here is my perceptron implementation in ANSI C: The training set I'm using: Data Set I have removed all irrelevant code. Basically what it does now it reads test1.txt file and loads values from it to three arrays: x, y, outputs. Then there is a perceptron learning algorithm which, for some reason, is not converging to 0 (globalError should converge to 0) and therefore I get an infinite do while loop. When I use a smaller training set (like 5 points), it works pretty well. Any ideas where could be the problem? I wrote this algorithm very similar to this C# Perceptron algorithm: EDIT: Here is an example with a smaller training set: In your current code, the perceptron successfully learns the direction of the decision boundary BUT is unable to translate it. (as someone pointed out, here is a more accurate version) The problem lies in the fact that your perceptron has no bias term, i.e. a third weight component connected to an input of value 1. The following is how I corrected the problem: ... with the following output: And here's a short animation of the code above using MATLAB, showing the decision boundary at each iteration:  It might help if you put the seeding of the random generator at the start of your main instead of reseeding on every call to randomFloat, i.e. Some small errors I spotted in your source code: Better change this to  so you doesn't have to rely on your x array to have the right size. You increase iterations inside the p loop, whereas the original C# code does this outside the p loop. Better move the printf and the iteration++ outside the p loop before the PAUSE statement - also I'd remove the PAUSE statement or change it to Even doing all those changes, your program still doesn't terminate using your data set, but the output is more consistent, giving an error oscillating somewhere between 56 and 60. The last thing you could try is to test the original C# program on this dataset, if it also doesn't terminate, there's something wrong with the algorithm (because your dataset looks correct, see my visualization comment). globalError will not become zero, it will converge to zero as you said, i.e. it will become very small. Change your loop like such: Give maxIterations and maxError values applicable to your problem.I've trained a sentiment classifier model using Keras library by following the below steps(broadly). Now for scoring using this model, I was able to save the model to a file and load from a file. However I've not found a way to save the Tokenizer object to file. Without this I'll have to process the corpus every time I need to score even a single sentence. Is there a way around this? The most common way is to use either pickle or joblib. Here you have an example on how to use pickle in order to save Tokenizer: Tokenizer class has a function to save date into JSON format: The data can be loaded using tokenizer_from_json function from keras_preprocessing.text: The accepted answer clearly demonstrates how to save the tokenizer. The following is a comment on the problem of (generally) scoring after fitting or saving. Suppose that a list texts is comprised of two lists Train_text and Test_text, where the set of tokens in Test_text is a subset of the set of tokens in Train_text (an optimistic assumption). Then fit_on_texts(Train_text) gives different results for texts_to_sequences(Test_text) as compared with first calling fit_on_texts(texts) and then text_to_sequences(Test_text). Concrete Example: Results: Of course, if the above optimistic assumption is not satisfied and the set of tokens in Test_text is disjoint from that of Train_test, then test 1 results in a list of empty brackets []. I've created the issue https://github.com/keras-team/keras/issues/9289 in  the keras Repo. Until the API is changed, the issue has a link to a gist that has code to demonstrate how to save and restore a tokenizer without having the original documents the tokenizer was fit on. I prefer to store all my model information in a JSON file (because reasons, but mainly mixed JS/Python environment), and this will allow for that, even with sort_keys=True  I found the following snippet provided at following link by @thusv89. Save objects: Load objects: Quite easy, because Tokenizer class has provided two funtions for save and load: save —— Tokenizer.to_json() load —— keras.preprocessing.text.tokenizer_from_json In to_json() method，it call "get_config" method which handle this:I am currently in the process of designing a recommender system for text articles (a binary case of 'interesting' or 'not interesting'). One of my specifications is that it should continuously update to changing trends.  From what I can tell, the best way to do this is to make use of machine learning algorithm that supports incremental/online learning.  Algorithms like the Perceptron and Winnow support online learning but I am not completely certain about Support Vector Machines. Does the scikit-learn python library support online learning and if so, is a support vector machine one of the algorithms that can make use of it? I am obviously not completely tied down to using support vector machines, but they are usually the go to algorithm for binary classification due to their all round performance. I would be willing to change to whatever fits best in the end. While online algorithms for SVMs do exist, it has become important to specify if you want kernel or linear SVMs, as many efficient algorithms have been developed for the special case of linear SVMs.  For the linear case, if you use the SGD classifier in scikit-learn with the hinge loss and L2 regularization you will get an SVM that can be updated online/incrementall. You can combine this with feature transforms that approximate a kernel to get similar to an online kernel SVM.  One of my specifications is that it should continuously update to changing trends. This is referred to as concept drift, and will not be handled well by a simple online SVM. Using the PassiveAggresive classifier will likely give you better results, as it's learning rate does not decrease over time.  Assuming you get feedback while training / running, you can attempt to detect decreases in accuracy over time and begin training a new model when the accuracy starts to decrease (and switch to the new one when you believe that it has become more accurate). JSAT has 2 drift detection methods (see jsat.driftdetectors) that can be used to track accuracy and alert you when it has changed.  It also has more online linear and kernel methods. (bias note: I'm the author of JSAT).  Maybe it's me being naive but I think it is worth mentioning how to actually update the sci-kit SGD classifier when you present your data incrementally: The short answer is no. Sklearn implementation (as well as most of the existing others) do not support online SVM training. It is possible to train SVM in  an incremental way, but it is not so trivial task. If you want to limit yourself to the linear case, than the answer is yes, as sklearn provides you with Stochastic Gradient Descent (SGD), which has option to minimize the SVM criterion. You can also try out pegasos library instead, which supports online SVM training. The problem of trend adaptation is currently very popular in ML community. As @Raff stated, it is called concept drift, and has numerous approaches, which are often kinds of meta models, which analyze "how the trend is behaving" and change the underlying ML model (by for example forcing it to retrain on the subset of the data). So you have two independent problems here: A way to scale SVM could be split your large dataset into batches that can be safely consumed by an SVM algorithm, then find support vectors for each batch separately, and then build a resulting SVM model on a dataset consisting of all the support vectors found in all the batches. Updating to trends could be achieved by maintaining a time window each time you run your training pipeline. For example, if you do your training once a day and there is enough information in a month's historical data, create your traning dataset from the historical data obtained in the recent 30 days. SGD for batch learning tasks normally has a decreasing learning rate and goes over training set multiple times. So, for purely online learning, make sure learning_rate is set to 'constant' in sklearn.linear_model.SGDClassifier() and eta0= 0.1 or any desired value. Therefore the process is as follows: If interested in online learning with concept drift then here is some previous work Learning under Concept Drift: an Overview
https://arxiv.org/pdf/1010.4784.pdf The problem of concept drift: definitions and related work
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.9085&rep=rep1&type=pdf A Survey on Concept Drift Adaptation
http://www.win.tue.nl/~mpechen/publications/pubs/Gama_ACMCS_AdaptationCD_accepted.pdf MOA Concept Drift Active Learning Strategies for Streaming Data
http://videolectures.net/wapa2011_bifet_moa/ A Stream of Algorithms for Concept Drift
http://people.cs.georgetown.edu/~maloof/pubs/maloof.heilbronn12.handout.pdf MINING DATA STREAMS WITH CONCEPT DRIFT
http://www.cs.put.poznan.pl/dbrzezinski/publications/ConceptDrift.pdf Analyzing time series data with stream processing and machine learning
http://www.ibmbigdatahub.com/blog/analyzing-time-series-data-stream-processing-and-machine-learningI have web application written in Flask. As suggested by everyone, I can't use Flask in production. So I thought of Gunicorn with Flask.   In Flask application I am loading some Machine Learning models. These are of size 8GB collectively. Concurrency of my web application can go upto 1000 requests. And the RAM of machine is 15GB.
So what is the best way to run this application? You can start your app with multiple workers or async workers with Gunicorn. Flask server.py Gunicorn with gevent async worker Gunicorn 1 worker 12 threads: Gunicorn with 4 workers (multiprocessing): More information on Flask concurrency in this post: How many concurrent requests does a single Flask process receive?. The best thing to do is to use pre-fork mode (preload_app=True). This will initialize your code in a "master" process and then simply fork off worker processes to handle requests. If you are running on linux and assuming your model is read-only, the OS is smart enough to reuse the physical memory amongst all the processes.What are the differences between all these cross-entropy losses? Keras is talking about While TensorFlow has What are the differences and relationships between them? What are the typical applications for them? What's the mathematical background? Are there other cross-entropy types that one should know? Are there any cross-entropy types without logits? There is just one cross (Shannon) entropy defined as: In machine learning usage, P is the actual (ground truth) distribution, and Q is the predicted distribution. All the functions you listed are just helper functions which accepts different ways to represent P and Q. There are basically 3 main things to consider: there are either 2 possibles outcomes (binary classification) or more. If there are just two outcomes, then Q(X=1) = 1 - Q(X=0) so a single float in (0,1) identifies the whole distribution, this is why neural network in binary classification has a single output (and so does logistic regresssion). If there are K>2 possible outcomes one has to define K outputs (one per each Q(X=...)) one either produces proper probabilities (meaning that Q(X=i)>=0 and SUM_i Q(X=i) =1 or one just produces a "score" and has some fixed method of transforming score to probability. For example a single real number can be "transformed to probability" by taking sigmoid, and a set of real numbers can be transformed by taking their softmax and so on. there is j such that P(X=j)=1 (there is one "true class", targets are "hard", like "this image represent a cat") or there are "soft targets" (like "we are 60% sure this is a cat, but for 40% it is actually a dog"). Depending on these three aspects, different helper function should be used: In the end one could just use "categorical cross entropy", as this is how it is mathematically defined, however since things like hard targets or binary classification are very popular - modern ML libraries do provide these additional helper functions to make things simpler. In particular "stacking" sigmoid and cross entropy might be numerically unstable, but if one knows these two operations are applied together - there is a numerically stable version of them combined (which is implemented in TF). It is important to notice that if you apply wrong helper function the code will usually still execute, but results will be wrong. For example if you apply softmax_* helper for binary classification with one output your network will be considered to always produce "True" at the output. As a final note - this answer considers classification, it is slightly different when you consider multi label case (when a single point can have multiple labels), as then Ps do not sum to 1, and one should use sigmoid_cross_entropy_with_logits despite having multiple output units. For this purpose, "logits" can be seen as the non-activated outputs of the model. Losses "with logits" will apply the activation internally.
Some functions allow you to choose logits=True or logits=False, which will tell the function whether to "apply" or "not apply" the activations.Sometimes I run into a problem: e.g. Where 1024 is my batch size and I don't know what's the rest. If I reduce the batch size or the number of neurons in the model, it runs fine. Is there a generic way to calculate optimal batch size based on model and GPU memory, so the program doesn't crash? In short: I want the largest batch size possible in terms of my model, which will fit into my GPU memory and won't crash the program. From the recent Deep Learning book by Goodfellow et al., chapter 8: Minibatch sizes are generally driven by the following factors: Which in practice usually means "in powers of 2 and the larger the better, provided that the batch fits into your (GPU) memory". You might want also to consult several good posts here in Stack Exchange: Just keep in mind that the paper by Keskar et al. 'On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima', quoted by several of the posts above, has received some objections by other respectable researchers of the deep learning community. Hope this helps... UPDATE (Dec 2017): There is a new paper by Yoshua Bengio & team, Three Factors Influencing Minima in SGD (Nov 2017); it is worth reading in the sense that it reports new theoretical & experimental results on the interplay between learning rate and batch size. UPDATE (Mar 2021): Of interest here is also another paper from 2018, Revisiting Small Batch Training for Deep Neural Networks (h/t to Nicolas Gervais), which runs contrary to the larger the better advice; quoting from the abstract: The best performance has been consistently obtained for mini-batch sizes between m=2 and m=32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands. You can estimate the largest batch size using: Max batch size= available GPU memory bytes / 4 / (size of tensors + trainable parameters) Use the summaries provided by pytorchsummary (pip install) or keras (builtin). E.g. Each instance you put in the batch will require a full forward/backward pass in memory, your model you only need once. People seem to prefer batch sizes of powers of two, probably because of automatic layout optimization on the gpu. Don't forget to linearly increase your learning rate when increasing the batch size. Let's assume we have a Tesla P100 at hand with 16 GB memory. Here is a function to find batch size for training the model: I ran into a similar GPU mem error which was solved by configuring the tensorflow session with the following: see: google colaboratory `ResourceExhaustedError` with GPUI trained my CNN (VGG) through google colab and generated .h5 file. Now problem is, I can predict my output successfully through google colab but when i download that .h5 trained model file and try to predict output on my laptop, I am getting error when loading the model. Here is the code: And the error: I ran into the same issue. After changing: from tensorflow import keras to: import keras life is once again worth living. I fixed the problem: Before: Works for me Wow I, just spent 6 Hours of my life trying to figure this out.. Dmitri posted a solution to this here: I trained a keras model on google colab. Now not able to load it locally on my system. I'm just basically reposting it here because it worked for me. This looks like some kind of a serialization bug in keras. 
If you wrap your load_model with the below CustomObjectScope thingy... all should work.. Changing to  solved my problem! To eliminate errors, import all things directly from Keras or TensorFlow. Mixing both of them in same project may result in problems. I had a same problem and was fixed this way. just don't save the optimizer with the model!
just change the save line like this: Second parameter tells Keras to overwrite the model if the file existed or not and the 3rd one tells it not to save the optimizer with the model. Edit:
I ran over the problem again on another system today and this did not helped me this time. so i saved the model conf as json and weights as h5 and used them to rebuild the model in another machine. you can do it like this.
save like this: rebuild the model like this: Something that helped me which wasn't in any of the answers: custom_objects={'GlorotUniform': glorot_uniform()} In either kaggle or colabs works well this worked for me when importing tensorflow keras if you are loading the architecture and weights separtly, while loading archtiecture of the model change : to :  and the problem is solved I had the same problem with a model built with tensorflow 1.11.0 (using tensorflow.python.keras.models.save_model) and loaded with tensoflow 1.11.0 (using tensorflow.python.keras.models.load_model). I solved it by upgrading everything to tensorflow 1.13.1, after building the model again with the new version, I could load it without this error. For the json file problem mentioned by @Derk in one of the comment, you can write the following: and in your import line, remember to write: instead of from keras.initializers import glorot_uniform. It worked out for me when I try to read a model saved in tf2.2 in the environment with only tf1.9.What is the difference between  and  in TensorFlow? What would be different in your computation graph when you construct your graph with + instead of tf.add()?  More generally, are  + or other operations overloaded for tensors? If at least one of x or y is a tf.Tensor object, the expressions tf.add(x, y) and x + y are equivalent. The main reason you might use tf.add() is to specify an explicit name keyword argument for the created op, which is not possible with the overloaded operator version. Note that if neither x nor y is a tf.Tensor—for example if they are NumPy arrays—then x + y will not create a TensorFlow op. tf.add() always creates a TensorFlow op and converts its arguments to tf.Tensor objects. Therefore, if you are writing a library function that might accept both tensors and NumPy arrays, you might prefer to use tf.add(). The following operators are overloaded in the TensorFlow Python API: Please note, __eq__ ( binary == ) is not overloaded. x == y will simply return a Python boolean whether x and y refer to the same tensor. You need to use tf.equal() explicitly to check for element-wise equality. Same goes for not equal, __ne__ ( binary != ). Mrry nicely explained that there is no real difference. I will just add when using tf.add is beneficial. tf.add has one important parameter which is name. It allows you to name the operation in a graph which will be visible in tensorboard. So my rule of thumb, if it will be beneficial to name an operation in tensorboard, I use tf. equivalent, otherwise I go for brevity and use overloaded version.I have a image with horizontal and vertical lines. In fact, this image is the BBC website converted to horizontal and vertical lines.
My problem is that I want to be able to find all the rectangles in the image. I want to write a computer program to find all the rectangles.
Does anyone know how to do this or suggest ideas on how to get started? This task is easy for me as a person to find the visual rectangles, but I am not sure how to describe it as a program. Image is the BBC website here http://www.bbc.co.uk/ Update to this, I wrote the code which converts the BBC website image to the horizontal and vertical line, the problem is these lines do not completely meet at the corners and sometimes they do not completely form a rectangle. Thanks! Opencv (image processing and computer vision library written in c) has implementation for hough transform (the simple hough transform find lines in an image, while the generalized one finds more complex objects) so that could be a good start. For the rectangles which do have closed corners there are also corner detectors such as cornerHarris which can help. I ran the houghlines demo provided with opencv and here's the result on the image you gave (detected lines marked in red):

(source: splintec.com)  I believe you are looking for the generalized Hough transform. In computer vision there is a algorithm called Generalized Hough Transform which maybe can solve your problem. There should be open source code having implemented this algorithm. Just search for it. Assuming it's a reasonably noise free image (not a video of a screen) then one of the simple floodfill algorithms should work. You might need to run a dilate/erode on the image to close up the gaps. The normal way to find lines is a Hough transform ( then find lines at right angles) 
Opencv is the easiest way. Take a look at this question OpenCV Object Detection - Center Point There are several different approaches to your problem. I'd use a morphological image processing tool like this one. You will have the flexibility to define "rectangle" even something that not "exactly closed" (where the fill algorithm will fail). Another possibility could be to use a machine learning approach, which basically is more data-driven than definition-driven like the previous one. You'll have to give your algorithm several "examples" of what a rectangle is, and it will eventually learn (with a bias and an error rate). iterate from left to right until you hit a color pixel then use modified flood fill algorithm. more info on the algo flood fill @ wiki another approach would be to find ANY colored pixel on the image then go with then do the same upwards.
now u have defined a single line. then use ends of the lines to approx match lines into rectangles. if they are not pixel perfect you could do some kind of tresholding. The flood fill would work, or you could use a modification of an edge tracking algorithm. what you do is:
create a 2d array (or any other d2 data struct)- each row represents a horizontal pixel line on screen, and each column a vertical line iterate through all the pixels, left to right, and whenever you find a coloured one add its coordinates to the array iterate through the array and findying lines and storing the begin and end pixel for each one (different data structure) knowing that the begin of each line is its left/top pixel, you can easily check to see if any 4 lines comprise a rectangle To get from the image you have with the nearly touching horizontal and vertical lines to just the rectangles: This will, with a bit of luck, first show the boxes with thick fat lines, leaving thick fat artifacts all over the image (after step 3) and then then after step 5 all thick fat artifacts will have been removed, while all boxes remain. You need to tweek the number of repeats in step 3 for best results. If you're interested in image morphology, this is the book of a really good introductory course I took.  Sample: (0=black, 1=white, pixels in the center of each 3x3 block are being considered, input left, output right)After spending a couple days trying to achieve this task, I would like to share my experience of how I went about answering the question: How do I use TS Object Detection to train using my own dataset? This assumes the module is already installed. Please refer to their documentation if not. Disclaimer This answer is not meant to be the right or only way of training the object detection module. This is simply I sharing my experience and what has worked for me. I'm open to suggestions and learning more about this as I am still new to ML in general. TL;DR Each section of this answer consists of a corresponding Edit (see below). After reading each section, please read its Edit as well for clarifications. Corrections and tips were added for each section. Tools used LabelImg: A tool for creating PASCAL VOC format annotations. 1. Create your own PASCAL VOC dataset PS: For simplicity, the folder naming convention of my answer follows that of Pascal VOC 2012 A peek into the May 2012 dataset, you'll notice the folder as having the following structure 
+VOCdevkit
  +VOC2012
    +Annotations
    +ImageSets
      +Action
      +Layout
      +Main
      +Segmentation
    +JPEGImages
    +SegmentationClass
    +SegmentationObject
 For the time being, amendments were made to the following folders: Annotations: This is were all the images' corresponding XML files will be placed in. Use the suggested tool above to create the annotations. Do not worry about <truncated> and <difficulty> tags as they will be ignored by the training and eval binaries. JPEGImages: Location of your actual images. Make sure they are of type JPEG because that's what is currently supported in order to create TFRecords using their provided script. ImageSets->Main: This simply consists of text files. For each class, there exists a corresponding train.txt, trainval.txt and val.txt. Below is a sample of the contents of the aeroplane_train.txt in the VOC 2012 folder The structure is basically image name followed by a boolean saying whether the corresponding object exists in that image or not. Take for example image 2008_000008 does not consist of an aeroplane hence marked with a -1 but image 2008_000033 does. I wrote a small Python script to generate these text files. Simply iterate through the image names and assign a 1 or -1 next to them for object existence. I added some randomness among my text files by shuffling the image names. The {classname}_val.txt files consist of the testing validation datasets. Think of this as the test data during training. You want to divide your dataset into training and validation. More info can be found here. The format of these files is similar to that of training. At this point, your folder structure should be 
+VOCdevkit
  +VOC2012
    +Annotations
     --(for each image, generated annotation)
    +ImageSets
      +Main
        --(for each class, generated *classname*_train.txt and *classname*_val.txt)
    +JPEGImages
     --(a bunch of JPEG images)
 1.1 Generating label map With the dataset prepared, we need to create the corresponding label maps.
Navigate to models/object_detection/data and open pascal_label_map.pbtxt. This file consists of a JSON that assigns an ID and name to each item. Make amendments to this file to reflect your desired objects. 2. Generate TFRecords If you look into their code especially this line, they explicitly grab the aeroplane_train.txt only. For curios minds, here's why. Change this file name to any of your class train text file. Make sure VOCdevkit is inside models/object_detection then you can go ahead and generate the TFRecords. Please go through their code first should you run into any problems. It is self explanatory and well documented. 3. Pipeline Configuration The instructions should be self explanatory to cover this segment. Sample configs can be found in object_detection/samples/configs. For those looking to train from scratch as I did, just make sure to remove the fine_tune_checkpoint and from_detection_checkpoint nodes. Here's what my config file looked like for reference. From here on you can continue with the tutorial and run the training process. 4. Visualize Be sure to run the eval in parallel to the training in order to be able to visualize the learning process. To quote Jonathan Huang the best way is to just run the eval.py binary. We typically run this
  binary in parallel to training, pointing it at the directory holding
  the checkpoint that is being trained. The eval.py binary will write
  logs to an eval_dir that you specify which you can then point to
  with Tensorboard. You want to see that the mAP has "lifted off" in the first few hours,
  and then you want to see when it converges. It's hard to tell without
  looking at these plots how many steps you need. EDIT I (28 July '17): I never expected my response to get this much attention so I decided to come back and review it. Tools For my fellow Apple users, you could actually use RectLabel for annotations. Pascal VOC After digging around, I finally realized that trainval.txt is actually the union of training and validation datasets. Please look at their official development kit to understand the format even better. Label Map Generation At the time of my writing, ID 0 represents none_of_the_above. It is recommended that your IDs start from 1. Visualize After running your evaluation and directed tensorboard to your Eval directory, it'll show you the mAP of each category along with each category's performance. This is good but I like seeing my training data as well in parallel with Eval. To do this, run tensorboard on a different port and point it to your train directory I wrote a blog post on Medium about my experience as well on how I trained an object detector (in particular, it's a Raccoon detector) with Tensorflow on my own dataset. This might also be useful for others and is complimentary to eshirima's answer.I want to create my own transformer for use with the sklearn Pipeline. I am creating a class that implements both fit and transform methods. The purpose of the transformer will be to remove rows from the matrix that have more than a specified number of NaNs. The issue I am facing is how can I change both the X and y matrices that are passed to the transformer? I believe this has to be done in the fit method since it has access to both X and y. Since python passes arguments by assignment once I reassign X to a new matrix with fewer rows the reference to the original X is lost (and of course the same is true for y). Is it possible to maintain this reference? I’m using a pandas DataFrame to easily drop the rows that have too many NaNs, this may not be the right way to do it for my use case. The current code looks like this: Modifying the sample axis, e.g. removing samples, does not (yet?) comply with the scikit-learn transformer API. So if you need to do this, you should do it outside any calls to scikit learn, as preprocessing. As it is now, the transformer API is used to transform the features of a given sample into something new. This can implicitly contain information from other samples, but samples are never deleted. Another option is to attempt to impute the missing values. But again, if you need to delete samples, treat it as preprocessing before using scikit learn. You have to modify the internal code of sklearn Pipeline. We define a transformer that removes samples where at least the value of a feature or the target is NaN during fitting (fit_transform). While it removes the samples where at least the value of a feature is NaN during inference (transform). Important to note that our transformer returns X and y in fit_transform so we need to handle this behaviour in the sklearn Pipeline. We only have to modify the original sklearn Pipeline in only two specific points in fit and in _fit method. The rest remains unchanged. This is required in order to unpack the values generated by Dropna().fit_transform(X, y) in the new X and y. Here is the full pipeline at work: Another trial with a further intermediate preprocessing step: More complex behaviors can be achieved with other simple modifications according to the needs. If you are interested also in Pipeline().fit_transform or Pipeline().fit_predict you need to operate the same changes. The package imblearn, which is built on top of sklearn, contains an estimator FunctionSampler that allows manipulating both the features array, X, and target array, y, in a pipeline step. Note that using it in a pipeline step requires using the Pipeline class in imblearn that inherits from the one in sklearn. Furthermore, by default, in the context of Pipeline, the method resample does nothing when it is not called immediately after fit (as in fit_resample). So, read the documentation ahead of time. Adding to @João Matias response: Here's an example of using imblearn to define a pipeline step that drops rows with missing values: Note, you have to use the imblearn pipeline. You can solve this easily by using the sklearn.preprocessing.FunctionTransformer method (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) You just need to put your alternations to X in a function then you get your transformer by calling  which you can use in the pipeline. The threshold can be set outside the drop_nans function. @eickenberg is the proper and clean answer. Nevertheless, I like to keep everything into one Pipeline, so if you are interested, I created a library (not yet deployed on pypi) that allow to apply transformation on Y: https://gitlab.com/thibaultB/transformers/ Usage is the following: Using this code, you can alter the number of rows if you put all the transformer that modify the numbers of rows before the "SplitXY" transformer. Transformer before the SplitXY transformer should keep columns name, it is why I also added a SklearnPandasWrapper that wrap sklearn transformer (that usually return numpy array) to keep columns name. You can use function transformer   Use "deep-copies" further on, down the pipeline and X, y remain protected .fit() can first assign on each call deep-copy to new class-variables and then reduce / transform these not to have more NaN-s than ordered by self.tresholdI'm using R to do machine learning. Following standard machine learning methodology, I would like to randomly split my data into training, validation, and test data sets. How do I do that in R? I know there are some related questions on how to split into 2 data sets (e.g. this post), but it is not obvious how to do it for 3 split data sets. By the way, the correct approach is to use 3 data sets (including a validation set to tune your hyperparameters). This linked approach for two groups (using floor) doesn't extend naturally to three. I'd do  To check the results: With set.seed(1) run just before, the result looks like Data.frames can be accessed like res$test or res[["test"]]. cut is the standard tool for partitioning based on shares. Following the approach shown in this post, here is working R code to divide a dataframe into three new dataframes for testing, validation, and test. The three subsets are non-overlapping. Some of these seem overly complex, here's a simple way using sample to split any dataset into 3, or even an arbitrary number of sets. If you'd rather reusable code: Here is one solution with a 60, 20 , 20 split that also ensures that there is no overlapping. However it is a trouble to adapt the split. If anyone could help me out, I appreciate it Caret also support data splitting with the function createDataPartition if your outcome y is Unbalanced factor ( yes >>> No and vice versa), ideally the random sampling occurs within each class and should preserve the overall class distribution of the data.
which is the case with createDataPartition Example: Note our outcome is unbalanced Splitting (80% train and 20% test): Verification: Note we preserve the overall class distribution I think my approach is the easiest one: First, it splits the data into 70% training data and the rest (idxNotTrain).
Then, the rest is again splitted into a validation data set (33%, 10% of the total data) and the rest (the testing data, 66%, 20% of the total data). Let me know if this would work. Just a simplified versionIn Keras (with Tensorflow backend), is the current input pattern available to my custom loss function? The current input pattern is defined as the input vector used to produce the prediction. For example, consider the following: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=False). Then the current input pattern is the current X_train vector associated with the y_train (which is termed y_true in the loss function). When designing a custom loss function, I intend to optimize/minimize a value that requires access to the current input pattern, not just the current prediction. I've taken a look through https://github.com/fchollet/keras/blob/master/keras/losses.py I've also looked through "Cost function that isn't just y_pred, y_true?" I am also familiar with previous examples to produce a customized loss function: Presumably (y_true,y_pred) are defined elsewhere. I've taken a look through the source code without success and I'm wondering whether I need to define the current input pattern myself or whether this is already accessible to my loss function. You can wrap the loss function as a inner function and pass your input tensor to it (as commonly done when passing additional arguments to the loss function). You can verify that input_tensor and the loss value (mostly, the K.mean(input_tensor) part) will change as different X is passed to the model. You can use add_loss to pass external layers to your loss, in your case the input tensor. Here an example: To use the model in inference mode (removing the target from inputs)Actually, there is a contradiction of 2 facts that are the possible answers to the question: The conventional answer is to do it after splitting as there can be information leakage, if done before, from the Test-Set. The contradicting answer is that, if only the Training Set chosen from the whole dataset is used for Feature Selection, then the feature selection or feature importance score orders is likely to be dynamically changed with change in random_state of the Train_Test_Split. And if the feature selection for any particular work changes, then no Generalization of Feature Importance can be done, which is not desirable. Secondly, if only Training Set is used for feature selection, then the test set may contain certain set of instances that defies/contradicts the feature selection done only on the Training Set as the overall historical data is not analyzed. Moreover, feature importance scores can only be evaluated when, given a set of instances rather than a single test/unknown instance. It is not actually difficult to demonstrate why using the whole dataset (i.e. before splitting to train/test) for selecting features can lead you astray. Here is one such demonstration using random dummy data with Python and scikit-learn: Since our data X are random ones (500 samples, 10,000 features) and our labels y are binary, we expect than we should never be able to exceed the baseline accuracy for such a setting, i.e. ~ 0.5, or around 50%. Let's see what happens when we apply the wrong procedure of using the whole dataset for feature selection, before splitting: Wow! We get 76% test accuracy on a binary problem where, according to the very basic laws of statistics, we should be getting something very close to 50%! Someone to call the Nobel Prize committee, and fast... ... the truth of course is that we were able to get such a test accuracy simply because we have committed a very basic mistake: we mistakenly think that our test data is unseen, but in fact the test data have already been seen by the model building process during feature selection, in particular here: How badly off can we be in reality? Well, again it is not difficult to see: suppose that, after we have finished with our model and we have deployed it (expecting something similar to 76% accuracy in practice with new unseen data), we get some really new data: where of course there is not any qualitative change, i.e. new trends or anything - these new data are generated by the very same underlying procedure. Suppose also we happen to know the true labels y, generated as above: How will our model perform here, when faced with these really unseen data? Not difficult to check: Well, it's true: we sent our model to the battle, thinking that it was capable of a ~ 76% accuracy, but in reality it performs just as a random guess... So, let's see now the correct procedure (i.e. split first, and select the features based on the training set only): Where the test accuracy 0f 0.528 is close enough to the theoretically predicted one of 0.5 in such a case (i.e. actually random guessing). Kudos to Jacob Schreiber  for providing the simple idea (check all the thread, it contains other useful examples), although in a slightly different context than the one you ask about here (cross-validation):  The conventional answer #1 is correct here; the arguments in the contradicting answer #2 do not actually hold. When having such doubts, it is useful to imagine that you simply do not have any access in any test set during the model fitting process (which includes feature importance); you should treat the test set as literally unseen data (and, since unseen, they could not have been used for feature importance scores). Hastie & Tibshirani have clearly argued long ago about the correct & wrong way to perform such processes; I have summarized the issue in a blog post, How NOT to perform feature selection! - and although the discussion is about cross-validation, it can be easily seen that the arguments hold for the case of train/test split, too. The only argument that actually holds in your contradicting answer #2 is that the overall historical data is not analyzed Nevertheless, this is the necessary price to pay in order to have an independent test set for performance assessment, otherwise, with the same logic, we should use the test set for training, too, shouldn't we? Wrap up: the test set is there solely for performance assessment of your model, and it should not be used in any stage of model building, including feature selection.  UPDATE (after comments): the trends in the Test Set may be different A standard (but often implicit) assumption here is that the training & test sets are qualitatively similar; it is exactly due to this assumption that we feel OK to just use simple random splits to get them. If we have reasons to believe that our data change in significant ways (not only between train & test, but during model deployment, too), the whole rationale breaks down, and completely different approaches are required. Also, on doing so, there can be a high probability of Over-fitting The only certain way of overfitting is to use the test set in any way during the pipeline (including for feature selection, as you suggest). Arguably, the linked blog post has enough arguments (including quotes & links) to be convincing. Classic example, the testimony in The Dangers of Overfitting or How to Drop 50 spots in 1 minute: as the competition went on, I began to use much more feature selection and preprocessing. However, I made the classic mistake in my cross-validation method by not including this in the cross-validation folds (for more on this mistake, see this short description or section 7.10.2 in The Elements of Statistical Learning). This lead to increasingly optimistic cross-validation estimates. As I have already said, although the discussion here is about cross-validation, it should not be difficult to convince yourself that it perfectly applies to the train/test case, too. feature selection should be done in such a way that Model Performance is enhanced Well, nobody can argue with this, of course! The catch is - which exact performance are we talking about? Because the Kaggler quoted above was indeed getting better "performance" as he was going along (applying a mistaken procedure), until his model was faced with real unseen data (the moment of truth!), and it unsurprisingly flopped. Admittedly, this is not trivial stuff, and it may take some time until you internalize them (it's no coincidence that, as Hastie & Tibshirani demonstrate, there are even research papers where the procedure is performed wrongly). Until then, my advice to keep you safe, is: during all stages of model building (including feature selection), pretend that you don't have access to the test set at all, and that it becomes available only when you need to assess the performance of your final model.I need a somehow descriptive example showing how to do a 10-fold SVM classification on a two class set of data. there is just one example in the MATLAB documentation but it is not with 10-fold. Can someone help me? Here's a complete example, using the following functions from the Bioinformatics Toolbox: SVMTRAIN, SVMCLASSIFY, CLASSPERF, CROSSVALIND. with the output: we obtained 99.33% accuracy with only one 'setosa' instance mis-classified as 'non-setosa' UPDATE: SVM functions have moved to Statistics toolbox in R2013aI am trying to approximate the sine() function using a neural network I wrote myself. I have tested my neural network on a simple OCR problem already and it worked, but I am having trouble applying it to approximate sine(). My problem is that during training my error converges on exactly 50%, so I'm guessing it's completely random. I am using one input neuron for the input (0 to PI), and one output neuron for the result. I have a single hidden layer in which I can vary the number of neurons but I'm currently trying around 6-10. I have a feeling the problem is because I am using the sigmoid transfer function (which is a requirement in my application) which only outputs between 0 and 1, while the output for sine() is between -1 and 1. To try to correct this I tried multiplying the output by 2 and then subtracting 1, but this didn't fix the problem. I'm thinking I have to do some kind of conversion somewhere to make this work. Any ideas? Use a linear output unit. Here is a simple example using R:  When you train the network, you should normalize the target (the sin function) to the range [0,1], then you can keep the sigmoid transfer function. Note that that we mapped the target before training. Once you train and simulate the network, you can map back the output of the net. The following is a MATLAB code to illustrate: 
 There is no reason your network shouldn't work, although 6 is definitely on the low side for approximating a sine wave. I'd try at least 10 maybe even 20. If that doesn't work then I think you need to give more detail about your system. i.e. the learning algorithm (back-propagation?), the learning rate etc. I get the same behavior if use vanilla gradient descent. Try using a different training algorithm. As far as the Java applet is concerned, I did notice something interesting: it does converge if I use a "bipolar sigmoid" and I start with some non-random weights (such as results from a previous training using a Quadratic function).I am getting a error when I try to use confusion matrix. I am doing my first deep learning project. I am new to it. I am using the mnist dataset provided by keras. I have trained and tested my model successfully.  However, when I try to use the scikit learn confusion matrix I get the error stated above. I have searched for an answer and while there are answers on this error, none of them worked for me. From what I found online it probably has something to do with the loss function (I use the categorical_crossentropy in my code). I tried changing it to sparse_categorical_crossentropy but that just gave me the  when I run the fit() function on the model.  This is the code. (I have left out the imports for the sake of brevity) How can i fix this?  Confusion matrix needs both labels & predictions as single-digits, not as one-hot encoded vectors; although you have done this with your predictions using model.predict_classes(), i.e. your test_labels are still one-hot encoded: So, you should convert them too to single-digit ones, as follows: After which, the confusion matrix should come up OK: The same problem is repeated here, and the solution is overall the same. That's why, that question is closed and unable to receive an answer. So I like to add an answer to this question here (hope that's not illegal). The below code is self-explanatory. @desertnaut gave exact reasons, so no need to explain more stuff. The author of the question tried to pass predicted features separately to the fit functions, which I believe can give a better understanding to the newcomer. Extract features from pre-trained weights (Transfer Learning). Reshape for further training process. The model with sequential API. Compile and Run. Evaluate.I have two numpy arrays light_points and time_points and would like to use some time series analysis methods on those data. I then tried this : This works but is not doing the correct thing.
Indeed, the measurements are not evenly time-spaced and if I just declare the time_points pandas DataFrame as the index of my frame, I get an error : I don't know how to correct this.
Also, it seems that pandas' TimeSeries are deprecated. I tried this : But it gives me a length mismatch : Nevertheless, I don't understand where it comes from, as rdf['light'] and 
tdf['time'] are of same length... Eventually, I tried by defining my rdf as a pandas Series : And I get this : Then, I tried instead replacing the index by  And it gives me an error on the seasonal_decompose method line : How can I work with unevenly spaced data ?
I was thinking about creating an approximately evenly spaced time array by adding many unknown values between the existing values and using interpolation to "evaluate" those points, but I think there could be a cleaner and easier solution. seasonal_decompose() requires a freq that is either provided as part of the DateTimeIndex meta information, can be inferred by pandas.Index.inferred_freq or else by the user as an integer that gives the number of periods per cycle. e.g., 12 for monthly (from docstring for seasonal_mean): To illustrate - using random sample data: So far, so good - now randomly dropping elements from the DateTimeIndex to create unevenly space data: Running the seasonal_decomp on this data 'works': The question is - how useful is the result. Even without gaps in the data that complicate inference of seasonal patterns (see example use of .interpolate() in the release notes, statsmodels qualifies this procedure as follows:I'm working with libsvm and I must implement the classification for multiclasses with one versus all.  How can I do it?
Does libsvm version 2011 use this? I think that my question is not very clear.
if libsvm don't use automatically one versus all,I will use one svm for every class, else how can i defined this parameters in the svmtrain function.
I had read README of libsvm.  According to the official libsvm documentation (Section 7): LIBSVM implements the "one-against-one" approach for multi-class
  classification. If k is the number of classes, then k(k-1)/2
  classifiers are constructed and each one trains data from two
  classes. In classification we use a voting strategy: each binary
  classification is considered to be a voting where votes can be cast
  for all data points x - in the end a point is designated to be in a
  class with the maximum number of votes. In the one-against-all approach, we build as many binary classifiers as there are classes, each trained to separate one class from the rest. To predict a new instance, we choose the classifier with the largest decision function value. As I mentioned before, the idea is to train k SVM models each one separating one class from the rest. Once we have those binary classifiers, we use the probability outputs (the -b 1 option) to predict new instances by picking the class with the highest probability. Consider the following example: Here is my implementation for the one-against-all approach for multi-class SVM:Could you give an example of classification of 4 classes using Support Vector Machines (SVM) in matlab something like: SVMs were originally designed for binary classification. They have then been extended to handle multi-class problems. The idea is to decompose the problem into many binary-class problems and then combine them to obtain the prediction. One approach called one-against-all, builds as many binary classifiers as there are classes, each trained to separate one class from the rest. To predict a new instance, we choose the classifier with the largest decision function value. Another approach called one-against-one (which I believe is used in LibSVM), builds k(k-1)/2 binary classifiers, trained to separate each pair of classes against each other, and uses a majority voting scheme (max-win strategy) to determine the output prediction. There are also other approaches such as using Error Correcting Output Code (ECOC) to build many somewhat-redundant binary-classifiers, and use this redundancy to obtain more robust classifications (uses the same idea as Hamming codes). Example (one-against-one): Here is a sample output: MATLAB does not support multiclass SVM at the moment. You could use svmtrain (2-classes) to achieve this, but it would be much easier to use a standard SVM package. I have used LIBSVM and can confirm that it's very easy to use.I'm new in the world of Tensorflow and I'm working on the simple example of mnist dataset classification. I would like to know how can I obtain other metrics (e.g precision, recall etc) in addition to accuracy and loss (and possibly to show them). Here's my code: Since I get only accuracy and loss, how can i get other metrics?
Thank you in advance, I'm sorry if it is a simple question or If was already answered somewhere. I am adding another answer because this is the cleanest way in order to compute these metrics correctly on your test set (as of 22nd of March 2020). The first thing you need to do is to create a custom callback, in which you send your test data: Starting from TensorFlow 2.X, precision and recall are both available as built-in metrics. Therefore, you do not need to implement them by hand. In addition to this, they were removed before in Keras 2.X versions because they were misleading --- as they were being computed in a batch-wise manner, the global(true) values of precision and recall would be actually different. You can have a look here : https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall Now they have a built-in accumulator, which ensures the correct calculation of those metrics. There is a list of available metrics in the Keras documentation. It includes recall, precision, etc.  For instance, recall: I could not get Timbus' answer to work and I found a very interesting explanation here. It says:
The meaning of 'accuracy' depends on the loss function. The one that corresponds to sparse_categorical_crossentropy is tf.keras.metrics.SparseCategoricalAccuracy(), not tf.metrics.Accuracy().
Which makes a lot of sense. So what metrics you can use depend on the loss you chose. E.g. using the metric 'TruePositives' won't work in the case of SparseCategoricalAccuracy, because that loss means you're working with more than 1 class, which in turn means True Positives cannot be defined because it is only used in binary classification problems. A loss like tf.keras.metrics.CategoricalCrossentropy() will work because it is designed with multiple classes in mind! Example: In my case the other 2 answers gave me shape mismatches. For a list of supported metrics, see: tf.keras MetricsI have a provided standardize function for a machine learning course that wasn't well documented and I'm still new to MATLAB so I'm just trying to break down the function. Any explanation of the syntax or the general idea of standardizing would greatly help. We use this function to standardize a set of training data provided in a large matrix. A break down of most of the lines of the code snippet would help me greatly. Thank you so much.  This code accepts a data matrix of size M x N, where M is the dimensionality of one data sample from this matrix and N is the total number of samples.  Therefore, one column of this matrix is one data sample.  Data samples are all stacked horizontally and are columns.   Now, the true purpose of this code is to take all of the columns of your matrix and standardize / normalize the data so that each data sample exhibits zero mean and unit variance.  This means that after this transform, if you found the mean value of any column in this matrix, it would be 0 and the variance would be 1.  This is a very standard method for normalizing values in statistical analysis, machine learning, and computer vision. This actually comes from the z-score in statistical analysis.  Specifically, the equation for normalization is:  Given a set of data points, we subtract the value in question by the mean of these data points, then divide by the respective standard deviation.  How you'd call this code is the following.  Given this matrix, which we will call X, there are two ways you can call this code: The first method automatically infers the mean of each column of X  and the standard deviation of each column of X.  mean_X and std_X will both return 1 x N vectors that give you the mean and standard deviation of each column in the matrix X.  The second method allows you to manually specify a mean (mu) and standard deviation (sigma) for each column of X.  This is possibly for use in debugging, but you would specify both mu and sigma as 1 x N vectors in this case.  What is returned for mean_X and std_X is identical to mu and sigma. The code is a bit poorly written IMHO, because you can certainly achieve this vectorized, but the gist of the code is that it finds the mean of every column of the matrix X if we are are using Method #1, duplicates this vector so that it becomes a M x N matrix, then we subtract this matrix with X.  This will subtract each column by its respective mean.  We also compute the standard deviation of each column before the mean subtraction.   Once we do that, we then normalize our X by dividing each column by its respective standard deviation.  BTW, doing std_X(:, i) is superfluous as std_X is already a 1 x N vector.  std_X(:, i) means to grab all of the rows at the ith column.  If we already have a 1 x N vector, this can simply be replaced with std_X(i) - a bit overkill for my taste. Method #2 performs the same thing as Method #1, but we provide our own mean and standard deviation for each column of X. For the sake of documentation, this is how I would have commented the code: If I can suggest another way to write this code, I would use the mighty and powerful bsxfun function.  This avoids having to do any duplication of elements and we can do this under the hood.  I would rewrite this function so that it looks like this: I would argue that the new code above is much faster than using for and repmat.  In fact, it is known that bsxfun is faster than the former approach - especially for larger matrices.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I am finding it hard to understand the process of Naive Bayes, and I was wondering if someone could explain it with a simple step by step process in English. I understand it takes comparisons by times occurred as a probability, but I have no idea how the training data is related to the actual dataset. Please give me an explanation of what role the training set plays. I am giving a very simple example for fruits here, like banana for example The accepted answer has many elements of k-NN (k-nearest neighbors), a different algorithm. Both k-NN and NaiveBayes are classification algorithms. Conceptually, k-NN uses the idea of "nearness" to classify new entities. In k-NN 'nearness' is modeled with ideas such as Euclidean Distance or Cosine Distance. By contrast, in NaiveBayes, the concept of 'probability' is used to classify new entities. Since the question is about Naive Bayes, here's how I'd describe the ideas and steps to someone. I'll try to do it with as few equations and in plain English as much as possible. Before someone can understand and appreciate the nuances of Naive Bayes', they need to know a couple of related concepts first, namely, the idea of Conditional Probability, and Bayes' Rule. (If you are familiar with these concepts, skip to the section titled Getting to Naive Bayes') Conditional Probability in plain English: What is the probability that something will happen, given that something else has already happened. Let's say that there is some Outcome O. And some Evidence E. From the way these probabilities are defined: The Probability of having both the Outcome O and Evidence E is:
(Probability of O occurring) multiplied by the (Prob of E given that O happened) One Example to understand Conditional Probability: Let say we have a collection of US Senators. Senators could be Democrats or Republicans. They are also either male or female. If we select one senator completely randomly, what is the probability that this person is a female Democrat? Conditional Probability can help us answer that. Probability of (Democrat and Female Senator)= Prob(Senator is Democrat) multiplied by Conditional Probability of Being Female given that they are a Democrat. We could compute the exact same thing, the reverse way: Conceptually, this is a way to go from P(Evidence| Known Outcome) to P(Outcome|Known Evidence). Often, we know how frequently some particular evidence is observed, given a known outcome. We have to use this known fact to compute the reverse, to compute the chance of that outcome happening, given the evidence. P(Outcome given that we know some Evidence) = P(Evidence given that we know the Outcome) times Prob(Outcome), scaled by the P(Evidence) The classic example to understand Bayes' Rule: Now, all this was just preamble, to get to Naive Bayes. So far, we have talked only about one piece of evidence. In reality, we have to predict an outcome given multiple evidence. In that case, the math gets very complicated. To get around that complication, one approach is to 'uncouple' multiple pieces of evidence, and to treat each of piece of evidence as independent. This approach is why this is called naive Bayes. Many people choose to remember this as: Notice a few things about this equation: Just run the formula above for each possible outcome. Since we are trying to classify, each outcome is called a class and it has a class label. Our job is to look at the evidence, to consider how likely it is to be this class or that class, and assign a label to each entity.
Again, we take a very simple approach: The class that has the highest probability is declared the "winner" and that class label gets assigned to that combination of evidences. Let's try it out on an example to increase our understanding: The OP asked for a 'fruit' identification example. Let's say that we have data on 1000 pieces of fruit. They happen to be Banana, Orange or some Other Fruit.
We know 3 characteristics about each fruit: This is our 'training set.' We will use this to predict the type of any new fruit we encounter. We can pre-compute a lot of things about our fruit collection. The so-called "Prior" probabilities. (If we didn't know any of the fruit attributes, this would be our guess.) These are our base rates. Probability of "Evidence" Probability of "Likelihood" Let's say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit? We can simply run the numbers for each of the 3 outcomes, one by one. Then we choose the highest probability and 'classify' our unknown fruit as belonging to the class that had the highest probability based on our prior evidence (our 1000 fruit training set): By an overwhelming margin (0.252 >> 0.01875), we classify this Sweet/Long/Yellow fruit as likely to be a Banana. Look at what it eventually comes down to. Just some counting and multiplication. We can pre-compute all these terms, and so classifying becomes easy, quick and efficient. Let z = 1 / P(evidence). Now we quickly compute the following three quantities. Assign the class label of whichever is the highest number, and you are done. Despite the name, Naive Bayes turns out to be excellent in certain applications. Text classification is one area where it really shines. Your question as I understand it is divided in two parts, part one being you need a better understanding of the Naive Bayes classifier & part two being the confusion surrounding Training set.  In general all of Machine Learning Algorithms need to be trained for supervised learning tasks like classification, prediction etc. or for unsupervised learning tasks like clustering. During the training step, the algorithms are taught with a particular input dataset (training set) so that later on we may test them for unknown inputs (which they have never seen before) for which they may classify or predict etc (in case of supervised learning) based on their learning. This is what most of the Machine Learning techniques like Neural Networks, SVM, Bayesian etc. are based upon. So in a general Machine Learning project basically you have to divide your input set to a Development Set (Training Set + Dev-Test Set) & a Test Set (or Evaluation set). Remember your basic objective would be that your system learns and classifies new inputs which they have never seen before in either Dev set or test set. The test set typically has the same format as the training set. However, it is very important that the test set be distinct from the training corpus: if we simply
reused the training set as the test set, then a model that simply memorized its input, without learning how to generalize to new examples, would receive misleadingly high scores. In general, for an example, 70% of our data can be used as training set cases. Also remember to partition the original set into the training and test sets randomly. Now I come to your other question about Naive Bayes. To demonstrate the concept of Naïve Bayes Classification, consider the example given below:  As indicated, the objects can be classified as either GREEN or RED. Our task is to classify new cases as they arrive, i.e., decide to which class label they belong, based on the currently existing objects. Since there are twice as many GREEN objects as RED, it is reasonable to believe that a new case (which hasn't been observed yet) is twice as likely to have membership GREEN rather than RED. In the Bayesian analysis, this belief is known as the prior probability. Prior probabilities are based on previous experience, in this case the percentage of GREEN and RED objects, and often used to predict outcomes before they actually happen. Thus, we can write: Prior Probability of GREEN: number of GREEN objects / total number of objects Prior Probability of RED: number of RED objects / total number of objects Since there is a total of 60 objects, 40 of which are GREEN and 20 RED, our prior probabilities for class membership are: Prior Probability for GREEN: 40 / 60 Prior Probability for RED: 20 / 60 Having formulated our prior probability, we are now ready to classify a new object (WHITE circle in the diagram below). Since the objects are well clustered, it is reasonable to assume that the more GREEN (or RED) objects in the vicinity of X, the more likely that the new cases belong to that particular color. To measure this likelihood, we draw a circle around X which encompasses a number (to be chosen a priori) of points irrespective of their class labels. Then we calculate the number of points in the circle belonging to each class label. From this we calculate the likelihood:   From the illustration above, it is clear that Likelihood of X given GREEN is smaller than Likelihood of X given RED, since the circle encompasses 1 GREEN object and 3 RED ones. Thus:   Although the prior probabilities indicate that X may belong to GREEN (given that there are twice as many GREEN compared to RED) the likelihood indicates otherwise; that the class membership of X is RED (given that there are more RED objects in the vicinity of X than GREEN). In the Bayesian analysis, the final classification is produced by combining both sources of information, i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes' rule (named after Rev. Thomas Bayes 1702-1761).  Finally, we classify X as RED since its class membership achieves the largest posterior probability. Naive Bayes comes under supervising machine learning which used to make classifications of data sets.
It is used to predict things based on its prior knowledge and independence assumptions. They call it naive because it’s assumptions (it assumes that all of the features in the dataset are equally important and independent) are really optimistic and rarely true in most real-world applications. It is classification algorithm which makes the decision for the unknown data set. It is based on Bayes Theorem which describe the probability of an event based on its prior knowledge. Below diagram shows how naive Bayes works  Formula to predict NB:  How to use Naive Bayes Algorithm ? Let's take an example of how N.B woks Step 1: First we find out Likelihood of table which shows the probability of yes or no in below diagram.
Step 2: Find the posterior probability of each class.  For more reference refer these blog. Refer GitHub Repository Naive-Bayes-Examples Ram Narasimhan explained the concept very nicely here below is an alternative explanation through the code example of Naive Bayes in action
It uses an example problem from this book on page 351
This is the data set that we will be using 
In the above dataset if we give the hypothesis = {"Age":'<=30', "Income":"medium", "Student":'yes' , "Creadit_Rating":'fair'} then what is the probability that he will buy or will not buy a computer.
The code below exactly answers that question.
Just create a file called named new_dataset.csv and paste the following content. Here is the code the comments explains everything we are doing here! [python] output: I try to explain the Bayes rule with an example. What is the chance that a random person selected from the society is a smoker? You may reply 10%, and let's assume that's right. Now, what if I say that the random person is a man and is 15 years old? You may say 15 or 20%, but why?. In fact, we try to update our initial guess with new pieces of evidence ( P(smoker) vs. P(smoker | evidence) ). The Bayes rule is a way to relate these two probabilities. Each evidence may increase or decrease this chance. For example, this fact that he is a man may increase the chance provided that this percentage (being a man) among non-smokers is lower. In the other words, being a man must be an indicator of being a smoker rather than a non-smoker. Therefore, if an evidence is an indicator of something, it increases the chance. But how do we know that this is an indicator? For each feature, you can compare the commonness (probability) of that feature under the given conditions with its commonness alone. (P(f | x) vs. P(f)). For example, if we know that 90% of smokers are men, it's not still enough to say whether being a man is an indicator of being smoker or not. For example if the probability of being a man in the society is also 90%, then knowing that someone is a man doesn't help us ((90% / 90%) = 1. But if men contribute to 40% of the society, but 90% of the smokers, then knowing that someone is a man increases the chance of being a smoker (90% / 40%) = 2.25, so it increases the initial guess (10%) by 2.25 resulting 22.5%. However, if the probability of being a man was 95% in the society, then regardless of the fact that the percentage of men among smokers is high (90%)! the evidence that someone is a man decreases the chance of him being a smoker! (90% / 95%) = 0.95). So we have: Note that in this formula we assumed that being a man and being under 20 are independent features so we multiplied them, it means that knowing that someone is under 20 has no effect on guessing that he is man or woman. But it may not be true, for example maybe most adolescence  in a society are men... To use this formula in a classifier The classifier is given with some features (being a man and being under 20) and it must decide if he is an smoker or not (these are two classes). It uses the above formula to calculate the probability of each class under the evidence (features), and it assigns the class with the highest probability to the input. To provide the required probabilities (90%, 10%, 80%...) it uses the training set. For example, it counts the people in the training set that are smokers and find they contribute 10% of the sample. Then for smokers checks how many of them are men or women .... how many are above 20 or under 20....In the other words, it tries to build the probability distribution of the features for each class based on the training data.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I've been reading some things on neural networks and I understand the general principle of a single layer neural network. I understand the need for aditional layers, but why are nonlinear activation functions used? This question is followed by this one: What is a derivative of the activation function used for in backpropagation? The purpose of the activation function is to introduce non-linearity into the network in turn, this allows you to model a response variable (aka target variable, class label, or score) that varies non-linearly with its explanatory variables non-linear means that the output cannot be reproduced from a linear combination of the inputs (which is not the same as output that renders to a straight line--the word for this is affine). another way to think of it: without a non-linear activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function (see definition just above). A common activation function used in backprop (hyperbolic tangent) evaluated from -2 to 2:  A linear activation function can be used, however on very limited occasions. In fact to understand activation functions better it is important to look at the ordinary least-square or simply the linear regression. A linear regression aims at finding the optimal weights that result in minimal vertical effect between the explanatory and target variables, when combined with the input. In short, if the expected output reflects the linear regression as shown below then linear activation functions can be used: (Top Figure). But  as in the second figure below linear function will not produce the desired results:(Middle figure). However, a non-linear function as shown below would produce the desired results:   Activation functions cannot be linear because neural networks with a linear activation function are effective only one layer deep, regardless of how complex their architecture is. Input to networks is usually linear transformation (input * weight), but real world and problems are non-linear. To make the incoming data nonlinear, we use nonlinear mapping called activation function. An activation function is a decision making function that determines the presence of a particular neural feature. It is mapped between 0 and 1, where zero means absence of the feature, while one means its presence. Unfortunately, the small changes occurring in the weights cannot be reflected in the activation values because it can only take either 0 or 1. Therefore, nonlinear functions must be continuous and differentiable between this range.
A neural network must be able to take any input from -infinity to +infinite, but it should be able to map it to an output that ranges between {0,1} or between {-1,1} in some cases - thus the need for activation function. Non-linearity is needed in activation functions because its aim in a neural network is to produce a nonlinear decision boundary via non-linear combinations of the weight and inputs. If we only allow linear activation functions in a neural network, the output will just be a linear transformation of the input, which is not enough to form a universal function approximator. Such a network can just be represented as a matrix multiplication, and you would not be able to obtain very interesting behaviors from such a network. The same thing goes for the case where all neurons have affine activation functions (i.e. an activation function on the form f(x) = a*x + c, where a and c are constants, which is a generalization of linear activation functions), which will just result in an affine transformation from input to output, which is not very exciting either. A neural network may very well contain neurons with linear activation functions, such as in the output layer, but these require the company of neurons with a non-linear activation function in other parts of the network. Note: An interesting exception is DeepMind's synthetic gradients, for which they use a small neural network to predict the gradient in the backpropagation pass given the activation values, and they find that they can get away with using a neural network with no hidden layers and with only linear activations. A feed-forward neural network with linear activation and any number of hidden layers is equivalent to just a linear neural neural network with no hidden layer. For example lets consider the neural network in figure with two hidden layers and no activation
 We can do the last step because combination of several linear transformation can be replaced with one transformation and combination of several bias term is just a single bias. The outcome is same even if we add some linear activation. So we could replace this neural net with a single layer neural net.This can be extended to n layers. This indicates adding layers doesn't increase the approximation power of a linear neural net at all. We need non-linear activation functions to approximate non-linear functions and most real world problems are highly complex and non-linear. In fact when the activation function is non-linear, then a two-layer neural network with sufficiently large number of hidden units can be proven to be a universal function approximator. Several good answers are here. It will be good to point out the book "Pattern Recognition and Machine Learning" by Christopher M. Bishop. It is a book worth referring to for getting a deeper insight about several ML related concepts. Excerpt from page 229 (section 5.1): If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units. This follows from the fact that the composition of successive linear transformations is itself a linear transformation. However, if the number of hidden units is smaller than either the number of input or output units, then the transformations that the network can generate are not the most general possible linear transformations from inputs to outputs because information is lost in the dimensionality reduction at the hidden units. In Section 12.4.2, we show that networks of linear units give rise to principal component analysis. In general, however, there is little interest in multilayer networks of linear units. "The present paper makes use of the Stone-Weierstrass Theorem and the cosine squasher of Gallant and White to establish that standard multilayer feedforward network architectures using abritrary squashing functions can approximate virtually any function of interest to any desired degree of accuracy, provided sufficently many hidden units are available." (Hornik et al., 1989, Neural Networks) A squashing function is for example a nonlinear activation function that maps to [0,1] like the sigmoid activation function. There are times when a purely linear network can give useful results. Say we have a network of three layers with shapes (3,2,3). By limiting the middle layer to only two dimensions, we get a result that is the "plane of best fit" in the original three dimensional space.  But there are easier ways to find linear transformations of this form, such as NMF, PCA etc. However, this is a case where a multi-layered network does NOT behave the same way as a single layer perceptron. Neural Networks are used in pattern recognition. And pattern finding is a very non-linear technique. Suppose for the sake of argument we use a linear activation function y=wX+b for every single neuron and set something like if y>0 -> class 1 else class 0. Now we can compute our loss using square error loss and back propagate it so that the model learns well, correct? WRONG. For the last hidden layer, the updated value will be w{l} = w{l} - (alpha)*X. For the second last hidden layer, the updated value will be w{l-1} = w{l-1} - (alpha)*w{l}*X. For the ith last hidden layer, the updated value will be w{i} = w{i} - (alpha)*w{l}...*w{i+1}*X. This results in us multiplying all the weight matrices together hence resulting in the possibilities:
A)w{i} barely changes due to vanishing gradient
B)w{i} changes dramatically and inaccurately due to exploding gradient
C)w{i} changes well enough to give us a good fit score In case C happens that means that our classification/prediction problem was most probably a simple linear/logistic regressor based one and never required a neural network in the first place! No matter how robust or well hyper tuned your NN is, if you use a linear activation function, you will never be able to tackle non-linear requiring pattern recognition problems As I remember - sigmoid functions are used because their derivative that fits in BP algorithm is easy to calculate, something simple like f(x)(1-f(x)). I don't remember exactly the math. Actually any function with derivatives can be used.   To understand the logic behind non-linear activation functions first you should understand why activation functions are used. In general, real world problems requires non-linear solutions which are not trivial. So we need some functions to generate the non-linearity. Basically what an activation function does is to generate this non-linearity while mapping input values into a desired range.  However, linear activation functions could be used in very limited set of cases where you do not need hidden layers such as linear regression. Usually, it is pointless to generate a neural network for this kind of problems because independent from number of hidden layers, this network will generate a linear combination of inputs which can be done in just one step. In other words, it behaves like a single layer.  There are also a few more desirable properties for activation functions such as continuous differentiability. Since we are using backpropagation the function we generate must be differentiable at any point. I strongly advise you to check the wikipedia page for activation functions from here to have a better understanding of the topic. It is important to use the nonlinear activation function in neural networks, especially in deep NNs and backpropagation. According to the question posed in the topic, first I will say the reason for the need to use the nonlinear activation function for the backpropagation. Simply put: if a linear activation function is used, the derivative of the cost function is a constant with respect to (w.r.t) input, so the value of input (to neurons) does not affect the updating of weights. This means that we can not figure out which weights are most effective in creating a good result and therefore we are forced to change all weights equally. Deeper: In general, weights are updated as follows: This means that the new weight is equal to the old weight minus the derivative of the cost function. If the activation function is a linear function, then its derivative w.r.t input is a constant, and the input values ​​have no direct effect on the weight update. For example, we intend to update the weights of last layer neurons using backpropagation. We need to calculate the gradient of the weight function w.r.t weight. With chain rule we have:  h and y are (estimated) neuron output and actual output value, respectively. And x is the input of neurons. grad (f) is derived from the input w.r.t activation function. The value calculated above (by a factor) is subtracted from the current weight and a new weight is obtained. We can now compare these two types of activation functions more clearly. 1- If the activating function is a linear function, such as:
F(x) = 2 * x then:  the new weight will be:  As you can see, all the weights are updated equally and it does not matter what the input value is!! 2- But if we use a non-linear activation function like Tanh(x) then:  and:  and now we can see the direct effect of input in updating weights! different input value makes different weights changes. I think the above is enough to answer the question of the topic but it is useful to mention other benefits of using the non-linear activation function. As mentioned in other answers, non-linearity enables NNs to have more hidden layers and deeper NNs. A sequence of layers with a linear activator function can be merged as a layer (with a combination of previous functions) and is practically a neural network with a hidden layer, which does not take advantage of the benefits of deep NN. Non-linear activation function can also produce a normalized output. A layered NN of several neurons can be used to learn linearly inseparable problems. For example XOR function can be obtained with two layers with step activation function. It's not at all a requirement.  In fact, the rectified linear activation function is very useful in large neural networks. Computing the gradient is much faster, and it induces sparsity by setting a minimum bound at 0. See the following for more details: https://www.academia.edu/7826776/Mathematical_Intuition_for_Performance_of_Rectified_Linear_Unit_in_Deep_Neural_Networks Edit: There has been some discussion over whether the rectified linear activation function can be called a linear function.  Yes, it is technically a nonlinear function because it is not linear at the point x=0, however, it is still correct to say that it is linear at all other points, so I don't think it's that useful to nitpick here,  I could have chosen the identity function and it would still be true, but I chose ReLU as an example because of its recent popularity.I can't figure out how the sklearn.pipeline.Pipeline works exactly. There are a few explanation in the doc. For example what do they mean by: Pipeline of transforms with a final estimator. To make my question clearer, what are steps? How do they work? Edit Thanks to the answers I can make my question clearer: When I call pipeline and pass, as steps, two transformers and one estimator, e.g: What happens when I call this? I can't figure out how an estimator can be a transformer and how a transformer can be fitted. Transformer in scikit-learn - some class that have fit and transform method, or fit_transform method. Predictor - some class that has fit and predict methods, or fit_predict method. Pipeline is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator. Here is a good example of Pipeline usage.
Pipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside, and now you can do something like: With just: With pipelines you can easily perform a grid-search over set of parameters for each step of this meta-estimator. As described in the link above. All steps except last one must be transforms, last step can be transformer or predictor.
Answer to edit:
When you call pipln.fit() - each transformer inside pipeline will be fitted on outputs of previous transformer (First transformer is learned on raw dataset).  Last estimator may be transformer or predictor, you can call fit_transform() on pipeline only if your last estimator is transformer (that implements fit_transform, or transform and fit methods separately), you can call fit_predict() or predict() on pipeline only if your last estimator is predictor. So you just can't call fit_transform or transform on pipeline, last step of which is predictor. I think that M0rkHaV has the right idea. Scikit-learn's pipeline class is a useful tool for encapsulating multiple different transformers alongside an estimator into one object, so that you only have to call your important methods once (fit(), predict(), etc). Let's break down the two major components: Transformers are classes that implement both fit() and transform(). You might be familiar with some of the sklearn preprocessing tools, like TfidfVectorizer and Binarizer. If you look at the docs for these preprocessing tools, you'll see that they implement both of these methods. What I find pretty cool is that some estimators can also be used as transformation steps, e.g. LinearSVC! Estimators are classes that implement both fit() and predict(). You'll find that many of the classifiers and regression models implement both these methods, and as such you can readily test many different models. It is possible to use another transformer as the final estimator (i.e., it doesn't necessarily implement predict(), but definitely implements fit()). All this means is that you wouldn't be able to call predict(). As for your edit: let's go through a text-based example. Using LabelBinarizer, we want to turn a list of labels into a list of binary values.  Now, when the binarizer is fitted on some data, it will have a structure called classes_ that contains the unique classes that the transformer 'knows' about. Without calling fit() the binarizer has no idea what the data looks like, so calling transform() wouldn't make any sense. This is true if you print out the list of classes before trying to fit the data. I get the following error when trying this: But when you fit the binarizer on the vec list: and try again  I get the following: And now, after calling transform on the vec object, we get the following: As for estimators being used as transformers, let us use the DecisionTree classifier as an example of a feature-extractor. Decision Trees are great for a lot of reasons, but for our purposes, what's important is that they have the ability to rank features that the tree found useful for predicting. When you call transform() on a Decision Tree, it will take your input data and find what it thinks are the most important features. So you can think of it transforming your data matrix (n rows by m columns) into a smaller matrix (n rows by k columns), where the k columns are the k most important features that the Decision Tree found. ML algorithms typically process tabular data. You may want to do preprocessing and post-processing of this data before and after your ML algorithm. A pipeline is a way to chain those data processing steps. A pipeline is a series of steps in which data is transformed. It comes from the old "pipe and filter" design pattern (for instance, you could think of unix bash commands with pipes “|” or redirect operators “>”). However, pipelines are objects in the code. Thus, you may have a class for each filter (a.k.a. each pipeline step), and then another class to combine those steps into the final pipeline. Some pipelines may combine other pipelines in series or in parallel, have multiple inputs or outputs, and so on. We like to view Pipelining Machine Learning as: Pipelines (or steps in the pipeline) must have those two methods: It's also possible to call this method to chain both: Scikit-Learn’s “pipe and filter” design pattern is simply beautiful. But how to use it for Deep Learning, AutoML, and complex production-level pipelines? Scikit-Learn had its first release in 2007, which was a pre deep learning era. However, it’s one of the most known and adopted machine learning library, and is still growing. On top of all, it uses the Pipe and Filter design pattern as a software architectural style - it’s what makes Scikit-Learn so fabulous, added to the fact it provides algorithms ready for use. However, it has massive issues when it comes to do the following, which we should be able to do in 2020 already: For sure, Scikit-Learn is very convenient and well-built. However, it needs a refresh. Here are our solutions with Neuraxle to make Scikit-Learn fresh and useable within modern computing projects! Note: if a step of a pipeline doesn’t need to have one of the fit or transform methods, it could inherit from NonFittableMixin or NonTransformableMixin to be provided a default implementation of one of those methods to do nothing. As a starter, it is possible for pipelines or their steps to also optionally define those methods: The following methods are provided by default to allow for managing hyperparameters: For more info on our suggested solutions, read the entries in the big list with links above.My problem: I have a dataset which is a large JSON file. I read it and store it in the trainList variable. Next, I pre-process it - in order to be able to work with it. Once I have done that I start the classification: Finally, I would use this to put in HTML in order to show a chart with the TPs of each label. Code: The variables I have for the moment: Most part of the method: For the multi-class case, everything you need can be found from the confusion matrix. For example, if your confusion matrix looks like this:  Then what you're looking for, per class, can be found like this:  Using pandas/numpy, you can do this for all classes at once like so: If you have two lists that have the predicted and actual values; as it appears you do, you can pass them to a function that will calculate TP, FP, TN, FN with something like this: From here I think you will be able to calculate rates of interest to you, and other performance measure like specificity and sensitivity. According to scikit-learn documentation, http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix By definition a confusion matrix C is such that C[i, j] is equal to the number of observations known to be in group i but predicted to be in group j. Thus in binary classification, the count of true negatives is C[0,0], false negatives is C[1,0], true positives is C[1,1] and false positives is C[0,1]. You can obtain all of the parameters from the confusion matrix.
The structure of the confusion matrix(which is 2X2 matrix) is as follows (assuming the first index is related to the positive label, and the rows are related to the true labels): So  More details at https://en.wikipedia.org/wiki/Confusion_matrix The one liner to get true postives etc. out of the confusion matrix is to ravel it: One should set the labels parameter in case the data contains only a single case, e.g. only true positives. Setting labels correctly ensures that the confusion matrix has a 2x2 shape. In the scikit-learn 'metrics' library there is a confusion_matrix method which gives you the desired output. You can use any classifier that you want. Here I used the KNeighbors as example. The docs: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix I wrote a version that works using only numpy.
I hope it helps you. Just in case some is looking for the same in MULTI-CLASS Example you can try sklearn.metrics.classification_report as below: output: In scikit version 0.22, you can do it  like this this works fine Source - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html if you have more than one classes in your classifier, you might want to use pandas-ml at that part. Confusion Matrix of pandas-ml give more detailed information. check that  I think both of the answers are not fully correct. For example, suppose that we have the following arrays;
y_actual = [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0] y_predic = [1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0] If we compute the FP, FN, TP and TN values manually, they should be as follows: FP: 3
FN: 1
TP: 3
TN: 4 However, if we use the first answer, results are given as follows: FP: 1
FN: 3
TP: 3
TN: 4 They are not correct, because in the first answer, False Positive should be where actual is 0, but the predicted is 1, not the opposite. It is also same for False Negative. And, if we use the second answer, the results are computed as follows: FP: 3
FN: 1
TP: 4
TN: 3 True Positive and True Negative numbers are not correct, they should be opposite. Am I correct with my computations? Please let me know if I am missing something. #FalseNegatives None of the answers given so far worked for me as I sometimes ended up having a confusion matrix with a single entry only. The following code is able to mitigate this issue: Please note that "y" is the groundtruth and "y_hat" is the prediction. Although it does not relate to scikit-learn, what you could also do is I have tried some of the answers and found them not working. This works for me:  Here's a fix to invoketheshell's buggy code (which currently appears as the accepted answer):OpenAI's REINFORCE and actor-critic example for reinforcement learning has the following code: REINFORCE: actor-critic: One is using torch.cat, the other uses torch.stack, for similar use cases. As far as my understanding goes, the doc doesn't give any clear distinction between them. I would be happy to know the differences between the functions. stack Concatenates sequence of tensors along a new dimension. cat Concatenates the given sequence of seq tensors in the given dimension. So if A and B are of shape (3, 4): These functions are analogous to numpy.stack and numpy.concatenate. The original answer lacks a good example that is self-contained so here it goes: output: for reference here are the definitions: cat: Concatenates the given sequence of seq tensors in the given dimension. The consequence is that a specific dimension changes size e.g. dim=0 then you are adding elements to the row which increases the dimensionality of the column space. stack: Concatenates sequence of tensors along a new dimension. I like to think of this as the torch "append" operation since you can index/get your original tensor by "poping it" from the front. With no arguments, it appends tensors to the front of the tensor. Related: here is a few unit tests (I didn't write more tests but it worked with my real code so I trust it's fine. Feel free to help me by adding more tests if you want): If someone is looking into the performance aspects of this, I've done a small experiment. In my case, I needed to convert a list of scalar tensors into a single tensor. My conclusion is that even if you want to have the additional dimension of torch.stack, using torch.cat and then reshape is better. Note: this post is taken from the PyTorch forum (I am the author of the original post)I have an assignment to make an AI Agent that will learn to play a video game using ML. I want to create a new environment using OpenAI Gym because I don't want to use an existing environment. How can I create a new, custom Environment? Also, is there any other way I can start to develop making AI Agent to play a specific video game without the help of OpenAI Gym? See my banana-gym for an extremely small environment. See the main page of the repository: https://github.com/openai/gym/blob/master/docs/creating_environments.md The steps are: It should look like this For the contents of it, follow the link above. Details which are not mentioned there are especially how some functions in foo_env.py should look like. Looking at examples and at gym.openai.com/docs/ helps. Here is an example: Its definitely possible. They say so in the Documentation page, close to the end. https://gym.openai.com/docs As to how to do it, you should look at the source code of the existing environments for inspiration. Its available in github: https://github.com/openai/gym#installation Most of their environments they did not implement from scratch, but rather created a wrapper around existing environments and gave it all an interface that is convenient for reinforcement learning. If you want to make your own, you should probably go in this direction and try to adapt something that already exists to the gym interface. Although there is a good chance that this is very time consuming. There is another option that may be interesting for your purpose. It's OpenAI's Universe https://universe.openai.com/ It can integrate with websites so that you train your models on kongregate games, for example. But Universe is not as easy to use as Gym. If you are a beginner, my recommendation is that you start with a vanilla implementation on a standard environment. After you get passed the problems with the basics, go on to increment...This question does not appear to be about programming within the scope defined in the help center. Closed 1 year ago. The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved How do I calculate the output size in a convolution layer? For example, I have a 2D convolution layer that takes a 3x128x128 input and has 40 filters of size 5x5. you can use this formula [(W−K+2P)/S]+1. So, we input into the formula: NOTE: Stride defaults to 1 if not provided and the 40 in (124, 124, 40) is the number of filters provided by the user. You can find it in two ways:
simple method: input_size - (filter_size - 1) But the second method is the standard to find the output size. Let me start simple; since you have square matrices for both input and filter let me get one dimension. Then you can apply the same for other dimension(s). Imagine your are building fences between trees, if there are N trees, you have to build N-1 fences. Now apply that analogy to convolution layers. Your output size will be: input size - filter size + 1 Because your filter can only have n-1 steps as fences I mentioned. Let's calculate your output with that idea.
128 - 5 + 1 = 124
Same for other dimension too. So now you have a 124 x 124 image. That is for one filter. If you apply this 40 times you will have another dimension: 124 x 124 x 40 Here is a great guide if you want to know more about advanced convolution arithmetic: https://arxiv.org/pdf/1603.07285.pdf Formula : n[i]=(n[i-1]−f[i]+2p[i])/s[i]+1 where, so, n[i]=(128-5+0)/1+1 =124 so the size of the output layer is: 124x124x40
Where '40' is the number of filters (124*124*3)*40 = 1845120 width = 124 height = 124 depth = 3 no. of filters = 40 stride = 1 padding = 0I have a dataset containing grayscale images and I want to train a state-of-the-art CNN on them. I'd very much like to fine-tune a pre-trained model (like the ones here). The problem is that almost all models I can find the weights for have been trained on the ImageNet dataset, which contains RGB images. I can't use one of those models because their input layer expects a batch of shape (batch_size, height, width, 3) or (64, 224, 224, 3) in my case, but my images batches are (64, 224, 224). Is there any way that I can use one of those models? I've thought of dropping the input layer after I've loaded the weights and adding my own (like we do for the top layers). Is this approach correct? The model's architecture cannot be changed because the weights have been trained for a specific input configuration. Replacing the first layer with your own would pretty much render the rest of the weights useless.  -- Edit: elaboration suggested by Prune--
CNNs are built so that as they go deeper, they can extract high-level features derived from the lower-level features that the previous layers extracted. By removing the initial layers of a CNN, you are destroying that hierarchy of features because the subsequent layers won't receive the features that they are supposed to as their input. In your case the second layer has been trained to expect the features of the first layer. By replacing your first layer with random weights, you are essentially throwing away any training that has been done on the subsequent layers, as they would need to be retrained. I doubt that they could retain any of the knowledge learned during the initial training.
--- end edit --- There is an easy way, though, which you can make your model work with grayscale images. You just need to make the image to appear to be RGB. The easiest way to do so is to repeat the image array 3 times on a new dimension. Because you will have the same image over all 3 channels, the performance of the model should be the same as it was on RGB images. In numpy this can be easily done like this: The way this works is that it first creates a new dimension (to place the channels) and then it repeats the existing array 3 times on this new dimension. I'm also pretty sure that keras' ImageDataGenerator can load grayscale images as RGB. Converting grayscale images to RGB as per the currently accepted answer is one approach to this problem, but not the most efficient. You most certainly can modify the weights of the model's first convolutional layer and achieve the stated goal. The modified model will both work out of the box (with reduced accuracy) and be finetunable. Modifying the weights of the first layer does not render the rest of the weights useless as suggested by others. To do this, you'll have to add some code where the pretrained weights are loaded. In your framework of choice, you need to figure out how to grab the weights of the first convolutional layer in your network and modify them before assigning to your 1-channel model. The required modification is to sum the weight tensor over the dimension of the input channels. The way the weights tensor is organized varies from framework to framework. The PyTorch default is [out_channels, in_channels, kernel_height, kernel_width]. In Tensorflow I believe it is [kernel_height, kernel_width, in_channels, out_channels]. Using PyTorch as an example, in a ResNet50 model from Torchvision (https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py), the shape of the weights for conv1 is [64, 3, 7, 7]. Summing over dimension 1 results in a tensor of shape [64, 1, 7, 7]. At the bottom I've included a snippet of code that would work with the ResNet models in Torchvision assuming that an argument (inchans) was added to specify a different number of input channels for the model. To prove this works I did three runs of ImageNet validation on ResNet50 with pretrained weights. There is a slight difference in the numbers for run 2 & 3, but it's minimal and should be irrelevant once finetuned. A simple way to do this is to add a convolution layer before the base model and then feed the output to the base model. Like this: Why not try to convert a grayscale image to a fake "RGB" image? Dropping the input layer will not work out. This will cause that the all following layers will suffer. What you can do is Concatenate 3 black and white images together to expand your color dimension. I faced the same problem while working with VGG16 along with gray-scale images. I solved this problem like follows: Let's say our training images are in train_gray_images, each row containing the unrolled gray scale image intensities. So if we directly pass it to fit function it will create an error as the fit function is expecting a 3 channel (RGB) image data-set instead of gray-scale data set. So before passing to fit function do the following: Create a dummy RGB image data set just like the gray scale data set with the same shape (here dummy_RGB_image). The only difference is here we are using the number of the channel is 3. Therefore just copy the whole data-set 3 times to each of the channels of the "dummy_RGB_images". (Here the dimensions are [no_of_examples, height, width, channel]) Finally pass the dummy_RGB_images instead of the gray scale data-set, like: numpy's depth-stack function, np.dstack((img, img, img)) is a natural way to go. If you're already using scikit-image, you can get the desired result by using gray2RGB. I believe you can use a pretrained resnet with 1 channel gray scale images without repeating 3 times the image. What I have done is to replace the first layer (this is pythorch not keras, but the idea might be similar): With the following layer: And then copy the sum (in the channel axis) of the weights to the new layer, for example, the shape of the original weights was: So I did: And then check that the output of the new model is the same than the output with the gray scale image: input_image_1: one channel image input_image_3: 3 channel image (gray scale - all channels equal) model_resnet_1: modified model model_resnet_3: Original resnet model It's really easy !
example for 'resnet50':
before do it you should have : Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),
bias=False) Just do this ! the final step is to update state_dict. so if run as follow : results would be : Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3,
3), bias=False) As you see input channel is for the grayscale images. what I did is to just simply expand grayscales into RGB images by using the following transform stage: When you add the Resnet to model, you should input the input_shape in Resnet definition like   .As from the title I am wondering what is the difference between StratifiedKFold with the parameter shuffle=True and StratifiedShuffleSplit and what is the advantage of using StratifiedShuffleSplit In stratKFolds, each test set should not overlap, even when shuffle is included. With stratKFolds and shuffle=True, the data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest. In ShuffleSplit, the data is shuffled every time, and then split. This means the test sets may overlap between the splits. See this block for an example of the difference. Note the overlap of the elements in the test sets for ShuffleSplit. Output: As for when to use them, I tend to use stratKFolds for any cross validation, and I use ShuffleSplit with a split of 2 for my train/test set splits. But I'm sure there are other use cases for both. @Ken Syme already has a very good answer. I just want to add something. With  shuffle = True, the data is shuffled by your random_state. Otherwise, 
the data is shuffled by np.random (as default).
For example, with n_splits = 4, and your data has 3 classes (label) for y (dependent variable). 4 test sets cover all the data without any overlap.   So, the difference here is that StratifiedKFold just shuffles and splits once, therefore the test sets do not overlap, while StratifiedShuffleSplit shuffles each time before splitting, and it splits n_splits times, the test sets can overlap.  Output examples of KFold, StratifiedKFold, StratifiedShuffleSplit:
 The above pictorial output is an extension of @Ken Syme's code:I'm using the MinMaxScaler model in sklearn to normalize the features of a model. Now I want to use the same scaler to normalize the test set: But I don't want so use the scaler.fit() with the training data all the time. Is there a way to save the scaler and load it later from a different file?     Update: sklearn.externals.joblib is deprecated. Install and use the pure joblib instead. Please see Engineero's answer below, which is otherwise identical to mine. Even better than pickle (which creates much larger files than this method), you can use sklearn's built-in tool: So I'm actually not an expert with this but from a bit of research and a few helpful links, I think pickle and sklearn.externals.joblib are going to be your friends here. The package pickle lets you save models or "dump" models to a file.  I think this link is also helpful. It talks about creating a persistence model. Something that you're going to want to try is: Here is where you can learn more about the sklearn  externals. Let me know if that doesn't help or I'm not understanding something about your model. Note: sklearn.externals.joblib is deprecated. Install and use the pure joblib instead Just a note that sklearn.externals.joblib has been deprecated and is superseded by plain old joblib, which can be installed with pip install joblib: Note that file extensions can be anything, but if it is one of ['.z', '.gz', '.bz2', '.xz', '.lzma'] then the corresponding compression protocol will be used. Docs for joblib.dump() and joblib.load() methods. You can use pickle, to save the scaler: Load it back:I have a few thousand audio files and I want to classify them using Keras and Theano. So far, I generated a 28x28 spectrograms (bigger is probably better, but I am just trying to get the algorithm work at this point) of each audio file and read the image into a matrix. So in the end I get this big image matrix to feed into the network for image classification. In a tutorial I found this mnist classification code: This code runs, and I get the result as expected: Up to this point everything runs perfectly, however when I apply the above algorithm to my dataset, accuracy gets stuck. My code is as follows: AudioProcessing.py ImageTools.py So I run the above code and recieve: I tried changing the network, adding more epochs, but I always get the same result no matter what. I don't understand why I am getting the same result. Any help would be appreciated. Thank you. Edit:
I found a mistake where pixel values were not read correctly. I fixed the ImageTools.py below as: Now I actually get grayscale pixel values from 0 to 255, so now my dividing it by 255 makes sense. However, I still get the same result. The most likely reason is that the optimizer is not suited to your dataset. Here is a list of Keras optimizers from the documentation. I recommend you first try SGD with default parameter values. If it still doesn't work, divide the learning rate by 10. Do that a few times if necessary. If your learning rate reaches 1e-6 and it still doesn't work, then you have another problem. In summary, replace this line: with this: and change the learning rate a few times if it doesn't work. If it was the problem, you should see the loss getting lower after just a few epochs. Another solution that I do not see mentioned here, but caused a similar problem for me was the activiation function of the last neuron, especialy if it is relu and not something non linear like sigmoid. In other words, it might help you to use a non-linear activation function in the last layer Last layer: Output: Now I used a non linear activation function: Output: This is not directly a solution to the original answer, but as the answer is #1 on Google when searching for this problem, it might benefit someone. If the accuracy is not changing, it means the optimizer has found a local minimum for the loss. This may be an undesirable minimum. One common local minimum is to  always predict the class with the most number of data points. You should use weighting on the classes to avoid this minimum. After some examination, I found that the issue was the data itself. It was very dirty as in same input had 2 different outputs, hence creating confusion. After clearing up the data now my accuracy goes up to %69. Still not enough to be good, but at least I can now work my way up from here now that the data is clear. I used the below code to test: Check out this one Check out the  documentation  I had better results with MNIST  By mistake I had added a softmax at the end instead of sigmoid. Try doing the latter. It worked as expected when I did this. For one output layer, softmax always gives values of 1 and this is what had happened. I faced a similar issue. One-hot encoding the target variable using nputils in Keras, solved the issue of accuracy and validation loss being stuck. Using weights for balancing the target classes further improved performance. Solution : I've the same problem as you 
my solution was a loop instead of epochs    I got 13% Accuracy increment using this 'sigmoid' activation    Or you can also test the following, where 'relu' in first and hidden layer. As mentioned above, the problem mainly arises from the type of optimizers chosen. However, it can also be driven from the fact of topping 2 Dense layers with the same activation functions(softmax, for example).
In this case, NN finds a local minimum and is not able to descent more from that point, rolling around the same acc (val_acc) values.
Hope it helps out. I had similar problem. I had binary class which was labeled by 1 and 2. After testing different kinds of optimizer and activation functions I found that the root of the problem was my labeling to classes. In the other words I changed the labels to 0 and 1 instead of 1 and 2, then this problem solved! I faced same problem for multi-class, Try to changing optimizer by default it is Adam change it to sgd. you can also try different Activation functions eg. (relu, sigmoid, softmax, softplus, etc.) Some imp links Optimizers Activations As pointed out by others, the optimizer probably doesn't suit your data/model which stuck in local minima. A neural network should at least be able to overfit the data (training_acc close to 1).
I once had a similar problem. I solved by trying different optimizers (in my case from SGD to RMSprop) In my case, my problem was binary and I was using the 'softmax' activation function and it doesn't work. I changed to 'sigmoid' it works properly for me. I had the exactly same problem: validation loss and accuracy remaining the same through the epochs.  I increased the batch size 10x times, reduced learning rate by 100x times, etc.  It did not work. My last try, inspired by monolingual's and Ranjab's answers, worked. my solution was to add Batchnormalization AND arrange the order as below: Conv - DropOut - BatchNorm - Activation - Pool. as recommended in Ordering of batch normalization and dropout?. I know this is an old question but as of today (14/06/2021), the comment from @theTechGuy works well on tf 2.3. The code is: I tried playing a lot with the optimizers and activation functions, but the only thing that worked was Batchnormalization1. And I guess it is a good practice too.
You can import it as: and simply add it before each hidden layer: I had the same problem, but in my case, it was caused by a non-regularized column on my data. This column had huge value. Fixing that solved it for me. So, I just converted it to values around 0 and 1. I had the same problem. My solution was to change the last layer activation function from "softmax" to "sigmoid" since i was dealing with a binary classification problem.I am attempting to apply k-means on a set of high-dimensional data points (about 50 dimensions) and was wondering if there are any implementations that find the optimal number of clusters.  I remember reading somewhere that the way an algorithm generally does this is such that the inter-cluster distance is maximized and intra-cluster distance is minimized but I don't remember where I saw that. It would be great if someone can point me to any resources that discuss this. I am using SciPy for k-means currently but any related library would be fine as well. If there are alternate ways of achieving the same or a better algorithm, please let me know. One approach is cross-validation.  In essence, you pick a subset of your data and cluster it into k clusters, and you ask how well it clusters, compared with the rest of the data: Are you assigning data points to the same cluster memberships, or are they falling into different clusters?  If the memberships are roughly the same, the data fit well into k clusters. Otherwise, you try a different k. Also, you could do PCA (principal component analysis) to reduce your 50 dimensions to some more tractable number. If a PCA run suggests that most of your variance is coming from, say, 4 out of the 50 dimensions, then you can pick k on that basis, to explore how the four cluster memberships are assigned. Take a look at this wikipedia page on determining the number of clusters in a data set.  Also you might want to try Agglomerative hierarchical clustering out. This approach does not need to know the number of clusters, it will incrementally form clusters of cluster till only one exists. This technique also exists in SciPy (scipy.cluster.hierarchy).  One interesting approach is that of evidence accumulation by Fred and Jain. This is based on combining multiple runs of k-means with a large number of clusters, aggregating them into an overall solution. Nice aspects of the approach include that the number of clusters is determined in the process and that the final clusters don't have to be spherical. There are visualization that should hint good parameters. For k-means you could visualize several runs with different k using Graphgrams (see the WEKA graphgram package - best obtained by the package manager or here. An introduction and examples can also be found here. You should also make sure that each dimension is in fact independent. Many so called multi-dimensional datasets have multiple representations of the same thing. It is not wrong to have these in your data. It is wrong to use multiple versions of the same thing as support for a cluster argument. http://en.wikipedia.org/wiki/Cronbach's_alpha One way to do it is to run k-means with large k (much larger than what you think is the correct number), say 1000. then, running mean-shift algorithm on the these 1000 point (mean shift uses the whole data but you will only "move" these 1000 points). mean shift will find the amount of clusters then.
Running mean shift without the k-means before is a possibility but it is just too slow usually O(N^2*#steps), so running k-means before will speed things up: O(NK#steps) If the cluster number is unknow, why not use Hierarchical Clustering instead? At the begining, every isolated one is a cluster, then every two cluster will be merged if their distance is lower than a threshold, the algorithm will end when no more merger goes. The Hierarchical clustering algorithm can carry out a suitable "K" for your data.I am new to machine learning and deep learning, and for learning purposes I tried to play with Resnet. I tried to overfit over small data (3 different images) and see if I can get almost 0 loss and 1.0 accuracy - and I did. The problem is that predictions on the training images (i.e. the same 3 images used for training) are not correct.. Training Images  
 Image labels [1,0,0], [0,1,0], [0,0,1] My python code The model does overfit the data: but predictions are: which means that all images got label=[0,1,0] why? and how can that happen? It's because of the batch normalization layers. In training phase, the batch is normalized w.r.t. its mean and variance. However, in testing phase, the batch is normalized w.r.t. the moving average of previously observed mean and variance. Now this is a problem when the number of observed batches is small (e.g., 5 in your example) because in the BatchNormalization layer, by default moving_mean is initialized to be 0 and moving_variance is initialized to be 1. Given also that the default momentum is 0.99, you'll need to update the moving averages quite a lot of times before they converge to the "real" mean and variance. That's why the prediction is wrong in the early stage, but is correct after 1000 epochs. You can verify it by forcing the BatchNormalization layers to operate in "training mode". During training, the accuracy is 1 and the loss is close to zero: Now if we evaluate the model, we'll observe high loss and low accuracy because after 5 updates, the moving averages are still pretty close to the initial values: However, if we manually specify the "learning phase" variable and let the BatchNormalization layers use the "real" batch mean and variance, the result becomes the same as what's observed in fit(). It's also possible to verify it by changing the momentum to a smaller value. For example, by adding momentum=0.01 to all the batch norm layers in ResNet50, the prediction after 20 epochs is: ResNet50V2 (the 2nd version) has the much higher accuracy than ResNet50in predicting a given image such as the classical Egyptian cat. Predicted: [[('n02124075', 'Egyptian_cat', 0.8233388), ('n02123159', 'tiger_cat', 0.103765756), ('n02123045', 'tabby', 0.07267675), ('n03958227', 'plastic_bag', 3.6531426e-05), ('n02127052', 'lynx', 3.647774e-05)]] Comparing with the EfficientNet(90% accuracy), the ResNet50/101/152 predicts quite a bad result(15~50% accuracy) while adopting the given weights provided by Francios Cholett. It is not related to the weights, but related to the inherent complexity of the above model. In other words, it is necessary to re-train the above model to predict an given image. But EfficientNet does not need such the training to predict an image. For instance, while given a classical cat image, it shows the final result as follows. 1. Adoption of the decode_predictions Predicted: [[('n01930112', 'nematode', 0.122968934), ('n03041632', 'cleaver', 0.04236396), ('n03838899', 'oboe', 0.03846453), ('n02783161', 'ballpoint', 0.027445247), ('n04270147', 'spatula', 0.024508419)]] 2. Adoption of the CV2 Predicted: [[('n04065272', 'recreational_vehicle', 0.46529356), ('n01819313', 'sulphur-crested_cockatoo', 0.31684962), ('n04074963', 'remote_control', 0.051597465), ('n02111889', 'Samoyed', 0.040776145), ('n04548362', 'wallet', 0.029898684)]] Therefore, ResNet50/101/152 models are not suitable to predict an image without training even provided with the weights. But users can feel its value after 100~1000 epochs training for prediction because it helps obtain a better moving average. If users want an easy prediction, EfficientNet is a good choice with the given weights. It seems that predicting with a batch of images will not work correctly in Keras. It is better to do prediction for each image individually and then calculate the accuracy manually.
As an example, in the following code, I don't use batch prediction, but use individual image prediction. What happens is basically that keras.fit() i.e your  is while having the best fit the precision is lost. As, the precision is lost the models fit gives problems and varied results.The keras.fit only has a good fit not the required precisionI am new to TensorFlow. While I am reading the existing documentation, I found the term tensor really confusing. Because of it, I need to clarify the following questions: TensorFlow doesn't have first-class Tensor objects, meaning that there are no notion of Tensor in the underlying graph that's executed by the runtime. Instead the graph consists of op nodes connected to each other, representing operations. An operation allocates memory for its outputs, which are available on endpoints :0, :1, etc, and you can think of each of these endpoints as a Tensor. If you have tensor corresponding to nodename:0 you can fetch its value as sess.run(tensor) or sess.run('nodename:0'). Execution granularity happens at operation level, so the run method will execute op which will compute all of the endpoints, not just the :0 endpoint. It's possible to have an Op node with no outputs (like tf.group) in which case there are no tensors associated with it. It is not possible to have tensors without an underlying Op node. You can examine what happens in underlying graph by doing something like this So with tf.constant you get a single operation node, and you can fetch it using sess.run("Const:0") or sess.run(value) Similarly, value=tf.placeholder(tf.int32) creates a regular node with name Placeholder, and you could feed it as feed_dict={"Placeholder:0":2} or feed_dict={value:2}. You can not feed and fetch a placeholder in the same session.run call, but you can see the result by attaching a tf.identity node on top and fetching that. For variable You'll see that it creates two nodes Variable and Variable/read, the :0 endpoint is a valid value to fetch on both of these nodes. However Variable:0 has a special ref type meaning it can be used as an input to mutating operations. The result of Python call tf.Variable is a Python Variable object and there's some Python magic to substitute Variable/read:0 or Variable:0 depending on whether mutation is necessary. Since most ops have only 1 endpoint, :0 is dropped. Another example is Queue -- close() method will create a new Close op node which connects to Queue op. To summarize -- operations on python objects like Variable and Queue map to different underlying TensorFlow op nodes depending on usage.  For ops like tf.split or tf.nn.top_k which create nodes with multiple endpoints, Python's session.run call automatically wraps output in tuple or collections.namedtuple of Tensor objects which can be fetched individually. From the glossary: A Tensor is a typed multi-dimensional array. For example, a 4-D array of floating point numbers representing a mini-batch of images with dimensions [batch, height, width, channel]. Basically, every data is a Tensor in TensorFlow (hence the name): However, in the graph, every node is an operation, which can have Tensors as inputs or outputs. As already mentioned by others, yes they are all tensors. The way I understood those is to first visualize and understand 1D, 2D, 3D, 4D, 5D, and 6D tensors as in the picture below. (source: knoldus)  Now, in the context of TensorFlow, you can imagine a computation graph like the one below,  Here, the Ops take two tensors a and b as input; multiplies the tensors with itself and then adds the result of these multiplications to produce the result tensor t3. And these multiplications and addition Ops happen at the nodes in the computation graph. And these tensors a and b can be constant tensors, Variable tensors, or placeholders. It doesn't matter, as long as they are of the same data type and compatible shapes(or broadcastable to it) to achieve the operations. Data is stored in matrices. A 28x28 pixel grayscale image fits into a
28x28 two-dimensional matrix. But for a color image, we need more
dimensions. There are 3 color values per pixel (Red, Green, Blue), so
a three-dimensional table will be needed with dimensions [28, 28, 3].
And to store a batch of 128 color images, a four-dimensional table is
needed with dimensions [128, 28, 28, 3]. These multi-dimensional tables are called "tensors" and the list of
their dimensions is their "shape". Source TensorFlow's central data type is the tensor. Tensors are the underlying components of computation and a fundamental data structure in TensorFlow. Without using complex mathematical interpretations, we can say a tensor (in TensorFlow) describes a multidimensional numerical array, with zero or n-dimensional collection of data, determined by rank, shape, and type.Read More: What is tensors in TensorFlow?I thought mask_zero=True will output 0's when the input value is 0, so the following layers could skip computation or something. How does mask_zero works?  Example:  The actual output is: (the numbers are random) However, I thought the output will be: Actually, setting mask_zero=True for the Embedding layer does not result in returning a zero vector. Rather, the behavior of the Embedding layer would not change and it would return the embedding vector with index zero. You can confirm this by checking the Embedding layer weights (i.e. in the example you mentioned it would be m.layers[0].get_weights()). Instead, it would affect the behavior of the following layers such as RNN layers.  If you inspect the source code of Embedding layer you would see a method called compute_mask: This output mask will be passed, as the mask argument, to the following layers which support masking. This has been implemented in the __call__ method of base layer, Layer: And this makes the following layers to ignore (i.e. does not consider in their computations) this inputs steps. Here is a minimal example: As you can see the outputs of the LSTM layer for the second and forth timesteps are the same as the output of first and third timesteps, respectively. This means that  those timesteps have been masked. Update: The mask will also be considered when computing the loss since the loss functions are internally augmented to support masking using weighted_masked_objective: when compiling the model: You can verify this using the following example: The process of informing the Model that some part of the Data is actually Padding and should be ignored is called Masking. There are three ways to introduce input masks in Keras models: Given below is the code to introduce Input Masks using keras.layers.Embedding Output of the above code is shown below: For more information, refer this Tensorflow Tutorial.Is it possible to have two fit_generator? I'm creating a model with two inputs,
The model configuration is shown below.  Label Y uses the same labeling for X1 and X2 data. The following error will continue to occur. Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected
  to see 2 array(s), but instead got the following list of 1 arrays:
  [array([[[[0.75686276, 0.75686276, 0.75686276],
           [0.75686276, 0.75686276, 0.75686276],
           [0.75686276, 0.75686276, 0.75686276],
           ...,
           [0.65882355, 0.65882355, 0.65882355... My code looks like this: Try this generator: Generator for 3 inputs: EDIT (add generator, output image and numpy array, and target) I have an implementation for multiple inputs for TimeseriesGenerator that I have adapted it (I have not been able to test it unfortunately) to meet this example with ImageDataGenerator. My approach was to build a wrapper class for the multiple generators from keras.utils.Sequence and then implement the base methods of it: __len__ and __getitem__: You can use this generator with model.fit_generator() once the generator has been instanced.If using a library like scikit-learn, how do I assign more weight on certain features in the input to a classifier like SVM? Is this something people do or not? First of all - you should probably not do it. The whole concept of machine learning is to use statistical analysis to assign optimal weights. You are interfering here with the whole concept, thus you need really strong evidence that this is crucial to the process you are trying to model, and for some reason your model is currently missing it. That being said - there is no general answer. This is purely model specific, some of which will allow you to weight features - in random forest you could bias distribution from which you sample features to analyse towards the ones that you are interested in; in SVM it should be enough to just multiply given feature by a constant - remember when you were told to normalize your features in SVM? This is why - you can use the scale of features to 'steer' your classifier towards given features. The ones with high values will be preffered. This will actually work for most linear weight norm-regularized models (regularized logistic regression, ridge regression, lasso etc.). The best way to do this is:
Assume you have f[1,2,..N] and weight of particular feature is w_f[0.12,0.14...N].
First of all, you need to normalize features by any feature scaling methods and then you need to also normalize the weights of features w_f to [0-1] range and then multiply the normalized weight by f[1,2,..N] with the new transformed features.
Remember you need to transform this in test data as well. Now you can check the performance of both models: without introducing the feature and with introducing the feature. As already mentioned, I wouldn't suggest using index weights as that is the job of ML. However, a ranking of weights in my opinion will have to be done in the original data source (database table, .txt, etc.) by updating an additional field and always with the range from 0 to 1 i.e., 0.1, 0.2. ... 0.7 ...), and certainly always in absolute correlation with the corresponding features(parameters).I'm trying to run a linear regression in PySpark and I want to create a table containing summary statistics such as coefficients, P-values and t-values for each column in my dataset. However, in order to train a linear regression model I had to create a feature vector using Spark's VectorAssembler, and now for each row I have a single feature vector and the target column.
When I try to access Spark's in-built regression summary statistics, they give me a very raw list of numbers for each of these statistics, and there's no way to know which attribute corresponds to which value, which is really difficult to figure out manually with a large number of columns.
How do I map these values back to the column names? For example, I have my current output as something like this: Coefficients: [-187.807832407,-187.058926726,85.1716641376,10595.3352802,-127.258892837,-39.2827730493,-1206.47228704,33.7078197705,99.9956812528] P-Value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18589731365614548, 0.275173571416679, 0.0] t-statistic: [-23.348593508995318, -44.72813283953004, 19.836508234714472, 144.49248881747755, -16.547272230754242, -9.560681351483941, -19.563547400189073, 1.3228378389036228, 1.0912415361190977, 20.383256127350474] Coefficient Standard Errors: [8.043646497811427, 4.182131353367049, 4.293682291754585, 73.32793120907755, 7.690626652102948, 4.108783841348964, 61.669402913526625, 25.481445101737247, 91.63478289909655, 609.7007361468519] These numbers mean nothing unless I know which attribute they correspond to. But in my DataFrame I only have one column called "features" which contains rows of sparse Vectors. This is an ever bigger problem when I have one-hot encoded features, because if I have one variable with an encoding of length n, I will get n corresponding coefficients/p-values/t-values etc. As of today Spark doesn't provide any method that can do it for you, so if you have to create your own. Let's say your data looks like this: and is processed using following pipeline: Get the LinearRegressionModel: Transform the data: Extract and flatten ML attributes: and map to the output: You can see the actual order of the columns here there will be two classes usually, ["binary] & ["numeric"] Should give the exact order of all the columns Here's the one line answer: Thanks to @pratiklodha for the core of this.How do you calculate a best fit line in python, and then plot it on a scatterplot in matplotlib?  I was I calculate the linear best-fit line using Ordinary Least Squares Regression as follows: This is multivariate (there are many x-values for each case). So, X is a list of lists, and y is a single list. 
For example:  But how do I do this with higher order polynomial functions. For example, not just linear (x to the power of M=1), but binomial (x to the power of M=2), quadratics (x to the power of M=4), and so on. For example, how to I get the best fit curves from the following? Extracted from Christopher Bishops's "Pattern Recognition and Machine Learning", p.7:  The accepted answer to this question
provides a small multi poly fit library which will do exactly what you need using numpy, and you can plug the result into the plotting as I've outlined below. You would just pass in your arrays of x and y points and the degree(order) of fit you require into multipolyfit. This returns the coefficients which you can then use for plotting using numpy's polyval. Note: The code below has been amended to do multivariate fitting, but the plot image was part of the earlier, non-multivariate answer.  Note: This was part of the answer earlier on, it is still relevant if you don't have multivariate data. Instead of coeffs = mpf(..., use coeffs = numpy.polyfit(x,y,3) For non-multivariate data sets, the easiest way to do this is probably with numpy's polyfit: numpy.polyfit(x, y, deg, rcond=None, full=False, w=None, cov=False) Least squares polynomial fit. Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). Returns a vector of coefficients p that minimises the squared error. Slightly out of context because the resulting function is not a polynomial, but still interesting perhaps. One major problem with polynomial fitting is Runge's phenomenon: The higher the degree, the more dramatic oscillations will occur. This isn't just constructed either but it will come back to bite you. As a remedy, I created smoothfit a while ago. It solves an appropriate least-squares problem and gives nice results, e.g.:I am implementing logistic regression using batch gradient descent. There are two classes into which the input samples are to be classified. The classes are 1 and 0. While training the data, I am using the following sigmoid function: where And I am using the following cost function to calculate cost, to determine when to stop training.  I am getting the cost at each step to be NaN as the values of htheta are either 1 or zero in most cases. What should I do to determine the cost value at each iteration?  This is the gradient descent code for logistic regression: There are two possible reasons why this may be happening to you.  This is because when you apply the sigmoid / logit function to your hypothesis, the output probabilities are almost all approximately 0s or all 1s and with your cost function, log(1 - 1) or log(0) will produce -Inf.  The accumulation of all of these individual terms in your cost function will eventually lead to NaN.   Specifically, if y = 0 for a training example and if the output of your hypothesis is log(x) where x is a very small number which is close to 0, examining the first part of the cost function would give us 0*log(x) and will in fact produce NaN.  Similarly, if y = 1 for a training example and if the output of your hypothesis is also log(x) where x is a very small number, this again would give us 0*log(x) and will produce NaN.  Simply put, the output of your hypothesis is either very close to 0 or very close to 1.   This is most likely due to the fact that the dynamic range of each feature is widely different and so a part of your hypothesis, specifically the weighted sum of x*theta for each training example you have will give you either very large negative or positive values, and if you apply the sigmoid function to these values, you'll get very close to 0 or 1. One way to combat this is to normalize the data in your matrix before performing training using gradient descent.  A typical approach is to normalize with zero-mean and unit variance.  Given an input feature x_k where k = 1, 2, ... n where you have n features, the new normalized feature x_k^{new} can be found by:  m_k is the mean of the feature k and s_k is the standard deviation of the feature k.  This is also known as standardizing data.  You can read up on more details about this on another answer I gave here: How does this code for standardizing data work? Because you are using the linear algebra approach to gradient descent, I'm assuming you have prepended your data matrix with a column of all ones.  Knowing this, we can normalize your data like so:  The mean and standard deviations of each feature are stored in mX and sX respectively.  You can learn how this code works by reading the post I linked to you above. I won't repeat that stuff here because that isn't the scope of this post. To ensure proper normalization, I've made the mean and standard deviation of the first column to be 0 and 1 respectively.  xnew contains the new normalized data matrix.  Use xnew with your gradient descent algorithm instead.  Now once you find the parameters, to perform any predictions you must normalize any new test instances with the mean and standard deviation from the training set.  Because the parameters learned are with respect to the statistics of the training set, you must also apply the same transformations to any test data you want to submit to the prediction model.  Assuming you have new data points stored in a matrix called xx, you would do normalize then perform the predictions: Now that you have this, you can perform your predictions: You can change the threshold of 0.5 to be whatever you believe is best that determines whether examples belong in the positive or negative class. As you mentioned in the comments, once you normalize the data the costs appear to be finite but then suddenly go to NaN after a few iterations. Normalization can only get you so far. If your learning rate or alpha is too large, each iteration will overshoot in the direction towards the minimum and would thus make the cost at each iteration oscillate or even diverge which is what is appearing to be happening. In your case, the cost is diverging or increasing at each iteration to the point where it is so large that it can't be represented using floating point precision.  As such, one other option is to decrease your learning rate alpha until you see that the cost function is decreasing at each iteration. A popular method to determine what the best learning rate would be is to perform gradient descent on a range of logarithmically spaced values of alpha and seeing what the final cost function value is and choosing the learning rate that resulted in the smallest cost.  Using the two facts above together should allow gradient descent to converge quite nicely, assuming that the cost function is convex. In this case for logistic regression, it most certainly is.  Let's assume you have an observation where:  Then your cost function will get a value of NaN because you're adding 0 * log(0), which is undefined. Hence: As @rayryeng pointed out, 0 * log(0) produces a NaN because 0 * Inf isn't kosher. This is actually a huge problem: if your algorithm believes it can predict a value perfectly, it incorrectly assigns a cost of NaN.  Instead of: You can avoid multiplying 0 by infinity by instead writing your cost function in Matlab as: The idea is if y_i is 1, we add -log(htheta_i) to the cost, but if y_i is 0, we add -log(1 - htheta_i) to the cost. This is mathematically equivalent to -y_i * log(htheta_i) - (1 - y_i) * log(1- htheta_i) but without running into numerical problems that essentially stem from htheta_i being equal to 0 or 1 within the limits of double precision floating point. It happened to me because an indetermination of the type: This can happen when one of the predicted values Y equals either 0 or 1.
In my case the solution was to add an if statement to the python code as follows: This way, when the actual value (y) and the predicted one (Y) are equal, no cost needs to be computed, which is the expected behavior.   (Notice that when a given Y is converging to 0 the left addend is canceled (because of y=0) and the right addend tends toward 0. The same happens when Y converges to 1, but with the opposite addend.) (There is also a very rare scenario, which you probably won't need to worry about, where y=0 and Y=1 or viceversa, but if your dataset is standarized and the weights are properly initialized it won't be an issue.)I know that Cross validation is used for selecting good parameters. After finding them, i need to re-train the whole data without the -v option. But the problem i face is that after i train with -v option, i get the cross-validation accuracy( e.g 85%). There is no model and i can't see the values of C and gamma. In that case how do i retrain? Btw i applying 10 fold cross validation.
e.g  Need some help on it.. To get the best C and gamma, i use this code that is available in the LIBSVM FAQ Another question : Is that cross-validation accuracy after using -v option similar to that we get when we train without -v option and use that model to predict? are the two accuracy similar? Another question : Cross-validation basically improves the accuracy of the model by avoiding the overfitting. So, it needs to have a model in place before it can improve. Am i right? Besides that, if i have a different model, then the cross-validation accuracy will be different? Am i right? One more question: In the cross-validation accuracy, what is the value of C and gamma then? The graph is something like this 
 Then the values of C are 2 and gamma = 0.0078125. But when i retrain the model with the new parameters. The value is not the same as 99.63%. Could there be any reason?
Thanks in advance... The -v option here is really meant to be used as a way to avoid the overfitting problem (instead of using the whole data for training, perform an N-fold cross-validation training on N-1 folds and testing on the remaining fold, one at-a-time, then report the average accuracy). Thus it only returns the cross-validation accuracy (assuming you have a classification problem, otherwise mean-squared error for regression) as a scalar number instead of an actual SVM model. If you want to perform model selection, you have to implement a grid search using cross-validation (similar to the grid.py helper python script), to find the best values of C and gamma. This shouldn't be hard to implement: create a grid of values using MESHGRID, iterate overall all pairs (C,gamma) training an SVM model with say 5-fold cross-validation, and choosing the values with the best CV-accuracy... Example:  If you use your entire dataset to determine your parameters, then train on that dataset, you are going to overfit your data.  Ideally, you would divide the dataset, do the parameter search on a portion (with CV), then use the other portion to train and test with CV.  Will you get better results if you use the whole dataset for both? Of course, but your model is likely to not generalize well.  If you want determine true performance of your model, you need to do parameter selection separately.Given a predefined Keras model, I am trying to first load in pre-trained weights, then remove one to three of the models internal (non-last few) layers, and then replace it with another layer. I can't seem to find any documentation on keras.io about to do such a thing or remove layers from a predefined model at all. The model I am using is a good ole VGG-16 network which is instantiated in a function as shown below: So as an example, I'd like to take the two Conv layers in Block 1 and replace them with just one Conv layer, after loading the original weights into all of the other layers. Any ideas?  Assuming that you have a model vgg16_model, initialized either by your function above or by keras.applications.VGG16(weights='imagenet'). Now, you need to insert a new layer in the middle in such a way that the weights of other layers will be saved. The idea is to disassemble the whole network to separate layers, then assemble it back. Here is the code specifically for your task: And the output of the above code is: Another way to do this is by building a Sequential model.
See the following example where I swap ReLU layers for PReLU.
You would need to simply not add the layers you don't want, and add a new layer.I have written an RNN language model using TensorFlow. The model is implemented as an RNN class. The graph structure is built in the constructor, while RNN.train and RNN.test methods run it. I want to be able to reset the RNN state when I move to a new document in the training set, or when I want to run a validation set during training. I do this by managing the state inside the training loop, passing it into the graph via a feed dictionary. In the constructor I define the the RNN like so The training loop looks like this x and y are batches of training data in a document. The idea is that I pass the latest state along after each batch, except when I start a new document, when I zero out the state by running self.reset_state. This all works.  Now I want to change my RNN to use the recommended state_is_tuple=True. However, I don't know how to pass the more complicated LSTM state object via a feed dictionary. Also I don't know what arguments to pass to the self.state = tf.placeholder(...) line in my constructor. What is the correct strategy here? There still isn't much example code or documentation for dynamic_rnn available. TensorFlow issues 2695 and 2838 appear relevant. A blog post on WILDML addresses these issues but doesn't directly spell out the answer. See also TensorFlow: Remember LSTM state for next batch (stateful LSTM). One problem with a Tensorflow placeholder is that you can only feed it with a Python list or Numpy array (I think). So you can't save the state between runs in tuples of LSTMStateTuple.  I solved this by saving the state in a tensor like this initial_state = np.zeros((num_layers, 2, batch_size, state_size)) You have two components in an LSTM layer, the cell state and hidden state, thats what the "2" comes from. (this article is great: https://arxiv.org/pdf/1506.00019.pdf) When building the graph you unpack and create the tuple state like this: Then you get the new state the usual way It shouldn't be like this... perhaps they are working on a solution. A simple way to feed in an RNN state is to simply feed in both components of the state tuple individually.As described in figure 1, I have 3 models which each apply to a particular domain. The 3 models are trained separately with different datasets.
 And inference is sequential :  I tried to parallelize the call of these 3 models thanks to the Multiprocess library of python but it is very unstable and it is not advised. Here's the idea I got to make sure to do this all at once: As the 3 models share a common pretrained-model, I want to make a single model that has multiple inputs and multiple outputs. As the following drawing shows:
 Like that during the inference, I will call a single model which will do all 3 operations at the same time.  I saw that with The Functional API of KERAS, it is possible but I have no idea how to do that.
The inputs of the datasets have the same dimension. These are pictures of (200,200,3). If anyone has an example of a Multi-Input Multi-output model that shares a common structure, I'm all ok. Here is the example of my code but it returns an error because of the layers. concatenate (...) line which propagates a shape that is not taken into account by the EfficientNet model. We can do that easily in tf. keras using its awesome Functional API. Here we will walk you through how to build multi-out with a different type (classification and regression) using Functional API. According to your last diagram, you need one input model and three outputs of different types. To demonstrate, we will use MNIST which is a handwritten dataset. It's normally a 10 class classification problem data set. From it, we will create an additionally 2 class classifier (whether a digit is even or odd) and also a 1 regression part (which is to predict the square of a digit, i.e for image input of 9, it should give approximately it's square). Data Set So, our training pairs will be xtrain and [y_out_a, y_out_b, y_out_c], the same as your last diagram. Model Building Let's build the model accordingly using the Functional API of tf. keras. See the model definition below. The MNIST samples are a 28 x 28 grayscale image. So our input is set in that way. I'm guessing your data set is probably RGB, so change the input dimension accordingly.  One thing to note, while defining out_a, out_b, and  out_c during model definition we set their name variable which is very important. Their names are set '10cls', '2cls', and '1rg' respectively. You can also see this from the above diagram (last 3 tails). Compile and Run Now, we can see why that name variable is important. In order to run the model, we need to compile it first with the proper loss function, metrics, and optimizer. Now, if you know that, for the classification and regression problem, the optimizer can be the same but for the loss function and metrics should be changed. And in our model, which has a multi-type output model (2 classifications and 1 regression), we need to set proper loss and metrics for each of these types. Please, see below how it's done. See, each last output of our above model, which is here represented by their name variables. And we set proper compilation to them. Hope you understand this part. Now, time to train the model. That's how each of the outputs of the last layer optimizes by their concern loss function. FYI, one thing to mention, there is an essential parameter while .compile the model which you might need: loss_weights - to weight the loss contributions of different model outputs. See my other answer here on this. Prediction / Inference Let's see some output. We now hope this model will predict 3 things: (1) is what the digit is, (2) is it even or odd, and (3) its square value.  If we like to quickly check the output layers of our model Passing this xtrain[0] (which we know 5) to the model to do predictions. Based on your comment, we can extend the above model to take multi-input too. We need to change things. To demonstrate, we will use train and test samples of the mnist data set to the model as a multi-input. Next, we need to modify some parts of the above model to take multi-input. And next if you now plot, you will see the new graph.  Now, we can train the model as follows Now, we can test the multi-input model and get multi-out from it.I want to select the top N=10,000 principal components from a matrix. After the pca is completed, MATLAB should return a pxp matrix, but it doesn't! It should be coefs:153600 x 153600? and scores:400 X 153600? When I use the below code it gives me an Out of Memory error:: I don't understand why MATLAB returns a lesser dimensional matrix. It
should return an error with pca: 153600*153600*8 bytes=188 GB Error with eigs: I think you are falling prey to the XY problem, since trying to find 153.600 dimensions in your data is completely non-physical, please ask about the problem (X) and not your proposed solution (Y) in order to get a meaningful answer. I will use this post only to tell you why PCA is not  a good fit in this case. I cannot tell you what will solve your problem, since you have not told us what that is. This is a mathematically unsound problem, as I will try to explain here. PCA is, as user3149915 said, a way to reduce dimensions. This means that somewhere in your problem you have one-hundred-fifty-three-thousand-six-hundred dimensions floating around. That's a lot. A heck of a lot. Explaining a physical reason for the existence of all of them might be a bigger problem than trying to solve the mathematical problem. Trying to fit that many dimensions to only 400 observations will not work, since even if all observations are linear independent vectors in your feature space, you can still extract only 399 dimensions, since the rest simply cannot be found since there are no observations. You can at most fit N-1 unique dimensions through N points, the other dimensions have an infinite number of possibilities of location. Like trying to fit a plane through two points: there's a line you can fit through those and the third dimension will be perpendicular to that line, but undefined in the rotational direction. Hence, you are left with an infinite number of possible planes that fit through those two points. After the first 400 components, there's no more dimensions left. You are fitting a void after that. You used all your data to get the dimensions and cannot create more dimensions. Impossible. All you can do is get more observations, some 1.5M, and do the PCA again. Why do you need more observations than dimensions? you might ask. Easy, you cannot fit a unique line through a point, nor a unique plane through two points, nor a unique 153.600 dimensional hyperplane through 400 points. Sadly, no. If you have two points and fit a line through it you get a 100% fit. No error, jay! Done for the day, let's go home and watch TV! Sadly, your boss will call you in the next morning since your fit is rubbish. Why? Well, if you'd have for instance 20 points scattered around, the fit would not be without errors, but at least closer to representing your actual data, since the first two could be outliers, see this very illustrative figure, where the red points would be your first two observations:  If you were to extract the first 10.000 components, that'd be 399 exact fits and 9601 zero dimensions. Might as well not even attempt to calculate beyond the 399th dimension, and stick that into a zero array with 10.000 entries. TL;DR You cannot use PCA and we cannot help you solve your problem as long as you do not tell us what your problem is. PCA is a dimension reduction algorithm, as such it tries to reduce the number of features to principal components (PC) that each represents some linear combination of the total features. All of this is done in order to reduce the dimensions of the feature space, i.e. transform the large feature space to one that is more manageable but still retains most if not all of the information.  Now for your problem, you are trying to explain the variance across your 400 observations using 153600 features, however, we don't need that much information 399 PC's will explain 100% of the variance across your sample (I will be very surprised if that is not the case). The reason for that is basicly overfitting, your algorithm finds noise that explain every observation in your sample.  So what the rayryeng was telling you is correct, if you want to reduce your feature space to 10,000 PC's you will need 100,000 observations for the PC's to mean anything (that is a rule of thumb but a rather stable one). And the reason that matlab was giving you 399 PC's because it was able to correctly extract 399 linear combinations that explained some #% of the variance across your sample.  If on the other hand what you are after are the most relevant features than you are not looking for dimensional reduction flows, but rather feature elimination processes. These will keep only the most relevant feature while nulling the irrelevant ones.   So just to make clear, if your feature space is rubbish and there isn't any information there just noise, the variance explained will be irrelevant and will indeed be less than 100% for example see the following    Again if you want to reduce your feature space there are ways to that even with a small m, but PCA is not one of them.  Good Luck     Matlab tries to not waste too much resources computing it.
But you still can do what you want, just use:I convert my image data to caffe db format (leveldb, lmdb) using C++ as example I use this code for imagenet. Is data need to be shuffled, can I write to db all my positives and then all my negatives like 00000000111111111, or data need to be shuffled and labels should look like 010101010110101011010? How caffe sample data from DB, is it true that it use random subset of all data with size = batch_size? Should you shuffle the samples? Think about the learning process if you don't shuffle; caffe sees only 0 samples - what do you expect the algorithm to deduce? simply predict 0 all the time and everything is cool. If you have plenty of 0 before you hit the first 1 caffe will be very confident in predicting always 0. It will be very difficult to move the model from this point.
On the other hand, if it constantly sees a mix of 0 and 1 it learns from the beginning meaningful features for separating the examples.
Bottom line: it is very advantageous to shuffle the training samples, especially when using SGD-based approaches.  AFAIK, caffe does not randomly sample batch_size samples, but rather goes sequentially over the input DB batch_size after batch_size samples. TL;DR
shuffle.I am very new to matplotlib and am working on simple projects to get acquainted with it. I was wondering how I might plot the decision boundary which is the weight vector of the form [w1,w2], which basically separates the two classes lets say C1 and C2, using matplotlib. Is it as simple as plotting a line from (0,0) to the point (w1,w2) (since W is the weight "vector") if so, how do I extend this like in both directions if I need to? Right now all I am doing is :  Thanks in advance. Decision boundary is generally much more complex then just a line, and so (in 2d dimensional case) it is better to use the code for generic case, which will also work well with linear classifiers. The simplest idea is to plot contour plot of the decision function some examples from sklearn documentationI am trying to convert my model in Tensorflow (.pb) format to Keras (.h5) format to view post hoc attention visualisation. 
I have tried below code. Can anyone help me with this? Is this even possible? In the Latest Tensorflow Version (2.2), when we Save the Model using tf.keras.models.save_model, the Model will be Saved in not just a pb file but it will be Saved in a Folder, which comprises Variables Folder and Assets Folder, in addition to the saved_model.pb file, as shown in the screenshot below:  For example, if the Model is Saved with the Name, "Model", we have to Load using the Name of the Folder, "Model", instead of saved_model.pb, as shown below: instead of One more change you can do is to replace with Complete working Code to convert a Model from Tensorflow Saved Model Format (pb) to Keras Saved Model Format (h5) is shown below: Output of the New_Model.summary command is: Continuing the code: Output of the command, print(loaded_model_from_h5.summary()) is shown below: ​
As can be seen from the Summary of both the Models above, both the Models are same.I'm trying to do image classification with two classes. I have 1000 images with balanced classes. When I train the model, I get a low constant validation accuracy but a decreasing validation loss. Is this a sign of overfitting or underfitting? I should also note that I'm attempting to retrain the Inception V3 model with new classes and a different dataset. Overfitting ( or underfitting) occurs when a model is too specific (or not specific enough) to the training data, and doesn't extrapolate well to the true domain. I'll just say overfitting from now on to save my poor typing fingers [*] I think the wikipedia image is good:  Clearly, the green line, a decision boundary trying to separate the red class from the blue, is "overfit", because although it will do well on the training data, it lacks the "regularized" form we like to see when generalizing [**]. These CMU slides on overfitting/cross validation also make the problem clear:  And here's some more intuition for good measure Overfitting is observed numerically when the testing error does not  reflect the training error Obviously, the testing error will always (in expectation) be worse than the training error, but at a certain number of iterations, the loss in testing will start to increase, even as the loss in training continues to decline. Overfitting can be observed by plotting the decision boundary (as in
  the wikipedia image above) when dimensionality allows, or by looking
  at testing loss in addition to training loss during the fit procedure You don't give us enough points to make these graphs, but here's an example (from someone asking a similar question) showing what those loss graphs would look like:
 While loss curves are sometimes more pretty and logarthmic, note the trend here that training error is still decreasing but testing error is on the rise. That's a big red flag for overfitting. SO discusses loss curves here The slightly cleaner and more real-life example is from this CMU lecture on ovefitting ANN's:  The top graph is overfitting, as before. The bottom graph is not. When does this occur? When a model has too many parameters, it is susceptible to overfitting (like a n-degree polynomial to n-1 points). Likewise, a model with not enough parameters can be underfit. Certain regularization techniques like dropout or batch normalization, or traditionally l-1 regularization combat this. I believe this is beyond the scope of your question. Footnotes [*] There's no reason to keep writing "overfitting/underfitting", since the reasoning is the same for both, but the indicators are flipped, obviously (a decision boundary that hasn't latched onto the true border enough, as opposed to being too tightly wrapped against individual points). In general, overfitting is the more common to avoid, since "more iterations/more parameters" is the current theme. If you have lots of data and not lot of parameters, maybe you really are worried about underfitting, but I doubt it.  [**] One way to formalize the idea that the black line is preferable than the green one in the first image from wikipedia is to penalize the number of parameters required by your model during model selectionI'm using Pydantic model (Basemodel) with FastAPI and converting the input into a dictionary, and then converting it into a Pandas DataFrame to assign it into model.predict() function for Machine Learning prediction, as shown below : It works fine, I'm just not quite sure if it's optimized or the right way to do it, since I convert the input two times to get the predictions. Also, I'm not sure if it is going to work fast in the case of having a huge number of inputs. Any improvements for this? If there's a way (even other than using Pydantic models, where I can work directly and avoid going through conversions and the loop. First, you should use more descriptive names for your variables/objects. For example: You cannot pass the Pydantic model directly to the predict() function, as it accepts a data array, not a Pydantic model. Available options are listed below. You could use: If you don't wish to use a Pandas DataFrame, as shown in your question, i.e., then, you could use the __dict__ method to get the values of all attributes in the model and convert it to a list: or, preferably, use the Pydantic's .dict() method: You could avoid looping over individual items and calling the predict() function multiple times, by using, instead, the below: or (in case you don't wish using Pandas DataFrame):Below is my code. I know why the error is occurring during transform. It is because of the feature list mismatch during fit and transform.
How can i solve this? How can i get 0 for all the rest features? After this i want to use this for partial fit of SGD classifier.  Instead of using pd.get_dummies() you need LabelEncoder + OneHotEncoder which can store the original values and then use them on the new data. Changing your code like below will give you required results. You encoder is fitted on refreshed_df which contains 10 columns and your refreshed_df1 contains only 4, literally what it is reported in the error. You have either to delete the columns not appearing on your refreshed_df1 or just fit your encoder to a new version of refreshed_df that only contains the 4 columns appearing in refreshed_df1 .I have a numpy array like this: I transform it like this to reduce the memory demand: resulting in this: However, when I do this: I get: Any ideas why? Ultimately, the numpy array contains the labels for a binary classification problem. So far, I have used it as float32 as is in a Keras ANN and it worked fine and I achieved pretty good performance. So is it actually necessary to run to_categorical? You don't need to use to_categorical since I guess you are doing multi-label classification. To avoid any confusion once and for all(!), let me explain this. If you are doing binary classification, meaning each sample may belong to only one 
of two classes e.g. cat vs dog or happy vs sad or positive review vs negative review, then: If you are doing multi-class classification, meaning each sample may belong to only one of many classes e.g. cat vs dog vs lion or happy vs neutral vs sad or positive review vs neutral review vs negative review, then: If you are doing multi-label classification, meaning each sample may belong to zero, one or more than one classes e.g. an image may contain both cat and dog, then: Ignoring the fact that the application of to_categorical is pointless in my scenario. The following solves the memory issue:This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. When we have to predict the value of a categorical (or discrete) outcome we use logistic regression. I believe we use linear regression to also predict the value of an outcome given the input values. Then, what is the difference between the two methodologies? Linear regression output as probabilities It's tempting to use the linear regression output as probabilities but it's a mistake because the output can be negative, and greater than 1 whereas probability can not. As regression might actually
produce probabilities that could be less than 0, or even bigger than
1, logistic regression was introduced.  Source: http://gerardnico.com/wiki/data_mining/simple_logistic_regression  Outcome In linear regression, the outcome (dependent variable) is continuous.
It can have any one of an infinite number of possible values.  In logistic regression, the outcome (dependent variable) has only a limited number of possible values.  The dependent variable Logistic regression is used when the response variable is categorical in nature. For instance, yes/no, true/false, red/green/blue,
1st/2nd/3rd/4th, etc.   Linear regression is used when your response variable is continuous. For instance, weight, height, number of hours,  etc. Equation Linear regression gives an equation which is of the form Y = mX + C,
means equation with degree 1.  However, logistic regression gives an equation which is of the form 
Y = eX + e-X Coefficient interpretation In linear regression, the coefficient interpretation of independent variables are quite straightforward (i.e. holding all other variables constant, with a unit increase in this variable, the dependent variable is expected to increase/decrease by xxx).  However, in logistic regression, depends on the family (binomial, Poisson,
etc.) and link (log, logit, inverse-log, etc.) you use, the interpretation is different.  Error minimization technique Linear regression uses ordinary least squares method to minimise the
errors and arrive at a best possible fit, while logistic regression
uses maximum likelihood method to arrive at the solution. Linear regression is usually solved by minimizing the least squares error of the model to the data, therefore large errors are penalized quadratically.  Logistic regression is just the opposite. Using the logistic loss function causes large errors to be penalized to an asymptotically constant. Consider linear regression on categorical {0, 1} outcomes to see why this is a problem. If your model predicts the outcome is 38, when the truth is 1, you've lost nothing. Linear regression would try to reduce that 38, logistic wouldn't (as much)2. In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values. For instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be any, there are so many possible values that a linear regression model would be chosen. If, instead, you wanted to predict, based on size, whether a house would sell for more than $200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than $200K, or No, the house will not. Just to add on the previous answers. Linear regression Is meant to resolve the problem of predicting/estimating the output value for a given element X (say f(x)). The result of the prediction is a continuous function where the values may be positive or negative. In this case you normally have an input dataset with lots of examples and the output value for each one of them. The goal is to be able to fit a model to this data set so you are able to predict that output for new different/never seen elements. Following is the classical example of fitting a line to set of points, but in general linear regression could be used to fit more complex models (using higher polynomial degrees):  Resolving the problem Linear regression can be solved in two different ways: Logistic regression Is meant to resolve classification problems where given an element you have to classify the same in N categories. Typical examples are, for example, given a mail to classify it as spam or not, or given a vehicle find to which category it belongs (car, truck, van, etc ..). That's basically the output is a finite set of discrete values. Resolving the problem Logistic regression problems could be resolved only by using Gradient descent. The formulation in general is very similar to linear regression the only difference is the usage of different hypothesis function. In linear regression the hypothesis has the form: where theta is the model we are trying to fit and [1, x_1, x_2, ..] is the input vector. In logistic regression the hypothesis function is different:  This function has a nice property, basically it maps any value to the range [0,1] which is appropiate to handle propababilities during the classificatin. For example in case of a binary classification g(X) could be interpreted as the probability to belong to the positive class. In this case normally you have different classes that are separated with a decision boundary which basically a curve that decides the separation between the different classes. Following is an example of dataset separated in two classes.  You can also use the below code to generate the linear regression
curve
q_df = details_df
# q_df = pd.get_dummies(q_df) lmod = sm.OLS(train_y, train_x).fit() lmod.summary() lmod.predict()[:10] lmod.get_prediction().summary_frame()[:10] sm.qqplot(lmod.resid,line="q") plt.title("Q-Q plot of Standardized
Residuals") plt.show() Simply put, linear regression is a regression algorithm, which outpus a possible continous and infinite value; logistic regression is considered as a binary classifier algorithm, which outputs the 'probability' of the input belonging to a label (0 or 1). The basic difference : Linear regression is basically a regression model which means its will give a non discreet/continuous output of a function. So this approach gives the value. For example : given x what is f(x) For example given a training set of different factors and the price of a property after training we can provide the required factors to determine what will be the property price. Logistic regression is basically a binary classification algorithm which means that here there will be discreet valued output for the function . For example : for a given x if f(x)>threshold classify it to be 1 else classify it to be 0. For example given a set of brain tumour size as training data we can use the size as input to determine whether its a benine or malignant tumour. Therefore here the output is discreet either 0 or 1. *here the function is basically the hypothesis function They are both quite similar in solving for the solution, but as others have said, one (Logistic Regression) is for predicting a category "fit" (Y/N or 1/0), and the other (Linear Regression) is for predicting a value. So if you want to predict if you have cancer Y/N (or a probability) - use logistic.  If you want to know how many years you will live to - use Linear Regression ! Regression means continuous variable, Linear means there is linear relation between y and x. 
Ex= You are trying to predict salary from no of years of experience. So here salary is independent variable(y) and yrs of experience is dependent variable(x).
y=b0+ b1*x1

We are trying to find optimum value of constant b0 and b1 which will give us best fitting line for your observation data.
It is a equation of line which gives continuous value from x=0 to very large value.
This line is called Linear regression model. Logistic regression is type of classification technique. Dnt be misled by term regression. Here we predict whether y=0 or 1. Here we first need to find p(y=1) (wprobability of y=1) given x from formuale below.  Probaibility p is related to y by below formuale  Ex=we can make classification of tumour having more than 50% chance of having cancer  as 1 and tumour having less than 50% chance of having cancer as 0.
 Here red point will be predicted as 0 whereas green point will be predicted as 1. Cannot agree more with the above comments. 
Above that, there are some more differences like In Linear Regression, residuals are assumed to be normally distributed. 
In Logistic Regression, residuals need to be independent but not normally distributed.  Linear Regression assumes that a constant change in the value of the explanatory variable results in constant change in the response variable. 
This assumption does not hold if the value of the response variable represents a probability (in Logistic Regression) GLM(Generalized linear models) does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model. In short:
Linear Regression gives continuous output. i.e. any value between a range of values.
Logistic Regression gives discrete output. i.e. Yes/No, 0/1 kind of outputs. To put it simply, if in linear regression model more test cases arrive which are far away from the threshold(say =0.5)for a prediction of y=1 and y=0. Then in that case the hypothesis will change and become worse.Therefore linear regression model is not used for classification problem. Another Problem is that if the classification is y=0 and y=1, h(x) can be > 1 or < 0.So we use Logistic regression were 0<=h(x)<=1. Logistic Regression is used in predicting categorical outputs like Yes/No, Low/Medium/High etc. You have basically 2 types of logistic regression Binary Logistic Regression (Yes/No, Approved/Disapproved) or Multi-class Logistic regression (Low/Medium/High, digits from 0-9 etc) On the other hand, linear regression is if your dependent variable (y) is continuous. 
y = mx + c is a simple linear regression equation (m = slope and c is the y-intercept). Multilinear regression has more than 1 independent variable (x1,x2,x3 ... etc)  In linear regression the outcome is continuous whereas in logistic regression, the outcome has only a limited number of possible values(discrete). example:
In a scenario,the given value of x is size of a plot in square feet then predicting y ie rate of the plot comes under linear regression.  If, instead, you wanted to predict, based on size, whether the plot would sell for more than 300000 Rs, you would use logistic regression. The possible outputs are either Yes, the plot will sell for more than 300000 Rs, or No. In case of Linear Regression the outcome is continuous while in case of Logistic Regression outcome is discrete (not continuous) To perform Linear regression we require a linear relationship between the dependent and independent variables. But to perform Logistic regression we do not require a linear relationship between the dependent and independent variables. Linear Regression is all about fitting a straight line in the data while Logistic Regression is about fitting a curve to the data. Linear Regression is a regression algorithm for Machine Learning while Logistic Regression is a classification Algorithm for machine learning. Linear regression assumes gaussian (or normal) distribution of dependent variable. Logistic regression assumes binomial distribution of dependent variable. The basic difference between Linear Regression and Logistic Regression is :
Linear Regression is used to predict a continuous or numerical value but when we are looking for predicting a value that is categorical Logistic Regression come into picture. Logistic Regression is used for binary classification.I am unable to understand the page of the StandardScaler in the documentation of sklearn. Can anyone explain this to me in simple terms? I assume that you have a matrix X where each row/line is a sample/observation and each column is a variable/feature (this is the expected input for any sklearn ML function by the way -- X.shape should be [number_of_samples, number_of_features]). The main idea is to normalize/standardize i.e. μ = 0 and σ = 1 your features/variables/columns of X, individually,  before applying any machine learning model. StandardScaler() will normalize the features i.e. each
column of X, INDIVIDUALLY, so that each column/feature/variable will have μ = 0 and σ = 1. P.S: I find the most upvoted answer on this page, wrong.
I am quoting "each value in the dataset will have the sample mean value subtracted" -- This is neither true nor correct. See also: How and why to Standardize your data: A python tutorial Verify that the mean of each feature (column) is 0: Verify that the std of each feature (column) is 1:  UPDATE 08/2020: Concerning the input parameters with_mean and with_std to False/True, I have provided an answer here: StandardScaler difference between “with_std=False or True” and “with_mean=False or True” The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.
In case of multivariate data, this is done feature-wise (in other words independently for each column of the data).
Given the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case). StandardScaler performs the task of Standardization. Usually a dataset contains variables that are different in scale. For e.g. an Employee dataset will contain AGE column with values on scale 20-70 and SALARY column with values on scale 10000-80000. As these two columns are different in scale, they are Standardized to have common scale while building machine learning model. How to calculate it:  You can read more here: This is useful when you want to compare data that correspond to different units. In that case, you want to remove the units. To do that in a consistent way of all the data, you transform the data in a way that the variance is unitary and that the mean of the series is 0. Following is a simple working example to explain how standarization calculation works. The theory part is already well explained in other answers. Calculation As you can see in the output, mean is [6. , 2.5] and std deviation is [1.41421356, 0.8660254 ] Data is (0,1) position is 2
Standardization = (2 - 2.5)/0.8660254 = -0.57735027 Data in (1,0) position is 4
Standardization = (4-6)/1.41421356 = -1.414 Result After Standardization  Check Mean and Std Deviation After Standardization  Note: -2.77555756e-17 is very close to 0. References Compare the effect of different scalers on data with outliers What's the difference between Normalization and Standardization? Mean of data scaled with sklearn StandardScaler is not zero The answers above are great, but I needed a simple example to alleviate some concerns that I have had in the past. I wanted to make sure it was indeed treating each column separately. I am now reassured and can't find what example had caused me concern. All columns ARE scaled separately as described by those above. The scipy.stats module is correctly reporting the "sample" variance, which uses (n - 1) in the denominator. The "population" variance would use n in the denominator for the calculation of variance. To understand better, please see the code below that uses scaled data from the first column of the data set above: After applying StandardScaler(), each column in X will have mean of 0 and standard deviation of 1. Formulas are listed by others on this page. Rationale: some algorithms require data to look like this (see sklearn docs). We apply StandardScalar() on a row basis. So, for each row in a column (I am assuming that you are working with a Pandas DataFrame): Few points - It is called Standard Scalar as we are dividing it by the standard deviation of the distribution (distr. of the feature). Similarly, you can guess for MinMaxScalar(). The original distribution remains the same after applying StandardScalar(). It is a common misconception that the distribution gets changed to a Normal Distribution. We are just squashing the range into [0, 1].This: Gives the error: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same You get this error because your model is on the GPU, but your data is on the CPU. So, you need to send your input tensors to the GPU. Or like this, to stay consistent with the rest of your code: The same error will be raised if your input tensors are on the GPU but your model weights aren't. In this case, you need to send your model weights to the GPU. See the documentation for cuda(), and its opposite, cpu(). The new API is to use .to() method. The advantage is obvious and important.
Your device may tomorrow be something other than "cuda": So try to avoid model.cuda()
It is not wrong to check for the device or to hardcode it: same as: In general you can use this code: As already mentioned in the previous answers, the issue can be that your model is trained on the GPU, but it's tested on the CPU. If that's the case then you need to port your model's weights and the data from the GPU to the CPU like this: NOTE: Here we still check if the configuration arguments are set to GPU or CPU, so that this piece of code can be used for both training (on the GPU) and testing (on the CPU). When loading a model, both weights and inputs have to be in the same device, we can do this by using the .to(device) as pointed by others. However it might be the case that also the datatype of the saved weights and the input tensors are different. If this is the case then we must also change the datatype of both model weights and inputs: I have same problem,My CNN model: I put for Conv2d.to(device) its work for me. Notice that (from pytorch documentation): If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device. That is, you might need to do: Instead of just: With the first approach you'll be in the safe side. First check cuda is available or not: In case you want to load some model do this: Now you probably get this error: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same It is needed to convert the type of input data from torch.tensor to torch.cuda.tensor by : and then convert the result from torch.cuda.tensor to torch.tensor: Works, perfectly fine...I'm learning different methods to convert categorical variables to numeric for machine-learning classifiers.  I came across the pd.get_dummies method and sklearn.preprocessing.OneHotEncoder() and I wanted to see how they differed in terms of performance and usage.  I found a tutorial on how to use OneHotEncoder() on https://xgdgsc.wordpress.com/2015/03/20/note-on-using-onehotencoder-in-scikit-learn-to-work-on-categorical-features/ since the sklearn documentation wasn't too helpful on this feature. I have a feeling I'm not doing it correctly...but Can some explain the pros and cons of using pd.dummies over sklearn.preprocessing.OneHotEncoder() and vice versa? I know that OneHotEncoder() gives you a sparse matrix but other than that I'm not sure how it is used and what the benefits are over the pandas method.  Am I using it inefficiently?  For machine learning, you almost definitely want to use sklearn.OneHotEncoder. For other tasks like simple analyses, you might be able to use pd.get_dummies, which is a bit more convenient. Note that sklearn.OneHotEncoder has been updated in the latest version so that it does accept strings for categorical variables, as well as integers. The crux of it is that the sklearn encoder creates a function which persists and can then be applied to new data sets which use the same categorical variables, with consistent results. Note how we apply the same encoder we created via X_train to the new data set X_test. Consider what happens if X_test contains different levels than X_train for one of its variables. For example, let's say X_train["color"] contains only "red" and "green", but in addition to those, X_test["color"] sometimes contains "blue". If we use pd.get_dummies, X_test will end up with an additional "color_blue" column which X_train doesn't have, and the inconsistency will probably break our code later on, especially if we are feeding X_test to an sklearn model which we trained on X_train. And if we want to process the data like this in production, where we're receiving a single example at a time, pd.get_dummies won't be of use. With sklearn.OneHotEncoder on the other hand, once we've created the encoder, we can reuse it to produce the same output every time, with columns only for "red" and "green". And we can explicitly control what happens when it encounters the new level "blue": if we think that's impossible, then we can tell it to throw an error with handle_unknown="error"; otherwise we can tell it to continue and simply set the red and green columns to 0, with handle_unknown="ignore". OneHotEncoder cannot process string values directly. If your nominal features are strings, then you need to first map them into integers. pandas.get_dummies is kind of the opposite. By default, it only converts string columns into one-hot representation, unless columns are specified.  I really like Carl's answer and upvoted it.  I will just expand Carl's example a bit so that more people hopefully will appreciate that pd.get_dummies can handle unknown.  The two examples below shows that pd.get_dummies can accomplish the same thing in handling unknown as OHE .  why wouldn't you just cache or save the columns as variable col_list from the resulting get_dummies then use pd.reindex to align the train vs test datasets....   example:update: this question is related to Google Colab's "Notebook settings: Hardware accelerator: GPU". This question was written before the "TPU" option was added. Reading multiple excited announcements about Google Colaboratory providing free Tesla K80 GPU, I tried to run fast.ai lesson on it for it to never complete - quickly running out of memory. I started investigating of why. The bottom line is that “free Tesla K80” is not "free" for all - for some only a small slice of it is "free".  I connect to Google Colab from West Coast Canada and I get only 0.5GB of what supposed to be a 24GB GPU RAM. Other users get access to 11GB of GPU RAM. Clearly 0.5GB GPU RAM is insufficient for most ML/DL work. If you're not sure what you get, here is little debug function I scraped together (only works with the GPU setting of the notebook): Executing it in a jupyter notebook before running any other code gives me: The lucky users who get access to the full card will see: Do you see any flaw in my calculation of the GPU RAM availability, borrowed from GPUtil? Can you confirm that you get similar results if you run this code on Google Colab notebook? If my calculations are correct, is there any way to get more of that GPU RAM on the free box? update: I'm not sure why some of us get 1/20th of what other users get. e.g. the person who helped me to debug this is from India and he gets the whole thing! note: please don't send any more suggestions on how to kill the potentially stuck/runaway/parallel notebooks that might be consuming parts of the GPU. No matter how you slice it, if you are in the same boat as I and were to run the debug code you'd see that you still get a total of 5% of GPU RAM (as of this update still). So to prevent another dozen of answers suggesting invalid in the context of this thread suggestion to !kill -9 -1, let's close this thread: The answer is simple: As of this writing Google simply gives only 5% of GPU to some of us, whereas 100% to the others. Period. dec-2019 update: The problem still exists - this question's upvotes continue still. mar-2019 update: A year later a Google employee @AmiF commented on the state of things, stating that the problem doesn't exist, and anybody who seems to have this problem needs to simply reset their runtime to recover memory. Yet, the upvotes continue, which to me this tells that the problem still exists, despite @AmiF's suggestion to the contrary. dec-2018 update: I have a theory that Google may have a blacklist of certain accounts, or perhaps browser fingerprints, when its robots detect a non-standard behavior. It could be a total coincidence, but for quite some time I had an issue with Google Re-captcha on any website that happened to require it, where I'd have to go through dozens of puzzles before I'd be allowed through, often taking me 10+ min to accomplish. This lasted for many months. All of a sudden as of this month I get no puzzles at all and any google re-captcha gets resolved with just a single mouse click, as it used to be almost a year ago.  And why I'm telling this story? Well, because at the same time I was given 100% of the GPU RAM on Colab. That's why my suspicion is that if you are on a theoretical Google black list then you aren't being trusted to be given a lot of resources for free. I wonder if any of you find the same correlation between the limited GPU access and the Re-captcha nightmare. As I said, it could be totally a coincidence as well. Last night I ran your snippet and got exactly what you got: but today: I think the most probable reason is the GPUs are shared among VMs, so each time you restart the runtime you have chance to switch the GPU, and there is also probability you switch to one that is being used by other users. UPDATED:
It turns out that I can use GPU normally even when the GPU RAM Free is 504 MB, which I thought as the cause of ResourceExhaustedError I got last night.  If you execute a cell that just has
!kill -9 -1
in it, that'll cause all of your runtime's state (including memory, filesystem, and GPU) to be wiped clean and restarted.  Wait 30-60s and press the CONNECT button at the top-right to reconnect. Restart Jupyter IPython Kernel: Find the Python3 pid and kill the pid. Please see the below image Note: kill only python3(pid=130) not jupyter python(122). just give a heavy task to google colab, it will ask us to change to 25 gb of ram.   example run this code twice: then click on get more ram :)

  Im not sure if this blacklisting is true! Its rather possible, that the cores are shared among users. I ran also the test, and my results are the following: It seems im getting also full core. However i ran it a few times, and i got the same result. Maybe i will repeat this check a few times during the day to see if there is any change. I believe if we have multiple notebooks open. Just closing it doesn't actually stop the process. I haven't figured out how to stop it. But I used top to find PID of the python3 that was running longest and using most of the memory and I killed it. Everything back to normal now. Google Colab resource allocation is dynamic, based on users past usage. Suppose if a user has been using more resources recently and a new user who is less frequently uses Colab, he will be given relatively more preference in resource allocation. Hence to get the max out of Colab , close all your Colab tabs and all other active sessions, reset the runtime of the one you want to use. You'll definitely get better GPU allocation.I've been trying to use tensorflow for two days now installing and reinstalling it over and over again in python2.7 and 3.4.  No matter what I do, I get this error message when trying to use tensorflow.placeholder() It's very boilerplate code: No matter what I do I always get the trace back: Anyone know how I can fix this? If you have this error after an upgrade to TensorFlow 2.0, you can still use 1.X API by replacing: by Solution: Do not use "tensorflow" as your filename. Notice that you use tensorflow.py as your filename. And I guess you write code like: Then you are actually importing the script file "tensorflow.py" that is under your current working directory, rather than the "real" tensorflow module from Google. Here is the order in which a module will be searched when importing: The directory containing the input script (or the current directory when no file is specified).  PYTHONPATH (a list of directory names,
  with the same syntax as the shell variable PATH).  The installation-dependent default. It happened to me too. I had tensorflow and it was working pretty well, but when I install tensorflow-gpu along side the previous tensorflow this error arose then I did these 3 steps and it started working with no problem: conda remove tensorflow-gpu tensorflow tensorflow-base conda install tensorflow Instead of tf.placeholder(shape=[None, 2], dtype=tf.float32) use something like
tf.compat.v1.placeholder(shape=[None, 2], dtype=tf.float32) if you don't want to disable v2 completely. works.
I am using Python 3.7 and tensorflow 2.0. It appears that .placeholder() , .reset_default_graph() , and others were removed with version 2.  I ran into this issue using Docker image: tensorflow/tensorflow:latest-gpu-py3 which automatically pulls the latest version.  I was working in 1.13.1 and was 'upgraded to 2' automatically and started getting the error messages.  I fixed this by being more specific with my image: tensorflow/tensorflow:1.13.1-gpu-py3. More info can be found here:  https://www.tensorflow.org/alpha/guide/effective_tf2 Avoid using the below striked out statement in tensorflow=2.0 i̶m̶p̶o̶r̶t̶ ̶t̶e̶n̶s̶o̶r̶f̶l̶o̶w̶ ̶a̶s̶ ̶t̶f̶ ̶x̶ ̶=̶ ̶t̶f̶.̶p̶l̶a̶c̶e̶h̶o̶l̶d̶e̶r̶(̶s̶h̶a̶p̶e̶=̶[̶N̶o̶n̶e̶,̶ ̶2̶]̶,̶ ̶d̶t̶y̶p̶e̶=̶t̶f̶.̶f̶l̶o̶a̶t̶3̶2̶)̶ You can disable the v2 behavior by using the following code  This one is perfectly working for me. I also got the same error. May be because of the version of tensorflow. 
After installing tensorflow 1.4.0, I got relief from the error. If you are using TensorFlow 2.0, then some code developed for tf 1.x may not code work. Either you can follow the link : https://www.tensorflow.org/guide/migrate or you can install a previous version of tf by
pip3 install tensorflow==version Import the old version of tensorflow instead of the new version [https://inneka.com/ml/tf/tensorflow-module-object-has-no-attribute-placeholder/][1] import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() You need to use the keras model with tensorflow 2, as here Recent version 2.0 does not support placeholder. 
I uninstalled 2.0 using command: conda remove tensorflow.
then I installed 1.15.0 using command: conda install -c conda-forge tensorflow=1.15.0.
1.15 is latest in version 1 series. You can change as per you wish and requirement.
For seeing all version, use command: conda search tensorflow.
It worked for Anaconda3 in Windows. Try this: or this (if you have GPU): Please take a look at the Migrate your TensorFlow 1 code to TensorFlow 2. These codes: need to be migrated in TensorFlow 2 as below: If you get this on tensorflow 2.0.0+, it's very likely because the code isn't compatible with the newer version of tensorflow. To fix this, run the tf_upgrade_v2 script. Faced same issue on Ubuntu 16LTS when tensor flow was installed over existing python installation. Workaround:
1.)Uninstall tensorflow from pip and pip3 2.)Uninstall python & python3  3.)Install only a single version of  python(I used python 3) 4.)Install tensorflow to python3 for non GPU tensorflow, run this command  for GPU tensorflow, run below command Suggest not to install GPU and vanilla version of tensorflow The error shows up because we are using tensorflow version 2 and the command is from version 1. So if we use: It'll work Because you cant use placeholder in tensflow2.0version, so you need to use tensflow1*, or you need to change your code to fix tensflow2.0 I had the same problem before after tried to upgrade tensorflow, I solved it by reinstalling Tensorflow and Keras.  pip uninstall tensorflow  pip uninstall keras  Then: pip install tensorflow  pip install keras The problem is with TensorFlow version; the one you are running is 2.0 or something above 1.5, while placeholder can only work with 1.4. So simply uninstall TensorFlow, then install it again with version 1.4 and everything will work.  It may be the typo if you incorrectly wrote the placeholder word.
In my case I misspelled it as placehoder and got the error like this:
AttributeError: 'module' object has no attribute 'placehoder'I'm working on a classification problem with unbalanced classes (5% 1's). I want to predict the class, not the probability. In a binary classification problem, is scikit's classifier.predict() using 0.5 by default?
If it doesn't, what's the default method? If it does, how do I change it? In scikit some classifiers have the class_weight='auto' option, but not all do. With class_weight='auto', would .predict() use the actual population proportion as a threshold? What would be the way to do this in a classifier like MultinomialNB that doesn't support class_weight? Other than using predict_proba() and then calculation the classes myself. The threshold can be set using clf.predict_proba() for example: The threshold in scikit learn is 0.5 for binary classification and whichever class has the greatest probability for multiclass classification. In many problems a much better result may be obtained by adjusting the threshold. However, this must be done with care and NOT on the holdout test data but by cross validation on the training data. If you do any adjustment of the threshold on your test data you are just overfitting the test data. Most methods of adjusting the threshold is based on the receiver operating characteristics (ROC) and Youden's J statistic but it can also be done by other methods such as a search with a genetic algorithm. Here is a peer review journal article describing doing this in medicine: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2515362/  So far as I know there is no package for doing it in Python but it is relatively simple (but inefficient) to find it with a brute force search in Python. This is some R code that does it.  is scikit's classifier.predict() using 0.5 by default? In probabilistic classifiers, yes. It's the only sensible threshold from a mathematical viewpoint, as others have explained. What would be the way to do this in a classifier like MultinomialNB that doesn't support class_weight? You can set the class_prior, which is the prior probability P(y) per class y. That effectively shifts the decision boundary. E.g. You seem to be confusing concepts here. Threshold is not a concept for a "generic classifier" - the most basic approaches are based on some tunable threshold, but most of the existing methods create complex rules for classification which cannot (or at least shouldn't) be seen as a thresholding. So first - one cannot answer your question for scikit's classifier default threshold because there is no such thing. Second - class weighting is not about threshold, is about classifier ability to deal with imbalanced classes, and it is something dependent on a particular classifier. For example - in SVM case it is the way of weighting the slack variables in the optimization problem, or if you prefer - the upper bounds for the lagrange multipliers values connected with particular classes. Setting this to 'auto' means using some default heuristic, but once again - it cannot be simply translated into some thresholding. Naive Bayes on the other hand directly estimates the classes probability from the training set. It is called "class prior" and you can set it in the constructor with "class_prior" variable. From the documentation: Prior probabilities of the classes. If specified the priors are not adjusted according to the data. In case someone visits this thread hoping for ready-to-use function (python 2.7). In this example cutoff is designed to reflect ratio of events to non-events in original dataset df, while y_prob could be the result of .predict_proba method (assuming stratified train/test split). Feel free to criticize/modify. Hope it helps in rare cases when class balancing is out of the question and the dataset itself is highly imbalanced.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 4 years ago. By processing a time series graph, I Would like to detect patterns that look similar to this:  Using a sample time series as an example, I would like to be able to detect the patterns as marked here:  What kind of AI algorithm (I am assuming marchine learning techniques) do I need to use to achieve this? Is there any library (in C/C++) out there that I can use? Here is a sample result from a small project I did to partition ecg data.  My approach was a "switching autoregressive HMM" (google this if you haven't heard of it) where each datapoint is predicted from the previous datapoint using a Bayesian regression model. I created 81 hidden states: a junk state to capture data between each beat, and 80 separate hidden states corresponding to different positions within the heartbeat pattern. The pattern 80 states were constructed directly from a subsampled single beat pattern and had two transitions - a self transition and a transition to the next state in the pattern. The final state in the pattern transitioned to either itself or the junk state. I trained the model with Viterbi training, updating only the regression parameters. Results were adequate in most cases. A similarly structure Conditional Random Field would probably perform better, but training a CRF would require manually labeling patterns in the dataset if you don't already have labelled data. Edit: Here's some example python code - it is not perfect, but it gives the general approach. It implements EM rather than Viterbi training, which may be slightly more stable.
The ecg dataset is from http://www.cs.ucr.edu/~eamonn/discords/ECG_data.zip Why not using a simple matched filter? Or its general statistical counterpart called cross correlation. Given a known pattern x(t) and a noisy compound time series containing your pattern shifted in a,b,...,z like y(t) = x(t-a) + x(t-b) +...+ x(t-z) + n(t). The cross correlation function between x and y should give peaks in a,b, ...,z Weka is a powerful collection of machine-learning software, and supports some time-series analysis tools, but I do not know enough about the field to recommend a best method. However, it is Java-based; and you can call Java code from C/C++ without great fuss. Packages for time-series manipulation are mostly directed at the stock-market. I suggested Cronos in the comments; I have no idea how to do pattern recognition with it, beyond the obvious: any good model of a length of your series should be able to predict that, after small bumps at a certain distance to the last small bump, big bumps follow. That is, your series exhibits self-similarity, and the models used in Cronos are designed to model it. If you don't mind C#, you should request a version of TimeSearcher2 from the folks at HCIL - pattern recognition is, for this system, drawing what a pattern looks like, and then checking whether your model is general enough to capture most instances with a low false-positive rate. Probably the most user-friendly approach you will find; all others require quite a background in statistics or pattern recognition strategies. I'm not sure what package would work best for this. I did something similar at one point in college where I tried to automatically detect certain similar shapes on an x-y axis for a bunch of different graphs. You could do something like the following. Class labels like: Features like: I am using deep learning if it's an option for you.  It's done in Java, Deeplearning4j. I am experimenting with LSTM. I tried 1 hidden layer and 2 hidden layers to process time series.  Found a few things:I have a Java app which needs to perform partial least squares regression. It would appear there are no Java implementations of PLSR out there. Weka might have had something like it at some point, but it is no longer in the API. On the other hand, I have found a good R implementation, which has an added bonus to it. It was used by the people whose result I want to replicate, which means there is less chance that things will go wrong because of differences in the way PLSR is implemented. The question is: is there a good enough (and simple to use) package that enable Java to call R, pass in some parameters to a function and read back the results? My other option is to have Java spawn R in a Process and then monitor it. Data would be read and written to disk. Which of the two would you recommend? Am I missing the obvious third option? I have successfully used two alternatives in the past. JRI RServe Other alternatives I have never used : RCaller There has been work by Duncan Temple Lang: http://rss.acs.unt.edu/Rdoc/library/SJava/Docs/RFromJava.pdf .  My guess as to the most robust solution would be JGR. The developers of JGR have a mailing list, Stats-Rosuda and the mailing list Archive indicates the list remains active as of 2013. There is also code that has been put up at Googlecode, with an example here:
http://stdioe.blogspot.com/2011/07/rcaller-20-calling-r-from-java.html This is an old question.. but for anyone browsing through here that is still interested: I wrote a blog article that provides a detailed example of how to use JRI/rjava (a JNI based bridge) to do this type of thing (the how-to is focused on Linux dev environments).  I also compare and contrast alternative approaches for doing 'mathy' stuff by calling out to R and similar frameworks. URL > http://buildlackey.com/integrating-r-and-java-with-jrirjava-a-jni-based-bridge/ Renjin is an alternative that allows not only the integration of many packages of R also a easy going communication between Java and R through objects: http://www.renjin.org/ JRI has both low level and High level interface to Call R from Java. There is an eclipse plugin that helps in setting up the R Java environment at http://www.studytrails.com/RJava-Eclipse-Plugin/.  This seems to be an old question. However Rserve and rJava are two good packages to integrate R with Java. Following blogs explain usage of both these libraries. For rJava: http://www.codophile.com/how-to-integrate-r-with-java-using-rjava/ For Rserve: http://www.codophile.com/how-to-integrate-r-with-java-using-rserve/ I hope this will help. I had similar need a while back and tested a few of the interfaces to R.  The one I found to be the best for my needs (windows, c#) was Rserve which I believe is written in Java.  My only gripe with it is that it wasn't 64-bit.  I used a simple client written in c# and it worked very well.  I'm sure the Java client is a lot better. FastR is a GraalVM based implementation of R. Embedding it in a JVM application is as simple as: More details in this article: https://medium.com/graalvm/faster-r-with-fastr-4b8db0e0dcebI'd like to reset (randomize) the weights of all layers in my Keras (deep learning) model. The reason is that I want to be able to train the model several times with different data splits without having to do the (slow) model recompilation every time. Inspired by this discussion, I'm trying the following code: However, it only partly works. Partly, becuase I've inspected some layer.get_weights() values, and they seem to change. But when I restart the training, the cost values are much lower than the initial cost values on the first run. It's almost like I've succeeded resetting some of the weights, but not all of them. Save the initial weights right after compiling the model but before training it: and then after training, "reset" the model by reloading the initial weights: This gives you an apples to apples model to compare different data sets and should be quicker than recompiling the entire model. Reset all layers by checking for initializers: Update: kernel_initializer is kernel.initializer now. If you want to truly re-randomize the weights, and not merely restore the initial weights, you can do the following. The code is slightly different depending on whether you're using TensorFlow or Theano. I have found the clone_model function that creates a cloned network with the same architecture but new model weights. Example of use: Comparing the weights: If you execute this code several times, you will notice that the cloned model receives new weights each time. Tensorflow 2 answer: Original weights: New weights: Try set_weights. for example: build a model with say, two convolutional layers then define your weights (i'm using a simple w, but you could use np.random.uniform or anything like that if you want) Take a peek at what are the layers inside a model Set each weight for each convolutional layer (you'll see that the first layer is actually input and you don't want to change that, that's why the range starts from 1 not zero). Generate some input for your test and predict the output from your model You could change it again if you want and check again for the output: Sample output: From your peek at .layers you can see that the first layer is input and the others your convolutional layers. For tf2 the simplest way to actually reset weights would be: clone_model() as mentioned by @danielsaromo returns new model with trainable params initialized from scratch, we use its weights to reinitialize our model thus no model compilation (knowledge about its loss or optimizer) is needed. There are two caveats though, first is mentioned in clone_model()'s documentation: clone_model will not preserve the uniqueness of shared objects within the model (e.g. a single variable attached to two distinct layers will be restored as two separate variables). Another caveat is that for large models cloning might fail due to memory limit. To "random" re-initialize weights of a compiled untrained model in TF 2.0 (tf.keras): Note the "if wdim > 1 else w". You don't want to re-initialize the biases (they stay 0 or 1). use keras.backend.clear_session()Can anyone tell me why we set random state to zero in splitting train and test set. I have seen situations like this where random state is set to 1! What is the consequence of this random state in cross validation as well? It doesn't matter if the random_state is 0 or 1 or any other integer. What matters is that it should be set the same value, if you want to validate your processing over multiple runs of the code. By the way I have seen random_state=42 used in many official examples of scikit as well as elsewhere also. random_state as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case. In the documentation, it is stated that: If random_state is None or np.random, then a randomly-initialized RandomState object is returned. If random_state is an integer, then it is used to seed a new RandomState object. If random_state is a RandomState object, then it is passed through. This is to check and validate the data when running the code multiple times. Setting random_state a fixed value will guarantee that same sequence of random numbers are generated each time you run the code. And unless there is some other randomness present in the process, the results produced will be same as always. This helps in verifying the output. when random_state set to an integer, train_test_split will return same results for each execution. when random_state set to an None, train_test_split will return different results for each execution. see below example: Output: [2, 8, 4] [2, 8, 4] [2, 8, 4] [2, 8, 4] [2, 8, 4] [4, 7, 6] [4, 3, 7] [8, 1, 4] [9, 5, 8] [6, 4, 5] If you don't mention the random_state in the code, then whenever you execute your code a new random value is generated and the train and test datasets would have different values each time. However, if you use a particular value for random_state(random_state = 1 or any other value) everytime the result will be same,i.e, same values in train and test datasets. The random_state splits a randomly selected data but with a twist. And the twist is the order of the data will be same for a particular value of random_state.You need to understand that it's not a bool accpeted value. starting from 0 to any integer no, if you pass as random_state,it'll be a permanent order for it. Ex: the order you will get in random_state=0 remain same. After that if you execuit random_state=5 and again come back to random_state=0 you'll get the same order. And like 0 for all integer will go same.
How ever random_state=None splits randomly each time. If still having doubt watch this  If you don't specify the random_state in your code, then every time you run(execute) your code a new random value is generated and the train and test datasets would have different values each time. However, if a fixed value is assigned like random_state = 0 or 1 or 42 then no matter how many times you execute your code the result would be the same .i.e, same values in train and test datasets. random_state is None by default which means every time when you run your program you will get different output because of splitting between train and test varies within. random_state = any int value means every time when you run your program you will get tehe same output because of splitting between train and test does not varies within. The random_state is an integer value which implies the selection of a random combination of train and test. When you set the test_size as 1/4 the there is a set generated of permutation and combination of train and test and each combination has one state.
Suppose you have a dataset---> [1,2,3,4] We need it because while param tuning of model same state will considered again and again.
So that there won't be any inference with the accuracy. But in case of Random forest there is also similar story but in a different way w.r.t the variables. We used the random_state parameter for reproducibility of the initial shuffling of training datasets after each epoch. For multiple times of execution of our model, random state  make sure that data values  will be same for training and testing data sets. It fixes the order of data for train_test_split Lets say our dataset is having one feature and 10data points. X=[0,1,2,3,4,5,6,7,8,9]
and lets say 0.3(30% is testset) is  specified as test data percentage then we are going to have 10C3=120 different combinations of data.[Refer picture in link for tabular explanation]: https://i.stack.imgur.com/FZm4a.png Based  on the random number specified system will pick random state and assigns train and test data In addition to what already said, different values of random state may produce different results during the training phase. Internally, the train_test_split() function uses a seed that allows you to pseudorandomly separate the data into two groups: training and test set. The number is pseudorandom because the same data subdivision corresponds to the same seed value. This aspect is very useful to ensure the reproducibility of the experiments. Unfortunately, the use of one seed rather than another could lead to totally different datasets, and even modify the performance of the chosen Machine Learning model that receives the training set as input. You can read the following article to deepen this aspect:
https://towardsdatascience.com/why-you-should-not-trust-the-train-test-split-function-47cb9d353ad2 The article also shows a practical example. You can also find other considerations in this article:
https://towardsdatascience.com/is-a-small-dataset-risky-b664b8569a21I have a dataset and I want to train my model on that data. After training, I need to know the features that are major contributors in the classification for a SVM classifier.  There is something called feature importance for forest algorithms, is there anything similar? Yes, there is attribute coef_ for SVM classifier but it only works for SVM with linear kernel. For other kernels it is not possible because data are transformed by kernel method to another space, which is not related to input space, check the explanation. And the output of the function looks like this:
 If you're using rbf (Radial basis function) kernal, you can use sklearn.inspection.permutation_importance as follows to get feature importance. [doc]  In only one line of code: fit an SVM model: and implement the plot as follows: The resuit will be: the most contributing features of the SVM model in absolute values I created a solution which also works for Python 3 and is based on Jakub Macina's code snippet.Can the Keras deal with input images with different size? For example, in the fully convolutional neural network, the input images can have any size. However, we need to specify the input shape when we create a network by Keras. Therefore, how can we use Keras to deal with different input size without resizing the input images to the same size? Thanks for any help. Yes.
Just change your input shape to shape=(n_channels, None, None).
Where n_channels is the number of channels in your input image. I'm using Theano backend though, so if you are using tensorflow you might have to change it to (None,None,n_channels) You should use: input_shape=(1, None, None) None in a shape denotes a variable dimension. Note that not all layers
  will work with such variable dimensions, since some layers require
  shape information (such as Flatten).
  https://github.com/fchollet/keras/issues/1920 For example, using keras's functional API your input layer would be: For a RGB dataset For a Gray dataset Implementing arbitrarily sized input arrays with the same computational kernels can pose many challenges - e.g. on a GPU, you need to know how big buffers to reserve, and more weakly how much to unroll your loops, etc.  This is the main reason that Keras requires constant input shapes, variable-sized inputs are too painful to deal with. This more commonly occurs when processing variable-length sequences like sentences in NLP. The common approach is to establish an upper bound on the size (and crop longer sequences), and then pad the sequences with zeros up to this size. (You could also include masking on zero values to skip computations on the padded areas, except that the convolutional layers in Keras might still not support masked inputs...) I'm not sure if for 3D data structures, the overhead of padding is not prohibitive - if you start getting memory errors, the easiest workaround is to reduce the batch size.  Let us know about your experience with applying this trick on images! Just use None while specifying input shape. But I still do not know how to pass different-shaped images into fit function.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 2 years ago. It seems like R is really designed to handle datasets that it can pull entirely into memory. What R packages are recommended for signal processing and machine learning on very large datasets that can not be pulled into memory?  If R is simply the wrong way to do this, I am open to other robust free suggestions (e.g. scipy if there is some nice way to handle very large datasets) Have a look at the "Large memory and out-of-memory data" subsection of the high performance computing task view on CRAN. bigmemory and ff are two popular packages. For bigmemory (and the related biganalytics, and bigtabulate), the bigmemory website has a few very good presentations, vignettes, and overviews from Jay Emerson. For ff, I recommend reading Adler Oehlschlägel and colleagues' excellent slide presentations on the ff website.  Also, consider storing data in a database and reading in smaller batches for analysis. There are likely any number of approaches to consider. To get started, consdier looking through some of the examples in the biglm package, as well as this presentation from Thomas Lumley. And do investigate the other packages on the high-performance computing task view and mentioned in the other answers. The packages I mention above are simply the ones I've happened to have more experience with. I think the amount of data you can process is more limited by ones programming skills than anything else. Although a lot of standard functionality is focused on in memory analysis, cutting your data into chunks already helps a lot. Ofcourse, this takes more time to program than picking up standard R code, but often times it is quite possible.  Cutting up data can for exale be done using read.table or readBin which support only reading a subset of the data. Alternatively, you can take a look at the high performance computing task view for packages which deliver out of the box out of memory functionality. You could also put your data in a database. For spatial raster data, the excellent raster package provides out of memory analysis. For machine learning tasks I can recommend using biglm package, used to do "Regression for data too large to fit in memory". For using R with really big data, one can use Hadoop as a backend and then use package rmr to perform statistical (or other) analysis via MapReduce on a Hadoop cluster. It all depends on algorithms you need. If they may be translated into incremental form (when only small part of data is needed at any given moment, e.g. for Naive Bayes you can hold in memory only the model itself and current observation being processed), then the best suggestion is to perform machine learning incrementally, reading new batches of data from disk.  However, many algorithms and especially their implementations really require the whole dataset. If size of the dataset fits you disk (and file system limitations), you can use mmap package that allows to map file on disk to memory and use it in the program. Note however, that read-writes to disk are expensive, and R sometimes likes to move data back and forth frequently. So be careful.  If your data can't be stored even on you hard drive, you will need to use distributed machine learning systems. One such R-based system is Revolution R which is designed to handle really large datasets. Unfortunately, it is not open source and costs quite a lot of money, but you may try to get free academic license. As alternative, you may be interested in Java-based Apache Mahout - not so elegant, but very efficient solution, based on Hadoop and including many important algorithms.  If the memory is not sufficient enough, one solution is push data to disk and using distributed computing. I think RHadoop(R+Hadoop) may be one of the solution to tackle with large amount dataset.I need to cluster a simple univariate data set into a preset number of clusters. Technically it would be closer to binning or sorting the data since it is only 1D, but my boss is calling it clustering, so I'm going to stick to that name. 
The current method used by the system I'm on is K-means, but that seems like overkill. Is there a better way of performing this task? Answers to some other posts are mentioning KDE (Kernel Density Estimation), but that is a density estimation method, how would that work?  I see how KDE returns a density, but how do I tell it to split the data into bins?  How do I have a fixed number of bins independent of the data (that's one of my requirements) ?  More specifically, how would one pull this off using scikit learn?  My input file looks like:  I want to group the sls number into clusters or bins, such that: And my output file will look like:  Write code yourself. Then it fits your problem best! Boilerplate: Never assume code you download from the net to be correct or optimal... make sure to fully understand it before using it.  Your clusters therefore are and visually, we did this split:  We cut at the red markers. The green markers are our best estimates for the cluster centers. There is a little error in the accepted answer by @Has QUIT--Anony-Mousse (I can't comment nor suggest an edit due my reputation). The line: Should be edited into: That's because mi and ma is an index, where s[mi] and s[ma] is the value. If you use mi[0] as the limit, you risk and error splitting if your upper and lower linspace >> your upper and lower data. For example, run this code and see the difference in split result: result: Further improving the responses above by @yasirroni, to dynamically print all clusters (not just 3 from the above) the line: can be changed into: This would ensure that all the clusters are taken into account.How can you write a python script to read Tensorboard log files, extracting the loss and accuracy and other numerical data, without launching the GUI tensorboard --logdir=...? You can use TensorBoard's Python classes or script to extract the data: How can I export data from TensorBoard? If you'd like to export data to visualize elsewhere (e.g. iPython Notebook), that's possible too. You can directly depend on the underlying classes that TensorBoard uses for loading data: python/summary/event_accumulator.py (for loading data from a single run) or python/summary/event_multiplexer.py (for loading data from multiple runs, and keeping it organized). These classes load groups of event files, discard data that was "orphaned" by TensorFlow crashes, and organize the data by tag. As another option, there is a script (tensorboard/scripts/serialize_tensorboard.py) which will load a logdir just like TensorBoard does, but write all of the data out to disk as json instead of starting a server. This script is setup to make "fake TensorBoard backends" for testing, so it is a bit rough around the edges. Using EventAccumulator: size_guidance: To finish user1501961's answer, you can then just export the list of scalars to a csv file easily with pandas pd.DataFrame(ea.Scalars('Loss)).to_csv('Loss.csv') For anyone interested, I've adapted user1501961's answer into a function for parsing tensorboard scalars into a dictionary of pandas dataframes: Try this: bat is optimal.I'm building a model that converts a string to another string using recurrent layers (GRUs). I have tried both a Dense and a TimeDistributed(Dense) layer as the last-but-one layer, but I don't understand the difference between the two when using return_sequences=True, especially as they seem to have the same number of parameters. My simplified model is the following: The summary of the network is: This makes sense to me as my understanding of TimeDistributed is that it applies the same layer at all timepoints, and so the Dense layer has 16*15+15=255 parameters (weights+biases). However, if I switch to a simple Dense layer: I still only have 255 parameters: I wonder if this is because Dense() will only use the last dimension in the shape, and effectively treat everything else as a batch-like dimension. But then I'm no longer sure what the difference is between Dense and TimeDistributed(Dense). Update Looking at https://github.com/fchollet/keras/blob/master/keras/layers/core.py it does seem that Dense uses the last dimension only to size itself: It also uses keras.dot to apply the weights: The docs of keras.dot imply that it works fine on n-dimensional tensors. I wonder if its exact behavior means that Dense() will in effect be called at every time step. If so, the question still remains what TimeDistributed() achieves in this case. TimeDistributedDense applies a same dense to every time step during GRU/LSTM Cell unrolling. So the error function will be between predicted label sequence and the actual label sequence. (Which is normally the requirement for sequence to sequence labeling problems). However, with return_sequences=False, Dense layer is applied only once at the last cell. This is normally the case when RNNs are used for classification problem. If return_sequences=True then Dense layer is applied to every timestep just like TimeDistributedDense. So for as per your models both are same, but if you change your second model to return_sequences=False, then Dense will be applied only at the last cell. Try changing it and the model will throw as error because then the Y will be of size [Batch_size, InputSize], it is no more a sequence to sequence but a full sequence to label problem. In the above example architecture of model1 and model2 are sample (sequence to sequence models) and model3 is a full sequence to label model. Here is a piece of code that verifies TimeDistirbuted(Dense(X)) is identical to Dense(X): (2, 4, 3) (3, 5) (2, 4, 5) (2, ?, 5) And the difference is:I'm looking into clustering points on a map (latitude/longitude). Are there any recommendations as to a suitable algorithm that is fast and scalable? More specifically, I have a series of latitude/longitude coordinates and a map viewport. I'm trying to cluster the points that are close together in order to remove clutter. I already have a solution to the problem (see here), only I am wondering if there is any formal algorithm that solves the problem efficiently. For a virtual earth application I've used the clustering described 
here. It's lightning fast and easily extensible. Google Maps Hacks has a hack, "Hack 69. Cluster Markers at High Zoom Levels", on that. Also, see Wikipedia on clustering algorithms. You could look at indexing all your points using a QuadTile scheme, and then based upon the scale the further down the quad-splits you go. All similarly located points will then be near each other in your index, allowing the clustering to happen efficiently. QuadTiles are an example of Morton Codes, and there is a python example linked from that wikipedia article that may help. I looked at various libraries and found them so complex couldn't understand a word so I decided to make my own clustering algorithm Here goes my code in Java // This calculates the pixel distance between tow lat long points at a particular zoom level  // The main function which actually calculates the clusters
1. ArrayList of lat long points is iterated to length .
2. inner loop a copy of the same arraylist is iterated from i+1 position ie leaving the top loop's index
3. 0th element is taken as the centre of centroid and all other points are compared if their pixel distance is very less add it into cluster
4. remove all  elements from top arraylist and copy arraylist which have formed cluster
5 restart the process by reinitializing the index from 0;
6 if the centroid selected has no clusters then that element is not deletedIs it possible to use GridSearchCV without cross validation? I am trying to optimize the number of clusters in KMeans clustering via grid search, and thus I don't need or want cross validation.  The documentation is also confusing me because under the fit() method, it has an option for unsupervised learning (says to use None for unsupervised learning). But if you want to do unsupervised learning, you need to do it without cross validation and there appears to be no option to get rid of cross validation. After much searching, I was able to find this thread. It appears that you can get rid of cross validation in GridSearchCV if you use: cv=[(slice(None), slice(None))] I have tested this against my own coded version of grid search without cross validation and I get the same results from both methods. I am posting this answer to my own question in case others have the same issue. Edit: to answer jjrr's question in the comments, here is an example use case: I'm going to answer your question since it seems like it has been unanswered still. Using the parallelism method with the for loop, you can use the multiprocessing module. I think that using cv=ShuffleSplit(test_size=0.20, n_splits=1) with n_splits=1 is a better solution like this post suggested I recently came out with the following custom cross-validator, based on this answer. I passed it to GridSearchCV and it properly disabled the cross-validation for me: I hope it can help.I'm getting this error 'ValueError: Tensor Tensor("Placeholder:0", shape=(1, 1), dtype=int32)
  is not an element of this graph.' The code is running perfectly fine without with tf.Graph(). as_default():. However I need to call M.sample(...) multiple times and each time the memory won't be free after session.close(). Probably there is a memory leak but not sure where is it. I want to restore a pre-trained neural network, set it as default graph, and testing it multiple times (like 10000) over the default graph without making it larger each time. The code is: And the model is: and the output is: Try first:  Then, when you need to use predict: When you create a Model, the session hasn't been restored yet. All placeholders, variables and ops that are defined in Model.__init__ are placed in a new graph, which makes itself a default graph inside with block. This is the key line:  This means that this instance of tf.Graph() equals to tf.get_default_graph() instance inside with block, but not before or after it. From this moment on, there exist two different graphs. When you later create a session and restore a graph into it, you can't access the previous instance of tf.Graph() in that session. Here's a short example: The best way to deal with this is give names to all nodes, e.g. 'input', 'target', etc, save the model and then look up the nodes in the restored graph by name, something like this: This method guarantees that all nodes will be from the graph in session. If you are calling the python function that calls Tensorflow from an external module, make sure that you the model isn't being loaded as a global variable or else it may not be loaded in time for usage. This happened to me calling a Tensorflow model from the Flask server. Use this line before making models: This will make a new graph to use in new models. For me, this issue was resolved by using Keras' APIs to save and load model. I had more than one models being trained in my code and I had to use the particular model for prediction under a condition. So I saved the entire model to a HDF5 file after model training and then recreate/reload the saved model at the time of prediction This helped me get rid of error. Inside
def LoadPredictor(save):
Just after loading the model, add model._make_predict_function()
So the function becomes:
 I had this issue when trying to make a model using another class that uses keras to create a model. I got this issue corrected by doing the followingI am training a simple model in keras for NLP task with following code. Variable names are self explanatory for train, test and validation set. This dataset has 19 classes so final layer of the network has 19 outputs. Labels are also one-hot encoded. After first epoch, this gives me these outputs. Then I evaluate my model on testing dataset and this also shows me accuracy around 0.98. However, the labels are one-hot encoded, so I need prediction vector of classes so that I can generate confusion matrix etc. So I use, This shows that total predicted classes were 83% accurate however model1.evaluate shows 98% accuracy!! What am I doing wrong here? Is my loss function okay with categorical class labels? Is my choice of sigmoid activation function for prediction layer okay? or there is difference in the way keras evaluates a model? Please suggest on what can be wrong. This is my first try to make a deep model so I don't have much understanding of what's wrong here. I have found the problem. metrics=['accuracy'] calculates accuracy automatically from cost function. So using binary_crossentropy shows binary accuracy, not categorical accuracy. Using categorical_crossentropy automatically switches to categorical accuracy and now it is the same as calculated manually using model1.predict(). Yu-Yang was right to point out the cost function and activation function for multi-class problem. P.S: One can get both categorical and binary accuracy by using metrics=['binary_accuracy', 'categorical_accuracy']I'm trying to understand GMM by reading the sources available online. I have achieved clustering using K-Means and was seeing how GMM would compare to K-means. Here is what I have understood, please let me know if my concept is wrong: GMM is like KNN, in the sense that clustering is achieved in both cases. But in GMM each cluster has their own independent mean and covariance. Furthermore k-means performs hard assignments of data points to clusters whereas in GMM we get a collection of independant gaussian distributions, and for each data point we have a probability that it belongs to one of the distributions. To understand it better I have used MatLab to code it and achieve the desired clustering. I have used SIFT features for the purpose of feature extraction. And have used k-means clustering to initialize the values. (This is from the VLFeat documentation) Based on the above I have means, covariances and priors. My main question is, What now? I am kind of lost now. Also the means, covariances vectors are each of the size 128 x 50. I was expecting them to be 1 x 50 since each column is a cluster, wont each cluster have only one mean and covariance? (I know 128 are the SIFT features but I was expecting means and covariances). In k-means I used the the MatLab command knnsearch(X,Y) which basically finds the nearest neighbour in X for each point in Y.  So how to achieve this in GMM, I know its a collection of probabilities, and ofcourse the nearest match from that probability will be our winning cluster. And this is where I am confused. 
All tutorials online have taught how to achieve the means, covariances values, but do not say much in how to actually use them in terms of clustering. Thank you I think it would help if you first look at what a GMM model represents. I'll be using functions from the Statistics Toolbox, but you should be able to do the same using VLFeat. Let's start with the case of a mixture of two 1-dimensional normal distributions. Each Gaussian is represented by a pair of mean and variance. The mixture assign a weight to each component (prior). For example, lets mix two normal distributions with equal weights (p = [0.5; 0.5]), the first centered at 0 and the second at 5 (mu = [0; 5]), and the variances equal 1 and 2 respectively for the first and second distributions (sigma = cat(3, 1, 2)). As you can see below, the mean effectively shifts the distribution, while the variance determines how wide/narrow and flat/pointy it is. The prior sets the mixing proportions to get the final combined model.  The idea of EM clustering is that each distribution represents a cluster. So in the example above with one dimensional data, if you were given an instance x = 0.5, we would assign it as belonging to the first cluster/mode with 99.5% probability you can see how the instance falls well under the first bell-curve. Whereas if you take a point in the middle, the answer would be more ambiguous (point assigned to class=2 but with much less certainty): The same concepts extend to higher dimension with multivariate normal distributions. In more than one dimension, the covariance matrix is a generalization of variance, in order to account for inter-dependencies between features. Here is an example again with a mixture of two MVN distributions in 2-dimensions:  There is some intuition behind how the the covariance matrix affects the shape of the joint density function. For instance in 2D, if the matrix is diagonal it implies that the two dimensions don't co-vary. In that case the PDF would look like an axis-aligned ellipse stretched out either horizontally or vertically according to which dimension has the bigger variance. If they are equal, then the shape is a perfect circle (distribution spread out in both dimensions at an equal rate). Finally if the covariance matrix is arbitrary (non-diagonal but still symmetric by definition), then it will probably look like a stretched ellipse rotated at some angle. So in the previous figure, you should be able to tell the two "bumps" apart and what individual distribution each represent. When you go 3D and higher dimensions, think of the it as representing (hyper-)ellipsoids in N-dims.  Now when you're performing clustering using GMM, the goal is to find the model parameters (mean and covariance of each distribution as well as the priors) so that the resulting model best fits the data. The best-fit estimation translates into maximizing the likelihood of the data given the GMM model (meaning you choose model that maximizes Pr(data|model)). As other have explained, this is solved iteratively using the EM algorithm; EM starts with an initial estimate or guess of the parameters of the mixture model. It iteratively re-scores the data instances against the mixture density produced by the parameters. The re-scored instances are then used to update the parameter estimates. This is repeated until the algorithm converges. Unfortunately the EM algorithm is very sensitive to the initialization of the model, so it might take a long time to converge if you set poor initial values, or even get stuck in local optima. A better way to initial the GMM parameters is to use K-means as a first step (like you've shown in your code), and using the mean/cov of those clusters to initialize EM. As with other cluster analysis techniques, we first need to decide on the number of clusters to use. Cross-validation is a robust way to find a good estimate of the number of clusters. EM clustering suffers from the fact that there a lot parameters to fit, and usually requires lots of data and many iterations to get good results. An unconstrained model with M-mixtures and D-dimensional data involves fitting D*D*M + D*M + M parameters (M covariance matrices each of size DxD, plus M mean vectors of length D, plus a vector of priors of length M). That could be a problem for datasets with large number of dimensions. So it is customary to impose restrictions and assumption to simplify the problem (a sort of regularization to avoid overfitting problems). For instance you could fix the covariance matrix to be only diagonal or even have the covariance matrices shared across all Gaussians. Finally once you've fitted the mixture model, you can explore the clusters by computing the posterior probability of data instances using each mixture component (like I've showed with the 1D example). GMM assigns each instance to a cluster according to this "membership" likelihood. Here is a more complete example of clustering data using Gaussian mixture models:  You are right, there is the same insight behind clustering with K-Means or GMM.
But as you mentionned Gaussian Mixtures take data covariances into account.
To find the maximum likelihood parameters (or maximum a posteriori MAP) of the GMM statistical model, you need to use an iterative process called the EM algorithm. Each iteration is composed of a E-step (Expectation) and a M-step (Maximization) and repeat until convergence.
After convergence you can easily estimate the membership probabilities of each data vectors for each cluster model. Covariance tells you how the data varies in the space, if a distribution has large covariance, that means data is more spread and vice versa. When you have the PDF of a gaussian distribution (mean and covariance params), you can check the membership confidence of a test point under that distribution.  However GMM also suffers from the weakness of K-Means, that you have to pick the parameter K which is the number of clusters. This requires a good understanding of your data's multimodality.While tuning the hyperparameters to get my model to perform better, I noticed that the score I get (and hence the model that is created) is different every time I run the code despite fixing all the seeds for random operations. This problem does not happen if I run on CPU. I googled and found out that this is a common issue when using a GPU to train. Here is a very good/detailed example with short code snippets to verify the existence of that problem. They pinpointed the non-determinism to "tf.reduce_sum" function. However, that is not the case for me. it could be because I'm using different hardware (1080 TI) or a different version of CUDA libraries or Tensorflow. It seems like there are many different parts of the CUDA libraries that are non-deterministic and it doesn't seem easy to figure out exactly which part and how to get rid of it. Also, this must have been by design, so it's likely that there is a sufficient efficiency increase in exchange for non-determinism. So, my question is: Since GPUs are popular for training NNs, people in this field must have a way to deal with non-determinism, because I can't see how else you'd be able to reliably tune the hyperparameters. What is the standard way to handle non-determinism when using a GPU? TL;DR That, but much longer When you see neural network operations as mathematical operations, you would expect everything to be deterministic. Convolutions, activations, cross-entropy – everything here are mathematical equations and should be deterministic. Even pseudo-random operations such as shuffling, drop-out, noise and the likes, are entirely determined by a seed. When you see those operations from their computational implementation, on the other hand, you see them as massively parallelized computations, which can be source of randomness unless you are very careful. The heart of the problem is that, when you run operations on several parallel threads, you typically do not know which thread will end first. It is not important when threads operate on their own data, so for example, applying an activation function to a tensor should be deterministic. But when those threads need to synchronize, such as when you compute a sum, then the result may depend on the order of the summation, and in turn, on the order in which thread ended first. From there, you have broadly speaking two options: Keep non-determinism associated with simpler implementations. Take extra care in the design of your parallel algorithm to reduce or remove non-determinism in your computation. The added constraint usually results in slower algorithms Which route takes CuDNN? Well, mostly the deterministic one. In recent releases, deterministic operations are the norm rather than the exception. But it used to offer many non-deterministic operations, and more importantly, it used to not offer some operations such as reduction, that people needed to implement themselves in CUDA with a variable degree of consideration to determinism. Some libraries such as theano were more ahead of this topic, by exposing early on a deterministic flag that the user could turn on or off – but as you can see from its description, it is far from offering any guarantee. If more, sometimes we will select some implementations that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementation, e.g. when we do not have a GPU implementation that is deterministic. Also, see the dnn.conv.algo* flags to cover more cases. In TensorFlow, the realization of the need for determinism has been rather late, but it's slowly getting there – helped by the advance of CuDNN on that front also. For a long time, reductions have been non-deterministic, but now they seem to be deterministic. The fact that CuDNN introduced deterministic reductions in version 6.0 may have helped of course. It seems that currently, the main obstacle for TensorFlow towards determinism is the backward pass of the convolution. It is indeed one of the few operations for which CuDNN proposes a non-deterministic algorithm, labeled CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0. This algorithm is still in the list of possible choices for the backward filter in TensorFlow. And since the choice of the filter seems to be based on performance, it could indeed be picked if it is more efficient. (I am not so familiar with TensorFlow's C++ code so take this with a grain of salt.) Is this important? If you are debugging an issue, determinism is not merely important: it is mandatory. You need to reproduce the steps that led to a problem. This is currently a real issue with toolkits like TensorFlow. To mitigate this problem, your only option is to debug live, adding checks and breakpoints at the correct locations – not great. Deployment is another aspect of things, where it is often desirable to have a deterministic behavior, in part for human acceptance. While nobody would reasonably expect a medical diagnosis algorithm to never fail, it would be awkward that a computer could give the same patient a different diagnosis depending on the run. (Although doctors themselves are not immune to this kind of variability.) Those reasons are rightful motivations to fix non-determinism in neural networks. For all other aspects, I would say that we need to accept, if not embrace, the non-deterministic nature of neural net training. For all purposes, training is stochastic. We use stochastic gradient descent, shuffle data, use random initialization and dropout – and more importantly, training data is itself but a random sample of data. From that standpoint, the fact that computers can only generate pseudo-random numbers with a seed is an artifact. When you train, your loss is a value that also comes with a confidence interval due to this stochastic nature. Comparing those values to optimize hyper-parameters while ignoring those confidence intervals does not make much sense – therefore it is vain, in my opinion, to spend too much effort fixing non-determinism in that, and many other, cases. Starting from TF 2.9 (TF >= 2.9), if you want your TF models to run deterministically, the following lines need to be added at the beginning of the program. Important note: The first line sets the random seed for the following : Python, NumPy and TensorFlow. The second line makes each TensorFlow operation deterministic. To get a MNIST network (https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) to train deterministically on my GPU (1050Ti): Or: Note that the resulting loss is repeatable with either method for selecting deterministic algorithms from TF, but the two methods result in different losses. Also, the solution above doesn't make a more complicated model I'm using repeatable. Check out https://github.com/NVIDIA/framework-determinism for a more current answer. A side note: For cuda  cuDNN 8.0.1, non deterministic algorithms exist for: (from https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html)When trying to create a neural network and optimize it using Pytorch, I am getting ValueError: optimizer got an empty parameter list Here is the code. and then the call gives the very informative error ValueError: optimizer got an empty parameter list I find it hard to understand what exactly in the network's definition makes the network have parameters. I am following and expanding the example I found in Pytorch's tutorial code. I can't really tell the difference between my code and theirs that makes mine think it has no parameters to optimize. How to make my network have parameters like the linked example? Your NetActor does not directly store any nn.Parameter. Moreover, all other layers it eventually uses in forward are stored as a simple list in self.nn_layers.
If you want self.actor_nn.parameters() to know that the items stored in the list self.nn_layers may contain trainable parameters, you should work with containers.
Specifically, making self.nn_layers to be a nn.ModuleList instead of a simple list should solve your problem:I'm currently working on classifying images with different image-descriptors. Since they have their own metrics, I am using precomputed kernels. So given these NxN kernel-matrices (for a total of N images) i want to train and test a SVM. I'm not very experienced using SVMs though.  What confuses me though is how to enter the input for training. Using a subset of the kernel MxM (M being the number of training images), trains the SVM with M features. However, if I understood it correctly this limits me to use test-data with similar amounts of features. Trying to use sub-kernel of size MxN, causes infinite loops during training, consequently, using more features when testing gives poor results. This results in using equal sized training and test-sets giving reasonable results. But if i only would want to classify, say one image, or train with a given amount of images for each class and test with the rest, this doesn't work at all. How can i remove the dependency between number of training images and features, so i can test with any number of images? I'm using libsvm for MATLAB, the kernels are distance-matrices ranging between [0,1]. You seem to already have figured out the problem... According to the README file included in the MATLAB package: To use precomputed kernel, you must include sample serial number as
  the first column of the training and testing data. Let me illustrate with an example: The output:I m doing an assignment where I am trying to build a collaborative filtering model for the Netflix prize data. The data that I am using is in a CSV file which I easily imported into a data frame. Now what I need to do is create a sparse matrix consisting of the Users as the rows and Movies as the columns and each cell is filled up by the corresponding rating value. When I try to map out the values in the data frame I need to run a loop for each row in the data frame, which is taking a lot of time in R, please can anyone suggest a better approach. Here is the sample code and data: Sample of data in the dataframe from which the sparse matrix is being created: So in the end I want something like this:
The columns are the movie IDs and the rows are the user IDs So the interpretation is something like this: user 2 rated movie 1 as 3 star, user 3 rated the movie 2 as 3 star and so on for the other users and movies. There are about 8500000 rows in my data frame for which my code takes just about 30-45 mins to create this user item matrix, i would like to get any suggestions  The Matrix package has a constructor made especially for your type of data: Otherwise, you might like knowing about that cool feature of the [ function known as matrix indexing. Your could have tried: (but I would definitely recommend the sparseMatrix approach over this.) This will probably be faster than a loop. If you use data.tables, it will be a lot faster: And as I'm sure someone will point out, you can use this instead This converts df to a data.table in place (without making a copy). if your data set is enormous, that can make a difference...To get to grips with PyTorch (and deep learning in general) I started by working through some basic classification examples. One such example was classifying a non-linear dataset created using sklearn (full code available as notebook here)  This is then accurately classified using a pretty basic neural net As I have an interest in health data I then decided to try and use the same network structure to classify some a basic real-world dataset. I took heart rate data for one patient from here, and altered it so all values > 91 would be labelled as anomalies (e.g. a 1 and everything <= 91 labelled a 0). This is completely arbitrary, but I just wanted to see how the classification would work. The complete notebook for this example is here.  What is not intuitive to me is why the first example reaches a loss of 0.0016 after 1,000 epochs, whereas the second example only reaches a loss of 0.4296 after 10,000 epochs   Perhaps I am being naive in thinking that the heart rate example would be much easier to classify. Any insights to help me understand why this is not what I am seeing would be great! Your input data is not normalized. You'll get
 convergence in only 1000 iterations. The key difference between the two examples you have is that the data x in the first example is centered around (0, 0) and has very low variance.
On the other hand, the data in the second example is centered around 92 and has relatively large variance. This initial bias in the data is not taken into account when you randomly initialize the weights which is done based on the assumption that the inputs are  roughly normally distributed around zero.
It is almost impossible for the optimization process to compensate for this gross deviation - thus the model gets stuck in a sub-optimal solution. Once you normalize the inputs, by subtracting the mean and dividing by the std, the optimization process becomes stable again and rapidly converges to a good solution. For more details about input normalization and weights initialization, you can read section 2.2 in He et al Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (ICCV 2015). If, for some reason, you cannot compute mean and std data in advance, you can still use nn.BatchNorm1d to estimate and normalize the data as part of the training process.  For example This modification without any change to the input data, yields similar convergance after only 1000 epochs:
 For numerical stability, it is better to use nn.BCEWithLogitsLoss instead of nn.BCELoss. For this end, you need to remove the torch.sigmoid from the forward() output, the sigmoid will be computed inside the loss.
See, for example, this thread regarding the related sigmoid + cross entropy loss for binary predictions. Let's start first by understanding how neural networks work, neural networks observe patterns, hence the necessity for large datasets. In the case of the example, two what pattern you intend to find is when if HR < 91: label = 0, this if-condition can be represented by the formula, sigmoid((HR-91) * 1) , if you plug various values into the formula you can see you that all values < 91, label 0 and others label 1. I have inferred this formula and it could be anything as long as it gives the correct values. Basically, we apply the formula wx+b, where x in our input data and we learn the values for w and b. Now initially the values are all random, so getting the b value from 1030131190 (a random value), to maybe 98 is fast, since the loss is great, the learning rate allows the values to jump fast. But once you reach 98, your loss is decreasing, and when you apply the learning rate, it takes it more time to reach closer to 91, hence the slow decrease in loss. As the values get closer, the steps taken are even slower.  This can be confirmed via the loss values, they are constantly decreasing, initially, the deceleration is higher, but then it becomes smaller. Your network is still learning but slowly. Hence in deep learning, you use this method called stepped learning rate, wherewith the increase in epochs you decrease your learning rate so that your learning is fastersince in TensorFlow 2.0 they plan on unifying all high-level APIs under keras (which I'm not much familiar with) and removing Sessions altogether, I was wondering: How can I create a custom keras layer that has a custom gradient? I've seen the (quite limited) guide on creating custom layers in keras but it doesn't describe what we should do if we want our operation to have a custom gradient. First of all, the "unification" of the APIs (as you call it) under keras doesn't prevent you from doing things like you did in TensorFlow 1.x. Sessions might be gone but you can still define your model like any python function and train it eagerly without keras (i.e. through tf.GradientTape) Now, if you want to build a keras model with a custom layer that performs a custom operation and has a custom gradient, you should do the following: a) Write a function that performs your custom operation and define your custom gradient. More info on how to do this here. Note that in the function you should treat x and dy as Tensors and not numpy arrays (i.e. perform tensor operations) b) Create a custom keras layer that performs your custom_op. For this example I'll assume that your layer doesn't have any trainable parameters or change the shape of its input, but it doesn't make much difference if it does. For that you can refer to the guide that you posted check this one. Now you can use this layer in a keras model and it will work. For example:I trained GoogLeNet model from scratch. But it didn't give me the promising results.
As an alternative, I would like to do fine tuning of GoogLeNet model on my dataset. Does anyone know what are the steps should I follow?  Assuming you are trying to do image classification. These should be the steps for finetuning a model: The original classification layer "loss3/classifier" outputs predictions for 1000 classes (it's mum_output is set to 1000). You'll need to replace it with a new layer with appropriate num_output. Replacing the classification layer: You need to make a new training dataset with the new labels you want to fine tune to. See, for example, this post on how to make an lmdb dataset. When finetuning a model, you can train ALL model's weights or choose to fix some weights (usually filters of the lower/deeper layers) and train only the weights of the top-most layers. This choice is up to you and it ususally depends on the amount of training data available (the more examples you have the more weights you can afford to finetune).
Each layer (that holds trainable parameters) has param { lr_mult: XX }. This coefficient determines how susceptible these weights to SGD updates. Setting param { lr_mult: 0 } means you FIX the weights of this layer and they will not be changed during the training process.
Edit your train_val.prototxt accordingly. Run caffe train but supply it with caffemodel weights as an initial weights: Fine-tuning is a very useful trick to achieve a promising accuracy compared to past manual feature. @Shai already posted a good tutorial for fine-tuning the Googlenet using Caffe, so I just want to give some recommends and tricks for fine-tuning for general cases. In most of time, we face a task classification problem that new dataset (e.g. Oxford 102 flower dataset or Cat&Dog) has following four common situations CS231n: In practice, most of time we do not have enough data to train the network from scratch, but may be enough for pre-trained model. Whatever which cases I mentions above only thing we must care about is that do we have enough data to train the CNN? If yes, we can train the CNN from scratch. However, in practice it is still beneficial to initialize the weight from pre-trained model. If no, we need to check whether data is very different from original datasets? If it is very similar, we can just fine-tune the fully connected neural network or fine-tune with SVM. However, If it is very different from original dataset, we may need to fine-tune the convolutional neural network to improve the generalization.I am trying to train a very large model. Therefore, I can only fit a very small batch size into GPU memory. Working with small batch sizes results with very noisy gradient estimations.
What can I do to avoid this problem? You can change the iter_size in the solver parameters.
Caffe accumulates gradients over iter_size x batch_size instances in each stochastic gradient descent step.
So increasing iter_size can also get more stable gradient when you cannot use large batch_size due to the limited memory. As stated in this post, the batch size is not a problem in theory (the efficiency of stochastic gradient descent has been proven with a batch of size 1). Make sure you implement your batch correctly (the samples should be randomly picked over your data).I want to do a 10-fold cross-validation in my one-against-all support vector machine classification in MATLAB. I tried to somehow mix these two related answers: But as I'm new to MATLAB and its syntax, I didn't manage to make it work till now. On the other hand, I saw just the following few lines about cross validation in the LibSVM README files and I couldn't find any related example there: option -v randomly splits the data into n parts and calculates cross
  validation accuracy/mean squared error on them. See libsvm FAQ for the meaning of outputs. Could anyone provide me an example of 10-fold cross-validation and one-against-all classification? Mainly there are two reasons we do cross-validation: For the first case which we are interested in, the process involves training k models for each fold, and then training one final model over the entire training set.
We report the average accuracy over the k-folds. Now since we are using one-vs-all approach to handle the multi-class problem, each model consists of N support vector machines (one for each class). The following are wrapper functions implementing the one-vs-all approach: And here are functions to support cross-validation: Finally, here is simple demo to illustrate the usage: Compare that against the one-vs-one approach which is used by default by libsvm: It may be confusing you that one of the two questions is not about LIBSVM. You should try to adjust this answer and ignore the other. You should select the folds, and do the rest exactly as the linked question. Assume the data has been loaded into data and the labels into labels:This is my train.prototxt. And this is my deploy.prototxt. When I want to load my deploy file I get this error: So, I removed the data layer: Than, I removed bottom: "data" from conv1 layer. After it, I got this error: I removed bottom: "label" from loss layer. And I got this error: What should I do to fix it and create my deploy file? There are two main differences between a "train" prototxt and a "deploy" one: 1. Inputs: While for training data is fixed to a pre-processed training dataset (lmdb/HDF5 etc.), deploying the net require it to process other inputs in a more "random" fashion.
Therefore, the first change is to remove the input layers (layers that push "data" and "labels" during TRAIN and TEST phases). To replace the input layers you need to add the following declaration: This declaration does not provide the actual data for the net, but it tells the net what shape to expect, allowing caffe to pre-allocate necessary resources. 2. Loss: the top most layers in a training prototxt define the loss function for the training. This usually involve the ground truth labels. When deploying the net, you no longer have access to these labels. Thus loss layers should be converted to "prediction" outputs. For example, a "SoftmaxWithLoss" layer should be converted to a simple "Softmax" layer that outputs class probability instead of log-likelihood loss. Some other loss layers already have predictions as inputs, thus it is sufficient just to remove them. Update: see this tutorial for more information. Besides advices from @Shai, you may also want to disable the dropout layers. Although Jia Yangqing, author of Caffe once said that dropout layers have negligible impact on the testing results (google group conversation, 2014), other Deeplearning tools suggest to disable dropout in the deploy phase (for example, lasange).I am working on one deep learning model where I am trying to combine two different model's output : The overall structure is like this :  So the first model takes one matrix, for example [ 10 x 30 ] Now the second model takes two input matrix : I want to make these two matrices trainable like in TensorFlow I was able to do this by : I am not getting any clue how to make those matrix_a and matrix_b trainable and how to merge the output of both networks then give input. I went through this  question But couldn't find an answer because their problem statement is different from mine. What I have tried so far is : Overview of the model :  Update: Model b model a I am merging like this: Is it right way to matmul two keras model? I don't know if I am merging the output correctly and the model is correct. I would greatly appreciate it if anyone kindly gives me some advice on how should I make that matrix trainable and how to merge the model's output correctly then give input. Thanks in advance! Ok. Since you are going to have custom trainable weights, the way to do this in Keras is creating a custom layer. Now, since your custom layer has no inputs, we will need a hack that will be explained later. So, this is the layer definition for the custom weights: Now, this layer should be used like this: Having the layer defined, we can start modeling.
First, let's see the model_a side: For this, we are going to use our TrainableWeights layer.
But first, let's simulate a New_model() as mentioned. Now the entire branch: Finally, we can join the branches in a whole model.
Notice how I didn't have to use model_a or model_s here. You can do it if you want, but those submodels are not needed, unless you want later to get them individually for other usages. (Even if you created them, you don't need to change the code below to use them, they're already part of the same graph) Now train it: Since the output is 2D now, there is no problem about the 'categorical_crossentropy', my comment was because of doubts on the output shape.I'm trying to understand how to identify statistical outliers in groups of dataframe. I will need to group the rows by the conditions and then reduce those groups into a single row and later find the outliers in all reduced rows.  Using a dataset like this I would like to group by different conditions such as: At this step, I am reducing each data frame into a single row, for that, I have a few ideas, a straightforward way is to take the mean of each dataframe but the problem is some of the columns are categorical and some of them are continuous, to take the mean of the entire data frame, I am converting the categorical columns into freq count columns : which looks like this for each df group:  Now I can take the mean and reduce the data frames into a single row : concatinating all reduced rows in single dataframe : The final reduced rows data frame looks like this, where each row represents reduced data frame group:  I want to find the outliers in this reduced dataset, I tried to find outliers using zscore such as : But it doesn't seem to work.
I feel like there has to be a way to do this without too much complexity but I've been stuck on how to proceed. How can I reduce the groups into single rows and find the outliers in the reduced dataset? get the mean and std.
We need to loop over each column, get the mean and std, then set the max and min value we accept for this column. not knowing what the data represents makes it harder, so I had to try and explore. I used groupby with mean to reduce rows  I imagined the binary columns are input and decimal are output, so i pivot to make them into one row making new column named input:  pivoting the needed columns and adjusting the names:  reducing the columns with PCA  df7 seems the outlier. thanks for reading :) in practice for Gaussian distribution: data_cases that falls outside mean+/-3*st.dev. are considered to be outliers (as is outside the 99.7% range of distribution)... Thus, for Gaussian & Gaussian-like distributions I would better rewrite the previous answer to: with data - being taken in for-loop for each column_range that you need
___ P.S.  sometimes even 4 st.dev. can be used (covering 99.9% of distribution), but 1 st.dev. is only 68%, 2 st.dev - 95%... So, be sure what you really needIn the paper Girshick, R Fast-RCNN (ICCV 2015), section "3.1 Truncated SVD for faster detection", the author proposes to use SVD trick to reduce the size and computation time of a fully connected layer.   Given a trained model (deploy.prototxt and weights.caffemodel), how can I use this trick to replace a fully connected layer with a truncated one? Some linear-algebra background
Singular Value Decomposition (SVD) is a decomposition of any matrix W into three matrices: Where U and V are ortho-normal matrices, and S is diagonal with elements in decreasing magnitude on the diagonal. 
One of the interesting properties of SVD is that it allows to easily approximate W with a lower rank matrix: Suppose you truncate S to have only its k leading elements (instead of all elements on the diagonal) then is a rank k approximation of W. Using SVD to approximate a fully connected layer
Suppose we have a model deploy_full.prototxt with a fully connected layer Furthermore, we have trained_weights_full.caffemodel - trained parameters for deploy_full.prototxt model. Copy deploy_full.protoxt to deploy_svd.protoxt and open it in editor of your choice. Replace the fully connected layer with these two layers: In python, a little net surgery: Now we have deploy_svd.prototxt with trained_weights_svd.caffemodel that approximate the original net with far less multiplications, and weights. Actually, Ross Girshick's py-faster-rcnn repo includes an implementation for the SVD step: compress_net.py. BTW, you usually need to fine-tune the compressed model to recover the accuracy (or to compress in a more sophisticated way, see for example "Accelerating Very Deep Convolutional Networks for Classification and Detection", Zhang et al). Also, for me scipy.linalg.svd worked faster than numpy's svd.I just made a Adaboost Classifier with these parameters, 1.n_estimators = 50 2.base_estimator = svc (support vector classifier) 3.learning_rate = 1 here is my code: Dataset has 18 independent variables and 1 categorical dependent variable dataset has 10480 datapoints whenever i run this it will take so much time but no any result. Is there any way to check execution time? Or any better way to do this? In practice, we never use SVMs as base classifiers for Adaboost. Adaboost (and similar ensemble methods) were conceived using decision trees as base classifiers (more specifically, decision stumps, i.e. DTs with a depth of only 1); there is good reason why still today, if you don't specify explicitly the base_classifier argument, it assumes a value of DecisionTreeClassifier(max_depth=1). DTs are suitable for such ensembling because they are essentially unstable classifiers, which is not the case with SVMs, hence the latter are not expected to offer much when used as base classifiers. On top of this, SVMs are computationally much more expensive than decision trees (let alone decision stumps), which is the reason for the long processing times you have observed. Unless you have a very good reason to stick to SVMs as base classifiers (and I highly doubt that you do), remove the base_estimator = svc in order to revert to the default setting, and most probably you will be fine. I had a similar experience recently. In my case though, I realised I wasn't scaling the X before using SVM as the base estimator. Just make sure you scale the data from 0 to 1 (you can use StandardScaler() from sklearn) which is always required prior to using SVM.I have a dataframe as below:
 I want to get the name of the column if column of a particular row if it contains 1 in the that column. Use DataFrame.dot: If there is multiple 1 per row: Your question is very ambiguous and I recommend reading this link in @sammywemmy's comment. If I understand your problem correctly... we'll talk about this mask first: What's happening? Lets work our way outward starting from within df.columns[**HERE**] : "Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent". This gives us a handy Series to mask the column names with. We will use this example to automate for your solution below Automate to get an output of (<row index> ,[<col name>, <col name>,..]) where there is 1 in the row values. Although this will be slower on large datasets, it should do the trick: Next step is a for loop that iterates the contents of each df in df_dict, checks them with the mask we created earlier, and prints the intended results: You see how I generated sample data that can be easily reproduced? In the future, please try to ask questions with posted sample data that can be reproduced. This way it helps you understand your problem better and it is easier for us to answer it for you. Getting column name are dividing in 2 sections. If you want in a new column name then condition should be unique because it will only give 1 col name for each row. If you were looking for min or maximum 2nd case, If your condition is satisfied in multiple columns for example you are looking for columns that contain 1 and you are looking for list because its not possible to adjust in same dataframe. Or you are looking for numerical condition columns contains value more than 1 Happy learningI am following the tutorial over here : https://www.rpubs.com/loveb/som . This tutorial shows how to use the Kohonen Network (also called SOM, a type of machine learning algorithm) on the iris data. I ran this code from the tutorial: The above code fits a Kohonen Network on the iris data. Each observation from the data set is assigned to each one of the "colorful circles" (also called "neurons") in the below pictures. My question: In these plots, how would you identify which observations were assigned to which circles? Suppose I wanted to know which observations belong in the circles outlined in with the black triangles below: 
 Is it possible to do this? Right now, I am trying to use iris.som$classif to somehow trace which points are in which circle. Is there a better way to do this? UPDATE: @Jonny Phelps showed me how to identify observations within a triangular form (see answer below). But i am still not sure if it possible to identify irregular shaped forms. E.g.  In a previous post (Labelling Points on a Plot (R Language)), a user showed me how to assign arbitrary numbers to each circle on the grid:  Based on the above plot, how could you use the "som$classif" statement to find out which observations were in circles 92,91,82,81,72 and 71? Thanks EDIT: Now with Shiny App! A plotly solution is also possible, where you can mouse over individual neurons to display the associated iris rownames (called id here). Based on your iris.som data and Jonny Phelps' grid approach, you can just assign the row numbers as concatenated strings to the individual neurons and have these shown upon mouseover:  Here is a full Shiny app that allows lasso selection and shows a table with the data: From what I can see, using iris.som$unit.classif & iris.som$grid is the way to go in isolating circles within the plotting grid. I have made an assumption that the classifier value matches the row index of iris.som$grid so this will need some more validation. Let me know if this helps your problem :) Output data: Validation plotting on the grid:
 I elaborated the example in my post, however, not on the iris data set but I suppose it is no problem: R, SOM, Kohonen Package, Outlier Detection and also added code snippets you might need. They show I think this answers your questions. It would also be nice to compare the performance of SOM with t-SNE. I have only used SOM as an experiment on the data I generated and on the real wine data set. It would also be nice to prepare heat maps if you have more than 2 variables. All the best to you analysis!I'm trying to make a XOR gate by using 2 perceptron network but for some reason the network is not learning, when I plot the change of error in a graph the error comes to a static level and oscillates in that region.  I did not add any bias to the network at the moment. This is the error changing by the number of learning rounds. Is this correct? The red color line is the line I was expecting how the error should change.  Anything wrong I'm doing in the code? As I can't seem to figure out what's causing the error. Help much appreciated.  Thanks in advance Here is a one hidden layer network with backpropagation which can be customized to run experiments with relu, sigmoid and other activations. After several experiments it was concluded that with relu the network performed better and reached convergence sooner, while with sigmoid the loss value fluctuated. This happens because, "the gradient of sigmoids becomes increasingly small as the absolute value of x increases". End Result:  The weights obtained after training were: nn.w1 nn.w2 I found the following youtube series extremely helpful for understanding neural nets: Neural networks demystified There is only little which I know and also that can be explained in this answer. If you want an even better understanding of neural nets, then I would suggest you to go through the following link: cs231n: Modelling one neuron The error calculated in each epoch should be a sum total of all sum squared errors (i.e. error for every target)Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 2 years ago. In terms of artificial intelligence and machine learning, what is the difference between supervised and unsupervised learning?
Can you provide a basic, easy explanation with an example?  Since you ask this very basic question, it looks like it's worth specifying what Machine Learning itself is. Machine Learning is a class of algorithms which is data-driven, i.e. unlike "normal" algorithms it is the data that "tells" what the "good answer" is. Example: a hypothetical non-machine learning algorithm for face detection in images would try to define what a face is (round skin-like-colored disk, with dark area where you expect the eyes etc). A machine learning algorithm would not have such coded definition, but would "learn-by-examples": you'll show several images of faces and not-faces and a good algorithm will eventually learn and be able to predict whether or not an unseen image is a face. This particular example of face detection is supervised, which means that your examples must be labeled, or explicitly say which ones are faces and which ones aren't. In an unsupervised algorithm your examples are not labeled, i.e. you don't say anything. Of course, in such a case the algorithm itself cannot "invent" what a face is, but it can try to cluster the data into different groups, e.g. it can distinguish that faces are very different from landscapes, which are very different from horses. Since another answer mentions it (though, in an incorrect way): there are "intermediate" forms of supervision, i.e. semi-supervised and active learning. Technically, these are supervised methods in which there is some "smart" way to avoid a large number of labeled examples. In active learning, the algorithm itself decides which thing you should label (e.g. it can be pretty sure about a landscape and a horse, but it might ask you to confirm if a gorilla is indeed the picture of a face). In semi-supervised learning, there are two different algorithms which start with the labeled examples, and then "tell" each other the way they think about some large number of unlabeled data. From this "discussion" they learn. Supervised learning is when the data you feed your algorithm with is "tagged" or "labelled", to help your logic make decisions. Example: Bayes spam filtering, where you have to flag an item as spam to refine the results. Unsupervised learning are types of algorithms that try to find correlations without any external inputs other than the raw data. Example: data mining clustering algorithms. Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering Pattern Recognition and Machine Learning (Bishop, 2006) In supervised learning, the input x is provided with the expected outcome y (i.e., the output the model is supposed to produce when the input is x), which is often called the "class" (or "label") of the corresponding input x. In unsupervised learning, the "class" of an example x is not provided. So, unsupervised learning can be thought of as finding "hidden structure" in unlabelled data set.  Approaches to supervised learning include: Classification (1R, Naive Bayes, decision tree learning algorithm, such
as ID3 CART, and so on) Numeric Value Prediction Approaches to unsupervised learning include: Clustering (K-means, hierarchical clustering) Association Rule Learning I can tell you an example. Suppose you need to recognize which vehicle is a car and which one is a motorcycle. In the supervised learning case, your input (training) dataset needs to be labelled, that is, for each input element in your input (training) dataset, you should specify if it represents a car or a motorcycle. In the unsupervised learning case, you do not label the inputs. The unsupervised model clusters the input into clusters based e.g. on similar features/properties. So, in this case, there is are no labels like "car". For instance, very often training a neural network is supervised learning: you're telling the network to which class corresponds the feature vector you're feeding. Clustering is unsupervised learning: you let the algorithm decide how to group samples into classes that share common properties. Another example of unsupervised learning is Kohonen's self organizing maps. I have always found the distinction between unsupervised and supervised learning to be arbitrary and a little confusing. There is no real distinction between the two cases, instead there is a range of situations in which an algorithm can have more or less 'supervision'. The existence of semi-supervised learning is an obvious examples where the line is blurred. I tend to think of supervision as giving feedback to the algorithm about what solutions should be preferred. For a traditional supervised setting, such as spam detection, you tell the algorithm "don't make any mistakes on the training set"; for a traditional unsupervised setting, such as clustering, you tell the algorithm "points that are close to each other should be in the same cluster". It just so happens that, the first form of feedback is a lot more specific than the  latter. In short, when someone says 'supervised', think classification, when they say 'unsupervised' think clustering and try not to worry too much about it beyond that. Supervised Learning Supervised learning is based on training a data sample
from data source with correct classification already assigned.
Such techniques are utilized in feedforward or MultiLayer
Perceptron (MLP) models. These MLP has three distinctive
characteristics: These characteristics along with learning through training
solve difficult and diverse problems. Learning through
training in a supervised ANN model also called as error backpropagation algorithm. The error correction-learning
algorithm trains the network based on the input-output
samples and finds error signal, which is the difference of the
output calculated and the desired output and adjusts the
synaptic weights of the neurons that is proportional to the
product of the error signal and the input instance of the
synaptic weight. Based on this principle, error back
propagation learning occurs in two passes: Forward Pass:  Here, input vector is presented to the network. This input signal propagates forward, neuron by neuron through the network and emerges at the output end of
the network as output signal: y(n) = φ(v(n)) where v(n) is the induced local field of a neuron defined by v(n) =Σ w(n)y(n). The output that is calculated at the output layer o(n) is compared with the desired response d(n) and finds the error e(n) for that neuron. The synaptic weights of the network during this pass are remains same. Backward Pass:  The error signal that is originated at the output neuron of that layer is propagated backward through network. This calculates the local gradient for each neuron in each layer and allows the synaptic weights of the network to undergo changes in accordance with the delta rule as: This recursive computation is continued, with forward pass followed by the backward pass for each input pattern till the network is converged. Supervised learning paradigm of an ANN is efficient and finds solutions to several linear and non-linear problems such as classification, plant control, forecasting, prediction, robotics etc. Unsupervised Learning Self-Organizing neural networks learn using unsupervised learning algorithm to identify hidden patterns in unlabelled input data. This unsupervised refers to the ability to learn and organize information without providing an error signal to evaluate the potential solution. The lack of direction for the learning algorithm in unsupervised learning can sometime be advantageous, since it lets the algorithm to look back for patterns that have not been previously considered. The main characteristics of Self-Organizing Maps (SOM) are: The computational layer is also called as competitive layer since the neurons in the layer compete with each other to become active. Hence, this learning algorithm is called competitive algorithm. Unsupervised algorithm in SOM
works in three phases: Competition phase: for each input pattern x, presented to the network, inner product with synaptic weight w is calculated and the neurons in the competitive layer finds a discriminant function that induce competition among the neurons and the synaptic weight vector that is close to the input vector in the Euclidean distance is announced as winner in the competition. That neuron is called best matching neuron, Cooperative phase:  the winning neuron determines the center of a topological neighborhood h of cooperating neurons. This is performed by the lateral interaction d among the
cooperative neurons. This topological neighborhood reduces its size over a time period. Adaptive phase:  enables the winning neuron and its neighborhood neurons to increase their individual values of the discriminant function in relation to the input pattern
through suitable synaptic weight adjustments,  Upon repeated presentation of the training patterns, the synaptic weight vectors tend to follow the distribution of the input patterns due to the neighborhood updating and thus ANN learns without supervisor. Self-Organizing Model naturally represents the neuro-biological behavior, and hence is used in many real world applications such as clustering, speech recognition, texture segmentation, vector coding etc. Reference. There are many answers already which explain the differences in detail. I found these gifs on codeacademy and they often help me explain the differences effectively. 
Notice that the training images have labels here and that the model is learning the names of the images. 
Notice that what's being done here is just grouping(clustering) and that the model doesn't know anything about any image. Machine learning: 
It explores the study and construction of algorithms that can learn from and make predictions on data.Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions expressed as outputs,rather than following strictly static program instructions. Supervised learning: 
It  is the machine learning task of inferring a function from labeled training data.The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.Specifically, a supervised learning algorithm takes a known set of input data and known responses to the data (output), and trains a model to generate reasonable predictions for the response to new data. Unsupervised learning: 
It is learning without a teacher. One basic
thing that you might want to do with data is to visualize it. It is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning. Unsupervised learning uses procedures that attempt to find natural partitions
of patterns.  With unsupervised learning there is no feedback based on the prediction results, i.e., there is no teacher to correct you.Under the Unsupervised learning methods no labeled examples are provided and there is no notion of the output during the learning process. As a result, it is up to the learning scheme/model to find patterns or discover the groups of the input data You should use unsupervised learning methods when you need a large
  amount of data to train your models, and the willingness and ability
  to experiment and explore, and of course a challenge that isn’t well
  solved via more-established methods.With unsupervised learning it is
  possible to learn larger and more complex models than with supervised
  learning.Here is a good example on it . Supervised Learning: You give variously labelled example data as input, along with the correct answers. This algorithm will learn from it, and start predicting correct results based on the inputs thereafter. Example: Email Spam filter  Unsupervised Learning: You just give data and don't tell anything - like labels or correct answers. Algorithm automatically analyses patterns in the data. Example: Google News Supervised learning:
say a kid goes to kinder-garden. here teacher shows him 3 toys-house,ball and car. now teacher gives him 10 toys. 
he will classify them  in 3 box of house,ball and car based on his previous experience.
so kid was first supervised by teachers for getting right answers for few sets. then he was tested on unknown toys.
 Unsupervised learning:
again kindergarten example.A child is given 10 toys. he is told to segment similar ones.
so based on features like shape,size,color,function etc he will try to make 3 groups say A,B,C and group them.
 The word Supervise means you are giving supervision/instruction to machine to help it find answers. Once it learns instructions, it can easily predict for new case. Unsupervised  means there is no supervision or instruction how to find answers/labels and machine will use its intelligence to find some pattern in our data. Here it will not make prediction, it will just try to find clusters which has similar data. Supervised learning, given the data with an answer. Given email labeled as spam/not spam, learn a spam filter. Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabetes or not. Unsupervised learning, given the data without an answer, let the pc to group things. Given a set of news articles found on the web, group the into set of articles about the same story. Given a database of custom data, automatically discover market segments and group customers into different market segments. Reference Supervised Learning In this, every input pattern that is used to train the network is
  associated with an output pattern, which is the target or the desired
  pattern. A teacher is assumed to be present during the learning
  process, when a comparison is made between the network's computed
  output and the correct expected output, to determine the error. The
  error can then be used to change network parameters, which result in
  an improvement in performance. Unsupervised Learning In this learning method, the target output is not presented to the
  network. It is as if there is no teacher to present the desired
  pattern and hence, the system learns of its own by discovering and
  adapting to structural features in the input patterns. I'll try to keep it simple. Supervised Learning: In this technique of learning, we are given a data set and the system already knows the correct output of the data set. So here, our system learns by predicting a value of its own. Then, it does an accuracy check by using a cost function to check how close its prediction was to the actual output. Unsupervised Learning: In this approach, we have little or no knowledge of what our result would be. So instead, we derive structure from the data where we don't know effect of variable.
We make structure by clustering the data based on relationship among the variable in data.
Here, we don't have a feedback based on our prediction.   You have input x and a target output t. So you train the algorithm to generalize to the missing parts. It is supervised because the target is given. You are the supervisor telling the algorithm: For the example x, you should output t! Although segmentation, clustering and compression are usually counted in this direction, I have a hard time to come up with a good definition for it. Let's take auto-encoders for compression as an example. While you only have the input x given, it is the human engineer how tells the algorithm that the target is also x. So in some sense, this is not different from supervised learning. And for clustering and segmentation, I'm not too sure if it really fits the definition of machine learning (see other question). Supervised Learning: You have labeled data and have to learn from that. e.g house data along with price and then learn to predict price Unsupervised learning: you have to find the trend and then predict, no prior labels given.
e.g different people in the class and then a new person comes so what group does this new student belong to. In Supervised Learning we know what the input and output should be. For example , given a set of cars. We have to find out which ones red and which ones blue.  Whereas, Unsupervised learning is where we have to find out the answer with a very little or without any idea about how the output should be. For example, a learner might be able to build a model that detects when people are smiling based on correlation of facial patterns and words such as "what are you smiling about?".  Supervised learning can label a new item into one of the trained labels based on learning during training. You need to provide large numbers of training data set, validation data set and test data set. If you provide say pixel image vectors of digits along with training data with labels, then it can identify the numbers.   Unsupervised learning does not require training data-sets. In unsupervised learning it can group items into different clusters based on the difference in the input vectors. If you provide pixel image vectors of digits and ask it to classify into 10 categories, it may do that. But it does know how to labels it as you have not provided training labels. Supervised Learning is basically where you have input variables(x) and output variable(y) and use algorithm to learn the mapping function from input to the output. The reason why we called this as supervised is because algorithm learns from the training dataset, the algorithm iteratively makes predictions on the training data.
Supervised have two types-Classification and Regression.
Classification is when the output variable is category like yes/no, true/false.
Regression is when the output is real values like height of person, Temperature etc. UN supervised learning is where we have only input data(X) and no output variables.
This is called an unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data. Types of unsupervised learning are clustering and Association. Supervised Learning is basically a technique in which the training data from which the machine learns is already labelled that is suppose a simple even odd number classifier where you have already classified the data during training . Therefore it uses "LABELLED" data. Unsupervised learning on the contrary is a technique in which the machine by itself labels the data . Or you can say its the case when the machine learns by itself from scratch. In Simple 
     Supervised learning is  type of machine learning problem in which we have some labels and by using that labels we implement algorithm such as regression and classification .Classification is applied where our output is like in the form of 
0 or 1 ,true/false,yes/no. and regression is applied where out put a real value such a house of price  Unsupervised Learning is a type of machine learning problem in which we don't have any labels means we have some data only ,unstructured data and we have to cluster the data (grouping of data)using various unsupervised algorithm Supervised Machine Learning "The process of  an algorithm learning from  training dataset and
  predict the  output. " Accuracy of  predicted output  directly proportional   to the  training data (length) Supervised learning is where you have input variables (x) (training dataset) and an output variable (Y) (testing dataset) and you use an algorithm to learn the mapping function from the input to the output. Major types:  Algorithms:  Classification Algorithms: Predictive Algorithms: Application  areas:  Voice  Recognition  Predict  the HR select particular candidate or  not  Predict the stock market price Supervised learning: A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. Categories of problem: Regression:  Predict results within a continuous output => map input variables to some continuous function. Example: Given a picture of a person, predict his age Classification: Predict results in a discrete output =>  map input variables into discrete categories Example: Is this tumer cancerous?  Unsupervised learning: Unsupervised learning learns from test data that has not been labeled, classified or categorized. Unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. We can derive this structure by clustering the data based on relationships among the variables in the data. There is no feedback based on the prediction results. Categories of problem: Clustering: is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters) Example: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.  Popular use cases are listed here. Difference between classification and clustering in data mining? References: Supervised_learning Unsupervised_learning machine-learning from coursera towardsdatascience Supervised Learning  Unsupervised Learning  Example: Supervised Learning: One bag with orange => build model One mixed bag of apple and orange. => Please classify  Unsupervised Learning:  One mixed bag of apple and orange. => build model  Another mixed bag => Please classify In simple words.. :) It's my understanding, feel free to correct.
Supervised learning is, we know what we are predicting on the basis of provided data. So we have a column in the dataset which needs to be predicated.
Unsupervised learning is, we try to extract meaning out of the provided dataset. We don't have clarity on what to be predicted. So question is why we do this?.. :) Answer is - the outcome of Unsupervised learning is groups/clusters(similar data together). So if we receive any new data then we associate that with the identified cluster/group and understand it's features. I hope it will help you. supervised learning supervised learning is where we know the output of the raw input, i.e the data is labelled so that during the training of machine learning model it will understand what it need to detect in the give output, and it will guide the system during the training to detect the pre-labelled objects on that basis it will detect the similar objects which we have provided in training. Here the algorithms will know what's the structure and pattern of data. Supervised learning is used for classification  As an example, we can have a different objects whose shapes are square, circle, trianle our task is to arrange the same types of shapes 
the labelled dataset have all the shapes labelled, and we will train the machine learning model on that dataset, on the based of training dateset it will start detecting the shapes.   Un-supervised learning Unsupervised learning is a unguided learning where the end result is not known, it will cluster the dataset and based on similar properties of the object it will divide the objects on different bunches and detect the objects. Here algorithms will search for the different pattern in the raw data, and based on that it will cluster the data. Un-supervised learning is used for clustering. As an example, we can have different objects of multiple shapes square, circle, triangle, so it will make the bunches based on the object properties, if a object has four sides it will consider it square, and if it have three sides triangle and if no sides than circle, here the the data is not labelled, it will learn itself to detect the various shapes Machine learning is a field where you are trying to make machine to mimic the human behavior. You train machine just like a baby.The way humans learn, identify features, recognize patterns and train himself, same way you train machine by feeding data with various features. Machine algorithm identify the pattern within the data and classify it into particular category. Machine learning broadly divided into two category, supervised and unsupervised learning. Supervised learning is the concept where you have input vector / data with corresponding target value (output).On the other hand unsupervised learning is the concept where you only have input vectors / data without any corresponding target value. An example of supervised learning is handwritten digits recognition where you have image of digits with corresponding digit [0-9], and an example of unsupervised learning is grouping customers by purchasing behavior.Want to improve this question? Update the question so it can be answered with facts and citations by editing this post. Closed 4 years ago. Suppose I'm working on some classification problem. (Fraud detection and comment spam are two problems I'm working on right now, but I'm curious about any classification task in general.) How do I know which classifier I should use?  In which cases is one of these the "natural" first choice, and what are the principles for choosing that one? Examples of the type of answers I'm looking for (from Manning et al.'s Introduction to Information Retrieval book): a. If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes). I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data. b. If you have a ton of data, then the classifier doesn't really matter so much, so you should probably just choose a classifier with good scalability. What are other guidelines? Even answers like "if you'll have to explain your model to some upper management person, then maybe you should use a decision tree, since the decision rules are fairly transparent" are good. I care less about implementation/library issues, though. Also, for a somewhat separate question, besides standard Bayesian classifiers, are there 'standard state-of-the-art' methods for comment spam detection (as opposed to email spam)?  First of all, you need to identify your problem. It depends upon what kind of data you have and what your desired task is. If you are Predicting Category : If you are Predicting Quantity : Otherwise There are different algorithms within each approach mentioned above. The choice of a particular algorithm depends upon the size of the dataset. Source: http://scikit-learn.org/stable/tutorial/machine_learning_map/ Model selection using cross validation may be what you need. What you do is simply to split your dataset into k non-overlapping subsets (folds), train a model using k-1 folds and predict its performance using the fold you left out. This you do for each possible combination of folds (first leave 1st fold out, then 2nd, ... , then kth, and train with the remaining folds). After finishing, you estimate the mean performance of all folds (maybe also the variance/standard deviation of the performance). How to choose the parameter k depends on the time you have. Usual values for k are 3, 5, 10 or even N, where N is the size of your data (that's the same as leave-one-out cross validation). I prefer 5 or 10. Let's say you have 5 methods (ANN, SVM, KNN, etc) and 10 parameter combinations for each method (depending on the method). You simply have to run cross validation for each method and parameter combination (5 * 10 = 50) and select the best model, method and parameters. Then you re-train with the best method and parameters on all your data and you have your final model. There are some more things to say. If, for example, you use a lot of methods and parameter combinations for each, it's very likely you will overfit. In cases like these, you have to use nested cross validation. In nested cross validation, you perform cross validation on the model selection algorithm. Again, you first split your data into k folds. After each step, you choose k-1 as your training data and the remaining one as your test data. Then you run model selection (the procedure I explained above) for each possible combination of those k folds. After finishing this, you will have k models, one for each combination of folds. After that, you test each model with the remaining test data and choose the best one. Again, after having the last model you train a new one with the same method and parameters on all the data you have. That's your final model. Of course, there are many variations of these methods and other things I didn't mention. If you need more information about these look for some publications about these topics. The book "OpenCV" has a great two pages on this on pages 462-463. Searching the Amazon preview for the word "discriminative" (probably google books also) will let you see the pages in question. These two pages are the greatest gem I have found in this book. In short: Boosting - often effective when a large amount of training data is available. Random trees - often very effective and can also perform regression. K-nearest neighbors - simplest thing you can do, often effective but slow and requires lots of memory. Neural networks - Slow to train but very fast to run, still optimal performer for letter recognition. SVM - Among the best with limited data, but losing against boosting or random trees only when large data sets are available. Things you might consider in choosing which algorithm to use would include: Do you need to train incrementally (as opposed to batched)? If you need to update your classifier with new data frequently (or you have tons of data), you'll probably want to use Bayesian. Neural nets and SVM need to work on the training data in one go. Is your data composed of categorical only, or numeric only, or both? I think Bayesian works best with categorical/binomial data. Decision trees can't predict numerical values. Does you or your audience need to understand how the classifier works? Use Bayesian or decision trees, since these can be easily explained to most people. Neural networks and SVM are "black boxes" in the sense that you can't really see how they are classifying data. How much classification speed do you need? SVM's are fast when it comes to classifying since they only need to determine which side of the "line" your data is on.  Decision trees can be slow especially when they're complex (e.g. lots of branches). Complexity. Neural nets and SVMs can handle complex non-linear classification. As Prof Andrew Ng often states: always begin by implementing a rough, dirty algorithm, and then iteratively refine it. For classification, Naive Bayes is a good starter, as it has good performances, is highly scalable and can adapt to almost any kind of classification task. Also 1NN (K-Nearest Neighbours with only 1 neighbour) is a no-hassle best fit algorithm (because the data will be the model, and thus you don't have to care about the dimensionality fit of your decision boundary), the only issue is the computation cost (quadratic because you need to compute the distance matrix, so it may not be a good fit for high dimensional data). Another good starter algorithm is the Random Forests (composed of decision trees), this is highly scalable to any number of dimensions and has generally quite acceptable performances. Then finally, there are genetic algorithms, which scale admirably well to any dimension and any data with minimal knowledge of the data itself, with the most minimal and simplest implementation being the microbial genetic algorithm (only one line of C code! by Inman Harvey in 1996), and one of the most complex being CMA-ES and MOGA/e-MOEA. And remember that, often, you can't really know what will work best on your data before you try the algorithms for real. As a side-note, if you want a theoretical framework to test your hypothesis and algorithms theoretical performances for a given problem, you can use the PAC (Probably approximately correct) learning framework (beware: it's very abstract and complex!), but to summary, the gist of PAC learning says that you should use the less complex, but complex enough (complexity being the maximum dimensionality that the algo can fit) algorithm that can fit your data. In other words, use the Occam's razor. Sam Roweis used to say that you should try naive Bayes, logistic regression, k-nearest neighbour and Fisher's linear discriminant before anything else. My take on it is that you always run the basic classifiers first to get some sense of your data. More often than not (in my experience at least) they've been good enough. So, if you have supervised data, train a Naive Bayes classifier. If you have unsupervised data, you can try k-means clustering. Another resource is one of the lecture videos of the series of videos Stanford Machine Learning, which I watched a while back. In video 4 or 5, I think, the lecturer discusses some generally accepted conventions when training classifiers, advantages/tradeoffs, etc. You should always keep into account the inference vs prediction trade-off.  If you want to understand the complex relationship that is occurring in your data then you should go with a rich inference algorithm (e.g. linear regression or lasso). On the other hand, if you are only interested in the result you can go with high dimensional and more complex (but less interpretable) algorithms, like neural networks. Selection of Algorithm is depending upon the scenario and the type and size of data set.
There are many other factors. This is a brief cheat sheet for basic machine learning.I am trying to understand the role of the Flatten function in Keras. Below is my code, which is a simple two-layer network. It takes in 2-dimensional data of shape (3, 2), and outputs 1-dimensional data of shape (1, 4): This prints out that y has shape (1, 4). However, if I remove the Flatten line, then it prints out that y has shape (1, 3, 4). I don't understand this. From my understanding of neural networks, the model.add(Dense(16, input_shape=(3, 2))) function is creating a hidden fully-connected layer, with 16 nodes. Each of these nodes is connected to each of the 3x2 input elements. Therefore, the 16 nodes at the output of this first layer are already "flat". So, the output shape of the first layer should be (1, 16). Then, the second layer takes this as an input, and outputs data of shape (1, 4). So if the output of the first layer is already "flat" and of shape (1, 16), why do I need to further flatten it? If you read the Keras documentation entry for Dense, you will see that this call: would result in a Dense network with 3 inputs and 16 outputs which would be applied independently for each of 5 steps. So, if D(x) transforms 3 dimensional vector to 16-d vector, what you'll get as output from your layer would be a sequence of vectors: [D(x[0,:]), D(x[1,:]),..., D(x[4,:])] with shape (5, 16). In order to have the behavior you specify you may first Flatten your input to a 15-d vector and then apply Dense: EDIT:
As some people struggled to understand - here you have an explaining image:  
This is how Flatten works converting Matrix to single array. short read: Flattening a tensor means to remove all of the dimensions except for one. This is exactly what the Flatten layer does. long read: If we take the original model (with the Flatten layer) created in consideration we can get the following model summary: For this summary the next image will hopefully provide little more sense on the input and output sizes for each layer. The output shape for the Flatten layer as you can read is (None, 48). Here is the tip. You should read it (1, 48) or (2, 48) or ... or (16, 48) ... or (32, 48), ... In fact, None on that position means any batch size. For the inputs to recall, the first dimension means the batch size and the second means the number of input features. The role of the Flatten layer in Keras is super simple: A flatten operation on a tensor reshapes the tensor to have the shape that is equal to the number of elements contained in tensor non including the batch dimension.  Note: I used the model.summary() method to provide the output shape and parameter details. I came across this recently, it certainly helped me understand: https://www.cs.ryerson.ca/~aharley/vis/conv/ So there's an input, a Conv2D, MaxPooling2D etc, the Flatten layers are at the end and show exactly how they are formed and how they go on to define the final classifications (0-9). It is rule of thumb that the first layer in your network should be the same shape as your data. For example our data is 28x28 images, and 28 layers of 28 neurons would be infeasible, so it makes more sense to 'flatten' that 28,28 into a 784x1. Instead of wriitng all the code to handle that ourselves, we add the Flatten() layer at the begining, and when the arrays are loaded into the model later, they'll automatically be flattened for us. Flatten make explicit how you serialize a multidimensional tensor (tipically the input one). This allows the mapping between the (flattened) input tensor and the first hidden layer. If the first hidden layer is "dense" each element of the (serialized) input tensor will be connected with each element of the hidden array.
If you do not use Flatten, the way the input tensor is mapped onto the first hidden layer would be ambiguous. Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. In some architectures, e.g. CNN an image is better processed by a neural network if it is in 1D form rather than 2D.  Here I would like to present another alternative to Flatten function. This may help to understand what is going on internally. The alternative method adds three more code lines.
Instead of using we can use In the second case, we first create a tensor (using a placeholder)
and then create an Input layer. After, we reshape the tensor to flat form. So basically, Flatten is a convenient function, doing all this automatically. Of course both ways has its specific use cases. Keras provides enough flexibility to manipulate the way you want to create a model. Keras flatten class is very important when you have to deal with multi-dimensional inputs such as image datasets. Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension, so you can model your input layer and build your neural network model, then pass those data into every single neuron of the model effectively. You can understand this easily with the fashion MNIST dataset. The images in this dataset are 28 * 28 pixels. Hence if you print the first image in python you can see a multi-dimensional array, which we really can't feed into the input layer of our Deep Neural Network. first image of fashion MNIST To tackle this problem we can flatten the image data when feeding it into a neural network. We can do this by turning this multidimensional tensor into a one-dimensional array. In this flattened array now we have 784 elements (28 * 28). Then we can create out input layer with 784 neurons to handle each element of the incoming data. We can do this all by using a single line of code, sort of... As the name suggests it just flattens out the input Tensor. A very good visual to understand this is given below.
Please let me know if there is any confusion.
Flatten Input TensorI'm working in a sentiment analysis problem the data looks like this: So my data is unbalanced since 1190 instances are labeled with 5. For the classification Im using scikit's SVC. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches: First: Second: Third: However, Im getting warnings like this: How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics? I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;). The weights from the class_weight parameter are used to train the classifier.
They are not used in the calculation of any of the metrics you are using: with different class weights, the numbers will be different simply because the classifier is different. Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.
How they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it. Once you have a classifier, you want to know how well it is performing.
Here you can use the metrics you mentioned: accuracy, recall_score, f1_score... Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class. I will not detail all these metrics but note that, with the exception of accuracy, they are naturally applied at the class level: as you can see in this print of a classification report they are defined for each class. They rely on concepts such as true positives or false negative that require defining which class is the positive one. You get this warning because you are using the f1-score, recall and precision without defining how they should be computed!
The question could be rephrased: from the above classification report, how do you output one global number for the f1-score?
You could: These are 3 of the options in scikit-learn, the warning is there to say you have to pick one. So you have to specify an average argument for the score method. Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5. The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it. Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen.
This is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant. Here's a way to do it using StratifiedShuffleSplit, which gives you a random splits of your data (after shuffling) that preserve the label distribution. Lot of very detailed answers here but I don't think you are answering the right questions. As I understand the question, there are two concerns: You can use most of the scoring functions in scikit-learn with both multiclass problem as with single class problems. Ex.: This way you end up with tangible and interpretable numbers for each of the classes. Then... ... you can tell if the unbalanced data is even a problem. If the scoring for the less represented classes (class 1 and 2) are lower than for the classes with more training samples (class 4 and 5) then you know that the unbalanced data is in fact a problem, and you can act accordingly, as described in some of the other answers in this thread.
However, if the same class distribution is present in the data you want to predict on, your unbalanced training data is a good representative of the data, and hence, the unbalance is a good thing. Posed question Responding to the question 'what metric should be used for multi-class classification with imbalanced data': Macro-F1-measure. 
Macro Precision and Macro Recall can be also used, but they are not so easily interpretable as for binary classificaion, they are already incorporated into F-measure, and excess metrics complicate methods comparison, parameters tuning, and so on.  Micro averaging are sensitive to class imbalance: if your method, for example, works good for the most common labels and totally messes others, micro-averaged metrics show good results. Weighting averaging isn't well suited for imbalanced data, because it weights by counts of labels. Moreover, it is too hardly interpretable and unpopular: for instance, there is no mention of such an averaging in the following very detailed survey I strongly recommend to look through: Sokolova, Marina, and Guy Lapalme. "A systematic analysis of
  performance measures for classification tasks." Information Processing
  & Management 45.4 (2009): 427-437. Application-specific question However, returning to your task, I'd research 2 topics: Commonly used metrics.
As I can infer after looking through literature, there are 2 main evaluation metrics: Yu, April, and Daryl Chang. "Multiclass Sentiment Prediction using
  Yelp Business." (link) - note that the authors work with almost the same distribution of ratings, see Figure 5. Pang, Bo, and Lillian Lee. "Seeing stars: Exploiting class
  relationships for sentiment categorization with respect to rating
  scales." Proceedings of the 43rd Annual Meeting on Association for
  Computational Linguistics. Association for Computational Linguistics,
  2005. (link) Lee, Moontae, and R. Grafe. "Multiclass sentiment analysis with
  restaurant reviews." Final Projects from CS N 224 (2010). (link) - they explore both accuracy and MSE, considering the latter to be better Pappas, Nikolaos, Rue Marconi, and Andrei Popescu-Belis. "Explaining
  the Stars: Weighted Multiple-Instance Learning for Aspect-Based
  Sentiment Analysis." Proceedings of the 2014 Conference on Empirical
  Methods In Natural Language Processing. No. EPFL-CONF-200899. 2014. (link) - they utilize scikit-learn for evaluation and baseline approaches and state that their code is available; however, I can't find it, so if you need it, write a letter to the authors, the work is pretty new and seems to be written in Python. Cost of different errors.
If you care more about avoiding gross blunders, e.g. assinging 1-star to 5-star review or something like that, look at MSE; 
if difference matters, but not so much, try MAE, since it doesn't square diff; 
otherwise stay with Accuracy. About approaches, not metrics Try regression approaches, e.g. SVR, since they generally outperforms Multiclass classifiers like SVC or OVA SVM. First of all it's a little bit harder using just counting analysis to tell if your data is unbalanced or not. For example: 1 in 1000 positive observation is just a noise, error or a breakthrough in science? You never know.
So it's always better to use all your available knowledge and choice its status with all wise. Okay, what if it's really unbalanced?
Once again — look to your data. Sometimes you can find one or two observation multiplied by hundred times. Sometimes it's useful to create this fake one-class-observations.
If all the data is clean next step is to use class weights in prediction model. So what about multiclass metrics?
In my experience none of your metrics is usually used. There are two main reasons.
First: it's always better to work with probabilities than with solid prediction (because how else could you separate models with 0.9 and 0.6 prediction if they both give you the same class?)
And second: it's much easier to compare your prediction models and build new ones depending on only one good metric.
From my experience I could recommend logloss or MSE (or just mean squared error). How to fix sklearn warnings?
Just simply (as yangjie noticed) overwrite average parameter with one of these 
values: 'micro' (calculate metrics globally), 'macro' (calculate metrics for each label) or 'weighted' (same as macro but with auto weights). All your Warnings came after calling metrics functions with default average value 'binary' which is inappropriate for multiclass prediction.
Good luck and have fun with machine learning! Edit:
I found another answerer recommendation to switch to regression approaches (e.g. SVR) with which I cannot agree. As far as I remember there is no even such a thing as multiclass regression. Yes there is multilabel regression which is far different and yes it's possible in some cases switch between regression and classification (if classes somehow sorted) but it pretty rare. What I would recommend (in scope of scikit-learn) is to try another very powerful classification tools: gradient boosting, random forest (my favorite), KNeighbors and many more. After that you can calculate arithmetic or geometric mean between predictions and most of the time you'll get even better result.In the iOS email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. It not only works for emails in English, but in other languages also. I love this feature and would like to understand how they do it.  The naive way to do this would be to have many regular expressions and run them all. However I  this is not going to scale very well and will work for only a specific language or date format, etc. I think that Apple must be using some concept of machine learning to extract entities (8:00PM, 8PM, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.). Any idea how Apple is able to extract entities so quickly in its email client? What machine learning algorithm would you to apply accomplish such task?  They likely use Information Extraction techniques for this. Here is a demo of Stanford's SUTime tool: http://nlp.stanford.edu:8080/sutime/process You would extract attributes about n-grams (consecutive words) in a document: And then use a classification algorithm, and feed it positive and negative examples: You might get away with 50 examples of each, but the more the merrier. Then, the algorithm learns based on those examples, and can apply to future examples that it hasn't seen before. It might learn rules such as  Here is a decent video by a Google engineer on the subject That's a technology Apple actually developed a very long time ago called Apple Data Detectors. You can read more about it here: http://www.miramontes.com/writing/add-cacm/ Essentially it parses the text and detects patterns that represent specific pieces of data, then applies OS-contextual actions to it. It's neat. This is called temporal expression identification and parsing.  Here are some Google searches to get you started:  https://www.google.com/#hl=en&safe=off&sclient=psy-ab&q=timebank+timeml+timex https://www.google.com/#hl=en&safe=off&sclient=psy-ab&q=temporal+expression+tagger One part of the puzzle could be the NSDataDetector class. Its used to recognize some standard types like phone numbers. I once wrote a parser to do this, using pyparsing. It's really very simple, you just need to get all the different ways right, but there aren't that many. It only took a few hours and was pretty fast. Apple has a patent on how they did it System and method for performing an action on a structure in computer data, and here's a story on this patent apples-patent-on-nsdatadetectorIn Keras, we can return the output of model.fit to a history as follows: Now, how to save the history attribute of the history object to a file for further uses (e.g. draw plots of acc or loss against epochs)? What I use is the following: In this way I save the history as a dictionary in case I want to plot the loss or accuracy later on. Later, when you want to load the history again, you can use: The comment under this answer accurately states: [Storing the history as json] does not work anymore in tensorflow keras. I had issues with: TypeError: Object of type 'float32' is not JSON serializable. There are ways to tell json how to encode numpy objects, which you can learn about from this other question, so there's nothing wrong with using json in this case, it's just more complicated than simply dumping to a pickle file. As history.history is a dict, you can convert it as well to a pandas DataFrame object, which can then be saved to suit your needs. Step by step: The easiest way: Saving: Loading: Then history is a dictionary and you can retrieve all desirable values using the keys. The model history can be saved into a file as follows A history objects has a history field is a dictionary which helds different training metrics spanned across every training epoch. So e.g. history.history['loss'][99] will return a loss of your model in a 100th epoch of training. In order to save that you could pickle this dictionary or simple save different lists from this dictionary to appropriate file. I came across the problem that the values inside of the list in keras are not json seriazable. Therefore I wrote this two handy functions for my use cause. where saveHist just needs to get the path to where the json file should be saved, and the history object returned from the keras fit or fit_generator method. I'm sure there are many ways to do this, but I fiddled around and came up with a version of my own. First, a custom callback enables grabbing and updating the history at the end of every epoch. In there I also have a callback to save the model. Both of these are handy because if you crash, or shutdown, you can pick up training at the last completed epoch. Second, here are some 'helper' functions to do exactly the things that they say they do. These are all called from the LossHistory() callback. After that, all you need is to set history_filename to something like data/model-history.json, as well as set model_filename to something like data/model.h5. One final tweak to make sure not to mess up your history at the end of training, assuming you stop and start, as well as stick in the callbacks, is to do this: Whenever you want, history = loadHist(history_filename) gets your history back. The funkiness comes from the json and the lists but I wasn't able to get it to work without converting it by iterating. Anyway, I know that this works because I've been cranking on it for days now. The pickle.dump answer at https://stackoverflow.com/a/44674337/852795 might be better, but I don't know what that is. If I missed anything here or you can't get it to work, let me know. You can save History attribute of tf.keras.callbacks.History in .txt form The above answers are useful when saving history at the end of the training process. If you want to save the history during the training, the CSVLogger callback will be helpful.  Below code saves the model weight and history training in form of a datasheet file log.csv. Here is a callback that pickles the logs into a file.  Provide the model file path when instantiating the callback obj; this will create an associated file - given model path '/home/user/model.h5', the pickled path '/home/user/model_history_pickle'.  Upon reloading the model, the callback will continue from the epoch that it left off at.I am trying to grasp what TimeDistributed wrapper does in Keras. I get that TimeDistributed "applies a layer to every temporal slice of an input." But I did some experiment and got the results that I cannot understand. In short, in connection to LSTM layer, TimeDistributed and just Dense layer bear same results. For both models, I got output shape of (None, 10, 1). Can anyone explain the difference between TimeDistributed and Dense layer after an RNN layer? In keras - while building a sequential model - usually the second dimension (one after sample dimension) - is related to a time dimension. This means that if for example, your data is 5-dim with (sample, time, width, length, channel) you could apply a convolutional layer using TimeDistributed (which is applicable to 4-dim with (sample, width, length, channel)) along a time dimension (applying the same layer to each time slice) in order to obtain 5-d output. The case with Dense is that in keras from version 2.0 Dense is by default applied to only last dimension (e.g. if you apply Dense(10) to input with shape (n, m, o, p) you'll get output with shape (n, m, o, 10)) so in your case Dense and TimeDistributed(Dense) are equivalent.I want to make a simple neural network which uses the ReLU function. Can someone give me a clue of how can I implement the function using numpy. There are a couple of ways. If timing the results with the following code: We get: So the multiplication seems to be the fastest. You can do it in much easier way: I'm completely revising my original answer because of points raised in the other questions and comments. Here is the new benchmark script: It takes care to use a different ndarray for each implementation and iteration. Here are the results: EDIT As  jirassimok has mentioned below my function will change the data in place, after that it runs a lot faster in timeit. This causes the good results. It's some kind of cheating. Sorry for your inconvenience. I found a faster method for ReLU with numpy. You can use the fancy index feature of numpy as well. fancy index: 20.3 ms ± 272 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) Here is my benchmark: Richard Möhn's comparison  is not fair.
As Andrea Di Biagio's comment, the in-place method np.maximum(x, 0, x) will modify x at the first loop.
So here is my benchmark:   Timing it:   Get the results:   In-place maximum method is only a bit faster than the maximum method, and it may because it omits the variable assignment for 'out'. And it's still slower than the multiplication method.
And since you're implementing the ReLU func. You may have to save the 'x' for backprop through relu. E.g.:   So i recommend you to use multiplication method. numpy didn't have the function of relu, but you define it by yourself as follow: for example: If we have 3 parameters (t0, a0, a1) for Relu, that is we want to implement We can use the following code: X there is a matrix. ReLU(x) also is equal to (x+abs(x))/2   For a single neuron Where net is the net activity at the neuron's input(net=dot(w,x)), where dot() is the dot product of w and x (weight vector and input vector respectively). dot() is a function defined in numpy package in Python. For neurons in a layer with net vector This is more precise implementation:Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 2 years ago. The community reviewed whether to reopen this question 9 months ago and left it closed: Original close reason(s) were not resolved From the XGBoost guide: After training, the model can be saved. The model and its feature map can also be dumped to a text file. A saved model can be loaded as follows: My questions are following. Here is how I solved the problem: Both functions save_model and dump_model save the model, the difference is that in dump_model you can save feature name and save tree in text format. The load_model will work with model from save_model. The model from dump_model can be used  for example with xgbfi. During loading the model, you need to specify the path where your models is saved. In the example bst.load_model("model.bin") model is loaded from file model.bin - it is just a name of file with model. Good luck! EDIT: From Xgboost documentation (for version 1.3.3), the dump_model() should be used for saving the model for further interpretation. For saving and loading the model the save_model() and load_model() should be used. Please check the docs for more details. There is also a difference between Learning API and Scikit-Learn API of Xgboost. The latter saves the best_ntree_limit variable which is set during the training with early stopping. You can read details in my article How to save and load Xgboost in Python? The save_model() method recognize the format of the file name, if *.json is specified, then model is saved in JSON, otherwise it is text file. Don't use pickle or joblib as that may introduces dependencies on xgboost version. The canonical way to save and restore models is by load_model and save_model. If you’d like to store or archive your model for long-term storage, use save_model (Python) and xgb.save (R). This is the relevant documentation for the latest versions of XGBoost. It also explains the difference between dump_model and save_model. Note that you can serialize/de-serialize your models as json by specifying json as the extension when using bst.save_model. If the speed of saving and restoring the model is not important for you, this is very convenient, as it allows you to do proper version control of the model since it's a simple text file. An easy way of saving and loading a xgboost model is with joblib library. If you are using the sklearn api you can use the following: If you used the above booster method for loading, you will get the xgboost booster within the python api not the sklearn booster in the sklearn api. So yeah, this seems to be the most pythonic way to load in a saved xgboost model data if you are using the sklearn api.I am trying to do a transfer learning; for that purpose I want to remove the last two layers of the neural network and add another two layers. This is an example code which also output the same error. I removed the layer using pop() but when I tried to add its outputting this error I know the most probable reason for the error is improper use of model.add(). what other syntax should I use? EDIT: I tried to remove/add layers in keras but its not  allowing it to be added after loading external weights. its showing this error You can take the output of the last model and create a new model. The lower layers remains the same. Check How to use models from keras.applications for transfer learnig? Update on Edit: The new error is because you are trying to create the new model on global in_img which is actually not used in the previous model creation.. there you are actually defining a local in_img. So the global in_img is obviously not connected to the upper layers in the symbolic graph. And it has nothing to do with loading weights. To better resolve this problem you should instead use model.input to reference to the input. Another way to do it As of Keras 2.3.1 and TensorFlow 2.0, model.layers.pop() is not working as intended (see issue here). They suggested two options to do this. One option is to recreate the model and copy the layers. For instance, if you want to remove the last layer and add another one, you can do: Another option is to use the functional model: model.layers[-1].output means the last layer's output which is the final output, so in your code, you actually didn't remove any layers, you added another head/path. An alternative to Wesam Na's answer, if you don't know the layer names you can simply cut off the last layer via:I'm using scikit-learn in Python to develop a classification algorithm to predict the gender of certain customers. Amongst others, I want to use the Naive Bayes classifier but my problem is that I have a mix of categorical data (ex: "Registered online", "Accepts email notifications" etc) and continuous data (ex: "Age", "Length of membership" etc). I haven't used scikit much before but I suppose that that Gaussian Naive Bayes is suitable for continuous data and that Bernoulli Naive Bayes can be used for categorical data. However, since I want to have both categorical and continuous data in my model, I don't really know how to handle this. Any ideas would be much appreciated! You have at least two options: Transform all your data into a categorical representation by computing percentiles for each continuous variables and then binning the continuous variables using the percentiles as bin boundaries. For instance for the height of a person create the following bins: "very small", "small", "regular", "big", "very big" ensuring that each bin contains approximately 20% of the population of your training set. We don't have any utility to perform this automatically in scikit-learn but it should not be too complicated to do it yourself. Then fit a unique multinomial NB on those categorical representation of your data. Independently fit a gaussian NB model on the continuous part of the data and a multinomial NB model on the categorical part. Then transform all the dataset by taking the class assignment probabilities (with predict_proba method) as new features: np.hstack((multinomial_probas, gaussian_probas)) and then refit a new model (e.g. a new gaussian NB) on the new features. Hope I'm not too late. I recently wrote a library called Mixed Naive Bayes, written in NumPy. It can assume a mix of Gaussian and categorical (multinoulli) distributions on the training data features. https://github.com/remykarem/mixed-naive-bayes The  library is written such that the APIs are similar to scikit-learn's. In the example below, let's assume that the first 2 features are from a categorical distribution and the last 2 are Gaussian. In the fit() method, just specify categorical_features=[0,1], indicating that Columns 0 and 1 are to follow categorical distribution. Pip installable via pip install mixed-naive-bayes. More information on the usage in the README.md file. Pull requests are greatly appreciated :) The simple answer: multiply result!! it's the same. Naive Bayes based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features - meaning you calculate the Bayes probability dependent on a specific feature without holding the others - which means that the algorithm multiply each probability from one feature with the probability from the second feature (and we totally ignore the denominator - since it is just a normalizer). so the right answer is: @Yaron's approach needs an extra step (4. below): Step 4. is the normalization step. Take a look at @remykarem's mixed-naive-bayes as an example (lines 268-278): The probabilities of the Gaussian and Categorical models (t and p respectively) are multiplied together in line 269 (line 2 in extract above) and then normalized as in 4. in line 275 (fourth line from the bottom in extract above). For hybrid features, you can check this implementation. The author has presented mathematical justification in his Quora answer, you might want to check. You will need the following steps: It should be easy enough to see how you can add your own prior instead of using those learned from the data.Regression algorithms seem to be working on features represented as numbers. 
For example:  This data set doesn't contain categorical features/variables. It's quite clear how to do regression on this data and predict price. But now I want to do a regression analysis on data that contain categorical features:  There are 5 features: District, Condition, Material, Security, Type How can I do a regression on this data? Do I have to transform all the string/categorical data to numbers manually? I mean if I have to create some encoding rules and according to that rules transform all data to numeric values.  Is there any simple way to transform string data to numbers without having to create my own encoding rules manually? Maybe there are some libraries in Python that can be used for that? Are there some risks that the regression model will be somehow incorrect due to "bad encoding"? Yes, you will have to convert everything to numbers. That requires thinking about what these attributes represent. Usually there are three possibilities: You have to be carefull to not infuse information you do not have in the application case. If you have categorical data, you can create dummy variables with 0/1 values for each possible value. E. g. to This can easily be done with pandas: will result in: Create a mapping of your sortable categories,  e. g.
old < renovated < new → 0, 1, 2 This is also possible with pandas: Result: You could use the mean for each category over past (known events). Say you have a DataFrame with the last known mean prices for cities: Result: In linear regression with categorical variables you should be careful of the Dummy Variable Trap. The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others. This can produce singularity of a model, meaning your model just won't work. Read about it here Idea is to use dummy variable encoding with drop_first=True, this will omit one column from each category after converting categorical variable into dummy/indicator variables. You WILL NOT lose any relevant information by doing that simply because your all point in dataset can fully be explained by rest of the features.  Here is complete code on how you can do it for your housing dataset So you have categorical features:  And one numerical features that you are trying to predict: First you need to split your initial dataset on input variables and prediction, assuming its pandas dataframe it would look like this: Input variables: Prediction: Convert categorical variable into dummy/indicator variables and drop one in each category: So now if you check shape of X with drop_first=True you will see that it has 4 columns less - one for each of your categorical variables.  You can now continue to use them in your linear model. For scikit-learn implementation it could look like this: You can use "Dummy Coding" in this case.
There are Python libraries to do dummy coding, you have a few options: An example with pandas is below: One way to achieve regression with categorical  variables as independent variables  is as mentioned above - Using encoding. 
Another way of  doing is by using R like statistical formula using statmodels library. Here is a code  snippet Dataset Summary of regressionDoes tensorflow have something similar to scikit learn's one hot encoder for processing categorical data?  Would using a placeholder of tf.string behave as categorical data? I realize I can manually pre-process the data before sending it to tensorflow, but having it built in is very convenient. As of TensorFlow 0.8, there is now a native one-hot op, tf.one_hot that can convert a set of sparse labels to a dense one-hot representation.  This is in addition to tf.nn.sparse_softmax_cross_entropy_with_logits, which can in some cases let you compute the cross entropy directly on the sparse labels instead of converting them to one-hot. Previous answer, in case you want to do it the old way:
@Salvador's answer is correct - there (used to be) no native op to do it.  Instead of doing it in numpy, though, you can do it natively in tensorflow using the sparse-to-dense operators: The output, labels, is a one-hot matrix of batch_size x num_labels. Note also that as of 2016-02-12 (which I assume will eventually be part of a 0.7 release), TensorFlow also has the tf.nn.sparse_softmax_cross_entropy_with_logits op, which in some cases can let you do training without needing to convert to a one-hot encoding. Edited to add:  At the end, you may need to explicitly set the shape of labels.  The shape inference doesn't recognize the size of the num_labels component.  If you don't need a dynamic batch size with derived_size, this can be simplified. Edited 2016-02-12 to change the assignment of outshape per comment below. tf.one_hot() is available in TF and easy to use.  Lets assume you have 4 possible categories (cat, dog, bird, human) and 2 instances (cat, human). So your depth=4 and your indices=[0, 3] Keep in mind that if you provide index=-1 you will get all zeros in your one-hot vector. Old answer, when this function was not available. After looking though the python documentation, I have not found anything similar. One thing that strengthen my belief that it does not exist is that in their own example they write one_hot manually. You can also do this in scikitlearn. numpy does it! A simple and short way to one-hot encode any integer or list of intergers: Recent versions of TensorFlow (nightlies and maybe even 0.7.1) have an op called tf.one_hot that does what you want.  Check it out! On the other hand if you have a dense matrix and you want to look up and aggregate values in it, you would want to use the embedding_lookup function. Maybe it's due to changes to Tensorflow since Nov 2015, but @dga's answer produced errors. I did get it to work with the following modifications: Take a look at tf.nn.embedding_lookup. It maps from categorical IDs to their embeddings.  For an example of how it's used for input data, see here. You can use tf.sparse_to_dense: The sparse_indices argument indicates where the ones should go, output_shape should be set to the number of possible outputs (e.g. the number of labels), and sparse_values should be 1 with the desired type (it will determine the type of the output from the type of sparse_values). There's embedding_ops in Scikit Flow and examples that deal with categorical variables, etc.  If you just begin to learn TensorFlow, I would suggest you trying out examples in TensorFlow/skflow first and then once you are more familiar with TensorFlow it would be fairly easy for you to insert TensorFlow code to build a custom model you want (there are also examples for this).  Hope those examples for images and text understanding could get you started and let us know if you encounter any issues! (post issues or tag skflow in SO).  Current versions of tensorflow implement the following function for creating one-hot tensors: https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#one_hot As mentioned above by @dga, Tensorflow has tf.one_hot now: You need to specify depth, otherwise you'll get a pruned one-hot tensor. If you like to do it manually: Note arguments order in tf.concat() There are  a couple ways to do it. The other way to do it is. My version of @CFB and @dga example, shortened a bit to ease understanding. works on TF version 1.3.0. As of Sep 2017.  Tensorflow 2.0 Compatible Answer: You can do it efficiently using Tensorflow Transform.  Code for performing One-Hot Encoding using Tensorflow Transform is shown below: For more information, refer this Tutorial on TF_Transform.I have a binary prediction model trained by logistic regression algorithm. I want know which features(predictors) are more important for the decision of positive or negative class. I know there is coef_ parameter comes from the scikit-learn package, but I don't know whether it is enough to for the importance. Another thing is how I can evaluate the coef_ values in terms of the importance for negative and positive classes. I also read about standardized regression coefficients and I don't know what it is. Lets say there are features like size of tumor, weight of tumor, and etc to make a decision for a test case like malignant or not malignant. I want to know which of the features are more important for malignant and not malignant prediction. Does it make sort of sense? One of the simplest options to get a feeling for the "influence" of a given parameter in a linear classification model (logistic being one of those), is to consider the magnitude of its coefficient times the standard deviation of the corresponding parameter in the data. Consider this example: An alternative way to get a similar result is to examine the coefficients of the model fit on standardized parameters: Note that this is the most basic approach and a number of other techniques for finding feature importance or parameter influence exist (using p-values, bootstrap scores, various "discriminative indices", etc).  I am pretty sure you would get more interesting answers at https://stats.stackexchange.com/.I'm using TfidfVectorizer from scikit-learn to do some feature extraction from text data. I have a CSV file with a Score (can be +1 or -1) and a Review (text). I pulled this data into a DataFrame so I can run the Vectorizer. This is my code:  This is the traceback for the error I get:  I checked the CSV file and DataFrame for anything that's being read as NaN but I can't find anything. There are 18000 rows, none of which return isnan as True.  This is what df['Review'].head() looks like:  You need to convert the dtype object to unicode string as is clearly mentioned in the traceback. From the Doc page of TFIDF Vectorizer: fit_transform(raw_documents, y=None)  Parameters:     raw_documents : iterable 
an iterable which yields either str, unicode or file objects I find a more efficient way to solve this problem. Of course you can use df['Review'].values.astype('U') to convert the entire Series. But I found using this function will consume much more memory if the Series you want to convert is really big. (I test this with a Series with 800k rows of data, and doing this astype('U') will consume about 96GB of memory) Instead, if you use the lambda expression to only convert the data in the Series from str to numpy.str_, which the result will also be accepted by the fit_transform function, this will be faster and will not increase the memory usage. I'm not sure why this will work because in the Doc page of TFIDF Vectorizer: fit_transform(raw_documents, y=None) Parameters: raw_documents : iterable an iterable which yields either str, unicode or file objects But actually this iterable must yields np.str_ instead of str. I was getting MemoryError even after using .values.astype('U') for the reviews in my dataset.  So i tried .astype('U').values and it worked.  This is a answer from: Python: how to avoid MemoryError when transform text data into Unicode using astype('U')Cross entropy formula:  But why does the following give loss = 0.7437 instead of loss = 0 (since 1*log(1) = 0)? In your example you are treating output [0, 0, 0, 1] as probabilities as required by the mathematical definition of cross entropy.  But PyTorch treats them as outputs, that don’t need to sum to 1, and need to be first converted into probabilities for which it uses the softmax function. So H(p, q) becomes: Translating the output [0, 0, 0, 1] into probabilities: whence: Your understanding is correct but pytorch doesn't compute cross entropy in that way. Pytorch uses the following formula. Since, in your scenario, x = [0, 0, 0, 1] and class = 3, if you evaluate the above expression, you would get: Pytorch considers natural logarithm. I would like to add an important note, as this often leads to confusion. Softmax is not a loss function, nor is it really an activation function. It has a very specific task: It is used for multi-class classification to normalize the scores for the given classes. By doing so we get probabilities for each class that sum up to 1. Softmax is combined with Cross-Entropy-Loss to calculate the loss of a model. Unfortunately, because this combination is so common, it is often abbreviated. Some are using the term Softmax-Loss, whereas PyTorch calls it only Cross-Entropy-Loss. The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using
nn.CrossEntropyLoss. This terminology is a particularity of PyTorch, as the
nn.NLLoss [sic] computes, in fact, the cross entropy but with log probability predictions as inputs where nn.CrossEntropyLoss takes scores (sometimes called logits). Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs. PyTorch's CrossEntropyLoss expects unbounded scores (interpretable as logits / log-odds) as input, not probabilities (as the CE is traditionally defined).When you run a Keras neural network model you might see something like this in the console:  As time goes on the loss hopefully improves. I want to log these losses to a file over time so that I can learn from them. I have tried:  but this doesn't work. I am not sure what level of logging I need in this situation.  I have also tried using a callback like in:  but obviously this isn't writing to a file. Whatever the method, through a callback or the logging module or anything else, I would love to hear your solutions for logging loss of a keras neural network to a file. Thanks!  You can use CSVLogger callback. as example: Look at: Keras Callbacks There is a simple solution to your problem. Every time any of the fit methods are used - as a result the special callback called History Callback is returned. It has a field history which is a dictionary of all metrics registered after every epoch. So to get list of loss function values after every epoch you can easly do: It's easy to save such list to a file (e.g. by converting it to numpy array and using savetxt method). UPDATE: Try: UPDATE 2: The solution to the problem of recording a loss after every batch is written in Keras Callbacks Documentation in a Create a Callback paragraph. Old question, but here goes. Keras history output perfectly matches pandas DataSet input. If you want the entire history to csv in one line:

pandas.DataFrame(model.fit(...).history).to_csv("history.csv")
 Cheers You can redirect the sys.stdout object to a file before the model.fit method and reassign it to the standard console after model.fit method as follows: So In TensorFlow 2.0, it is quite easy to get Loss and Accuracy of each epoch because it returns a History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values  If you have validation Data If you don't have validation Data Then to save list data into text file use the below code Best is to create a LambdaCallback: Now,Just add it like this in the model.fit function:The problem is that my train data could not be placed into RAM due to train data size. So I need a method which first builds one tree on whole train data set, calculate residuals build another tree and so on (like gradient boosted tree do). Obviously if I call model = xgb.train(param, batch_dtrain, 2) in some loop - it will not help, because in such case it just rebuilds whole model for each batch. Try saving your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model. Here's a small experiment that I ran to convince myself that it works: First, split the boston dataset into training and testing sets.
Then split the training set into halves.
Fit a model with the first half and get a score that will serve as a benchmark.
Then fit two models with the second half; one model will have the additional parameter xgb_model. If passing in the extra parameter didn't make a difference, then we would expect their scores to be similar..
But, fortunately, the new model seems to perform much better than the first. reference: https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py There is now (version 0.6?) a process_update parameter that might help.  Here's an experiment with it: Output: I created a gist of jupyter notebook to demonstrate that xgboost model can be trained incrementally. I used boston dataset to train the model. I did 3 experiments - one shot learning, iterative one shot learning, iterative incremental learning. In incremental training, I passed the boston data to the model in batches of size 50. The gist of the gist is that you'll have to iterate over the data multiple times for the model to converge to the accuracy attained by one shot (all data) learning. Here is the corresponding code for doing iterative incremental learning with xgboost. XGBoost version: 0.6 looks like you don't need anything other than call your xgb.train(....) again but provide the model result from the previous batch: this is based on https://xgboost.readthedocs.io/en/latest/python/python_api.html
  If your problem is regarding the dataset size and you do not really need Incremental Learning (you are not dealing with an Streaming app, for instance), then you should check out Spark or Flink.  This two frameworks can train on very large datasets with a small RAM, leveraging disk memory. Both framework deal with memory issues internally. While Flink had it solved first, Spark has caught up in recent releases. Take a look at: To paulperry's code, If change one line from "train_split = round(len(train_idx) / 2)" to "train_split = len(train_idx) - 50". model 1+update2 will changed from 14.2816257268 to 45.60806270012028. And a lot of "leaf=0" result in dump file. Updated model is not good when update sample set is relative small.
For binary:logistic, updated model is unusable when update sample set has only one class. One possible solution that I have not tested is to used a dask dataframe which  should act the same as a pandas dataframe but (I assume) utilize disk and reads in and out of RAM. here are some helpful links.
this link mentions how to use it with xgboost also see
also see.
further there is an experimental options from XGBoost as well here but it is "not ready for production" I agree with @desertnaut in his solution. I have a dataset where I split it into 4 batches.  I have to do an initial fit without the xgb_model parameter first, then the next fits will have the  xgb_model parameter, like in this (I'm using the Sklearn API): It's not based on xgboost, but there is a C++ incremental decision tree.
see gaenari. Continuous chunking data can be inserted and updated, and rebuilds can be run if concept drift reduces accuracy.I'm trying to learn scikit-learn and Machine Learning by using the Boston Housing Data Set. Based on this new model clf_sgd, I am trying to predict the y based on the first instance of X_train. However, the result is quite odd for me (1.34032174, instead of 20-30, the range of the price of the houses) I guess that this 1.34032174 value should be scaled back, but I am trying to figure out how to do it with no success. Any tip is welcome. Thank you very much. You can use inverse_transform using your scalery object: Bit late to the game: 
Just don't scale your y. With scaling y you actually loose your units. The regression or loss optimization is actually determined by the relative differences between the features. BTW for house prices (or any other monetary value) it is common practice to take the logarithm. Then you obviously need to do an numpy.exp() to get back to the actual dollars/euros/yens...I am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools. Additionally, I will be grateful if you point any Python based solution / library for this. Thanks One way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document. To identify keywords like this, you could use the point-wise mutual information of the keyword and the document. This is given by PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection. To identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score.  If you want to extract multiword tags, see the StackOverflow question How to extract common / significant phrases from a series of text entries.  Borrowing from my answer to that question, the NLTK collocations how-to covers how to do 
extract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.: First, the key python library for computational linguistics is NLTK ("Natural Language Toolkit"). This is a stable, mature library created and maintained by professional computational linguists. It also has an extensive collection of tutorials, FAQs, etc. I recommend it highly. Below is a simple template, in python code, for the problem raised in your Question; although it's a template it runs--supply any text as a string (as i've done) and it will return a list of word frequencies as well as a ranked list of those words in order of 'importance' (or suitability as keywords) according to a very simple heuristic. Keywords for a given document are (obviously) chosen from among important words in a document--ie, those words that are likely to distinguish it from another document. If you had no a priori knowledge of the text's subject matter, a common technique is to infer the importance or weight of a given word/term from its frequency, or importance = 1/frequency. http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation tries to represent each document in a training corpus as mixture of topics, which in turn are distributions mapping words to probabilities.   I had used it once to dissect a corpus of product reviews into the latent ideas that were being spoken about across all the documents such as 'customer service', 'product usability', etc.. The basic model does not advocate a way to convert the topic models into a single word describing what a topic is about.. but people have come up with all kinds of heuristics to do that once their model is trained.   I recommend you try playing with http://mallet.cs.umass.edu/ and seeing if this model fits your needs..   LDA is a completely unsupervised algorithm meaning it doesn't require you to hand annotate anything which is great, but on the flip side, might not deliver you the topics you were expecting it to give. A very simple solution to the problem would be: I'm sure there are cleverer, stats based solutions though. If you need a solution to use in a larger project rather than for interests sake, Yahoo BOSS has a key term extraction method. Latent Dirichlet allocation or Hierarchical Dirichlet Process can be used to generate tags for individual texts within a greater corpus (body of texts) by extracting the most important words from the derived topics. A basic example would be if we were to run LDA over a corpus and define it to have two topics, and that we find further that a text in the corpus is 70% one topic, and 30% another. The top 70% of the words that define the first topic and 30% that define the second (without duplication) could then be considered as tags for the given text. This method provides strong results where tags generally represent the broader themes of the given texts. With a general reference for preprocessing needed for these codes being found here, we can find tags through the following process using gensim. A heuristic way of deriving the optimal number of topics for LDA is found in this answer. Although HDP does not require the number of topics as an input, the standard in such cases is still to use LDA with a derived topic number, as HDP can be problematic. Assume here that the corpus is found to have 10 topics, and we want 5 tags per text: Assume further that we have a variable corpus, which is a preprocessed list of lists, with the subslist entries being word tokens. Initialize a Dirichlet dictionary and create a bag of words where texts are converted to their indexes for their component tokens (words): Create an LDA or HDP model: The following code produces ordered lists for the most important words per topic (note that here is where num_tags defines the desired tags per text): Then find the coherence of the topics across the texts: From here we have the percentage that each text coheres to a given topic, and the words associated with each topic, so we can combine them for tags with the following: corpus_tags will be a list of tags for each text based on how coherent the text is to the derived topics. See this answer for a similar version of this that generates tags for a whole text corpus.I have a dataset from sklearn and I plotted the distribution of the load_diabetes.target data (i.e. the values of the regression that the load_diabetes.data are used to predict).  I used this because it has the fewest number of variables/attributes of the regression sklearn.datasets. Using Python 3, How can I get the distribution-type and parameters of the distribution this most closely resembles?  All I know the target values are all positive and skewed (positve skew/right skew). . . Is there a way in Python to provide a few distributions and then get the best fit for the target data/vector? OR, to actually suggest a fit based on the data that's given? That would be realllllly useful for people who have theoretical statistical knowledge but little experience with applying it to "real data".  Bonus
Would it make sense to use this type of approach to figure out what your posterior distribution would be with "real data" ? If no, why not?  Use this approach To the best of my knowledge, there is no automatic way of obtaining the distribution type and parameters of a sample (as inferring the distribution of a sample is a statistical problem by itself). In my opinion, the best you can do is: (for each attribute) Try to fit each attribute to a reasonably large list of possible distributions 
(e.g. see Fitting empirical distribution to theoretical ones with Scipy (Python)? for an example with Scipy) Evaluate all your fits and pick the best one. This can be done by performing a Kolmogorov-Smirnov test between your sample and each of the distributions of the fit (you have an implementation in Scipy, again), and picking the one that minimises D, the test statistic (a.k.a. the difference between the sample and the fit). Bonus: It would make sense - as you'll be building a model on each of the variables as you pick a fit for each one - although the goodness of your prediction would depend on the quality of your data and the distributions you are using for fitting. You are building a model, after all. You can use that code to fit (according to the maximum likelihood) different distributions with your datas:  You can see a sample snippet about how to use the parameters obtained here: Fitting empirical distribution to theoretical ones with Scipy (Python)? Then, you can pick the distribution with the best log likelihood (there are also other criteria to match the "best" distribution, such as Bayesian posterior probability, AIC, BIC or BICc values, ...).  For your bonus question, there's I think no generic answer. If your set of data is significant and obtained under the same conditions as the real word datas, you can do it.  This code also works: fitter provides an iteration process over possible fitting distributions.
It also outputs a plot and summary table with statistic values. fitter package provides a simple class to identify the distribution
from which a data samples is generated from. It uses 80 distributions
from Scipy and allows you to plot the results to check what is the
most probable distribution and the best parameters. So basically the same iterative fit test procedure as described in other answers, but conveniently executed by the module.  Result for your SR_y series:  Code: Parameters of those fitted distributions as a dict: To get a list of all available distributions: Testing for all of them takes a long time, so it's best to use the implemened get_common_distributions() and potentially extend them with likely distribution as done in the code above. Make sure to have the current (1.4.1 or newer) fitter version installed: I had logging errors with a previous one and for my conda environment I needed: On a similar question (see here) you may be interrested in @Michel_Baudin answer explaining. His code assesses around 40 different distributions available OpenTURNS library and chooses the best one according to the BIC criterion. Looks something like that:I'm slightly confused in regard to how I save a trained classifier. As in, re-training a classifier each time I want to use it is obviously really bad and slow, how do I save it and the load it again when I need it? Code is below, thanks in advance for your help. I'm using Python with NLTK Naive Bayes Classifier.  To save: To load later: I went thru the same problem, and you cannot save the object since is a ELEFreqDistr NLTK class. Anyhow NLTK is hell slow. Training took 45 mins on a decent set and I decided to implement my own version of the algorithm (run it with pypy or rename it .pyx and install cython). It takes about 3 minutes with the same set and it can simply save data as json (I'll implement pickle which is faster/better).  I started a simple github project, check out the code here To Retrain the Pickled Classifer :I was using the Decision Tree and this error was raised. The same situation appeared when I used Back Propagation. How can I solve it? Traceback (most recent call last):
  File "<ipython-input-40-4359c06ae1f0>", line 1, in <module>
    runfile('C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py', wdir='C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib')
  File "C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py", line 710, in runfile
    execfile(filename, namespace)
  File "C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File "C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py", line 9, in <module>
    from numpy.testing.nosetester import import_nose ModuleNotFoundError: No module named 'numpy.testing.nosetester' This is happening due to a version incompatibility between numpy and scipy. numpy in its latest versions have deprecated numpy.testing.nosetester. and Triggers the error. Upgrade your scipy to a higher version. But not limited to this. By upgrading the above libraries to the latest stable, you should be able to get rid of this error. I needed to upgrade scipy pip3 install -U scipy I was facing the same error while using lexnlp package 
Got it fixed by installing: (Only install lexnlp if know you're explicitly using it in your project and you know what you're doing) I solved this by: and using: I also faced same issue while loading the model and fixed by upgrading below libraries try installing numpy version 1.17.0 using pip or pip3 
(assuming you already installed pip3) If you are using Jetson TX2 or any other aarch64 based device. You can solve the issue by installing latest numpy and scipy libraries. This also works for x86 based systems. (You can skip libatlas-base-dev and gfortran for x86 based systems) For me it solved by this link, apparently an open issue.
Downgrade to numpy==1.16.4I would like to know if there is a way to implement the different score function from the scikit learn package like this one : into a tensorflow model to get the different score. Will i have to run the session again to get the prediction ? You do not really need sklearn to calculate precision/recall/f1 score. You can easily express them in TF-ish way by looking at the formulas:  Now if you have your actual and predicted values as vectors of 0/1, you can calculate TP, TN, FP, FN using tf.count_nonzero: Now your metrics are easy to calculate: Maybe this example will speak to you :     Previous answers do not specify how to handle the multi-label case so here is such a version implementing three types of multi-label f1 score in tensorflow: micro, macro and weighted (as per scikit-learn) Update (06/06/18): I wrote a blog post about how to compute the streaming multilabel f1 score in case it helps anyone (it's a longer process, don't want to overload this answer) outputs: Since i have not enough reputation to add a comment to Salvador Dalis answer this is the way to go: tf.count_nonzero casts your values into an tf.int64 unless specified otherwise. Using: is a realy good idea. Use the metrics APIs provided in tf.contrib.metrics, for example:I've trained a Linear Regression model with R caret. I'm now trying to generate a confusion matrix and keep getting the following error: Error in confusionMatrix.default(pred, testing$Final) : 
the data and reference factors must have the same number of levels The error occurs when generating the confusion matrix. The levels are the same on both objects. I cant figure out what the problem is. Their structure and levels are given below. They should be the same. Any help would be greatly appreciated as its making me cracked!! I had the same issue. 
I guess it happened because data argument was not casted as factor as I expected.
Try:  hope it helps Do table(pred) and table(testing$Final). You will see that there is at least one number in the testing set that is never predicted (i.e. never present in pred). This is what is meant why "different number of levels". There is an example of a custom made function to get around this problem here. However, I found that this trick works fine: It should give you exactly the same confusion matrix as with the function.  Whenever you try to build a confusion matrix, make sure that both the true values and prediction values are of factor datatype.  Here both pred and testing$Final must be of type factor. Instead of check for levels, check the type of both the variables and convert them to factor if they are not. Here testing$final is of type int. conver it to factor and then build the confusion matrix. Something like the follows seem to work for me. The idea is similar to that of @nayriz: The key is to make sure the factor levels match. On a similar error, I forced the GLM predictions to have the same class as the dependent variable. For example, a GLM will predict a "numeric" class. But with the target variable being a "factor" class, I ran into an error. erroneous code: Result: corrected code: Result: I had this problem due to NAs for the target variable in the dataset. If you're using the tidyverse, you can use the drop_na function to remove rows that contain NAs. Like this: For base R, it might look something like: We get this error when creating the confusion matrix. When creating a confusion matrix, we need to make sure that the predicted value and the actual value of the data type are "factors". If there are other data types, we must convert them to "factor" data factors before generating a confusion matrix. After this conversion, start compiling the confusion matrix. Your are using regression and trying to generate a confusion matrix. I believe confusion matrix is used for classification task. Generally people use R^2 and RMSE metrics.I have tried many examples with F1 micro and Accuracy in scikit-learn and in all of them, I see that F1 micro is the same as Accuracy. Is this always true? Script Output F1 micro = Accuracy In classification tasks for which every test case is guaranteed to be assigned to exactly one class, micro-F is equivalent to accuracy. It won't be the case in multi-label classification. This is because we are dealing with a multi class classification , where every test data should belong to only 1 class and not multi label , in such case where there is no TN , we can call True Negatives as True Positives. Formula wise ,  correction : F1 score is 2* precision* recall / (precision + recall)  Micoaverage precision, recall, f1 and accuracy are all equal for cases in which every instance must be classified into one (and only one) class. A simple way to see this is by looking at the formulas precision=TP/(TP+FP) and recall=TP/(TP+FN). The numerators are the same, and every FN for one class is another classes's FP, which makes the denominators the same as well. If precision = recall, then f1 will also be equal. For any inputs should should be able to show that: I had the same issue so I investigated and came up with this: Just thinking about the theory, it is impossible that accuracy and the f1-score are the very same for every single dataset. The reason for this is that the f1-score is independent from the true-negatives while accuracy is not. By taking a dataset where f1 = acc and adding true negatives to it, you get f1 != acc.I just try to find out how I can use Caffe. To do so, I just took a look at the different .prototxt files in the examples folder. There is one option I don't understand: Possible values seem to be: Could somebody please explain those options? It is a common practice to decrease the learning rate (lr) as the optimization/learning process progresses. However, it is not clear how exactly the learning rate should be decreased as a function of the iteration number. If you use DIGITS as an interface to Caffe, you will be able to visually see how the different choices affect the learning rate. fixed: the learning rate is kept fixed throughout the learning process. inv: the learning rate is decaying as ~1/T
 step: the learning rate is piecewise constant, dropping every X iterations
  multistep: piecewise constant at arbitrary intervals
 You can see exactly how the learning rate is computed in the function SGDSolver<Dtype>::GetLearningRate (solvers/sgd_solver.cpp line ~30). Recently, I came across an interesting and unconventional approach to learning-rate tuning: Leslie N. Smith's work "No More Pesky Learning Rate Guessing Games". In his report, Leslie suggests to use lr_policy that alternates between decreasing and increasing the learning rate. His work also suggests how to implement this policy in Caffe. If you look inside the /caffe-master/src/caffe/proto/caffe.proto file (you can find it online here) you will see the following descriptions:I'm dealing with an imbalanced dataset and want to do a grid search to tune my model's parameters using scikit's gridsearchcv. To oversample the data, I want to use SMOTE, and I know I can include that as a stage of a pipeline and pass it to gridsearchcv.
My concern is that I think smote will be applied to both train and validation folds, which is not what you are supposed to do. The validation set should not be oversampled.
Am I right that the whole pipeline will be applied to both dataset splits? And if yes, how can I turn around this?
Thanks a lot in advance Yes, it can be done, but with imblearn Pipeline. You see, imblearn has its own Pipeline to handle the samplers correctly. I described this in a similar question here. When called predict() on a imblearn.Pipeline object, it will skip the sampling method and leave the data as it is to be passed to next transformer.
You can confirm that by looking at the source code here: So for this to work correctly, you need the following: Fill the details as necessary, and the pipeline will take care of the rest.I am fine-tuning a MobileNet with 14 new classes. When I add new layers by: I get the error: Also using: I get the error: What does lower mean? I saw other fine-tuning scripts and there were no other arguments other than the name of the model which is x in this case. The tensor must be passed to the layer when you are calling it, and not as an argument. Therefore it must be like this: To make it more clear, it is equivalent to this:Confused about random_state parameter, not sure why decision tree training needs some randomness. My thoughts http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html This is explained in the documentation The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement. So, basically, a sub-optimal greedy algorithm is repeated a number of times using random selections of features and samples (a similar technique used in random forests). The random_state parameter allows controlling these random choices. The interface documentation specifically states: If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. So, the random algorithm will be used in any case. Passing any value (whether a specific int, e.g., 0, or a RandomState instance), will not change that. The only rationale for passing in an int value (0 or otherwise) is to make the outcome consistent across calls: if you call this with random_state=0 (or any other value), then each and every time, you'll get the same result. The above cited part of the documentation is misleading, the underlying problem is not greediness of the algorithm. The CART algorithm is deterministic (see e.g. here) and finds a global minimum of the weighted Gini indices. Repeated runs of the decision tree can give different results because it is sometimes possible to split the data using different features and still achieve the same Gini index. This is described here:
https://github.com/scikit-learn/scikit-learn/issues/8443. Setting the random state simply assures that the CART implementation works through the same randomized list of features when looking for the minimum. Decision trees use heuristics process. Decision tree do not guarantee the same solution globally. There will be variations in the tree structure each time you build a model. Passing a specific seed to random_state ensures the same result is generated each time you build the model. The random_state parameter present for decision trees in scikit-learn determines which feature to select for a split if (and only if) there are two splits that are equally good (i.e. two features yield the exact same improvement in the selected splitting criteria (e.g. gini)). If this is not the case, the random_state parameter has no effect. The issue linked in teatrader's answer discusses this in more detail and as a result of that discussion the following section was added to the docs (emphasis added): random_state int, RandomState instance or None, default=None Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to "best". When max_features < n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See Glossary for details. To illustrate, let's consider the following example with the iris sample data set and a shallow decision tree containing just a single split: The output of this code will alternate between the two following trees based on which random_state is used.   The reason for this is that splitting on either petal length <= 2.45 or petal width <= 0.8 will both perfectly separate out the setosa class from the other two classes (we can see that the leftmost setosa node contains all 50 of the setosa observations). If we change just one observation of the data so that one of the previous two  splitting criteria no longer produces a perfect separation, the random_state will have no effect and we will always end up with the same result, for example:  The first split will now always be petal length <= 2.45 since the split petal width <= 0.8 can only separate out 49 of the 50 setosa classes (in other words a lesser decreases in the gini score). For a random forest (which consists of many decision trees), we would create each individual tree with a random selections of features and samples (see https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters for details), so there is a bigger role for the random_state parameter, but this is not the case when training just a single decision tree (this is true with the default parameters, but it is worth noting that some parameters could be affected by randomness if they are changed from the default value, most notably setting splitter="random"). A couple of related issues: Many machine learning models allow some randomness in model training. Specifying a number for random_state ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won't depend meaningfully on exactly what value you choose.If I want to train a model with train_generator, is there a significant difference between choosing and Currently I am training for 10 epochs, because each epoch takes a long time, but any graph showing improvement looks very "jumpy" because I only have 10 datapoints. I figure I can get a smoother graph if I use 100 Epochs, but I want to know first if there is any downside to this Based on what you said it sounds like you need a larger batch_size, and of course there are implications with that which could impact the steps_per_epoch and number of epochs. To solve for jumping-around Implications of a larger batch-size When to reduce epochs When to adjust steps-per-epoch Steps per epoch does not connect to epochs. Naturally what you want if to 1 epoch your generator pass through all of your training data one time. To achieve this you should provide steps per epoch equal to number of batches like this: as from above equation the largest the batch_size, the lower the steps_per_epoch.  Next you will choose epoch based on chosen validation. (choose what you think best) steps_per_epoch tells the network how many batches to include in an epoch. By definition, an epoch is considered complete when the dataset has been run through the model once in its entirety. With other words, it means that all training samples have been run through the model. (For further discussion, let us assume that the size of the training examples is 'm'). Also by definition, we know that `batch size' is between [1, m]. Below is what TensorFlow page says about steps_per_epoch If you want to run training only on a specific number of batches from this Dataset, you can pass the steps_per_epoch argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch. Now suppose that your training_size, m = 128 and batch_size, b = 16, which means that your data is grouped into 8 batches. According to the above quote, the maximum value you can assign to steps_per_epoch is 8, as computed in one of the answers by @Ioannis Nasios. However, it is not necessary that you set the value to 8 only (as in our example). You can choose any value between 1 and 8. You just need to be aware that the training will be performed only with this number of batches. The reason for the jumpy error values could be the size of your batch, as correctly mentioned in this answer by @Chris Farr. Training & evaluation from tf.data Datasets If you do this, the dataset is not reset at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset). The advantage of a low value for steps_per_epoch is that different epochs are trained with different data sets (a kind of regularization). However, if you have a limited training size, using only a subset of stacks would not be what we want. It is a decision one has to make. The Steps per epoch denote the number of batches to be selected for one epoch.
If 500 steps are selected then the network will train for 500 batches to complete one epoch.
If we select the large number of epochs it can be computationalI wish to implement early stopping with Keras and sklean's GridSearchCV. The working code example below is modified from How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras. The data set may be downloaded from here. The modification adds the Keras EarlyStopping callback class to prevent over-fitting. For this to be effective it requires the monitor='val_acc' argument for monitoring validation accuracy. For val_acc to be available KerasClassifier requires the validation_split=0.1 to generate validation accuracy, else EarlyStopping raises RuntimeWarning: Early stopping requires val_acc available!. Note the FIXME: code comment! Note we could replace val_acc by val_loss!  Question: How can I use the cross-validation data set generated by the GridSearchCV k-fold algorithm instead of wasting 10% of the training data for an early stopping validation set?  [Answer after the question was edited & clarified:] Before rushing into implementation issues, it is always a good practice to take some time to think about the methodology and the task itself; arguably, intermingling early stopping with the cross validation procedure is not a good idea. Let's make up an example to highlight the argument. Suppose that you indeed use early stopping with 100 epochs, and 5-fold cross validation (CV) for hyperparameter selection. Suppose also that you end up with a hyperparameter set X giving best performance, say 89.3% binary classification accuracy. Now suppose that your second-best hyperparameter set, Y, gives 89.2% accuracy. Examining closely the individual CV folds, you see that, for your best case X, 3 out of the 5 CV folds exhausted the max 100 epochs, while in the other 2 early stopping kicked in, say in 95 and 93 epochs respectively. Now imagine that, examining your second-best set Y, you see that again 3 out of the 5 CV folds exhausted the 100 epochs, while the other 2 both stopped early enough at ~ 80 epochs. What would be your conclusion from such an experiment? Arguably, you would have found yourself in an inconclusive situation; further experiments might reveal which is actually the best hyperparameter set, provided of course that you would have thought to look into these details of the results in the first place. And needless to say, if all this was automated through a callback, you might have missed your best model despite the fact that you would have actually tried it. The whole CV idea is implicitly based on the "all other being equal" argument (which of course is never true in practice, only approximated in the best possible way). If you feel that the number of epochs should be a hyperparameter, just include it explicitly in your CV as such, rather than inserting it through the back door of early stopping, thus possibly compromising the whole process (not to mention that early stopping has itself a hyperparameter, patience). Not intermingling these two techniques doesn't mean of course that you cannot use them sequentially: once you have obtained your best hyperparameters through CV, you can always employ early stopping when fitting the model in your whole training set (provided of course that you do have a separate validation set). The field of deep neural nets is still (very) young, and it is true that it has yet to establish its "best practice" guidelines; add the fact that, thanks to an amazing community, there are all sort of tools available in open source implementations, and you can easily find yourself into the (admittedly tempting) position of mixing things up just because they happen to be available. I am not necessarily saying that this is what you are attempting to do here - I am just urging for more caution when combining ideas that may have not been designed to work along together... [Old answer, before the question was edited & clarified - see updated & accepted answer above] I am not sure I have understood your exact issue (your question is quite unclear, and you include many unrelated details, which is never good when asking a SO question - see here). You don't have to (and actually should not) include any arguments about validation data in your model = KerasClassifier() function call (it is interesting why you don't feel the same need for training data here, too). Your grid.fit() will take care of both the training and validation folds. So provided that you want to keep the hyperparameter values as included in your example, this function call should be simply You can see some clear and well-explained examples regarding the use of GridSearchCV with Keras here. Here is how to do it with only a single split. If you want more splits, you can use 'cl__validation_split' with a fixed ratio and construct splits that meet that criteria. It might be too paranoid, but I don't use the early stopping data set as a validation data set since it was indirectly used to create the model. I also think if you are using early stopping with your final model, then it should also be done when you are doing hyper-parameter search.Suppose I want to have the general neural network architecture: Input1 is image data, input2 is non-image data. I have implemented this architecture in Tensorflow. All pytorch examples I have found are one input go through each layer. How can I define forward func to process 2 inputs separately then combine them in a middle layer?  By "combine them" I assume you mean to concatenate the two inputs.
Assuming you concat along the second dimension: Note that when you define the number of inputs to self.fc2 you need to take into account both out_channels of self.conv as well as the output spatial dimensions of c.I'm building a model in Keras using some tensorflow function (reduce_sum and l2_normalize) in the last layer while encountered this problem. I have searched for a solution but all of it related to "Keras tensor". Here is my code: and then the error:  ValueError: Output tensors to a Model must be the output of a
  TensorFlow Layer (thus holding past layer metadata). Found:
  Tensor("l2_normalize_3:0", shape=(?, 3), dtype=float32) I noticed that without passing fc2 layer to these functions, the model works fine: Can someone please explain to me this problem and some suggestion on how to fix it? I have found a way to work around to solve the problem.
For anyone who encounters the same issue, you can use the Lambda layer to wrap your tensorflow operations, this is what I did: I had this issue because I was adding 2 tensors as x1+x2 somewhere in my model instead of using Add()([x1,x2]).  That solved the problem.I'm trying to convert a string array of categorical variables to an integer array of categorical variables. Ex. I realize this can be done with a loop but I imagine there is an easier way. Thanks. np.unique has some optional returns return_inverse gives the integer encoding, which I use very often it can be used to recreate the original array from uniques ... years later....  For completeness (because this isn't mentioned in the answers) and personal reasons (I always have pandas imported in my modules but not necessarily sklearn), this is also quite straightforward with pandas.get_dummies() One way is to use the categorical function from scikits.statsmodels.  For example: The return value from categorical (b) is actually a design matrix, hence the call to argmax above to get it close to your desired format. Another option is to use a categorical pandas Series: Another way is to use sklearn.preprocessing.LabelEncoder It can convert hashable labels like strings to numerical values ranging between 0 and n_classes-1. It is done like this: If you insist on having the values start from 1 in the resulting array you could simply do c + 1 afterwards. It might not be worth it to bring in sklearn as a dependency for a project only to do this, but it is a good option if you have sklearn already imported.  Another approach is to use Pandas factorize to map items to a number: Well, this is a hack... but does it help? ...some more years pass... Thought I would provide a pure python solution for completeness: You can also try something like this: It would be better if you know what's in there and wish to set specific index for each values. If there's only two categories, next code will work like a charm:LSTM/RNN can be used for text generation.
This shows way to use pre-trained GloVe word embeddings for Keras model. Sample approach tried: Sample code / psuedocode to train LSTM and predict will be appreciated.  I've created a gist with a simple generator that builds on top of your initial idea: it's an LSTM network wired to the pre-trained word2vec embeddings, trained to predict the next word in a sentence. The data is the list of abstracts from arXiv website. I'll highlight the most important parts here. Your code is fine, except for the number of iterations to train it. The default iter=5 seems rather low. Besides, it's definitely not the bottleneck -- LSTM training takes much longer. iter=100 looks better. The result embedding matrix is saved into pretrained_weights array which has a shape (vocab_size, emdedding_size). Your code is almost correct, except for the loss function. Since the model predicts the next word, it's a classification task, hence the loss should be categorical_crossentropy or sparse_categorical_crossentropy. I've chosen the latter for efficiency reasons: this way it avoids one-hot encoding, which is pretty expensive for a big vocabulary. Note passing the pre-trained weights to weights. In order to work with sparse_categorical_crossentropy loss, both sentences and labels must be word indices. Short sentences must be padded with zeros to the common length. This is pretty straight-forward: the model outputs the vector of probabilities, of which the next word is sampled and appended to the input. Note that the generated text would be better and more diverse if the next word is sampled, rather than picked as argmax. The temperature based random sampling I've used is described here. Doesn't make too much sense, but is able to produce sentences that look at least grammatically sound (sometimes). The link to the complete runnable script.I'm learning keras API in tensorflow(2.3). In this guide on tensorflow website, I found an example of custom loss funciton: The reduce_mean function in this custom loss function will return an scalar. Is it right to define loss function like this? As far as I know, the first dimension of the shapes of y_true and y_pred is the batch size. I think the loss function should return loss values for every sample in the batch. So the loss function shoud give an array of shape (batch_size,). But the above function gives a single value for the whole batch. Maybe the above example is wrong? Could anyone give me some help on this problem? p.s. Why do I think the loss function should return an array rather than a single value? I read the source code of Model class. When you provide a loss function (please note it's a function, not a loss class) to Model.compile() method, ths loss function is used to construct a LossesContainer object, which is stored in Model.compiled_loss. This loss function passed to the constructor of LossesContainer class is used once again to construct a LossFunctionWrapper object, which is stored in LossesContainer._losses. According to the source code of LossFunctionWrapper class, the overall loss value for a training batch is calculated by the LossFunctionWrapper.__call__() method (inherited from Loss class), i.e. it returns a single loss value for the whole batch. But the LossFunctionWrapper.__call__() first calls the LossFunctionWrapper.call() method to obtain an array of losses for every sample in the training batch. Then these losses are fianlly averaged to get the single loss value for the whole batch. It's in the LossFunctionWrapper.call() method that the loss function provided to the Model.compile() method is called. That's why I think the custom loss funciton should return an array of losses, insead of a single scalar value. Besides, if we write a custom Loss class for the Model.compile() method, the call() method of our custom Loss class should also return an array, rather than a signal value. I opened an issue on github. It's confirmed that custom loss function is required to return one loss value per sample. The example will need to be updated to reflect this. Actually, as far as I know, the shape of return value of the loss function is not important, i.e. it could be a scalar tensor or a tensor of one or multiple values per sample. The important thing is how it should be reduced to a scalar value so that it could be used in optimization process or shown to the user. For that, you can check the reduction types in Reduction documentation. Further, here is what the compile method documentation says about the loss argument, partially addressing this point: loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses. An objective function is any callable with the signature loss = fn(y_true,y_pred), where y_true = ground truth values with shape = [batch_size, d0, .. dN], except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1]. y_pred = predicted values with shape = [batch_size, d0, .. dN]. It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. In addition, it's worth noting that most of the built-in loss functions in TF/Keras are usually reduced over the last dimension (i.e. axis=-1). For those who doubt that a custom loss function which returns a scalar value would work: you can run the following snippet and you will see that the model would train and converge properly. I opened an issue on github. It's confirmed that custom loss function is required to return one loss value per sample. The example will need to be updated to reflect this. I think the question posted by @Gödel is totally legit and is correct. The custom loss function should return a loss value per sample. And, an explanation provided by @today is also correct. In the end, it all depends on the kind of reduction used. So if one uses class API to create a loss function, then, reduction parameter is automatically inherited in the custom class. Its default value "sum_over_batch_size" is used (which is simply averaging of all the loss values in a given batch). Other options are "sum", which computes a sum instead of averaging and the last option is "none", where an array of loss values are returned. It is also mentioned in the Keras documentation that these differences in reduction are irreverent when one is using model.fit() because reduction is then automatically handled by TF/Keras. And, lastly, it is also mentioned that when a custom loss function is created, then, an array of losses (individual sample losses) should be returned. Their reduction is handled by the framework. Links: The tf.math.reduce_mean takes the average for the batch and returns it. That's why it is a scalar. In machine learning, the loss we use is sum of losses of individual training examples, so it should be a scalar value. (Since for all the examples, we are using a single network, thus we need a single loss value to update the parameters.) When using parallel computation, making container is a simpler and feasible way to keep track of indices of losses computed as we are using batches to train and not the whole training set. The tensorflow documentation missed it, but this is clearly stated and clarified on the Keras documentation. It says: Note that this is an important difference between loss functions like
tf.keras.losses.mean_squared_error and default loss class instances
like tf.keras.losses.MeanSquaredError: the function version does not
perform reduction, but by default the class instance does. And it also states: By default, loss functions return one scalar loss value per input
sample. The dimensionality can be increased because of multiple channels...however, each channel should only have a scalar value for loss.I have some question about pytorch's backward function I don't think I'm getting the right output : the output is maybe it's 2*a*a but i think the output suppose to be 2*a. cause d(x^2)/dx=2x Please read carefully the documentation on backward() to better understand it. By default, pytorch expects backward() to be called for the last output of the network - the loss function. The loss function always outputs a scalar and therefore, the gradients of the scalar loss w.r.t all other variables/parameters is well defined (using the chain rule). Thus, by default, backward() is called on a scalar tensor and expects no arguments. For example: yields As expected: d(a^2)/da = 2a. However, when you call backward on the 2-by-3 out tensor (no longer a scalar function) - what do you expects a.grad to be? You'll actually need a 2-by-3-by-2-by-3 output: d out[i,j] / d a[k,l](!) Pytorch does not support this non-scalar function derivatives.  Instead, pytorch assumes out is only an intermediate tensor and somewhere "upstream" there is a scalar loss function, that through chain rule provides d loss/ d out[i,j]. This "upstream" gradient is of size 2-by-3 and this is actually the argument you provide backward in this case: out.backward(g) where g_ij = d loss/ d out_ij. The gradients are then calculated by chain rule d loss / d a[i,j] = (d loss/d out[i,j]) * (d out[i,j] / d a[i,j]) Since you provided a as the "upstream" gradients you got If you were to provide the "upstream" gradients to be all ones yields As expected. It's all in the chain rule.I'm currently working on a problem which compares three different machine learning algorithms performance on the same data-set. I divided the data-set into 70/30 training/testing sets and then performed grid search for the best parameters of each algorithm using GridSearchCV and X_train, y_train. First question, am I suppose to perform grid search on the training set or is it suppose to be on the whole data-set? Second question, I know that GridSearchCV uses K-fold in its' implementation, does it mean that I performed cross-validation if I used the same X_train, y_train for all three algorithms I compare in the GridSearchCV? Any answer would be appreciated, thank you. All estimators in scikit where name ends with CV perform cross-validation.
But you need to keep a separate test set for measuring the performance. So you need to split your whole data to train and test. Forget about this test data for a while.  And then pass this train data only to grid-search. GridSearch will split this train data further into train and test to tune the hyper-parameters passed to it. And finally fit the model on the whole train data with best found parameters. Now you need to test this model on the test data you kept aside in the beginning. This will give you the near real world performance of model.  If you use the whole data into GridSearchCV, then there would be leakage of test data into parameter tuning and then the final model may not perform that well on newer unseen data. You can look at my other answers which describe the GridSearch in more detail: Yes, GridSearchCV performs cross-validation. If I understand the concept correctly - you want to keep part of your data set unseen for the model in order to test it. So you train your models against train data set and test them on a testing data set. Here I was doing almost the same - you might want to check it...I need to transform the independent field from string to arithmetical notation. I am using OneHotEncoder for the transformation. My dataset has many independent columns of which some are as: I have to encode the Country column like I succeed to get the desire transformation via using OneHotEncoder as Now I'm getting the depreciation message to use categories='auto'. If I do so the transformation is being done for the all independent columns like country, age, salary etc. How to achieve the transformation on the dataset 0th column only? There is actually 2 warnings :  FutureWarning: The handling of integer data will change in version
  0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the
  unique values. If you want the future behaviour and silence this
  warning, you can specify "categories='auto'". In case you used a
  LabelEncoder before this OneHotEncoder to convert the categories to
  integers, then you can now use the OneHotEncoder directly. and the second : The 'categorical_features' keyword is deprecated in version 0.20 and
  will be removed in 0.22. You can use the ColumnTransformer instead.
  "use the ColumnTransformer instead.", DeprecationWarning) In the future, you should not define the columns in the OneHotEncoder directly, unless you want to use "categories='auto'". The first message also tells you to use OneHotEncoder directly, without the LabelEncoder first.
Finally, the second message tells you to use ColumnTransformer, which is like a Pipe for columns transformations. Here is the equivalent code for your case :  See also : ColumnTransformer documentation For the above example; Encoding Categorical data (Basically Changing Text to Numerical data i.e, Country Name) As of version 0.22, you can write the same code as below: As you can see, you don't need to use LabelEncoder anymore. Reminder will keep previous data while [0]th column will replace will be encoded Dont use the labelencoder and directly use OneHotEncoder. There is a way that you can do one hot encoding with pandas.
Python: Give names to the newly formed columns add it to your dataframe. Check the pandas documentation here. I had the same issue and the following worked for me: Hope this helps  Use the following code :-  This code should solve the error. When updating the code from this: To this: Note that I had to add dtype=np.float to fix the error message TypeError: can't convert np.ndarray of type numpy.object_. Where my colums were [0, 1, 4, 5, 6] and 'one_hot_encoder' is anything. My imports were:I basically have the same question as this guy.. The example in the NLTK book for the Naive Bayes classifier considers only whether a word occurs in a document as a feature.. it doesn't consider the frequency of the words as the feature to look at ("bag-of-words"). One of the answers seems to suggest this can't be done with the built in NLTK classifiers.  Is that the case?  How can I do frequency/bag-of-words NB classification with NLTK? scikit-learn has an implementation of multinomial naive Bayes, which is the right variant of naive Bayes in this situation. A support vector machine (SVM) would probably work better, though. As Ken pointed out in the comments, NLTK has a nice wrapper for scikit-learn classifiers. Modified from the docs, here's a somewhat complicated one that does TF-IDF weighting, chooses the 1000 best features based on a chi2 statistic, and then passes that into a multinomial naive Bayes classifier. (I bet this is somewhat clumsy, as I'm not super familiar with either NLTK or scikit-learn.) This printed for me: Not perfect, but decent, considering it's not a super easy problem and it's only trained on 100/100. The features in the NLTK bayes classifier are "nominal", not numeric. This means they can take a finite number of discrete values (labels), but they can't be treated as frequencies. So with the Bayes classifier, you cannot directly use word frequency as a feature-- you could do something like use the 50 more frequent words from each text as your feature set, but that's quite a different thing But maybe there are other classifiers in the NLTK that depend on frequency. I wouldn't know, but have you looked? I'd say it's worth checking out. If your sentence has the same word multiple times, it will just add the probs multiple times.  If the word appears multiple times in the same class, your training data should reflect that in the word count. For added accuracy, count all bi-grams, tri-grams, etc as separate features. It helps to manually write your own classifiers so that you understand exactly what is happening and what you need to do to imporve accuracy.  If you use a pre-packaged solution and it doesn't work well enough, there is not much you can do about it.I need a help understanding the error in while executing the above code. Below is the error: "raise ValueError("x and y must be the same size")" I have .csv file with 1398 rows and 2 column. I have taken 40% as y_test set, as it is visible in the above code. Print X_train shape. What do you see? I'd bet X_train is 2d (matrix with a single column), while y_train 1d (vector). In turn you get different sizes.  I think using X_train[:,0] for plotting (which is from where the error originates) should solve the problem Slicing with [:, :-1] will give you a 2-dimensional array (including all rows and all columns excluding the last column). Slicing with [:, 1] will give you a 1-dimensional array (including all rows from the second column). To make this array also 2-dimensional use [:, 1:2] or [:, 1].reshape(-1, 1) or [:, 1][:, None] instead of [:, 1]. This will make x and y comparable. An alternative to making both arrays 2-dimensional is making them both one dimensional. For this one would do [:, 0] (instead of [:, :1]) for selecting the first column and [:, 1] for selecting the second column. Try this: It will make an evenly spaced array and your error will be gone permanently.I train a RandomForestRegressor model on 64bit python.
I pickle the object.
When trying to unpickle the object on 32bit python I get the following error: 'ValueError: Buffer dtype mismatch, expected 'SIZE_t' but got 'long long'' I really have no idea how to fix this, so any help would be hugely appreciated. Edit: more detail This occurs because the random forest code uses different types for indices on 32-bit and 64-bit machines. This can, unfortunately, only be fixed by overhauling the random forests code. Since several scikit-learn devs are working on that anyway, I put it on the todo list. For now, the training and testing machines need to have the same pointer size. For ease, please use python  64 bit version to decentralize your model. I faced the same issue recently. after taking that step it was resolved. So try running it on a 64 bit version. I hope this helps I fixed this problem with training the model in the same machine. I was training the model on Jupyter Notebook(Windows PC) and trying to load into Raspberry Pi but I got the error. Therefore, I trained the model in Raspberry Pi and maintained again then I fixed the problem. I had the same problem when I trained the model with python 3.7.0 32bit installed on my system. It got solved after installing the python 3.8.10 64bit version and training the model again.This code attempts to utilize a custom implementation of dropout :  Custom dropout is implemented as :  It seems I've implemented the dropout function incorrectly ? : How to modify in order to correctly utilize dropout ? These posts were useful in getting to this point :  Hinton's Dropout in 3 Lines of Python : 
https://iamtrask.github.io/2015/07/28/dropout/ Making a Custom Dropout Function : https://discuss.pytorch.org/t/making-a-custom-dropout-function/14053/2 It seems I've implemented the dropout function incorrectly? In fact, the above implementation is known as Inverted Dropout. Inverted Dropout is how Dropout is implemented in practice in the various deep learning frameworks. What is inverted dropout? Before jump into the inverted dropout, it can be helpful to see how Dropout works for a single neuron:  Since during train phase a neuron is kept on with probability q (=1-p), during the testing phase we have to emulate the behavior of the ensemble of networks used in the training phase. To this end, the authors suggest scaling the activation function by a factor of q during the test phase in order to use the expected output produced in the training phase as the single output required in the test phase (Section 10, Multiplicative Gaussian Noise). Thus:  Inverted dropout is a bit different. This approach consists in the scaling of the activations during the training phase, leaving the test phase untouched. The scale factor is the inverse of the keep probability 1/1-p = 1/q, thus:  Inverted dropout helps to define the model once and just change a parameter (the keep/drop probability) to run train and test on the same model. Direct Dropout, instead, force you to modify the network during the test phase because if you don’t multiply by q the output the neuron will produce values that are higher respect to the one expected by the successive neurons (thus the following neurons can saturate or explode): that’s why Inverted Dropout is the more common implementation. References: Dropout Regularization, coursera by Andrew NG What is inverted dropout? Dropout: scaling the activation versus inverting the dropout Analysis of Dropout How implement inverted dropout Pytorch? How to implement in Numpy? How to implement in Tensorflow? Implementation with Torch and bernoulli..I want the classifier to run faster and stop early if the patience reaches the number I set. In the following code it does 10 iterations of fitting the model. Here is the resulting error- I changed the cross_val_score in the following- and now I get this error- This code came from here. The code is by far the most accurate I've used so far. The problem is that there is no defined model.fit() anywhere in the code. It also takes forever to fit. The fit() operation occurs at the results = cross_val_score(...) and there's no parameters to throw a callback in there. How do I go about doing this?
Also, how do I run the model trained on a test set? I need to be able to save the trained model for later use... Reading from here, which is the source code of KerasClassifier, you can pass it the arguments of fit and they should be used.
I don't have your dataset so I cannot test it, but you can tell me if this works and if not I will try and adapt the solution. Change this line : A small explaination of what's happening : KerasClassifier is taking all the possibles arguments for fit, predict, score and uses them accordingly when each method is called. They made a function that filters the arguments that should go to each of the above functions that can be called in the pipeline. 
I guess there are several fit and predict calls inside the StratifiedKFold step to train on different splits everytime.  The reason why it takes forever to fit and it fits 10 times is because one fit is doing 300 epochs, as you asked. So the KFold is repeating this step over the different folds : EDIT : Ok, so I took the time to download the dataset and try your code... First of all you need to correct a "few" things in your network :  your input have a 60 features. You clearly show it in your data prep : so why would you have this : please change to :  About your targets/labels. You changed the objective from the original code (binary_crossentropy) to categorical_crossentropy. But you didn't change your Y array. So either do this in your data preparation : or change your objective back to binary_crossentropy. Now the network's output size : 122 on the last dense layer? your dataset obviously has 2 categories so why are you trying to output 122 classes? it won't match the target. Please change back your last layer to : if you choose to use categorical_crossentropy, or  if you go back to binary_crossentropy. So now that your network compiles, I could start to troubleshout. here is your solution So now I could get the real error message. It turns out that when you feed fit_params=whatever in the cross_val_score() function, you are feeding those parameters to a pipeline. In order to know to which part of the pipeline you want to send those parameters you have to specify it like this : Your error was saying that the process couldn't unpack 'callbacks'.split('__', 1) into 2 values. It was actually looking for the name of the pipeline's step to apply this to. It should be working now :) BUT, you should be aware of what's happening here... the cross validation actually calls the create_baseline() function to recreate the model from scratch 10 times an trains it 10 times on different parts of the dataset. So it's not doing epochs as you were saying, it's doing 300 epochs 10 times. 
What is also happening as a consequence of using this tool : since the models are always differents, it means the fit() method is applied 10 times on different models, therefore, the callbacks are also applied 10 different times and the files saved by ModelCheckpoint() get overriden and you find yourself only with the best model of the last run. This is intrinsec to the tools you use, I don't see any way around this. This comes as consequence to using different general tools that weren't especially thought to be used together with all the possible configurations. Try: where list_of_callbacks is a list of callbacks you want to apply. You could find details here. It's mentioned there that parameters fed to KerasClassifier could be legal fitting parameters. It's also worth to mention that if you are using multiple runs with GPUs there might be a problem due to several reported memory leaks especially when you are using theano. I also noticed that running multiple fits consequently may show results which seem to be not independent when using sklearn API.  Edit: Try also: Instead of putting callbacks list in a wrapper instantiation. This is what I have done and has worked so far Despite the TensorFlow, Keras & SciKeras documentation suggesting you can define training callbacks via the fit method, for my setup it turns out (like @NassimBen suggests) you should do it through the model constructor instead. Rather than this: Try this:I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops? Here are the original input variables: A is a 2x4 array.
B is a 3x4 array. We want to compute the Euclidean distance matrix operation in one entirely vectorized operation, where dist[i,j] contains the distance between the ith instance in A and jth instance in B. So dist is 2x3 in this example. The distance   could ostensibly be written with numpy as However, as shown above, the problem is that the element-wise subtraction operation A-B involves incompatible array sizes, specifically the 2 and 3 in the first dimension. In order to do element-wise subtraction, we have to pad either A or B to satisfy numpy's broadcast rules. I'll choose to pad A with an extra dimension so that it becomes 2 x 1 x 4, which allows the arrays' dimensions to line up for broadcasting. For more on numpy broadcasting, see the tutorial in the scipy manual and the final example in this tutorial. You can perform the padding with either np.newaxis value or with the np.reshape command. I show both below: As you can see, using either approach will allow the dimensions to line up. I'll use the first approach with np.newaxis. So now, this will work to create A-B, which is a 2x3x4 array: Now we can put that difference expression into the dist equation statement to get the final result: Note that the sum is over axis=2, which means take the sum over the 2x3x4 array's third axis (where the axis id starts with 0). If your arrays are small, then the above command will work just fine. However, if you have large arrays, then you may run into memory issues. Note that in the above example, numpy internally created a 2x3x4 array to perform the broadcasting. If we generalize A to have dimensions a x z and B to have dimensions b x z, then numpy will internally create an a x b x z array for broadcasting. We can avoid creating this intermediate array by doing some mathematical manipulation. Because you are computing the Euclidean distance as a sum-of-squared-differences, we can take advantage of the mathematical fact that sum-of-squared-differences can be rewritten.  Note that the middle term involves the sum over element-wise multiplication. This sum over multiplcations is better known as a dot product. Because A and B are each a matrix, then this operation is actually a matrix multiplication. We can thus rewrite the above as:  We can then write the following numpy code: Note that the answer above is exactly the same as the previous implementation. Again, the advantage here is the we do not need to create the intermediate 2x3x4 array for broadcasting. For completeness, let's double-check that the dimensions of each summand in threeSums allowed broadcasting. So, as expected, the final dist array has dimensions 2x3. This use of the dot product in lieu of sum of element-wise multiplication is also discussed in this tutorial. I had the same problem recently working with deep learning(stanford cs231n,Assignment1),but when I used There was a error That means I ran out of memory(In fact,that produced a array of 500*5000*1024 in the middle.It's so huge!) To prevent that error,we can use a formula to simplify:  code: Simply use np.newaxis at the right place: This functionality is already included in scipy's spatial module and I recommend using it as it will be vectorized and highly optimized under the hood. But, as evident by the other answer, there are ways you can do this yourself. Using numpy.linalg.norm also works well with broadcasting. Specifying an integer value for axis will use a vector norm, which defaults to Euclidean norm.Here's the code I'm working with (pulled from Kaggle mostly): I have 4 classes that are very imbalanced. Class A equals 70%, class B = 15%, class C = 10%, and class D = 5%. However, I care most about class D. So I did the following type of calculations: D_weight = A/D = 70/5 = 14 and so on for the weight for class B and A. (if there are better methods to select these weights, then feel free) In the last line, I'm trying to properly set class_weights and I'm doing it as so: class_weights = {0: 1.0, 1: 6, 2: 7, 3: 14}. However, when I do this, I get the following error.  class_weight not supported for 3+ dimensional targets. Is it possible that I add a dense layer after the last layer and just use it as a dummy layer so I can pass the class_weights and then only use the output of the last conv2d layer to do the prediction?  If this is not possible, how would I modify the loss function (I'm aware of this post, however, just passing in the weights in to the loss function won't cut it, because the loss function is called separately for each class) ? Currently, I'm using the following loss function:  But I don't see any way in which I can input class weights. If someone wants the full working code see this post. But remember to change the final conv2d layer's num classes to 4 instead of 1.  You can always apply the weights yourself. The originalLossFunc below you can import from keras.losses.
The weightsList is your list with the weights ordered by class.  For using this in compile: You can change the balance of the input samples too.  For instance, if you have 5 samples from class 1 and 10 samples from class 2, pass the samples for class 5 twice in the input arrays. .   Instead of working "by class", you can also work "by sample". Create an array of weights for each sample in your input array: len(x_train) == len(weights)  And fit passing this array to the sample_weight argument.
(If it's fit_generator, the generator will have to return the weights along with the train/true pairs: return/yield inputs, targets, weights)I am struggling to use Random Forest in Python with Scikit learn. My problem is that I use it for text classification (in 3 classes - positive/negative/neutral) and the features that I extract are mainly words/unigrams, so I need to convert these to numerical features. I found a way to do it with DictVectorizer's fit_transform: My problem is that the fit_transform method is working on the train dataset, which contains around 8000 instances, but when I try to convert my test set to numerical features too, which is around 80000 instances, I get a memory error saying that: What could possibly cause this and is there any workaround? Many thanks! You are not supposed to do fit_transform on your test data, but only transform. Otherwise, you will get different vectorization than the one used during training. For the memory issue, I recommend TfIdfVectorizer, which has numerous options of reducing the dimensionality (by removing rare unigrams etc.). UPDATE If the only problem is fitting test data, simply split it to small chunks. Instead of something like you can do and record results/stats and analyze them afterwards. in particularI have built a simple neural network, and I would get its weights by: but, in this way, I only get the weights matrices (5x20 , 1x20) without the biases. How can I get the biases values? Quite simple, its just the second element in the array returned by get_weights() (For Dense layers): Here's a complete working example (implemented with TensorFlow 2 and Keras). YOu can view and output biases and weights using the following code: if you're looking for weights and bias from the validation dataset, you need to do model.predict on each vector from the dataset.I am running elastic net regularization in caret using glmnet. I pass sequence of values to trainControl for alpha and lambda, then I perform repeatedcv to get the optimal tunings of alpha and lambda. Here is an example where the optimal tunings for alpha and lambda are 0.7 and 0.5 respectively: My question?  When I run as.matrix(coef(model.test$finalModel)) which I would assume give me the coefficients corresponding to the best model, I get 100 different sets of coefficients.  So how do I get the coefficients corresponding to the best tuning? I've seen this recommendation to get the best model coef(model.test$finalModel, model.test$bestTune$lambda) However, this returns NULL coefficients, and In any case, would only be returning the best tunings related to lambda, and not to alpha in addition. EDIT: After searching everywhere on the internet, all I can find now which points me in the direction of the correct answer is this blog post, which says that model.test$finalModel returns the model corresponding to the best alpha tuning, and coef(model.test$finalModel, model.caret$bestTune$lambda) returns the set of coefficients corresponding to the best values of lambda. If this is true then this is the answer to my question. However, as this is a single blog post, and I can't find anything else to back up this claim, I am still skeptical. Can anyone validate this claim that model.test$finalModel returns the model corresponding to the best alpha?? If so then this question would be solved. Thanks! After a bit of playing with your code I find it very odd that glmnet train chooses different lambda ranges depending on the seed. Here is an example: optimum lambda is: and this works: giving the coefficients at best alpha and lambda when using this model to predict some y are predicted as X1 and some as X2 now with the seed you used lambda values are 10 times smaller and this gives empty coefficients since lambdaOpt is not in the range of tested lambda: now when predicting upon this model only X0 is predicted (the first level): quite odd behavior, probably worth reportingI am getting the following error c50 code called exit with value 1 I am doing this on the titanic data available from Kaggle Output :- Then I tried using C5.0 dtree So running the above lines gives me this error I'm not able to figure out what's going wrong? I was using similar code on different dataset and it was working fine. Any ideas about how can I debug my code? -Thanks For anyone interested, the data can be found here: http://www.kaggle.com/c/titanic-gettingStarted/data. I think you need to be registered in order to download it. Regarding your problem, first of I think you meant to write Next, notice the structure of the Cabin and Embarked Columns. These two factors have an empty character as a level name (check with levels(train$Embarked)). This is the point where C50 falls over. If you modify your data such that your algorithm will now run without an error. Just in case. You can take a look to the error by Also this error occurs when there are a special characters in the name of a variable. For example, one will get this error if there is "я"(it's from Russian alphabet) character in the name of a variable. Here is what worked finally:- Got this idea after reading this post The intuition behind this is that in this way both the train and test data set will have consistent factor levels. I had the same error, but I was using a numeric dataset without missing values. After a long time, I discovered that my dataset had a predictive attribute called "outcome" and the C5.0Control use this name, and this was the error cause :'( My solution was changing the column name. Other way, would be create a  C5.0Control object and change the value of the label attribute and then pass this object as parameter for the C50 method. I also struggled some hours with the same Problem (Return code "1") when building a model as well as when predicting.
With the hint of answer of Marco I have written a small function to remove all factor levels equal to "" in a data frame or vector, see code below. However, since R does not allow for pass by reference to functions, you have to use the result of the function (it can not change the original dataframe): Call of the functions may look like this: However, it seems, that C50 has a similar Problem with character columns containing an empty cell, so you will have probably to extend this to handle also character attributes if you have some. I also got the same error, but it was because of some illegal characters in the factor levels of one the columns. I used make.names function and corrected the factor levels: Then the problem was resolved.I am trying to implement batch gradient descent on a data set with a single feature and multiple training examples (m). When I try using the normal equation, I get the right answer but the wrong one with this code below which performs batch gradient descent in MATLAB. y is the vector with target values, X is a matrix with the first column full of ones and second columns of values (variable). I have implemented this using vectorization, i.e  ... where delta is a 2 element column vector initialized to zeroes. The cost function J(Theta) is 1/(2m)*(sum from i=1 to m [(h(theta)-y)^2]). The error is very simple.  Your delta declaration should be inside the first for loop.  Every time you accumulate the weighted differences between the training sample and output, you should start accumulating from the beginning.   By not doing this, what you're doing is accumulating the errors from the previous iteration which takes the error of the the previous learned version of theta into account which isn't correct.  You must put this at the beginning of the first for loop. In addition, you seem to have an extraneous computeCost call.  I'm assuming this calculates the cost function at every iteration given the current parameters, and so I'm going to create a new output array called cost that shows you this at each iteration.  I'm also going to call this function and assign it to the corresponding elements in this array: FWIW, I don't consider this implementation completely vectorized.  You can eliminate the second for loop by using vectorized operations.  Before we do that, let me cover some theory so we're on the same page.  You are using gradient descent here in terms of linear regression.  We want to seek the best parameters theta that are our linear regression coefficients that seek to minimize this cost function:  m corresponds to the number of training samples we have available and x^{i} corresponds to the ith training example.  y^{i} corresponds to the ground truth value we have associated with the ith training sample.  h is our hypothesis, and it is given as:  Note that in the context of linear regression in 2D, we only have two values in theta we want to compute - the intercept term and the slope. We can minimize the cost function J to determine the best regression coefficients that can give us the best predictions that minimize the error of the training set.  Specifically, starting with some initial theta parameters... usually a vector of zeroes, we iterate over iterations from 1 up to as many as we see fit, and at each iteration, we update our theta parameters by this relationship:  For each parameter we want to update, you need to determine the gradient of the cost function with respect to each variable and evaluate what that is at the current state of theta.  If you work this out using Calculus, we get:  If you're unclear with how this derivation happened, then I refer you to this nice Mathematics Stack Exchange post that talks about it:   https://math.stackexchange.com/questions/70728/partial-derivative-in-gradient-descent-for-two-variables Now... how can we apply this to our current problem?  Specifically, you can calculate the entries of delta quite easily analyzing all of the samples together in one go.  What I mean is that you can just do this: The operations on delta(1) and delta(2) can completely be vectorized in a single statement for both.  What you are doing theta^{T}*X^{i} for each sample i from 1, 2, ..., m.  You can conveniently place this into a single sum statement. We can go even further and replace this with purely matrix operations.  First off, what you can do is compute theta^{T}*X^{i} for each input sample X^{i} very quickly using matrix multiplication.  Suppose if:  Here, X is our data matrix which composes of m rows corresponding to m training samples and n columns corresponding to n features.  Similarly, theta is our learned weight vector from gradient descent with n+1 features accounting for the intercept term. If we compute X*theta, we get:  As you can see here, we have computed the hypothesis for each sample and have placed each into a vector.  Each element of this vector is the hypothesis for the ith training sample.  Now, recall what the gradient term of each parameter is in gradient descent:  We want to implement this all in one go for all of the parameters in your learned vector, and so putting this into a vector gives us:  Finally:  Therefore, we know that y is already a vector of length m, and so we can very compactly compute gradient descent at each iteration by: .... so your code is now just:I am  trying to build a classifier with LightGBM on a very imbalanced dataset. Imbalance is in the ratio 97:3, i.e.: Params I used and the code for training is as shown below. I ran CV to get the best model and best round. I got 0.994 AUC on CV and similar score in Validation set. But when I am predicting on the test set I am getting very bad results. I am sure that the train set is sampled perfectly. What parameters are needed to be tuned.? What is the reason for the problem.? Should I resample the dataset such that the highest class is reduced.? The issue is that, despite the extreme class imbalance in your dataset, you are still using the "default" threshold of 0.5 when deciding the final hard classification in This should not be the case here. This is a rather big topic, and I strongly suggest you do your own research (try googling for threshold or cut off probability imbalanced data), but here are some pointers to get you started... From a relevant answer at Cross Validated (emphasis added): Don't forget that you should be thresholding intelligently to make predictions. It is not always best to predict 1 when the model probability is greater 0.5. Another threshold may be better. To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold. From a relevant academic paper, Finding the Best Classification Threshold in Imbalanced Classification: 2.2. How to set the classification threshold for the testing set Prediction
results
are
ultimately
determined
according
to
prediction
probabilities.
The
threshold
is
typically
set
to
0.5.
If
the
prediction
probability
exceeds
0.5,
the
sample
is
predicted
to
be
positive;
otherwise,
negative.
However,
0.5
is
not
ideal
for
some
cases,
particularly
for
imbalanced
datasets. The post Optimizing Probability Thresholds for Class Imbalances from the (highly recommended) Applied Predictive Modeling blog is also relevant. Take home lesson from all the above: AUC is seldom enough, but the ROC curve itself is often your best friend... On a more general level regarding the role of the threshold itself in the classification process (which, according to my experience at least, many practitioners get wrong), check also the Classification probability threshold thread (and the provided links) at Cross Validated; key point: the statistical component of your exercise ends when you output a probability for each class of your new sample. Choosing a threshold beyond which you classify a new observation as 1 vs. 0 is not part of the statistics any more. It is part of the decision component.I have read the CNN Tutorial on the TensorFlow and I am trying to use the same model for my project. 
The problem is now in data reading. I have around 25000 images for training and around 5000 for testing and validation each. The files are in png format and I can read them and convert them into the numpy.ndarray.  The CNN example in the tutorials use a queue to fetch the records from the file list provided. I tried to create my own such binary file by reshaping my images into 1-D array and attaching a label value in the front of it. So my data looks like this  The single row of the above array is of length 22501 size where the first element is the label. I dumped the file to using pickle and the tried to read from the file using the 
tf.FixedLengthRecordReader to read from the file as demonstrated in example I am doing the same things as given in the cifar10_input.py to read the binary file and putting them into the record object. Now when I read from the files the labels and the image values are different. I can understand the reason for this to be that pickle dumps the extra information of braces and brackets also in the binary file and they change the fixed length record size.  The above example uses the filenames and pass it to a queue to fetch the files and then the queue to read a single record from the file.  I want to know if I can pass the numpy array as defined above instead of the filenames to some reader and it can fetch records one by one from that array instead of the files. Probably the easiest way to make your data work with the CNN example code is to make a modified version of read_cifar10() and use it instead: Write out a binary file containing the contents of your numpy array. This file is similar to the format used in CIFAR10 datafiles. You might want to generate multiple files in order to get read parallelism. Note that ndarray.tofile() writes binary data in row-major order with no other metadata; pickling the array will add Python-specific metadata that TensorFlow's parsing routines do not understand. Write a modified version of read_cifar10() that handles your record format. Modify distorted_inputs() to use your new dataset: This is intended to be a minimal set of steps, given your starting point. It may be more efficient to do the PNG decoding using TensorFlow ops, but that would be a larger change. In your question, you specifically asked: I want to know if I can pass the numpy array as defined above instead of the filenames to some reader and it can fetch records one by one from that array instead of the files. You can feed the numpy array to a queue directly, but it will be a more invasive change to the cifar10_input.py code than my other answer suggests. As before, let's assume you have the following array from your question: You can then define a queue that contains the entire data as follows: ...then call sess.run(enqueue_op) to populate the queue. Another—more efficient—approach would be to feed records to the queue, which you could do from a parallel thread (see this answer for more details on how this would work): Alternatively, to enqueue a batch at a time, which will be more efficient: I want to know if I can pass the numpy array as defined above instead
  of the filenames to some reader and it can fetch records one by one
  from that array instead of the files. tf.py_func, that wraps a python function and uses it as a TensorFlow operator, might help. Here's an example.  However, since you've mentioned that your images are stored in png files, I think the simplest solution would be to replace this: with this:NLTK package provides a method show_most_informative_features() to find the most important features for both class, with output like:  As answered in this question How to get most informative features for scikit-learn classifiers? , this can also work in scikit-learn. However, for binary classifier, the answer in that question only outputs the best feature itself.  So my question is, how can I identify the feature's associated class, like the example above (outstanding is most informative in pos class, and seagal is most informative in negative class)? EDIT: actually what I want is a list of most informative words for each class. How can I do that? Thanks! In the case of binary classification, it seems like the coefficient array has been flatten. Let's try to relabel our data with only two labels: [out]: So let's do some diagnostics: [out]: Seems like the features are counted and then when vectorized it was flattened to save memory, so let's try: [out]: Now we see some patterns... Seems like the higher coefficient favors a class and the other tail favors the other, so you can simply do this: [out]: Actually if you've read @larsmans comment carefully, he gave the hint on the binary classes' coefficient in How to get most informative features for scikit-learn classifiers? Basically you need: classifier.classes_ accesses the index of the class labels you have in the classifier vectorizer.get_feature_names() is self-explanatory sorted(zip(classifier.coef_[labelid], feature_names))[-n:] retrieves the coefficient of the classifier for a given class label and then sorts it in ascending order.  I'm going to use a simple example from https://github.com/alvations/bayesline Input file train.txt: Code: [out]: You can get the same with two classes on the left and right side:My model is a simple fully connected network like this: So, after saving the model I want to give input to layer 3. What I am doing right now is this: But it isn't working, i.e. I am getting errors like incompatible input, inputs should be tuple etc. The error message is: Is there any way I can pass my own inputs in middle of network and get the output instead of giving an input at the start and getting output from the end? Any help will be highly appreciated. First you must learn that in Keras when you apply a layer on an input, a new node is created inside this layer which connects the input and output tensors. Each layer may have multiple nodes connecting different input tensors to their corresponding output tensors. To build a model, these nodes are traversed and a new graph of the model is created which consists all the nodes needed to reach output tensors from input tensors (i.e. which you specify when creating a model: model = Model(inputs=[...], outputs=[...]). Now you would like to feed an intermediate layer of a model and get the output of the model. Since this is a new data-flow path, we need to create new nodes for each layer corresponding to this new computational graph. We can do it like this: Fortunately, your model consists of one-branch and we could simply use a for loop to construct the new model. However, for more complex models it may not be easy to do so and you may need to write more codes to construct the new model. Here is another method for achieving the same result. Initially create a new input layer and then connect it to the lower layers(with weights). For this purpose, first re-initialize these layers(with same name) and reload the corresponding weights from the parent model using  new_model.load_weights("parent_model.hdf5", by_name=True) This will load the required weights from the parent model.Just make sure you name your layers properly beforehand. This method will work for complex models with multiple inputs or branches.You just need to copy the same code for required layers, connect the new inputs and finally load the corresponding weights. You can easily use keras.backend.function for this purpose: Sorry for ugly function naming - do it best) I was having the same problem and the proposed solutions worked for me but I was looking for something more explicit, so here it is for future reference:  Reference:
https://keras.io/guides/functional_api/#shared-layersI am trying to teach my SVM algorithm using data of clicks and conversion by people who see the banners. The main problem is that the clicks is around 0.2% of all data so it's big disproportion in it. When I use simple SVM in testing phase it always predict only "view" class and never "click" or "conversion". In average it gives 99.8% right answers (because of disproportion), but it gives 0% right prediction if you check "click" or "conversion" ones. How can you tune the SVM algorithm (or select another one) to take into consideration the disproportion? The most basic approach here is to use so called "class weighting scheme" - in classical SVM formulation there is a C parameter used to control the missclassification count. It can be changed into C1 and C2 parameters used for class 1 and 2 respectively. The most common choice of C1 and C2 for a given C is to put where n1 and n2 are sizes of class 1 and 2 respectively. So you "punish" SVM for missclassifing the less frequent class much harder then for missclassification the most common one. Many existing libraries (like libSVM) supports this mechanism with class_weight parameters. Example using python and sklearn In particular, in sklearn you can simply turn on the automatic weighting by setting class_weight='auto'.   This paper describes a variety of techniques. One simple (but very bad method for SVM) is just replicating the minority class(s) until you have a balance: http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdfI am running Ridge regression with the use of glmnet R package. I noticed that the coefficients I obtain from glmnet::glmnet function are different from those I get by computing coefficients by definition (with the use of the same lambda value). Could somebody explain me why? Data (both: response Y and design matrix X) are scaled.   If you read ?glmnet, you will see that the penalized objective function of Gaussian response is: In case the ridge penalty 1/2 * ||beta_j||_2^2 is used, we have which is proportional to This is different to what we usually see in textbook regarding ridge regression: The formula you write: is for the textbook result; for glmnet we should expect: So, the textbook uses penalized least squares, but glmnet uses penalized mean squared error. Note I did not use your original code with t(), "%*%" and solve(A) %*% b; using crossprod and solve(A, b) is more efficient! See Follow-up section in the end. Now let's make a new comparison:  Note that I have set intercept = FALSE when I call cv.glmnet (or glmnet). This has more conceptual meaning than what it will affect in practice. Conceptually, our textbook computation has no intercept, so we want to drop intercept when using glmnet. But practically, since your X and Y are standardized, the theoretical estimate of intercept is 0. Even with intercepte = TRUE (glment default), you can check that the estimate of intercept is ~e-17 (numerically 0), hence estimate of other coefficients is not notably affected. The other answer is just showing this. Follow-up As for the using crossprod and solve(A, b) - interesting! Do you by chance have any reference to simulation comparison for that?  t(X) %*% Y will first take transpose X1 <- t(X), then do X1 %*% Y, while crossprod(X, Y) will not do the transpose. "%*%" is a wrapper for DGEMM for case op(A) = A, op(B) = B, while crossprod is a wrapper for op(A) = A', op(B) = B. Similarly tcrossprod for op(A) = A, op(B) = B'. A major use of crossprod(X) is for t(X) %*% X; similarly the tcrossprod(X) for X %*% t(X), in which case DSYRK instead of DGEMM is called. You can read the first section of Why the built-in lm function is so slow in R? for reason and a benchmark.  Be aware that if X is not a square matrix, crossprod(X) and tcrossprod(X) are not equally fast as they involve different amount of floating point operations, for which you may read the side notice of Any faster R function than “tcrossprod” for symmetric dense matrix multiplication? Regarding solvel(A, b) and solve(A) %*% b, please read the first section of How to compute diag(X %% solve(A) %% t(X)) efficiently without taking matrix inverse? Adding on top of Zheyuan's interesting post, did some more experiments to see that we can get the same results with intercept as well, as follows:As part of the Enron project, built the attached model, Below is the summary of the steps, Used Kbest to find out the scores and sorted the features and trying a combination of higher and lower scores. Used SVM with a GridSearch using a StratifiedShuffle  Used the best_estimator_  to predict and calculate the precision and recall.  The problem is estimator is spitting out perfect scores, in some case 1  But when I refit the best classifier on training data then run the test it gives reasonable scores.  My doubt/question was what exactly GridSearch does with the test data after the split using the Shuffle split object we send in to it. I assumed it would not fit anything on Test data, if that was true then when I predict using the same test data, it should not give this high scores right.? since i used random_state value, the shufflesplit should have created the same copy for the Grid fit and also for the predict.  So, is using the same Shufflesplit for two wrong?  GridSearchCV as @Gauthier Feuillen said is used to search best parameters of an estimator for given data.
Description of GridSearchCV:- Because of last step, you are getting different scores in first and second approach. Because in the first approach, all data is used for training and you are predicting for that data only. Second approach has prediction on previously unseen data. Basically the grid search will: So your second case is the good one. Otherwise you are actually predicting data that you trained with (which is not the case in the second option, there you only keep the best parameters from your gridsearch)I have the following Python test code (the arguments to ALS.train are defined elsewhere): Which works, because it has a count of 1 against the predictions variable and outputs: However, when I try and use an RDD I created myself using the following code, it doesn't appear to work anymore: Which outputs: As you can see, predictAllcomes back empty when passed the mapped RDD. The values going in are both of the same format. The only noticeable difference that I can see is that the first example uses parallelize and produces a ParallelCollectionRDDwhereas the second example just uses a map which produces a PythonRDD. Does predictAll only work if passed a certain type of RDD? If so, is it possible to convert between RDD types? I'm not sure how to get this working.  There are two basic conditions under which MatrixFactorizationMode.predictAll may return a RDD with lower number of items than the input: You can easily reproduce this behavior and check that it is is not dependent on the way how RDD has been created. First lets use example data to build a model: Next lets see which products and users are present in the training data: Now lets create test data and check predictions: So far so good. Next lets map it using the same logic as in your code: Still fine. Next lets create invalid data and repeat experiment: As expected there are no predictions for invalid input. Finally you can confirm this is really the case by using ML model which is completely independent in training / prediction from Python code: As you can see no corresponding user / item in the training data means no prediction.My input is a array of 64 integers. I have 10,000 of these arrays in my training set.  And I supposed to be specifying this in order for conv1D to work? I am getting the dreaded error and I really don't understand what I need to do. Don't let the name confuse you. The layer tf.keras.layers.Conv1D needs the following shape: (time_steps, features). If your dataset is made of 10,000 samples with each sample having 64 values, then your data has the shape (10000, 64), which is not directly applicable to the tf.keras.layers.Conv1D layer. You are missing the time_steps dimension. What you can do is use the tf.keras.layers.RepeatVector, which repeats your array input n times, in the example 5. This way your Conv1D layer gets an input of the shape (5, 64). Check out the documentation for more information: As a side note, you should ask yourself if using a tf.keras.layers.Conv1D layer is the right option for your use case. This layer is usually used for NLP and other time series tasks. For example, in sentence classification, each word in a sentence is usually mapped to a high-dimensional word vector representation, as seen in the image. This results in data with the shape (time_steps, features).                                            If you want to use character one hot encoded embeddings it would look something like this:                                            This is a simple example of one single sample with the shape (10, 10) --> 10 characters along the time series dimension and 10 features. It  should help you understand the tutorial I mentioned a bit better. The Conv1D layer does temporal convolution, that is, along the first dimension (not the batch dimension of course), so you should put something like this: You will need to slice your data into time_steps temporal slices to feed the network. However, if your arrays don't have a temporal structure, then conv1D is not the layer you are looking for.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 3 years ago. I have worked recently with the DARPA network traffic packets and the derived version of it used in KDD99 for intrusion detection evaluation.  Excuse my limited domain knowledge in computer networks, I could only derive 9 features from the DARPA packet headers. and Not the 41 features used in KDD99.  I am intending to continue my work on the UNB ISCX Intrusion Detection Evaluation DataSet. However, I want to derive from the pcap files the 41 features used in the KDD99 and save it in a CSV format. Is there a fast/easy way to achieve this? Be careful with this data set. http://www.kdnuggets.com/news/2007/n18/4i.html Some excerpts: the artificial data was generated using a closed network, some proprietary network traffic generators, and hand-injected attacks Among the issues raised, the most important seemed to be that no validation was ever performed to show that the DARPA dataset actually looked like real network traffic. In 2003, Mahoney and Chan built a trivial intrusion detection system and ran it against the DARPA tcpdump data. They found numerous irregularities, including that -- due to the way the data was generated -- all the malicious packets had a TTL of 126 or 253 whereas almost all the benign packets had a TTL of 127 or 254. the DARPA dataset (and by extension, the KDD Cup '99 dataset) was fundamentally broken, and one could not draw any conclusions from any experiments run using them we strongly recommend that (1) all researchers stop using the KDD Cup '99 dataset As for the feature extraction used. IIRC the majority of features simply were attributes of the parsed IP/TCP/UDP headers. Such as, port number, last octet of IP, and some packet flags. As such, these findings no longer reflect realistic attacks anymore anyway. Todays TCP/IP stacks are much more robust than at the time the data set was created, where a "ping of death" would instantly lock up a windows host. Every developer of a TCP/IP stack should by now be aware of the risk of such malformed packets and stress-test the stack against such things. With this, these features have become pretty much meaningless. Incorrectly set SYN flags etc. are no longer used in network attacks; these are much more sophisticated; and most likely no longer attacking the TCP/IP stack, but the services running on the next layer. So I would not bother finding out which low level packet flags were used in that '99 flawed simulation using attacks that worked in the early '90s...If I am using two method (NN and KNN) with caret and then I want to provide significance test, how can I do wilcoxon test. I provided sample of my data as follows How to perform wilcox.test() here. One way to deal with your problem is to generate several performance values for knn and NN which you can compare using a statistical test. This can be achieved using Nested resampling. In nested resampling you are performing train/test splits multiple times and evaluating the model on each test set. Lets for instance use BostonHousing data: lets just select numerical columns for the example to make it simple: As far as I know there is no way to perform nested CV in caret out of the box so a simple wrapper is needed: generate outer folds for nested CV: Lets use bootstrap resampling as the inner resample loop to tune the hyper parameters: now loop over the outer folds and perform hyper parameter optimization using the train set and predict on the test set: extract just MAE from the results: Do the same for glmnet learner for instance: now compare the two using wilcox.test. Since the performance for both learners was generated using the same data splits a paired test is appropriate: If comparing more than two algorithms one can use friedman.test Does this work for you?What I want to do: I wish to compute a cross_val_score using roc_auc on a multiclass problem What I tried to do: Here is a reproducible example made with iris data set. I one hot encode my target  I use a decision tree classifier Finaly I perform cross val What is failing: This last line throw the following error My env: python==3.7.2 sklearn==0.19.2 My question: Is it a bug, or I'm making a miss-use? An unnecessary annoyance with the cross-validation functionality of scikit-learn is that, by default, the data are not shuffled; it would arguably be a good idea to make shuffling the default choice - of course, this would pre-suppose that a shuffling argument would be available for cross_val_score in the first place, but unfortunately it is not (docs). So, here is what is happening; the 150 samples of the iris dataset are stratified: Now, a 3-fold CV procedure with 150 samples stratified as shown above and an error message saying: should hopefully start making sense: in each one of your 3 validation folds only one label is present, so no ROC calculation is possible (let alone the fact that in each validation fold the model sees labels unseen in the respective training folds). So, just shuffle your data before: and you should be fine.Given a vector of bits v, compute the collection of bits that have Hamming distance 1 with v, then with distance 2, up to an input parameter t. So for How to efficiently compute this? The vector won't be always of dimension 3, e.g. it could be 6. This will run numerous time in my real code, so some efficiency would be welcome as well (even by paying more memory). My attempt: Output: First: There is a bijection between hamming dist k bit-vectors and subsets (of n aka v.size()) of kardinality k (the set of indices with changed bits). Hence, I'd  enumerate the subsets of changed indices instead. A quick glance at the SO history shows this reference. You'd have to keep track of the correct cardinalitites of course. Considering efficiency is probably pointless, since the solution to your problem is exponential anyways. If Hamming distance h(u, v) = k, then u^v has exactly k bits set. In other words, computing u ^ m over all masks m with k bits set gives all words with the desired Hamming distance. Notice that such set of mask does not depend on u. That is, for n and t reasonably small, precompute sets of masks with k bits set, for all k in 1,t, and iterate over these sets as required. If you don't have enough memory, you may generate the k-bit patterns on the fly. See this discussion for details. Output: In response to Kastrinis' answer, I would like to verify that this can be extended to my basis example, like this: where the output is identical.  PS - I am also toggling the bit with a different way.I would like to build a GBM model with H2O. My data set is imbalanced, so I am using the balance_classes parameter. For grid search (parameter tuning) I would like to use 5-fold cross validation. I am wondering how H2O deals with class balancing in that case. Will only the training folds be rebalanced? I want to be sure the test-fold is not rebalanced. In class imbalance settings, artificially balancing the test/validation set does not make any sense: these sets must remain realistic, i.e. you want to test your classifier performance in the real world setting, where, say, the negative class will include the 99% of the samples, in order to see how well your model will do in predicting the 1% positive class of interest without too many false positives. Artificially inflating the minority class or reducing the majority one will lead to performance metrics that are unrealistic, bearing no real relation to the real world problem you are trying to solve. For corroboration, here is Max Kuhn, creator of the caret R package and co-author of the (highly recommended) Applied Predictive Modelling textbook, in Chapter 11: Subsampling For Class Imbalances of the caret ebook: You would never want to artificially balance the test set; its class frequencies should be in-line with what one would see “in the wild”. Re-balancing makes sense only in the training set, so as to prevent the classifier from simply and naively classifying all instances as negative for a perceived accuracy of 99%. Hence, you can rest assured that in the setting you describe the rebalancing takes action only for the training set/folds. A way to force balancing is using a weight columns to use different weights for different classes, in H2O weights_columnI'm currently using the Cross Entropy Loss function but with the imbalance data-set the performance is not great. Is there better lost function? It's a very broad subject, but IMHO, you should try focal loss: It was introduced by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Dollar to handle imbalance prediction in object detection. Since introduced it was also used in the context of segmentation.
The idea of the focal loss is to reduce both loss and gradient for correct (or almost correct) prediction while emphasizing the gradient of errors. As you can see in the graph:
 Blue curve is the regular cross entropy loss: it has on the one hand non-negligible loss and gradient even for well classified examples, and on the other hand it has weaker gradient for the erroneously classified examples.
In contrast, focal loss (all other curves) has smaller loss and weaker gradient for the well classified examples and stronger gradients for the  erroneously classified examples.I was trying sklearn pipeline for the first time and using Titanic dataset. I want to first impute missing value in Embarked and then do one hot encoding. While in Sex attribute, I just want to do one hot encoding. So, I have the below steps in which two steps are for Embarked. But it is not working as expected as the Embarked column remains in addition to its one hot encoding as shown in the output(column having 'S'). If I do imputation and one hot encoding for Embarked in single step, it is working as expected. What is the reason behind this or I am doing something wrong? Also, I didn't find any information related to this.  ColumnTransformer transformers are applied in parallel, not sequentially.  So in your example, Embarked ends up in your transformed data twice: once from the first transformer, keeping its string type, and again from the second transformer, this time one-hot encoded (but not imputed first!(?)). So just uncomment the second step in the embarked pipeline, and remove Embarked from categorical_cols. See also Consistent ColumnTransformer for intersecting lists of columns (but I don't think it's quite a duplicate).I want to develop a CNN model to identify 24 hand signs in American Sign Language. I created a custom dataset that contains 3000 images for each hand sign i.e. 72000 images in the entire dataset. For training the model, I would be using 80-20 dataset split (2400 images/hand sign in the training set and 600 images/hand sign in the validation set).  My question is:
Should I randomly shuffle the images when creating the dataset? And Why? Based on my previous experience, it led to validation loss being lower than training loss and validation accuracy more than training accuracy. Check this link. Random shuffling of data is a standard procedure in all machine learning pipelines, and image classification is not an exception; its purpose is to break possible biases during data preparation - e.g. putting all the cat images first and then the dog ones in a cat/dog classification dataset.  Take for example the famous iris dataset: As you can clearly see, the dataset has been prepared in such a way that the first 50 samples are all of label 0, the next 50 of label 1, and the last 50 of label 2. Try to perform a 5-fold cross validation in such a dataset without shuffling and you'll find most of your folds containing only a single label; try a 3-fold CV, and all your folds will include only one label. Bad... BTW, it's not just a theoretical possibility, it has actually happened. Even if no such bias exists, shuffling never hurts, so we do it always just to be on the safe side (you never know...).  Based on my previous experience, it led to validation loss being lower than training loss and validation accuracy more than training accuracy. Check this link. As noted in the answer there, it is highly unlikely that this was due to shuffling. Data shuffling is not anything sophisticated - essentially, it is just the equivalent of shuffling a deck of cards; it may have happened once that you insisted on "better" shuffling and subsequently you ended up with a straight flush hand, but obviously this was not due to the "better" shuffling of the cards. Here is my two cents on the topic.  First of all make sure to extract a test set that has equal number of samples for each hand sign. (hand sign #1 - 500 samples, hand sign #2 - 500 samples and so on)
I think this is referred to as stratified sampling. When it comes to the training set, there is no huge mistake in shuffling the entire set. However, when splitting the training set into training and validation set make sure that the validation set is good enough to be a representation for the test set.  One of my personal experiences with shuffling:
After splitting the training set into training and validation sets, the validation set turned out to be very easy to predict. Therefore, I saw  good learning metric values. However, the performance of the model on the test set was horrible.I am learning about sklearn custom transformers and read about the two core ways to create custom transformers: I wanted to compare these two approaches by implementing a "meta-vectorizer" functionality: a vectorizer that supports either CountVectorizer or TfidfVectorizer and transforms the input data according to the specified vectorizer type. However, I can't seem to get any of the two work when passing them to a sklearn.pipeline.Pipeline. I am getting the following error message in the fit_transform() step: My code for option 1 (using a custom class): And my code for option 2 (creating a custom transformer from a function using FunctionTransformer): Imports and sample data: The issue is that both CountVectorizer and TfidfVectorizer require their input to be 1D (and not 2D). In such cases the doc of ColumnTransformer states that parameter columns of the transformers tuple should be passed as a string rather than as a list. columns: str, array-like of str, int, array-like of int, array-like of bool, slice or callable Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector. Therefore, the following will work in your case (i.e. changing ['Text'] into 'Text'). You can adjust the example with FunctionTransformer accordingly. Observe, as a final remark, that I had to pass handle_unknown='ignore' to OneHotEncoder to prevent the possibility that an error would have arisen in case of unknown categories seen during the test phase of your cross-validation (and not seen during the training phase).I am learning about the "kohonen" package in R for the purpose of making Self Organizing Maps (SOM, also called Kohonen Networks - a type of Machine Learning algorithm). I am following this R language tutorial over here: https://www.rpubs.com/loveb/som I tried to create my own data (this time with both "factor" and "numeric" variables) and run the SOM algorithm (this time using the "supersom()" function instead): From here, I was able to successfully make some of the basic plots: However, the problem arises when I try to make individual plots for each variable: This produces an error: "Error: Incorrect Number of Dimensions" A similar error (NAs by coercion) is produced when attempting to cluster the SOM Network: Can someone please tell me what I am doing wrong?
Thanks Sources: https://www.rdocumentation.org/packages/kohonen/versions/2.0.5/topics/supersom getCodes() produces a list and as such you have to treat it like one. Calling getCodes(som) produces a list containing 7 items named a-g as such you should be selecting items from the list either using $ or [[]] e.g or or if you must set the variable prior to calling the plot you can do so like: Regarding kmeans() kmeans() needs a matrix or an object that can be coerced into a matrix, you have factors (categorical data) which cannot be coerced into numeric, either drop the factors, or find another method. drop the factors: edit:
Alternatively you can specify the code directly from getCodes() by using idx like so:Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 3 years ago. Can someone explain what the difference is between classification and clustering in data mining? If you can, please give examples of both to understand the main idea. In general, in classification you have a set of predefined classes and want to know which class a new object belongs to. Clustering tries to group a set of objects and find whether there is some relationship between the objects. In the context of machine learning, classification is supervised learning and clustering is unsupervised learning. Also have a look at Classification and Clustering at Wikipedia. Please read the following information:  
 If you have asked this question to any data mining or machine learning persons they will use the terms supervised learning and unsupervised learning to explain you the difference between clustering and classification. So let me first explain you about the key word supervised and unsupervised. Supervised learning:
suppose you have a basket and it is filled with some fresh fruits and your task is to arrange the same type fruits at one place. suppose the fruits are apple,banana,cherry, and grape.
so you already know from your previous work that, the shape of each and every fruit so it is easy to arrange the same type of fruits at one place.
here your previous work is called as trained data in data mining.
so you already learn the things from your trained data, This is because of you have a response variable which says you that if some fruit have so and so features it is grape, like that for each and every fruit. This type of data you will get from the trained data.
This type of learning is called as supervised learning.
This type solving problem comes under Classification.
So you already learn the things so you can do you job confidently. unsupervised :
suppose you have a basket and it is filled with some fresh fruits and your task is to arrange the same type fruits at one place. This time you don't  know any thing about that fruits, you are first time seeing these fruits so how  will you arrange the same type of fruits. What you will do first  is you take on the fruit and you will select any physical character of that particular fruit. suppose you taken color. Then you will arrange them based on the color, then the  groups will be some thing like this.
RED COLOR GROUP: apples & cherry fruits.
GREEN COLOR GROUP: bananas & grapes.
so now you will take another physical character as size, so now the groups will be some thing like this.
RED COLOR AND BIG SIZE: apple.
RED COLOR AND SMALL SIZE: cherry fruits.
GREEN COLOR AND BIG SIZE: bananas.
GREEN COLOR AND SMALL SIZE: grapes.
job done happy ending. here you didn't  learn any thing before ,means no train data and no response variable.
This type of learning is known unsupervised learning.
clustering comes under unsupervised learning. +Classification:
you are given some new data, you have to set new label for them. For example, a company wants to classify their prospect customers. When a new customer comes, they have to determine if this is a customer who is going to buy their products or not. +Clustering:
you're given a set of history transactions which recorded who bought what. By using clustering techniques, you can tell the segmentation of your customers.  I am sure a number of you have heard about machine learning. A dozen of you might even know what it is. And a couple of you might have worked with machine learning algorithms too. 
You see where this is going? Not a lot of people are familiar with the technology that will be absolutely essential 5 years from now. Siri is machine learning. Amazon’s Alexa is machine learning. Ad and shopping item recommender systems are machine learning. 
Let’s try to understand machine learning with a simple analogy of a 2 year old boy. Just for fun, let’s call him Kylo Ren  Let’s assume Kylo Ren saw an elephant. What will his brain tell him ?(Remember he has minimum thinking capacity, even if he is the successor to Vader). His brain will tell him that he saw a big moving creature which was grey in color. He sees a cat next, and his brain tells him that it is a small moving creature which is golden in color. Finally, he sees a light saber next and his brain tells him that it is a non-living object which he can play with! His brain at this point knows that saber is different from the elephant and the cat, because the saber is something to play with and doesn’t move on its own. His brain can figure this much out even if Kylo doesn’t know what movable means. This simple phenomenon is called Clustering .  Machine learning is nothing but the mathematical version of this process.
A lot of people who study statistics realized that they can make some equations work in the same way as brain works. 
Brain can cluster similar objects, brain can learn from mistakes and brain can learn to identify things. All of this can be represented with statistics, and the computer based simulation of this process is called Machine Learning. Why do we need the computer based simulation? because computers can do heavy math faster than human brains. 
I would love to go into the mathematical/statistical part of machine learning but you don’t wanna jump into that without clearing some concepts first. Let’s get back to Kylo Ren. Let’s say Kylo picks up the saber and starts playing with it. He accidentally hits a stormtrooper and the stormtrooper gets injured. He doesn’t understand what’s going on and continues playing. Next he hits a cat and the cat gets injured. This time Kylo is sure he has done something bad, and tries to be somewhat careful. But given his bad saber skills, he hits the elephant and is absolutely sure that he is in trouble. 
He becomes extremely careful thereafter, and only hits his dad on purpose as we saw in Force Awakens!!  This entire process of learning from your mistake can be mimicked with equations, where the feeling of doing something wrong is represented by an error or cost. This process of identifying what not to do with a saber is called Classification . 
Clustering and Classification are the absolute basics of machine learning. Let’s look at the difference between them. Kylo differentiated between animals and light saber because his brain decided that light sabers cant move by themselves and are therefore, different. The decision was based solely upon the objects present (data) and no external help or advice was provided. 
In contrast to this, Kylo differentiated the importance of being careful with light saber by first observing what hitting an object can do. The decision wasn’t completely based on the saber, but on what it could do to different objects . In short, there was some help here.  Because of this difference in learning, Clustering is called an unsupervised learning method and Classification is called a supervised learning method. 
They are very different in the machine learning world, and are often dictated by the kind of data present. Obtaining labelled data (or things that help us learn , like stormtrooper,elephant and cat in Kylo’s case) is often not easy and becomes very complicated when the data to be differentiated is large. On the other hand, learning without labels can have it’s own disadvantages , like not knowing what are the label titles. 
If Kylo was to learn being careful with the saber without any examples or help, he wouldn’t know what it would do. He would just know that it is not suppose to be done. It’s kind of a lame analogy but you get the point! We are just getting started with Machine Learning. Classification itself can be classification of continuous numbers or classification of labels. For instance, if Kylo had to classify what each stormtrooper’s height is, there would be a lot of answers because the heights can be 5.0, 5.01, 5.011, etc. But a simple classification like types of light sabers (red,blue.green) would have very limited answers. Infact they can be represented with simple numbers. Red can be 0 , Blue can be 1 and Green can be 2. If you know basic math, you know that 0,1,2 and 5.1,5.01,5.011 are different and are called discrete and continuous numbers respectively. The classification of discrete numbers is called Logistic Regression , and classification of continuous numbers is called Regression. 
Logistic Regression is also known as categorical classification, so don’t be confused when you read this term elsewhere This was a very basic introduction to Machine Learning. I will dwell into the statistical side in my next post. Please let me know if I need any corrections :) Second part posted here.
 I'm a new comer to Data Mining, but as my textbook says, CLASSICIATION is supposed to be supervised learning, and CLUSTERING unsupervised learning. The difference between supervised learning and unsupervised learning can be found here. Is the assignment of predefined classes to new observations, based on learning from examples. It is one of the key tasks in machine learning. While popularly dismissed as "unsupervised classification" it is quite different. In contrast to what many machine learners will teach you, it is not about assigning "classes" to objects, but without having them predefined. This is the very limited view of people who did too much classification; a typical example of if you have a hammer (classifier), everything looks like a nail (classification problem) to you. But it is also why classification people do not get a hang of clustering. Instead, consider it as structure discovery. The task of clustering is to find structure (e.g. groups) in your data that you did not know before. Clustering has been successful if you learned something new. It failed, if you only got the structure you already knew. Cluster analysis is a key task of data mining (and the ugly duckling in machine-learning, so don't listen to machine learners dismissing clustering). This has been iterated up and down the literature, but unsupervised learning is bllsht. It does not exist, but it is an oxymoron like "military intelligence". Either the algorithm learns from examples (then it is "supervised learning"), or it does not learn. If all the clustering methods are "learning", then computing the minimum, maximum and average of a data set is "unsupervised learning", too. Then any computation "learned" its output. Thus the term 'unsupervised learning' is totally meaningless, it means everything and nothing. Some "unsupervised learning" algorithms do, however, fall into the optimization category. For example k-means is a least-squares optimization. Such methods are all over statistics, so I don't think we need to label them "unsupervised learning", but instead should continue to call them "optimization problems". It's more precise, and more meaningful.
There are plenty of clustering algorithms who do not involve optimization, and who do not fit into machine-learning paradigms well. So stop squeezing them in there under the umbrella "unsupervised learning". There is some "learning" associated with clustering, but it is not the program that learns. It is the user that is supposed to learn new things about his data set. By clustering, you can group data with your desired properties such as the number, the shape, and other properties of extracted clusters. While, in classification, the number and the shape of groups are fixed.
Most of the clustering algorithms give the number of clusters as a parameter. However, there are some approaches to find out the appropriate number of clusters. First of all, like many answers state here: classification is supervised learning and clustering is unsupervised. This means: Classification needs labeled data so the classifiers can be trained on this data, and after that start classifying new unseen data based on what he knows. Unsupervised learning like clustering does not uses labeled data, and what it actually does is to discover intrinsic structures in the data like groups.  Another difference between both techniques (related to the previous one), is the fact that classification is a form of discrete regression problem where the output is a categorical dependent variable. Whereas clustering's output yields a set of subsets called groups. The way to evaluate these two models is also different for the same reason: in classification you often have to check for the precision and recall, things like overfitting and underfitting, etc. Those things will tell you how good is the model. But in clustering you usually need the vision of and expert to interpret what you find, because you don't know what type of structure you have (type of group or cluster). That's why clustering belongs to exploratory data analysis.  Finally, i would say that applications are the main difference between both. Classification as the word says, is used to discriminate instances that belong to a class or another, for example a man or a woman, a cat or a dog, etc. Clustering is often used in the diagnosis of medical illness, discovery of patterns, etc.  Classification: Predict results in a discrete output => map input variables into discrete categories  Popular use cases: Email classification : Spam or non-Spam Sanction loan to customer : Yes if he is capable of paying EMI for the sanctioned loan amount. No if he can't Cancer tumour cells identification : Is it critical or non-critical? Sentiment analysis of tweets : Is the tweet positive or negative or neutral Classification of news  : Classify the news into one of predefined classes - Politics, Sports, Health etc Clustering: is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)   Popular use cases: Marketing : Discover customer segments for marketing purposes Biology : Classification among different species of plants and animals Libraries : Clustering different books on the basis of topics and information Insurance : Acknowledge the customers, their policies and identifying the frauds City Planning : Make groups of houses and to study their values based on their geographical locations and other factors. Earthquake studies : Identify  dangerous zones Recommendation system :  References: geeksforgeeks dataaspirant 3leafnodes Classification
– Predicts categorical class labels
– Classifies data (constructs a model) based on a training set and the values (class labels) in a class label attribute 
– Uses the model in classifying new data Cluster: a collection of data objects
– Similar to one another within the same cluster
– Dissimilar to the objects in other clusters Clustering aims at finding groups in data. “Cluster” is an intuitive concept and does
not have a mathematically rigorous definition. The members of one cluster should be
similar to one another and dissimilar to the members of other clusters. A clustering
algorithm operates on an unlabeled data set Z and produces a partition on it. For Classes and Class Labels,
class contains similar objects, whereas objects from different classes
are dissimilar. Some classes have a clear-cut meaning, and in the simplest case
are mutually exclusive. For example, in signature verification, the signature is either
genuine or forged. The true class is one of the two, no matter that we might not be
able to guess correctly from the observation of a particular signature. Clustering is a method of grouping objects in such a way that objects with similar features come together, and objects with dissimilar features go apart. It is a common technique for statistical data analysis used in machine learning and data mining..  Classification is a process of categorization where objects are recognized, differentiated and understood on the basis of the training set of data. Classification is a supervised learning technique where a training set and correctly defined observations are available. From book Mahout in Action, and I think it explains the difference very well: Classification algorithms are related to, but still quite different from, clustering algorithms such as the k-means algorithm. Classification algorithms are a form of supervised learning, as opposed to unsupervised learning, which happens with clustering algorithms. A supervised learning algorithm is one that’s given examples that contain the desired value of a target variable. Unsupervised algorithms aren’t given the desired answer, but instead must find something plausible on their own. One liner for Classification: Classifying data into pre-defined categories One liner for Clustering: Grouping data into a set of categories Key difference: Classification is taking data and putting it into pre-defined categories and in Clustering the set of categories, that you want to group the data into, is not known beforehand. Conclusion: I have written a long post on the same topic which you can find here:  https://neelbhatt40.wordpress.com/2017/11/21/classification-and-clustering-machine-learning-interview-questions-answers-part-i/ If you are trying to file up a large number of sheets on to your shelf(based on date or some other specification of the file), you are CLASSIFYING. If you were to create clusters from the set of sheets, it would mean that there is something similar among the sheets. There are two definitions in data mining "Supervised" and "Unsupervised".
When someone tells the computer, algorithm, code, ... that this thing is like an apple and that thing is like an orange, this is supervised learning and using supervised learning (like tags for each sample in a data set) for classifying the data, you'll get classification. But on the other hand if you let the computer find out what is what and differentiate between features of the given data set, in fact learning unsupervised, for classifying the data set this would be called clustering. In this case data that are fed to the algorithm don't have tags and the algorithm should find out different classes. Machine Learning or AI is largely perceived by the task it Performs/achieves. In my opinion, by thinking about Clustering and Classification in notion of task they achieve can really help to understand the difference between the two. Clustering is to Group things and Classification is to, kind of, label things. Let's assume you are in a party hall where all men are in Suits and women are in Gowns. Now, you ask your friend few questions: Q1: Heyy, can you help me group people? Possible answers that your friend can give are: 1: He can group people based on Gender, Male or Female 2: He can group people based on their clothes, 1 wearing suits other wearing gowns 3: He can group people based on color of their hairs 4: He can group people based on their age group, etc. etc. etc. Their are numerous ways your friend can complete this task. Of course, you can influence his decision making process by providing extra inputs like: Can you help me group these people based on gender (or age group, or hair color or dress etc.) Q2: Before Q2, you need to do some pre-work. You have to teach or inform your friend so that he can take informed decision. So, let's say you said to your friend that: People with long hair are Women. People with short hair are Men. Q2.  Now, you point out to a Person with long hair and ask your friend - Is it a Man or a Woman? The only answer that you can expect is: Woman. Of course, there can be men with long hairs and women with short hairs in the party. But, the answer is correct based on the learning you provided to your friend. You can further improve the process by teaching more to your friend on how to differentiate between the two. In above example, Q1 represents the task what Clustering achieves. In Clustering you provide the data(people) to the algorithm(your friend) and ask it to group the data.  Now, it's up to algorithm to decide what's the best way to group is? (Gender, Color or age group). Again,you can definitely influence the decision made by the algorithm by providing extra inputs. Q2 represents the task Classification achieves. There, you give your algorithm(your friend) some data(People), called as Training data, and made him learn which data corresponds to which label(Male or Female). Then you point your algorithm to certain data, called as Test data, and ask it to determine whether it is Male or Female. The better your teaching is, the better it's prediction. And the Pre-work in Q2 or Classification is nothing but just training your model so that it can learn how to differentiate. In Clustering or Q1 this pre-work is the part of grouping. Hope this helps someone. Thanks  Classification- A data-set can have different groups/ classes. red, green and black. Classification will try to find rules that divides them in different classes.      Custering- if a data-set is not having any class and you want to put them in some class/grouping, you do clustering. The purple circles above.  If classification rules are not good, you will have mis-classification in testing or ur rules are not correct enough.
  if clustering is not good, you will have lot of outliers ie. data points not able to fall in any cluster. The Key Differences Between Classification and Clustering  are:
Classification is the process of classifying the data with the help of class labels. On the other hand, Clustering is similar to classification but there are no predefined class labels.
Classification is geared with supervised learning. As against, clustering is also known as unsupervised learning.
Training sample is provided in the classification method while in the case of clustering training data is not provided. Hope this will help! I believe classification is classifying records in a data set into predefined classes or even defining classes on the go. I look at it as pre-requisite for any valuable data mining, I like to think of it at unsupervised learning i.e. one does not know what he/she is looking for while mining the data and classification serves as a good starting point Clustering on the other end falls under supervised learning i.e. one know what parameters to look for, the correlation between them along with critical levels. I believe it requires some understanding of statistics and mathsI recently started studying deep learning and other ML techniques, and I started searching for frameworks that simplify the process of build a net and training it, then I found TensorFlow, having little experience in the field, for me, it seems that speed is a big factor for making a big ML system even more if working with deep learning, so why python was chosen by Google to make TensorFlow? Wouldn't it be better to make it over an language that can be compiled and not interpreted? What are the advantages of using Python over a language like C++ for machine learning? The most important thing to realize about TensorFlow is that, for the most part, the core is not written in Python:  It's written in a combination of highly-optimized C++ and CUDA (Nvidia's language for programming GPUs).  Much of that happens, in turn, by using Eigen (a high-performance C++ and CUDA numerical library) and NVidia's cuDNN (a very optimized DNN library for NVidia GPUs, for functions such as convolutions). The model for TensorFlow is that the programmer uses "some language" (most likely Python!) to express the model.  This model, written in the TensorFlow constructs such as: is not actually executed when the Python is run.  Instead, what's actually created is a dataflow graph that says to take particular inputs, apply particular operations, supply the results as the inputs to other operations, and so on.  This model is executed by fast C++ code, and for the most part, the data going between operations is never copied back to the Python code. Then the programmer "drives" the execution of this model by pulling on nodes -- for training, usually in Python, and for serving, sometimes in Python and sometimes in raw C++: This one Python (or C++ function call) uses either an in-process call to C++ or an RPC for the distributed version to call into the C++ TensorFlow server to tell it to execute, and then copies back the results. So, with that said, let's re-phrase the question:  Why did TensorFlow choose  Python as the first well-supported language for expressing and controlling the training of models? The answer to that is simple:  Python is probably the most comfortable language for a large range of data scientists and machine learning experts that's also that easy to integrate and have control a C++ backend, while also being general, widely-used both inside and outside of Google, and open source.  Given that with the basic model of TensorFlow, the performance of Python isn't that important, it was a natural fit.  It's also a huge plus that NumPy makes it easy to do pre-processing in Python -- also with high performance -- before feeding it in to TensorFlow for the truly CPU-heavy things. There's also a bunch of complexity in expressing the model that isn't used when executing it -- shape inference (e.g., if you do matmul(A, B), what is the shape of the resulting data?) and automatic gradient computation.  It turns out to have been nice to be able to express those in Python, though I think in the long term they'll probably move to the C++ backend to make adding other languages easier. (The hope, of course, is to support other languages in the future for creating and expressing models.  It's already quite straightforward to run inference using several other languages -- C++ works now, someone from Facebook contributed Go bindings that we're reviewing now, etc.) TF is not written in python. It is written in C++ (and uses high-performant numerical libraries and CUDA code) and you can check this by looking at their github. So the core is written not in python but TF provide an interface to many other languages (python, C++, Java, Go)  If you come from a data analysis world, you can think about it like numpy (not written in python, but provides an interface to Python) or if you are a web-developer - think about it as a database (PostgreSQL, MySQL, which can be invoked from Java, Python, PHP) Python frontend (the language in which people write models in TF) is the most popular due to many reasons. In my opinion the main reason is historical: majority of ML users already use it (another popular choice is R) so if you will not provide an interface to python, your library is most probably doomed to obscurity. But being written in python does not mean that your model is executed in python. On the contrary, if you written your model in the right way Python is never executed during the evaluation of the TF graph (except of tf.py_func(), which exists for debugging and should be avoided in real model exactly because it is executed on Python's side). This is different from for example numpy. For example if you do np.linalg.eig(np.matmul(A, np.transpose(A)) (which is eig(AA')), the operation will compute transpose in some fast language (C++ or fortran), return it to python, take it from python together with A, and compute a multiplication in some fast language and return it to python, then compute eigenvalues and return it to python. So nonetheless expensive operations like matmul and eig are calculated efficiently, you still lose time by moving the results to python back and force. TF does not do it, once you defined the graph your tensors flow not in python but in C++/CUDA/something else. Python allows you to create extension modules using C and C++, interfacing with native code, and still getting the advantages that Python gives you. TensorFlow uses Python, yes, but it also contains large amounts of C++. This allows a simpler interface for experimentation with less human-thought overhead with Python, and add performance by programming the most important parts in C++. The latest ratio you can check from here shows inside TensorFlow C++ takes ~50% of code, and Python takes ~40% of code. Both C++ and Python are the official languages at Google so there is no wonder why this is so. If I would have to provide fast regression where C++ and Python are present...  C++ is inside the computational algebra, and Python is used for everything else including for the testing. Knowing how ubiquitous the testing is today it is no wonder why Python code contributes that much to TF.I have five text files that I input to a CountVectorizer. When specifying min_df and max_df to the CountVectorizer instance what does the min/max document frequency exactly mean? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (five text files)? What are the differences when min_df and max_df are provided as integers or as floats? The documentation doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of these two parameters. Could someone provide an explanation or example demonstrating min_df and max_df? max_df is used for removing terms that appear too frequently, also known as "corpus-specific stop words". For example: The default max_df is 1.0, which means "ignore terms that appear in more than 100% of the documents". Thus, the default setting does not ignore any terms. min_df is used for removing terms that appear too infrequently. For example: The default min_df is 1, which means "ignore terms that appear in less than 1 document". Thus, the default setting does not ignore any terms. I would add this point also for understanding min_df and max_df in tf-idf better. If you go with the default values, meaning considering all terms, you have generated definitely more tokens. So your clustering process (or any other thing you want to do with those terms later) will take a longer time.  BUT the quality of your clustering should NOT be reduced.  One might think that allowing all terms (e.g. too frequent terms or stop-words) to be present might lower the quality but in tf-idf it doesn't. Because tf-idf measurement instinctively will give a low score to those terms, effectively making them not influential (as they appear in many documents). So to sum it up, pruning the terms via min_df and max_df is to improve the performance, not the quality of clusters (as an example). And the crucial point is that if you set the min and max mistakenly, you would lose some important terms and thus lower the quality. So if you are unsure about the right threshold (it depends on your documents set), or if you are sure about your machine's processing capabilities, leave the min, max parameters unchanged. As per the CountVectorizer documentation here. When using a float in the range [0.0, 1.0] they refer to the document frequency. That is the percentage of documents that contain the term. When using an int it refers to absolute number of documents that hold this term. Consider the example where you have 5 text files (or documents). If you set max_df = 0.6 then that would translate to 0.6*5=3 documents. If you set max_df = 2 then that would simply translate to 2 documents. The source code example below is copied from Github here and shows how the max_doc_count is constructed from the max_df. The code for min_df is similar and can be found on the GH page. The defaults for min_df and max_df are 1 and 1.0, respectively. This basically says "If my term is found in only 1 document, then it's ignored. Similarly if it's found in all documents (100% or 1.0) then it's ignored." max_df and min_df are both used internally to calculate max_doc_count and min_doc_count, the maximum and minimum number of documents that a term must be found in. This is then passed to self._limit_features as the keyword arguments high and low respectively, the docstring for self._limit_features is The defaults for min_df and max_df are 1 and 1.0, respectively. These defaults really don't do anything at all.   That being said, I believe the currently accepted answer by @Ffisegydd answer isn't quite correct. For example, run this using the defaults, to see that when min_df=1 and max_df=1.0, then  1) all tokens that appear in at least one document are used  (e.g., all tokens!) 2) all tokens that appear in all documents are used (we'll test with one candidate: everywhere).   We get:   All tokens are kept. There are no stopwords.   Further messing around with the arguments will clarify other configurations.   For fun and insight, I'd also recommend playing around with stop_words = 'english' and seeing that, peculiarly, all the words except 'seven' are removed! Including `everywhere'. The goal of MIN_DF is to ignore words that have very few occurrences to be considered meaningful. For example, in your text you may have names of people that may appear in only 1 or two documents. In some applications, this may qualify as noise and could be eliminated from further analysis. Similarly, you can ignore words that are too common with MAX_DF. Instead of using a minimum/maximum term frequency (total occurrences of a word) to eliminate words, MIN_DF and MAX_DF look at how many documents contained a term, better known as document frequency. The threshold values can be an absolute value (e.g. 1, 2, 3, 4) or a value representing proportion of documents (e.g. 0.25 meaning, ignore words that have appeared in 25% of the documents) . See some usage examples here. I just looked at the documentation for sklearn CountVectorizer.This is how I think about it. Common words have higher frequency values, while rare words have lower frequency values. The frequency values range between 0 - 1 as fractions. max_df is the upper ceiling value of the frequency values, while min_df is just the lower cutoff value of the frequency values. If we want to remove more common words, we set max_df to a lower ceiling value between 0 and 1. If we want to remove more rare words, we set min_df to a higher cutoff value between 0 and 1. We keep everything between max_df and min_df. Let me know, not sure if this makes sense.Currently I use the following code: It tells Keras to stop training when loss didn't improve for 2 epochs. But I want to stop training after loss became smaller than some constant "THR": I've seen in documentation there are possibility to make your own callback:
http://keras.io/callbacks/
But nothing found how to stop training process. I need an advice. I found the answer. I looked into Keras sources and find out code for EarlyStopping. I made my own callback, based on it: And usage: The keras.callbacks.EarlyStopping callback does have a min_delta argument. From Keras documentation: min_delta: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. One solution is to call model.fit(nb_epoch=1, ...) inside a for loop, then you can put a break statement inside the for loop and do whatever other custom control flow you want.   I solved the same problem using custom callback. In the following custom callback code assign THR with the value at which you want to stop training and add the callback to your model. While I was taking the TensorFlow in practice specialization, I learned a very elegant technique. Just little modified from the accepted answer.  Let's set the example with our favorite MNIST data.  So, here I set the metrics=['accuracy'], and thus in the callback class the condition is set to 'accuracy'> 0.90.  You can choose any metric and monitor the training like this example. Most importantly you can set different conditions for different metric and use them simultaneously.  Hopefully this helps!         For me the model would only stop training if I added a return statement after setting the stop_training parameter to True because I was calling after self.model.evaluate. So either make sure to put stop_training = True at the end of the function or add a return statement. If you're using a custom training loop, you can use a collections.deque, which is a "rolling" list which can be appended, and the left-hand items gets popped out when the list is longer than maxlen. Here's the line: Here's a full example:We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 4 years ago. I am looking for a method on how to calculate the number of layers and the number of neurons per layer. As input I only have the size of the input vector, the size of the output vector and the size of the training set. Usually the best net is determined by trying different net topologies and selecting the one with the least error. Unfortunately I cannot do that. This is a really hard problem. The more internal structure a network has, the better that network will be at representing complex solutions.  On the other hand, too much internal structure is slower, may cause training to diverge, or lead to overfitting -- which would prevent your network from generalizing well to new data. People have traditionally approached this problem in several different ways: Try different configurations, see what works best.  You can divide your training set into two pieces -- one for training, one for evaluation -- and then train and evaluate different approaches.  Unfortunately it sounds like in your case this experimental approach isn't available. Use a rule of thumb.  A lot of people have come up with a lot of guesses as to what works best.  Concerning the number of neurons in the hidden layer, people have speculated that (for example) it should (a) be between the input and output layer size, (b) set to something near (inputs+outputs) * 2/3, or (c) never larger than twice the size of the input layer.
The problem with rules of thumb is that they don't always take into account vital pieces of information, like how "difficult" the problem is, what the size of the training and testing sets are, etc.  Consequently, these rules are often used as rough starting points for the "let's-try-a-bunch-of-things-and-see-what-works-best" approach. Use an algorithm that dynamically adjusts the network configuration.  Algorithms like Cascade Correlation start with a minimal network, then add hidden nodes during training.  This can make your experimental setup a bit simpler, and (in theory) can result in better performance (because you won't accidentally use an inappropriate number of hidden nodes). There's a lot of research on this subject -- so if you're really interested, there is a lot to read.  Check out the citations on this summary, in particular: Lawrence, S., Giles, C.L., and Tsoi, A.C. (1996), "What size neural network gives optimal generalization? Convergence properties of backpropagation".  Technical Report UMIACS-TR-96-22 and CS-TR-3617, Institute for Advanced Computer Studies, University of Maryland, College Park. Elisseeff, A., and Paugam-Moisy, H. (1997), "Size of multilayer networks for exact learning: analytic approach".  Advances in Neural Information Processing Systems 9, Cambridge, MA: The MIT Press, pp.162-168. In practice, this is not difficult (based on having coded and trained dozens of MLPs). In a textbook sense, getting the architecture "right" is hard--i.e., to tune your network architecture such that performance (resolution) cannot be improved by further optimization of the architecture is hard, i agree. But only in rare cases is that degree of optimization required.  In practice, to meet or exceed the prediction accuracy from a neural network required by your spec, you almost never need to spend a lot of time with the network architecture--three reasons why this is true:  most of the parameters required to specify the network architecture
are fixed once you have decided on your data model (number of
features in the input vector, whether the desired response variable
is numerical or categorical, and if the latter, how many unique class
labels you've chosen); the few remaining architecture parameters that are in fact tunable,
are nearly always (100% of the time in my experience) highly constrained by those fixed architecture
parameters--i.e., the values of those parameters are tightly bounded by a max and min value; and the optimal architecture does not have to be determined before
training begins, indeed, it is very common for neural network code to
include a small module to programmatically tune the network
architecture during training (by removing nodes whose weight values
are approaching zero--usually called "pruning.")
  According to the Table above, the architecture of a neural network is completely specified by six parameters (the six cells in the interior grid). Two of those (number of layer type for the input and output layers) are always one and one--neural networks have a single input layer and a single output layer. Your NN must have at least one input layer and one output layer--no more, no less. Second, the number of nodes comprising each of those two layers is fixed--the input layer, by the size of the input vector--i.e., the number of nodes in the input layer is equal to the length of the input vector (actually one more neuron is nearly always added to the input layer as a bias node). Similarly, the output layer size is fixed by the response variable (single node for numerical response variable, and (assuming softmax is used, if response variable is a class label, the the number of nodes in the output layer simply equals the number of unique class labels). That leaves just two parameters for which there is any discretion at all--the number of hidden layers and the number of nodes comprising each of those layers. 
 if your data is linearly separable (which you often know by the time you begin coding a NN) then you don't need any hidden layers at all. (If that's in fact the case, i would not use a NN for this problem--choose a simpler linear classifier). 
The first of these--the number of hidden layers--is nearly always one. There is a lot of empirical weight behind this presumption--in practice very few problems that cannot be solved with a single hidden layer become soluble by adding another hidden layer. Likewise, there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very small. One hidden layer is sufficient for the large majority of problems. In your question, you mentioned that for whatever reason, you cannot find the optimum network architecture by trial-and-error. Another way to tune your NN configuration (without using trial-and-error) is 'pruning'. The gist of this technique is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look for weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on a network architecture, err on the side of more neurons, if you add a pruning step.  Put another way, by applying a pruning algorithm to your network during training, you can much closer to an optimized network configuration than any a priori theory is ever likely to give you.
 but what about the number of nodes comprising the hidden layer? Granted this value is more or less unconstrained--i.e., it can be smaller or larger than the size of the input layer. Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the famous NN FAQ for an excellent summary of that commentary). There are many empirically-derived rules-of-thumb, but of these, the most commonly relied on is the size of the hidden layer is between the input and output layers. Jeff Heaton, author of "Introduction to Neural Networks in Java" offers a few more, which are recited on the page i just linked to. Likewise, a scan of the application-oriented neural network literature, will almost certainly reveal that the hidden layer size is usually between the input and output layer sizes. But between doesn't mean in the middle; in fact, it is usually better to set the hidden layer size closer to the size of the input vector. The reason is that if the hidden layer is too small, the network might have difficultly converging. For the initial configuration, err on the larger size--a larger hidden layer gives the network more capacity which helps it converge, compared with a smaller hidden layer. Indeed, this justification is often used to recommend a hidden layer size larger than (more nodes) the input layer--ie, begin with an initial architecture that will encourage quick convergence, after which you can prune the 'excess' nodes (identify the nodes in the hidden layer with very low weight values and eliminate them from your re-factored network). I have used an MLP for a commercial software which has only one hidden layer which has only one node. As input nodes and output nodes are fixed, I only ever got to change the number of hidden layers and play with the generalisation achieved. I never really got a great difference in what I was achieving with just one hidden layer and one node by changing the number of hidden layers. I just used one hidden layer with one node. It worked quite well and also reduced computations were very tempting in my software premise.I want to separate my data into train and test set, should I apply normalization over data before or after the split? Does it make any difference while building predictive model? You first need to split the data into training and test set (validation set could be useful too). Don't forget that testing data points represent real-world data.
Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance). Therefore, you should perform feature normalisation over the training data.  Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points. For a more comprehensive read, you can read my article Feature Scaling and Normalisation in a nutshell As an example, assuming we have the following data: where X represents our features: and Y contains the corresponding label Step 1: Create training/testing sets Step 2: Normalise training data Step 3: Normalize testing data In the specific setting of a train/test split, we need to distinguish between two transformations: Two common examples of (1) are mean-centering (subtracting the mean of the feature) or scaling to unit variance (dividing by the standard deviation). Subtracting the mean and dividing by the standard deviation is a common transformation. In sklearn, it is implemented in sklearn.preprocessing.StandardScaler. Importantly, this is not the same as Normalizer. See below for exhaustive detail. An example of (2) is transforming a feature by taking the logarithm, or raising each value to a power (e.g. squaring). Transformations of the first type are best applied to the training data, with the centering and scaling values retained and applied to the test data afterwards. This is because using information about the test set to train the model may bias model comparison metrics to be overly optimistic. This can result in over-fitting & selection of a bogus model. Transformations of the second type can be applied without regard to train/test splits, because the modified value of each observation depends only on the data about the observation itself, and not on any other data or observation(s). This question has garnered some misleading answers. The rest of this answer is dedicated to showing how and why they are misleading. The term "normalization" is ambiguous, and different authors and disciplines will use the term "normalization" in different ways. In the absence of a specific articulation of what "normalization" means, I think it's best to approach the question in the most general sense possible. In this view, the question is not about sklearn.preprocessing.Normalizer specifically. Indeed, the Normalizer class is not mentioned in the question. For that matter, no software, programming language or library is mentioned, either. Moreover, even if the intent is to ask about Normalizer, the answers are still misleading because they mischaracterize what Normalizer does. Even within the same library, the terminology can be inconsistent. For example, PyTorch implements normalize torchvision.transforms.Normalize and torch.nn.functional.normalize. One of these can be used to create output tensors with mean 0 and standard deviation 1, while the other creates outputs that have a norm of 1. The Normalizer class is an example of (2) because it rescales each observation (row) individually so that the sum-of-squares is 1 for every row. (In the corner-case that a row has sum-of-squares equal to 0, no rescaling is done.) The first sentence of the documentation for the Normalizer says Normalize samples individually to unit norm. This simple test code validates this understanding: This prints True because the result is an array of 1s, as described in the documentation. The normalizer implements fit, transform and fit_transform methods even though some of these are just "pass-through" methods. This is so that there is a consistent interface across preprocessing methods, not because the methods' behaviors needs to distinguish between different data partitions. The Normalizer class does not subtract the column means Another answer writes: Don't forget that testing data points represent real-world data. Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. Ok, so let's try this out. Using the code snippet from the answer, we have This is the value of column_means_train_X. It is not zero! If the column means had been subtracted from the columns, then the centered column means would be 0.0. (This is simple to prove. The sum of n numbers x=[x1,x2,x3,...,xn] is S. The mean of those numbers is S / n. Then we have sum(x - S/n) = S - n * (S / n) = 0.) We can write similar code to show that the columns have not been divided by the variance. (Neither have the columns been divided by the standard deviation, which would be the more usual choice). Applying the Normalizer class to the whole data set does not change the result. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance). This claim is true as far as it goes, but it has absolutely no bearing on the Normalizer class. Indeed, Giorgos Myrianthous's chosen example is actually immune to the effect that they are describing. If the Normalizer class did involve the means of the features, then we would expect that the normalize results will change depending on which of our data are included in the training set. For example, the sample mean is a weighted sum of every observation in the sample. If we were computing column means and subtracting them, the results of applying this to all of the data would differ from applying it to only the training data subset. But we've already demonstrated that Normalizer doesn't subtract column means. Furthermore, these tests show that applying Normalizer to all of the data or just some of the data makes no difference for the results. If we apply this method separately, we have And if we apply it together, we have where the only difference is that we have 2 arrays in the first case, due to partitioning. Let's just double-check that the combined arrays are the same: No exception is raised; they're numerically identical. But sklearn's transformers are sometimes stateful, so let's make a new object just to make sure this isn't some state-related behavior. In the second case, we still have no exception raised. We can conclude that for the Normalizer class, it makes no difference if the data are partitioned or not. you can use fit then transform
learn transform Ask yourself if your data will look different depending on whether you transform before or after your split. If you're doing a log2 transformation, the order doesn't matter because each value is transformed independently of the others. If you're scaling and centering your data, the order does matter because an outlier can drastically change the final distribution. You're allowing the test set to "spill over" and affect your training set, potentially causing overly optimistic performance measures. For R uses, the caret package is good at handling test/train splits. You can add the argument preProcess = c("scale", "center") to the train function and it will automatically apply any transformation from the training data onto the test data. Tl;dr - if the data is different depending on whether your normalize before or after your split, do it beforeWant to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 5 years ago. Could you please explain what the "fit" method in scikit-learn does? Why is it useful? In a nutshell: fitting is equal to training. Then, after it is trained, the model can be used to make predictions, usually with a .predict() method call. To elaborate: Fitting your model to (i.e. using the .fit() method on) the training data is essentially the training part of the modeling process. It finds the coefficients for the equation specified via the algorithm being used (take for example umutto's linear regression example, above). Then, for a classifier, you can classify incoming data points (from a test set, or otherwise) using the predict method. Or, in the case of regression, your model will interpolate/extrapolate when predict is used on incoming data points. It also should be noted that sometimes the "fit" nomenclature is used for non-machine-learning methods, such as scalers and other preprocessing steps. In this case, you are merely "applying" the specified function to your data, as in the case with a min-max scaler, TF-IDF, or other transformation. Note: here are a couple of references...Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 4 years ago. Although both of the above methods provide a better score for the better closeness of prediction, still cross-entropy is preferred. Is it in every case or there are some peculiar scenarios where we prefer cross-entropy over MSE?  Cross-entropy is prefered for classification, while mean squared error is one of the best choices for regression. This comes directly from the statement of the problems itself - in classification you work with very particular set of possible output values thus MSE is badly defined (as it does not have this kind of knowledge thus penalizes errors in incompatible way).  To better understand the phenomena it is good to follow and understand the relations between You will notice that both can be seen as a maximum likelihood estimators, simply with different assumptions about the dependent variable. When you derive the cost function from the aspect of probability and distribution, you can observe that MSE happens when you assume the error follows Normal Distribution and cross entropy when you assume binomial distribution. It means that implicitly when you use MSE, you are doing regression (estimation) and when you use CE, you are doing classification. Hope it helps a little bit.  If you do logistic regression for example, you will use the sigmoid function to estimate de probability, the cross entropy as the loss function and gradient descent to minimize it. Doing this but using MSE as the loss function might lead to a non-convex problem where you might find local minima. Using cross entropy will lead to a convex problem where you might find the optimum solution. https://www.youtube.com/watch?v=rtD0RvfBJqQ&list=PL0Smm0jPm9WcCsYvbhPCdizqNKps69W4Z&index=35 There is also an interesting analysis here:
https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/I used Keras biomedical image segmentation to segment brain neurons. I used model.evaluate() it gave me Dice coefficient: 0.916. However, when I used model.predict(), then loop through the predicted images by calculating the Dice coefficient, the Dice coefficient is 0.82. Why are these two values different? The model.evaluate function predicts the output for the given input and then computes the metrics function specified in the model.compile and based on y_true and y_pred and returns the computed metric value as the output. The model.predict just returns back the y_pred So if you use model.predict and then compute the metrics yourself, the computed metric value should turn out to be the same as model.evaluate For example, one would use model.predict instead of model.evaluate in evaluating an RNN/ LSTM based models where the output needs to be fed as input in next time step The problem lies in the fact that every metric in Keras is evaluated in a following manner: Most of the most popular metrics (like mse, categorical_crossentropy, mae) etc. - as a mean of loss value of each example - have a property that such evaluation ends up with a proper result. But in case of Dice Coefficient - a mean of its value across all of the batches is not equal to actual value computed on a whole dataset and as model.evaluate() uses such way of computations - this is the direct cause of your problem. The keras.evaluate() function will give you the loss value for every batch. The keras.predict() function will give you the actual predictions for all samples in a batch, for all batches. So even if you use the same data, the differences will be there because the value of a loss function will be almost always different than the predicted values. These are two different things. It is about regularization. model.predict() returns the final output of the model, i.e. answer. While model.evaluate() returns the loss. The loss is used to train the model (via backpropagation) and it is not the answer. This video of ML Tokyo should help to understand the difference between model.evaluate() and model.predict().In my understanding, I thought PCA can be performed only for continuous features. But while trying to understand the difference between onehot encoding and label encoding came through a post in the following link: When to use One Hot Encoding vs LabelEncoder vs DictVectorizor? It states that one hot encoding followed by PCA is a very good method, which basically means PCA is applied for categorical features.
Hence confused, please suggest me on the same. I disagree with the others. While you can use PCA on binary data (e.g. one-hot encoded data) that does not mean it is a good thing, or it will work very well. PCA is designed for continuous variables. It tries to minimize variance (=squared deviations). The concept of squared deviations breaks down when you have binary variables. So yes, you can use PCA. And yes, you get an output. It even is a least-squared output: it's not as if PCA would segfault on such data. It works, but it is just much less meaningful than you'd want it to be; and supposedly less meaningful than e.g. frequent pattern mining. MCA is a known technique for categorical data dimension reduction. In R there is a lot of package to use MCA and even mix with PCA in mixed contexts. In python exist a a mca library too. MCA apply similar maths that PCA, indeed the French statistician used to say, "data analysis is to find correct matrix to diagonalize" http://gastonsanchez.com/visually-enforced/how-to/2012/10/13/MCA-in-R/ Basically, PCA finds and eliminate less informative (duplicate) information on feature set and reduce the dimension of feature space. In other words, imagine a N-dimensional hyperspace, PCA finds such M (M < N) features that the data variates most. In this way data may be represented as M-dimensional feature vectors. Mathematically, it is some-kind of a eigen-values & eigen vectors calculation of a feature space. So, it is not important whether the features are continuous or not.  PCA is used widely on many application. Mostly for eliminating noisy, less informative data that comes from some sensor or hardware before classification/recognition. Edit: Statistically speaking, categorical features can be seen as discrete random variables in interval [0,1]. Computation for expectation E{X} and variance E{(X-E{X})^2) are still valid and meaningful for discrete rvs. I still stand for the applicability of PCA in case of categorical features.  Consider a case where you would like to predict whether "It is going to rain for a given day or not". You have categorical feature X which is "Do I have to go to work for the given day", 1 for yes and 0 for no. Clearly weather conditions do not depend on our work schedule, so P(R|X)=P(R). Assuming 5 days of work for every week, we have more 1s than 0s for X in our randomly collected dataset. PCA would probably lead to dropping this low-variance dimension in your feature representation.  At the end of the day, PCA is for dimension reduction with minimal loss of information. Intuitively, we rely on variance of the data on a given axis to measure its usefulness for the task. I don't think there is any theoretical limitation for applying it to categorical features. Practical value depends on application and data which is also the case for continuous variables.  The following publication shows great and meaningful results when computing PCA on categorical variables treated as simplex vertices: Niitsuma H., Okada T. (2005) Covariance and PCA for Categorical Variables. In: Ho T.B., Cheung D., Liu H. (eds) Advances in Knowledge Discovery and Data Mining. PAKDD 2005. Lecture Notes in Computer Science, vol 3518. Springer, Berlin, Heidelberg https://doi.org/10.1007/11430919_61 It is available via https://arxiv.org/abs/0711.4452 (including as a PDF). In this paper, the author's use PCA to combine categorical features of high cardinality. If I understood correctly, they first calculate conditional probabilities for each target class. Then they choose a threshold hyperparameter and create a new binary variable for each conditional class probability for each categorical feature to be combined. PCA is performed to combine the new binary variables with the number of components retained specified as a hyperparameter. PCA is a dimensionality reduction method that can be applied any set of features. Here is an example using OneHotEncoded (i.e. categorical) data: I think  pca is reducing var by leverage the linear relation between vars.
If there's only one categoral var coded in onehot, there's not linear relation between the onehoted cols. so it can't reduce by pca.  But if there exsits other vars, the onehoted cols may be can presented by linear relation of other vars. So may be it can reduce by pca, depends on the relation of vars.Is there a built-in way for getting accuracy scores for each class separatetly? I know in sklearn we can get overall accuracy by using metric.accuracy_score. Is there a way to get the breakdown of accuracy scores for individual classes? Something similar to metrics.classification_report. classification_report does not give accuracy scores: Accuracy score gives only the overall accuracy: You can use sklearn's confusion matrix to get the accuracy References I am adding my answer as I haven't found any answer to this exact question online, and because I think that the other calculation methods suggested here before me are incorrect. Remember that accuracy is defined as: Or to put it into words; it is the ratio between the number of correctly classified examples (either positive or negative) and the total number of examples in the test set. One thing that is important to note is that for both TN and FN, the "negative" is class agnostic, meaning "not predicted as the specific class in question". For example, consider the following: Here, both the second 'cat' prediction and the second 'dog' prediction are false negatives simply because they are not 'bird'. To your question: As far as I know, there is currently no package that provides a method that does what you are looking for, but based on the definition of accuracy, we can use the confusion matrix method from sklearn to calculate it ourselves. The original question was posted a while ago but this might help anyone who comes here through Google, like me. You can code it by yourself : the accuracy is nothing more than the ratio between the well classified samples (true positives and true negatives) and the total number of samples you have. Then, for a given class, instead of considering all the samples, you only take into account those of your class. You can then try this:
Let's first define a handy function. The function above will return the indices in the list l of a certain value val The last function will return the in-class accuracy that you look for. Your question makes no sense. Accuracy is a global measure, and there is no such thing as class-wise accuracy. The suggestions to normalize by true cases (rows) yields something called true-positive rate, sensitivity or recall, depending on the context. Likewise, if you normalize by prediction (columns), it's called precision or positive predictive value. The question is misleading. Accuracy scores for each class equal the overall accuracy score. Consider the confusion matrix: This gives you: Accuracy is calculated as the proportion of correctly classified samples to all samples:  Regarding the confusion matrix, the numerator (TP + TN) is the sum of the diagonal. The denominator is the sum of all cells. Both are the same for every class. In my opinion, accuracy is generic term that has different dimensions, e.g. precision, recall, f1-score, (or even specificity, sensitivity), etc. that provide accuracy measures in different perspectives. Hence, the function 'classification_report' outputs a range of accuracy measures for each class. For instance, precision provides the proportion of accurately retrieved instances (i.e. true positives) with total number of instances (both true positives and false negatives) available in a particular class. here the solution bro: For the multilabel case you can use this No, There is no built-in way for getting accuracy scores for each class separately. But you can use the following snippet to get accuracy, sensitivity, and specificity. You can use sklearn.metrics.classification_report: This will show precision, recall and F1 score for each class. You can also checkout these links from the official documentation:It seems that Google Colab GPU's doesn't come with CUDA Toolkit, how can I install CUDA in Google Colab GPU's. I am getting this error in installing mxnet  in Google Colab. ERROR: Incomplete installation for leveraging GPUs for computations.
  Please make sure you have CUDA installed and run the following line in
  your terminal and try again: Adjust 'cu90' depending on your CUDA version ('cu75' and 'cu80' are
  also available).
      You can also disable GPU usage altogether by invoking turicreate.config.set_num_gpus(0). 
      An exception has occurred, use %tb to see the full traceback. Cuda is not showing on your notebook because you have not enabled GPU in Colab. The Google Colab comes with both options GPU or without GPU.
You can enable or disable GPU in runtime settings Change hardware acceleration to GPU.  To check if GPU is running or not, run the following command If the output is like the following image it means your GPU and cuda are working. You can see the CUDA version also. After that to check if PyTorch is capable of using GPU, run the following code. To check if TensorFlow is capable of using GPU, run the following code. I pretty much believe that Google Colab has Cuda pre-installed... You can make sure by opening a new notebook and type !nvcc --version which would return the installed Cuda version. Here is mine:
 If you switch to using GPU then CUDA will be available on your VM. Basically what you need to do is to match MXNet's version with installed CUDA version. Here's what I used to install MXNet on Colab: First check the CUDA version For me it outputted #define TF_CUDA_VERSION "8.0"
 Then I installed MXNet with  I think the easiest way here is to install mxnet-cu80. Just use the following code: And you could check whether it works by: I think colab right now just support cu80 and higher versions won't work. For more information, you could see the following two websites: Google Colab Free GPU Tutorial Installing mxnet This solution worked for me in November, 2022. Query the version of Ubuntu that Colab is running on (run in notebook using ! or in terminal without): Query the current cuda version in Colab (only for comparision): Next, got to the cuda toolkit archive or latest builds and configure the desired cuda version and os version. The Distribution is Ubuntu.
 Copy the installation instructions: Change the last line to include your cuda-version e.g., apt-get -y install cuda-11-7. Otherwise a more recent version might be installed. Your cuda version will now be updated: To run in Colab, you need CUDA 8 (mxnet 1.1.0 for cuda 9+ is broken). But Google Colab runs now 9.2. There is, however the way to uninstall 9.2, install 8.0 and then install mxnet 1.1.0 cu80.  The complete jupyter code is here : MediumI am starting to work with TensorFlow library for deep learning, https://www.tensorflow.org/.  I found a explicit guide to work on it on linux and Mac but I did not find how to work with it under Windows. I try over the net, but the information are lacking.  I use Visual Studio 2015 for my projects, and I am trying to compile the library with Visual studio Compiler VC14. How to install it and to use it under Windows? Can I use Bazel for Windows  for production use? How to install TensorFlow and to use it under Windows? Updated on 8/4/16  Windows 10 now has a Ubuntu Bash environment, AKA Bash on Ubuntu on Windows, available as a standard option (as opposed to Insider Preview updates for developers). (StackOverflow tag wsl) This option came with the Windows 10 anniversary update (Version 1607) released on 8/2/2016. This allows the use of apt-get to install software packages such as Python and TensorFlow. Note: Bash on Ubuntu on Windows does not have access to the GPU, so all of the GPU options for installing TensorFlow will not work. The dated installation instructions for Bash on Ubuntu on Windows are basically correct, but only these steps are necessary:
Prerequisites
Enable the Windows Subsystem for Linux feature (GUI)
Reboot when prompted
Run Bash on Windows   Steps no longer needed:
Turn on Developer Mode
Enable the Windows Subsystem for Linux feature (command-line)   Then install TensorFlow using apt-get and now test TensorFlow and run an actual neural network After learning about the developer preview of Bash on Windows. See Playing with TensorFlow on Windows by Scott Hanselman which uses Bash on Windows 10  Bazel is the problem TensorFlow is not made with build automation tools such as make, but with Google's in-house build tool Bazel. Bazel only works on systems based on Unix such as Linux and OS X.   Since the current published/known means to build TensorFlow uses Bazel and Bazel does not work on Windows, one can not install or run TensorFlow natively on Windows.   From Bazel FAQ What about Windows? Due to its UNIX heritage, porting Bazel to Windows is significant
  work. For example, Bazel uses symlinks extensively, which has varying
  levels of support across Windows versions. We are currently actively working on improving Windows support, but
  it's still ways from being usable. Status See: TensorFlow issue #17
See: Bazel issue #276 The solutions are listed in the order of complexity and work needed; from about an hour to may not even work. Docker installation Docker is a system to build self contained versions of a Linux operating system running on your machine. When you install and run TensorFlow via Docker it completely isolates the installation from pre-existing packages on your machine. Also look at TensorFlow - which Docker image to use? If you have a current Mac running OS X then see: Installation for Mac OS X The recommend Linux system tends to be Ubuntu 14.04 LTS (Download page). a. Virtual Machine - Hardware Virtualization - Full Virtualization
     ~ 3 hours Download and install a virtual machine such as the commercial VMware or the free Virtual Box, after which you can install Linux and then install TensorFlow.   When you go to install TensorFlow you will be using Pip - Python's package management system. Visual Studio users should think NuGet. The packages are known as wheels. See: Pip Installation If you need to build from the source then see: Installing From Sources
~ 4 hours Note: If you plan on using a Virtual Machine and have never done so before, consider using the Docker option instead, since Docker is the Virtual Machine, OS and TensorFlow all packaged together. b. Dual boot
     ~ 3 hours If you want to run TensorFlow on the same machine that you have Windows and make use of the GPU version then you will most likely have to use this option as running on a hosted Virtual Machine, Type 2 hypervisor, will not allow you access to the GPU.  If you have remote access to another machine that you can install the Linux OS and TensorFlow software on and allow remote connections to, then you can use your Windows machine to present the remote machine as an application running on Windows. Cloud services such as AWS are being used.   From TensorFlow Features Want to run the model as a service in the cloud?
  Containerize with Docker and TensorFlow just works. From Docker Running Docker on AWS provides a highly reliable, low-cost way to
  quickly build, ship, and run distributed applications at scale. Deploy
  Docker using AMIs from the AWS Marketplace. Currently it appears the only hold up is Bazel, however Bazel's roadmap list working on Windows should be available this year. There are two features listed for Windows: Remember that Bazel is only used to build TensorFlow. If you get the commands Bazel runs and the correct source code and libraries you should be able to build TensorFlow on Windows. See: How do I get the commands executed by Bazel.   While I have not researched this more, you can look at the continuous integration info for needed files and info on how to they build it for testing. (Readme) (site) There is a public experimental source code version of Bazel that boots on Windows. You may be able to leverage this into getting Bazel to work on Windows, etc.   Also these solutions require the use of Cygwin or MinGW which adds another layer of complexity. This currently does not exist for TensorFlow. It is a feature request. See: TensorFlow issue 380 You build TensorFlow on Linux using Bazel but change the build process to output a wheel that can be installed on Windows. This will require detailed knowledge of Bazel to change the configuration, and locating the source code and libraries that work with Windows. An option I would only suggest as a last resort. It may not even be possible. See: Windows Subsystem for Linux Overview You will know as much as I do by reading the referenced article.  Can I use Bazel for Windows for production use? Since it is experimental software I would not use on a production machine. Remember that you only need Bazel to build TensorFlow. So use the experimental code on a non production machine to build the wheel, then install the wheel on a production machine. See: Pip Installation TLDR; Currently I have several versions for learning. Most use a VMWare 7.1 Workstation to host Ubuntu 14.04 LTS or Ubuntu 15 or Debian. I also have one dual boot of Ubuntu 14.04 LTS on my Windows machine to access the GPU as the machine with VMware does not have the proper GPU. I would recommend that you give these machines at least 8G of memory either as RAM or RAM and swap space as I have run out of memory a few times. I can confirm that it works in the Windows Subsystem for Linux!
And it is also very straightforward. In the Ubuntu Bash on Windows 10, first update the package index: Then install pip for Python 2: Install tensorflow: The package is now installed an you can run the CNN sample on the MNIST set: I just tested the CPU package for now. I blogged about it: http://blog.mosthege.net/2016/05/11/running-tensorflow-with-native-linux-binaries-in-the-windows-subsystem-for-linux/ cheers ~michael Sorry for the excavation, but this question is quite popular, and now it has a different answer. Google officially announced the addition of Windows (7, 10, and Server 2016) support for TensorFlow:
developers.googleblog.com The Python module can be installed using pip with a single command: And if you need GPU support: TensorFlow manual - How to install pip on windows Another useful information are included in release notes:
https://github.com/tensorflow/tensorflow/releases UPD: As @m02ph3u5 right mentioned in the comments TF for windows supports only Python 3.5.x Installing TensorFlow on Windows with native pip TensorFlow currently supports only Python 3.5 64-bit. Both CPU and GPU are supported. Here are some installation instructions assuming you do not have Python 3.5 64-bit: You can now run something like following to test whether TensorFlow is working fine: TensorFlow comes with a few models, which are located in C:\Python35\Lib\site-packages\tensorflow\models\ (assuming you installed python in C:\Python35). For example, you can run in the console:  or Initial support for building TensorFlow on Microsoft Windows was added on  2016-10-05 in commit 2098b9abcf20d2c9694055bbfd6997bc00b73578: This PR contains an initial version of support for building TensorFlow
  (CPU only) on Windows using CMake. It includes documentation for
  building with CMake on Windows, platform-specific code for
  implementing core functions on Windows, and CMake rules for building
  the C++ example trainer program and a PIP package (Python 3.5 only).
  The CMake rules support building TensorFlow with Visual Studio 2015. Windows support is a work in progress, and we welcome your feedback
  and contributions. For full details of the features currently supported and instructions
  for how to build TensorFlow on Windows, please see the file
  tensorflow/contrib/cmake/README.md. The Microsoft Windows support was introduced in TensorFlow in version 0.12 RC0 (release notes): TensorFlow now builds and runs on Microsoft Windows (tested on Windows 10, Windows 7, and Windows Server 2016). Supported languages include Python (via a pip package) and C++. CUDA 8.0 and cuDNN 5.1 are supported for GPU acceleration. Known limitations include: It is not currently possible to load a custom op library. The GCS and HDFS file systems are not currently supported. The following ops are not currently implemented: DepthwiseConv2dNative, DepthwiseConv2dNativeBackpropFilter, DepthwiseConv2dNativeBackpropInput, Dequantize, Digamma, Erf, Erfc, Igamma, Igammac, Lgamma, Polygamma, QuantizeAndDequantize, QuantizedAvgPool, QuantizedBatchNomWithGlobalNormalization, QuantizedBiasAdd, QuantizedConcat, QuantizedConv2D, QuantizedMatmul, QuantizedMaxPool, QuantizeDownAndShrinkRange, QuantizedRelu, QuantizedRelu6, QuantizedReshape, QuantizeV2, RequantizationRange, and Requantize. Now Tensorflow is officially supported in Windows, you can install it using pip command of Python 3.5 without compile it yourself CPU Version cp35 indicates python 3.5 wheel, 0.12.0 the version, you can edit these according your preference, or to install latest CPU version available you can use GPU Version cp35 indicates python 3.5 wheel, 0.12.0 the version, you can edit these according your preference, or to install latest GPU version available you can use More Info  Following may work for you: install Virtual Box, create Linux VM and install Linux into it. I'd recommend Ubuntu, because Google often uses it internally. Then, install TensorFlow in Linux VM. You can't at the moment. The problem is that tensorflow uses the bazel build another Google internal tool that has been exposed as an open source project and it has only support for mac and unix.
Until bazel is ported to windows or another build system is added to tensorflow there is a little chance to run tensorflow natively on windows. That said you can install virtualbox and then install docker-machine and run a linux container with tensorflow inside it. I managed to install TensorFlow on Win8.1 without Docker using advice from
https://discussions.udacity.com/t/windows-tensorflow-and-visual-studio-2015/45636 I tried a lot of stuff before that, and i won't try to install it twice but here is what i did:
- install VS2015 (make sure Visual C++ installed as well)
- install Python Tools for VS2015
- install Python2.7 with Anaconda2
- install pip and conda for Python
- install numpy with pip inside VS2015
- install tensorflow with pip inside VS2015 i didn't manage to do it with Python3.5 I managed also to install on Win8.1 via Cloud9
There is a video tutorial on Youtube. https://www.youtube.com/watch?v=kMtrOIPLpR0 EDIT: actually for the above, (not Cloud9 which is fine) i have problems:
TensorFlow LOOKS LIKE it's installed (i can see it in the list of modules installed in VS2015 when clicking in Solution Explorer on Python 64-bit 2.7)
but if i type in a script or in Python Interactive import tensorflow as TF then i get an error message   As of writing this answer, I wasn't able to get tensorflow to install properly with python version 3.5.2.  Reverting to python 3.5.0 did the trick. Then I was able to install with  C:> pip install tensorflow If you have already installed anaconda on your windows, there is an easier way as I found out: Then Then This is similar to virtualenv and I found this helpful. Follow this link to install Tensorflow on Windows and you can also use it in Visual StudioHow train_on_batch() is different from fit()? What are the cases when we should use train_on_batch()? For this question, it's a simple answer from the primary author: With fit_generator, you can use a generator for the validation data as
  well. In general, I would recommend using fit_generator, but using
  train_on_batch works fine too. These methods only exist for the sake of
  convenience in different use cases, there is no "correct" method. train_on_batch allows you to expressly update weights based on a collection of samples you provide, without regard to any fixed batch size. You would use this in cases when that is what you want: to train on an explicit collection of samples. You could use that approach to maintain your own iteration over multiple batches of a traditional training set but allowing fit or fit_generator to iterate batches for you is likely simpler. One case when it might be nice to use train_on_batch is for updating a pre-trained model on a single new batch of samples. Suppose you've already trained and deployed a model, and sometime later you've received a new set of training samples previously never used. You could use train_on_batch to directly update the existing model only on those samples. Other methods can do this too, but it is rather explicit to use train_on_batch for this case. Apart from special cases like this (either where you have some pedagogical reason to maintain your own cursor across different training batches, or else for some type of semi-online training update on a special batch), it is probably better to just always use fit (for data that fits in memory) or fit_generator (for streaming batches of data as a generator). train_on_batch() gives you greater control of the state of the LSTM, for example, when using a stateful LSTM and controlling calls to model.reset_states() is needed. You may have multi-series data and need to reset the state after each series, which you can do with train_on_batch(), but if you used .fit() then the network would be trained on all the series of data without resetting the state.  There's no right or wrong, it depends on what data you're using, and how you want the network to behave. Train_on_batch will also see a performance increase over fit and fit generator if youre using large datasets and don't have easily serializable data (like high rank numpy arrays), to write to tfrecords. In this case you can save the arrays as numpy files and load up smaller subsets of them (traina.npy, trainb.npy etc) in memory, when the whole set won't fit in memory. You can then use tf.data.Dataset.from_tensor_slices and then using train_on_batch with your subdataset, then loading up another dataset and calling train on batch again, etc, now you've trained on your entire set and can control exactly how much and what of your dataset trains your model. You can then define your own epochs, batch sizes, etc with simple loops and functions to grab from your dataset. Indeed @nbro answer helps, just to add few more scenarios, lets say you are training some seq to seq model or a large network with one or more encoders. We can create custom training loops using train_on_batch and use a part of our data to validate on the encoder directly without using callbacks. Writing callbacks for a complex validation process could be difficult. There are several cases where we wish to train on batch. Regards,
Karthick From Keras - Model training APIs: We can use it in GAN when we update the discriminator and generator using a batch of our training data set at a time. I saw Jason Brownlee used train_on_batch in on his tutorials (How to Develop a 1D Generative Adversarial Network From Scratch in Keras) Tip for quick search: Type Control+F and type in the search box the term that you want to search (train_on_batch, for example).We are writing a small ANN which is supposed to categorize 7000 products into 7 classes based on 10 input variables. In order to do this we have to use k-fold cross validation but we are kind of confused. We have this excerpt from the presentation slide:  What are exactly the validation and test sets?  From what we understand is that we run through the 3 training sets and adjust the weights (single epoch). Then what do we do with the validation? Because from what I understand is that the test set is used to get the error of the network. What happens next is also confusing to me. When does the crossover take place? If it's not too much to ask, a bullet list of step would be appreciated You seem to be a bit confused (I remember I was too) so I am going to simplify things for you. ;) Whenever you are given a task such as devising a neural network you are often also given a sample dataset to use for training purposes. Let us assume you are training a simple neural network system Y = W · X where Y is the output computed from calculating the scalar product (·) of the weight vector W with a given sample vector X. Now, the naive way to go about this would be using the entire dataset of, say, 1000 samples to train the neural network. Assuming that the training converges and your weights stabilise you can then safely say that you network will correctly classify the training data. But what happens to the network if presented with previously unseen data? Clearly the purpose of such systems is to be able to generalise and correctly classify data other than the one used for training.  In any real-world situation, however, previously-unseen/new data is only available once your neural network is deployed in a, let's call it, production environment. But since you have not tested it adequately you are probably going to have a bad time. :) The phenomenon by which any learning system matches its training set almost perfectly but constantly fails with unseen data is called overfitting. Here come in the validation and testing parts of the algorithm. Let's go back to the original dataset of 1000 samples. What you do is you split it into three sets -- training, validation and testing (Tr, Va and Te) -- using carefully selected proportions. (80-10-10)% is usually a good proportion, where: Now what happens is that the neural network is trained on the Tr set and its weights are correctly updated. The validation set Va is then used to compute the classification error E = M - Y using the weights resulting from the training, where M is the expected output vector taken from the validation set and Y is the computed output resulting from the classification (Y = W * X). If the error is higher than a user-defined threshold then the whole training-validation epoch is repeated. This training phase ends when the error computed using the validation set is deemed low enough. Now, a smart ruse here is to randomly select which samples to use for training and validation from the total set Tr + Va at each epoch iteration. This ensures that the network will not over-fit the training set. The testing set Te is then  used to measure the performance of the network. This data is perfect for this purpose as it was never used throughout the training and validation phase. It is effectively a small set of previously unseen data, which is supposed to mimic what would happen once the network is deployed in the production environment. The performance is again measured in term of classification error as explained above. The performance can also (or maybe even should) be measured in terms of precision and recall so as to know where and how the error occurs, but that's the topic for another Q&A. Having understood this training-validation-testing mechanism, one can further strengthen the network against over-fitting by performing K-fold cross-validation. This is somewhat an evolution of the smart ruse I explained above. This technique involves performing K rounds of training-validation-testing on, different, non-overlapping, equally-proportioned Tr, Va and Te sets. Given k = 10, for each value of K you will split your dataset into Tr+Va = 90% and Te = 10% and you will run the algorithm, recording the testing performance. I am taking the world-famous plot below from wikipedia to show how the validation set helps prevent overfitting. The training error, in blue, tends to decrease as the number of epochs increases: the network is therefore attempting to match the training set exactly. The validation error, in red, on the other hand follows a different, u-shaped profile. The minimum of the curve is when ideally the training should be stopped as this is the point at which the training and validation error are lowest.  For more references this excellent book will give you both a sound knowledge of machine learning as well as several migraines. Up to you to decide if it's worth it. :) Divide your data into K non-overlapping folds. Have each fold K contain an equal number of items from each of the m classes (stratified cross-validation; if you have 100 items from class A and 50 from class B and you do 2 fold validation, each fold should contain a random 50 items from A and 25 from B). For i in 1..k: You have now collected aggregate results across all the folds. This is your final performance. If you're going to apply this for real, in the wild, use the best parameters from the grid search to train on all the data.I'm trying to do image classification with the Inception V3 model. Does ImageDataGenerator from Keras create new images which are added onto my dataset? If I have 1000 images, will using this function double it to 2000 images which are used for training? Is there a way to know how many images were created and now fed into the model? Short answer: 1) All the original images are just transformed (i.e. rotation, zooming, etc.) every epoch and then used for training, and 2) [Therefore] the number of images in each epoch is equal to the number of original images you have. Long answer: In each epoch, the ImageDataGenerator applies a transformation on the images you have and use the transformed images for training. The set of transformations includes rotation, zooming, etc. By doing this you're somehow creating new data (i.e. also called data augmentation), but obviously the generated images are not totally different from the original ones. This way the learned model may be more robust and accurate as it is trained on different variations of the same image. You need to set the steps_per_epoch argument of fit method to n_samples / batch_size, where n_samples is the total number of training data you have (i.e. 1000 in your case). This way in each epoch, each training sample is augmented only one time and therefore 1000 transformed images will be generated in each epoch. Further, I think it's worth clarifying the meaning of "augmentation" in this context: basically we are augmenting the images when we use ImageDataGenerator and enabling its augmentation capabilities. But the word "augmentation" here does not mean, say, if we have 100 original training images we end up having 1000 images per epoch after augmentation (i.e. the number of training images does not increase per epoch). Instead, it means we use a different transformation of each image in each epoch; hence, if we train our model for, say, 5 epochs, we have used 5 different versions of each original image in training (or 100 * 5 = 500 different images in the whole training, instead of using just the 100 original images in the whole training). To put it differently, the total number of unique images increases in the whole training from start to finish, and not per epoch. Here is my attempt to answer as I also had this question on my mind. ImageDataGenerator will NOT add new images to your data set in a sense that it will not make your epochs bigger. Instead, in each epoch it will provide slightly altered images (depending on your configuration). It will always generate new images, no matter how many epochs you have. So in each epoch model will train on different images, but not too different. This should prevent overfitting and in some way simulates online learning. All these alterations happen in memory, but if you want to see these images you can save them to disc, inspect them, see how many of them were generated and get the sense of how ImageDataGenerator works. To do this pass save_to_dir=/tmp/img-data-gen-outputs to function flow_from_directory. See docs. As is officially written here ImageDataGenerator is a  batches Generator of tensor image data with real-time data augmentation. The data will be looped over (in batches). This means that will on the fly apply transformations to batch of images randomly. For instance: At every new epoch new random transformations will be applied and in this way we train with a little different set of images each time. Obtaining more data is not always achievable or possible, using ImageDataGenerator is helpful this way. It all depends on how many epochs you run, as @today answered, fitting the model with the generator will make the generator provide as many images as needed, depending on steps_per_epoch. To make things easier to understand, put i.e. 20 images into two whatever folders (mimicking classified data), create a generator out of the parent folder and run a simple for loop The first thing you should confirm that you see the message Found 20 images belonging to 2 classes., and the loop itself will NOT stop after 20 iterations, but it will just keep incrementing and printing endlessly (I got mine to 10k and stopped it manually). The generator will provide as many images as are requested, whether they were augmented or not. Also note that: These augmented images are not stored in the memory, they are generated on the fly while training and lost after training. You can't read again those augmented images.  Not storing those images is a good idea because we'd run out of memory very soon storing huge no of images ImageDataGenerator class ensures that the model receives new
variations of the images at each epoch. But it only returns the
transformed images and does not add it to the original corpus of
images. If it was, in fact, the case, then the model would be seeing
the original images multiple times which would definitely overfit our
model. https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/ Let me try and tell u in the easiest way possible with the help of an example.
For example:I would like to calculate NN model certainty/confidence (see What my deep model doesn't know) - when NN tells me an image represents "8", I would like to know how certain it is. Is my model 99% certain it is "8" or is it 51% it is "8", but it could also be "6"? Some digits are quite ambiguous and I would like to know for which images the model is just "flipping a coin". I have found some theoretical writings about this but I have trouble putting this in code. If I understand correctly, I should evaluate a testing image multiple times while "killing off" different neurons (using dropout) and then...? Working on MNIST dataset, I am running the following model: How should I predict with this model so that I get its certainty about predictions too? I would appreciate some practical examples (preferably in Keras, but any will do). To clarify, I am looking for an example of how to get certainty using the method outlined by Yurin Gal (or an explanation of why some other method yields better results).  If you want to implement dropout approach to measure uncertainty you should do the following: Implement function which applies dropout also during the test time: Use this function as uncertainty predictor e.g. in a following manner: Of course you may use any different function to compute uncertainty.  Made a few changes to the top voted answer. Now it works for me.  It's a way to estimate model uncertainty. For other source of uncertainty, I found https://eng.uber.com/neural-networks-uncertainty-estimation/ helpful. Your model uses a softmax activation, so the simplest way to obtain some kind of uncertainty measure is to look at the output softmax probabilities: The probs array will then be a 10-element vector of numbers in the [0, 1] range that sum to 1.0, so they can be interpreted as probabilities. For example the probability for digit 7 is just probs[7]. Then with this information you can do some post-processing, typically the predicted class is the one with highest probability, but you can also look at the class with second highest probability, etc. A simpler way is to set training=True on any dropout layers you want to run during inference as well (essentially tells the layer to operate as if it's always in training mode - so it is always present for both training and inference). Code above is from this issue.From my research, I found three conflicting results: Can someone explain when to use LinearSVC vs. SVC(kernel="linear")? It seems like LinearSVC is marginally better than SVC and is usually more finicky. But if scikit decided to spend time on implementing a specific case for linear classification, why wouldn't LinearSVC outperform SVC? Mathematically, optimizing an SVM is a convex optimization problem, usually with a unique minimizer. This means that there is only one solution to this mathematical optimization problem. The differences in results come from several aspects: SVC and LinearSVC are supposed to optimize the same problem, but in fact all liblinear estimators penalize the intercept, whereas libsvm ones don't (IIRC). This leads to a different mathematical optimization problem and thus different results. There may also be other subtle differences such as scaling and default loss function (edit: make sure you set loss='hinge' in LinearSVC). Next, in multiclass classification, liblinear does one-vs-rest by default whereas libsvm does one-vs-one. SGDClassifier(loss='hinge') is different from the other two in the sense that it uses stochastic gradient descent and not exact gradient descent and may not converge to the same solution. However the obtained solution may generalize better. Between SVC and LinearSVC, one important decision criterion is that LinearSVC tends to be faster to converge the larger the number of samples is. This is due to the fact that the linear kernel is a special case, which is optimized for in Liblinear, but not in Libsvm. The actual problem is in the problem with scikit approach, where they call SVM something which is not SVM. LinearSVC is actually minimizing squared hinge loss, instead of just hinge loss, furthermore, it penalizes size of the bias (which is not SVM), for more details refer to other question:
Under what parameters are SVC and LinearSVC in scikit-learn equivalent? So which one to use? It is purely problem specific. As due to no free lunch theorem it is impossible to say "this loss function is best, period". Sometimes squared loss will work better, sometimes normal hinge.I am having trouble fully understanding the K-Means++ algorithm.  I am interested exactly how the first k centroids are picked, namely the initialization as the rest is like in the original K-Means algorithm. I will appreciate a step by step explanation and an example.  The one in Wikipedia is not clear enough.  Also a very well commented source code would also help.  If you are using 6 arrays then please tell us which one is for what. Interesting question. Thank you for bringing this paper to my attention - K-Means++: The Advantages of Careful Seeding In simple terms, cluster centers are initially chosen at random from the set of input observation vectors, where the probability of choosing vector x is high if x is not near any previously chosen centers. Here is a one-dimensional example. Our observations are [0, 1, 2, 3, 4]. Let the first center, c1, be 0. The probability that the next cluster center, c2, is x is proportional to ||c1-x||^2. So, P(c2 = 1) = 1a, P(c2 = 2) = 4a, P(c2 = 3) = 9a, P(c2 = 4) = 16a, where a = 1/(1+4+9+16). Suppose c2=4. Then, P(c3 = 1) = 1a, P(c3 = 2) = 4a, P(c3 = 3) = 1a, where a = 1/(1+4+1). I've coded the initialization procedure in Python; I don't know if this helps you. EDIT with clarification: The output of cumsum gives us boundaries to partition the interval [0,1]. These partitions have length equal to the probability of the corresponding point being chosen as a center. So then, since r is uniformly chosen between [0,1], it will fall into exactly one of these intervals (because of break). The for loop checks to see which partition r is in. Example:  One Liner.  Say we need to select 2 cluster centers, instead of selecting them all randomly{like we do in simple k means}, we will select the first one randomly, then find the points that are farthest to the first center{These points most probably do not belong to the first cluster center as they are far from it} and assign the second cluster center nearby those far points. I have prepared a full source implementation of k-means++ based on the book "Collective Intelligence" by Toby Segaran and the k-menas++ initialization provided here. Indeed there are two distance functions here. For the initial centroids a standard one is used based numpy.inner and then for the centroids fixation the Pearson one is used. Maybe the Pearson one can be also be used for the initial centroids. They say it is better.     data.txt:

p1 1 5 6
p2 9 4 3
p3 2 3 1
p4 4 5 6
p5 7 8 9
p6 4 5 4
p7 2 5 6
p8 3 4 5
p9 6 7 8Is it possible to have missing values in scikit-learn ? How should they be represented? I couldn't find any documentation about that. Missing values are simply not supported in scikit-learn. There has been discussion on the mailing list about this before, but no attempt to actually write code to handle them. Whatever you do, don't use NaN to encode missing values, since many of the algorithms refuse to handle samples containing NaNs. The above answer is outdated; the latest release of scikit-learn has a class Imputer that does simple, per-feature missing value imputation. You can feed it arrays containing NaNs to have those replaced by the mean, median or mode of the corresponding feature. I wish I could provide a simple example, but I have found that RandomForestRegressor does not handle NaN's gracefully. Performance gets steadily worse when adding features with increasing percentages of NaN's. Features that have "too many" NaN's are completely ignored, even when the nan's indicate very useful information. This is because the algorithm will never create a split on the decision "isnan" or "ismissing". The algorithm will ignore a feature at a particular level of the tree if that feature has a single NaN in that subset of samples. But, at lower levels of the tree, when sample sizes are smaller, it becomes more likely that a subset of samples won't have a NaN in a particular feature's values, and a split can occur on that feature. I have tried various imputation techniques to deal with the problem (replace with mean/median, predict missing values using a different model, etc.), but the results were mixed. Instead, this is my solution: replace NaN's with a single, obviously out-of-range value (like -1.0). This enables the tree to split on the criteria "unknown-value vs known-value". However, there is a strange side-effect of using such out-of-range values: known values near the out-of-range value could get lumped together with the out-of-range value when the algorithm tries to find a good place to split. For example, known 0's could get lumped with the -1's used to replace the NaN's. So your model could change depending on if your out-of-range value is less than the minimum or if it's greater than the maximum (it could get lumped in with the minimum value or maximum value, respectively). This may or may not help the generalization of the technique, the outcome will depend on how similar in behavior minimum- or maximum-value samples are to NaN-value samples. I have come across very similar issue, when running the RandomForestRegressor on data. The presence of NA values were throwing out "nan" for predictions. From scrolling around several discussions, the Documentation by Breiman recommends two solutions for continuous and categorical data respectively. According to Breiman the random nature of the algorithm and the number of trees will allow for the correction without too much effect on the accuracy of the prediction. This I feel would be the case if the presence of NA values is sparse, a feature containing many NA values I think will most likely have an affect. Replacing a missing value with a mean/median/other stat may not solve the problem as the fact that the value is missing may be significant. For example in a survey on physical characteristics a respondent may not put their height if they were embarrassed about being abnormally tall or small. This would imply that missing values indicate the respondent was unusually tall or small - the opposite of the median value. What is necessary is a model that has a separate rule for missing values, any attempt to guess the missing value will likely reduce the predictive power of the model. e.g: Orange is another python machine learning library that has facilities dedicated to imputation. I have not had a chance to use them, but might be soon, since the simple methods of replacing nan's with zeros, averages, or medians all have significant problems. I do encounter this problem. In a practical case, I found a package in R called missForest that can handle this problem well, imputing the missing value and greatly enhance my prediction. 
Instead of simply replacing NAs with median or mean, missForest replaces them with a prediction of what it thinks the missing value should be. It makes the predictions using a random forest trained on the observed values of a data matrix. It can run very slow on a large data set that contains a high number of missing values. So there is a trade-off for this method.
A similar option in python is predictive_imputer When you run into missing values on input features, the first order of business is not how to impute the missing. The most important question is WHY SHOULD you. Unless you have clear and definitive mind what the 'true' reality behind the data is, you may want to curtail urge to impute. This is not about technique or package in the first place.  Historically we resorted to tree methods like decision trees mainly because some of us at least felt that imputing missing to estimate regression like linear regression, logistic regression, or even NN is distortive enough that we should have methods that do not require imputing missing 'among the columns'. The so-called missing informativeness. Which should be familiar concept to those familiar with, say, Bayesian.  If you are really modeling on big data, besides talking about it, the chance is you face large number of columns. In common practice of feature extraction like text analytics, you may very well say missing means count=0. That is fine because you know the root cause. The reality, especially when facing structured data sources, is you don't know or simply don't have time to know the root cause. But your engine forces to plug in a value, be it NAN or other place holders that the engine can tolerate, I may very well argue your model is as good as you impute, which does not make sense.  One intriguing question is : if we leave missingness to be judged by its close context inside the splitting process, first or second degree surrogate, does foresting actually make the contextual judgement a moot because the context per se is random selection? This, however, is a 'better' problem. At least it does not hurt that much. It certainly should make preserving missingness unnecessary. As a practical matter, if you have large number of input features, you probably cannot have a 'good' strategy to impute after all. From the sheer imputation perspective, the best practice is anything but univariate. Which is in the contest of RF pretty much means to use the RF to impute before modeling with it.  Therefore, unless somebody tells me (or us), "we are not able to do that", I think we should enable carrying forward missing 'cells', entirely bypassing the subject of how 'best' to impute.I am trying to use a pre-trained model. Here's where the problem occurs Isn't the model supposed to take in a simple colored image? Why is it expecting a 4-dimensional input? Where  As Usman Ali wrote in his comment, pytorch (and most other DL toolboxes) expects a batch of images as an input. Thus you need to call Inserting a singleton "batch" dimension to your input data. Please also note that the model you are using might expect a different input size (3x229x229) and not 3x224x224. From the Pytorch documentation on convolutional layers, Conv2d layers expect input with the shape Passing grayscale images in their usual format (224, 224) won't work. To get the right shape, you will need to add a channel dimension. You can do it as follows: The unsqueeze() method adds a dimensions at the specified index. The result would have shape: As the model expects a batch of images, we need to pass a 4 dimensional tensor, which can be done as follows: Method-1: output = model(data[0:1])
Method-2: output = model(data[0].unsqueeze(0)) This will only send the first image of the whole batch. Similarly for ith image we can do: Method-1: output = model(data[i:i+1])
Method-2: output = model(data[i].unsqueeze(0))Can someone explain me what random_state means in below example? Why is it hard coded to 42? Isn't that obvious? 42 is the Answer to the Ultimate Question of Life, the Universe, and Everything. On a serious note, random_state simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time. Relevant documentation: random_state : int, RandomState instance or None, optional
  (default=None)
  If int, random_state is the seed used by the random
  number generator; If RandomState instance, random_state is the random
  number generator; If None, the random number generator is the
  RandomState instance used by np.random. If you don't specify the random_state in the code, then every time you run(execute) your code a new random value is generated and the train and test datasets would have different values each time. However, if a fixed value is assigned like random_state = 0 or 1 or 42 or any other integer then no matter how many times you execute your code the result would be the same .i.e, same values in train and test datasets. Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to generate the splits. The random state that you provide is used as a seed to the random number generator. This ensures that the random numbers are generated in the same order. When the Random_state is not defined in the code for every run train data will change and accuracy might change for every run.
When the Random_state = " constant integer" is defined then train data will be constant For every run so that it will make easy to debug. The random state is simply the lot number of the set generated randomly in any operation. We can specify this lot number whenever we want the same set again.In the excel sheet , i have two columns with large numbers. But when i read the excel file with read_excel() and display the dataframe, those two columns are printed in scientific format with exponential. How can  get rid of this format? Thanks  Output in Pandas  The way scientific notation is applied is controled via pandas' display options: If this is simply for presentational purposes, you may convert your
data to strings while formatting them on a column-by-column basis: An alternative more straightforward method would to put the following line at the top of your code that would format floats only: try '{:.0f}' with Sergeys, worked for me.I am trying to feed a huge sparse matrix to Keras model. As the dataset doesn`t fit into RAM, the way around is to train the model on a data generated batch-by-batch by a generator.  To test this approach and make sure my solution works fine, I slightly modified a Kera`s simple MLP on the Reuters newswire topic classification task. So, the idea is to compare original and edited models.  I just convert numpy.ndarray into scipy.sparse.csr.csr_matrix and feed it to the model. But my model crashes at some point and I need a hand to figure out a reason. Here is the original model and my additions below It outputs:   Finally, here is my part  The crash:   I believe the problem is due to wrong setup of samples_per_epoch. I`d trully appreciate if someone could comment on this.   Here is my solution.  In my case, X - sparse matrix, y - array. If you can use Lasagne instead of Keras I've written a small MLP class with the following features: supports both dense and sparse matrices supports drop-out and hidden layer Supports complete probability distribution instead of one-hot labels so supports multilabel training. Supports scikit-learn like API (fit, predict, accuracy, etc.) Is very easy to configure and modifyIs there a way to calculate the total number of parameters in a LSTM network. I have found a example but I'm unsure of how correct this is or If I have understood it correctly. For eg consider the following example:- As per My understanding n is the input vector lenght.
And m is the number of time steps. and in this example they consider the number of hidden layers to be 1. Hence according to the formula in the post. 4(nm+n^2) in my example m=16;n=4096;num_of_units=256 Why is there such a difference? 
Did I misunderstand the example or was the formula wrong ? No - the number of parameters of a LSTM layer in Keras equals to: Additional 1 comes from bias terms. So n is size of input (increased by the bias term) and m is size of output of a LSTM layer. So finally : 
image via this post num_units + input_dim:  concat [h(t-1), x(t)] + 1: bias * 4: there are 4 neural network layers (yellow box) {W_forget, W_input, W_output, W_cell} [(256 + 4096 + 1) * 256] * 4 = 4457472 PS: num_units = num_hidden_units = output_dims I think it would be easier to understand if we start with a simple RNN. Let's assume that we have 4 units (please ignore the ... in the network and concentrate only on visible units), and the input size (number of dimensions) is 3:  The number of weights is 28 = 16 (num_units * num_units) for the recurrent connections + 12 (input_dim * num_units) for input. The number of biases is simply num_units. Recurrency means that each neuron output is fed back into the whole network, so if we unroll it in time sequence, it looks like two dense layers:  and that makes it clear why we have num_units * num_units weights for the recurrent part. The number of parameters for this simple RNN is 32 = 4 * 4 + 3 * 4 + 4, which can be expressed as num_units * num_units + input_dim * num_units + num_units or num_units * (num_units + input_dim + 1) Now, for LSTM, we must multiply the number of of these parameters by 4, as this is the number of sub-parameters inside each unit, and it was nicely illustrated in the answer by @FelixHo Formula expanding for @JohnStrong :  4 means we have different weight and bias variables for 3 gates (read / write / froget) and - 4-th - for the cell state (within same hidden state).
(These mentioned are shared among timesteps along particular hidden state vector) as LSTM output (y) is h (hidden state) by approach, so, without an extra projection, for LSTM outputs we have :  let's say it's d : Then  LSTM Equations (via deeplearning.ai Coursera)   The others have pretty much answered it. But just for further clarification, on creating an LSTM layer. The number of params is as follows: No of params= 4*((num_features used+1)*num_units+
num_units^2) The +1 is because of the additional bias we take. Where the num_features is the num_features in your input shape to the LSTM:
Input_shape=(window_size,num_features)Is there a way to let Tensorflow print extra training metrics (e.g. batch accuracy) when using the Estimator API? One can add summaries and view the result in Tensorboard (see another post), but I was wondering if there is an elegant way to get the scalar summary values printed while training. This already happens for training loss, e.g.:  but it would be nice to have e.g. without to much trouble. I am aware that most of the time it is more useful to plot test set accuracy (I am already doing this with a validation monitor), but in this case I am also interested in training batch accuracy. From what I've read it is not possible to change it by passing parameter.
You can try to do by creating a logging hook and passing it into to estimator run. In the body of model_fn function for your estimator: EDIT: To see the output you must also set logging verbosity high enough (unless its your default):
tf.logging.set_verbosity(tf.logging.INFO) You can also use the TensorBoard to see some graphics of the desired metrics. To do that, add the metric to a TensorFlow summary like this: The cool thing when you use the tf.estimator.Estimator is that you don't need to add the summaries to a FileWriter, since it's done automatically (merging and saving them periodically by default - on average every 100 steps). Don't forget to change this line as well, based on the accuracy parameter you just added: In order to see the TensorBoard you need to open a new terminal and type: After that you will be able to see the graphics in your browser at localhost:6006.I was playing with TensorFlow's brand new Object Detection API and decided to train it on some other publicly available datasets. I happened to stumble upon this grocery dataset which consists of images of various brands of cigarette boxes on the supermarket shelf along with a text file which lists out the bounding boxes of each cigarette box in each image. 10 major brands have been labeled in the dataset and all other brands fall into the 11th "miscellaneous" category. I followed their tutorial and managed to train the model on this dataset. Due to limitations on processing power, I used only a third of the dataset and performed a 70:30 split for training and testing data. I used the faster_rcnn_resnet101 model. All parameters in my config file are the same as the default parameters provided by TF. After 16491 global steps, I tested the model on some images but I am not too happy with the results - 
Failed to detect the Camels in top-shelf whereas it detects the product in other images 
Why does it fail to detect the Marlboros in the top row? 
Another issue I had is that the model never detected any other label except for label 1  Doesn't detected a crop instance of the product from the training data  It detects cigarette boxes with 99% confidence even in negative images! Can somebody help me with what is going wrong? What can I do to improve the accuracy? And why does it detect all products to belong in category 1 even though I have mentioned that there are 11 classes in total? Edit Added my label map: So I think I figured out what is going on. I did some analysis on the dataset and found out that it is skewed towards objects of category 1. This is the frequency distribution of the each category from 1 to 11 (in 0 based indexing) I guess the model is hitting a local minima where just labelling everything as category 1 is good enough. About the problem of not detecting some boxes : I tried training again, but this time I didn't differentiate between brands. Instead, I tried to teach the model what a cigarette box is. It still wasn't detecting all the boxes. Then I decided to crop the input image and provide that as an input. Just to see if the results improve and it did! It turns out that the dimensions of the input image were much larger than the 600 x 1024 that is accepted by the model. So, it was scaling down these images to 600 x 1024 which meant that the cigarette boxes were losing their details :) So, I decided to test the original model which was trained on all classes on cropped images and it works like a charm :)  This was the output of the model on the original image  This is the output of the model when I crop out the top left quarter and provide it as input.  Thanks everyone who helped! And congrats to the TensorFlow team for an amazing job for the API :) Now everybody can train object detection models! How many images are there in the dataset? The more training data that you have the better the API performs. I tried training it on about 20 images per class, the accuracy was pretty bad. I pretty much faced all the problems that you have mentioned above. When I generated more data, the accuracy improved considerably. PS: Sorry I couldn't comment since I don't have enough reputation Maybe it is too late now, but I wanted to post the comments if anyone struggles with this in the future: Unfortunately, TF documentation is not the best and I struggled with this a lot before finding the reason. The way the model is constructed is that it allows for a MAXIMUM of x amount of predictions per single image. In your case I think it is 20. You can easily test my hypothesis by editing the original photo like this:  Obviously before the boxes are actually drawn and you should see some better results. Pretty nasty limitation. There are some parameters to configure like: They are very important. By default they are set to 100. It seems, the dataset size is rather small. Resnet is a large network, which will require even more data to train properly. What to do:I am guessing that it is conditional probability given that the above (tree branch) condition exists. However, I am not clear on it. If you want to read more about the  data used or how do we get this diagram then go to : http://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/ For a classification tree with 2 classes {0,1}, the value of the leaf node represent the raw score for class 1. It can be converted to a probability score by using the logistic function. The calculation below use the left most leaf as an example. What this means is if a data point ends up being distributed to this leaf, the probability of this data point being class 1 is 0.5417843204057448. If it is a regression model (objective can be reg:squarederror), then the leaf value is the prediction of that tree for the given data point. The leaf value can be negative based on your target variable. The final prediction for that data point will be sum of leaf values in all the trees for that point. If it is a classification model (objective can be binary:logistic), then the leaf value is representative (like raw score) for the probability of the data point belonging to the positive class. The final probability prediction is obtained by taking sum of leaf values (raw scores) in all the trees and then transforming it between 0 and 1 using a sigmoid function. The leaf value (raw score) can be negative, the value 0 actually represents probability being 1/2. Please find more details about the parameters and outputs at - https://xgboost.readthedocs.io/en/latest/parameter.html Attribute leaf is the predicted value. In other words, if the evaluation of a tree model ends at that terminal node (aka leaf node), then this is the value that is returned. In pseudocode (the left-most branch of your tree model): You are correct. Those probability values associated with leaf nodes are representing the conditional probability of reaching leaf nodes given a specific branch of the tree. Branches of trees can be presented as a set of rules. For example, @user1808924 mentioned in his answer; one rule which is representing the left-most branch of your tree model. So, in short: The tree can be linearized into decision rules, where the outcome is the contents of the leaf node, and the conditions along the path form a conjunction in the if clause. In general, the rules have the form: Decision rules can be generated by constructing association rules with the target variable on the right. They can also denote temporal or causal relations.I am importing tensorflow in my ubuntu python 
using following commands- And the program exits.
Please specify the solution. I had the same problem and had to downgrade tensorflow to 1.5.0: Edit: As @Tobsta points out in the comments, the other option is to compile the binaries from source. The precompiled binaries of versions >1.5 use AVX instructions that are not supported by older CPUs I see same message on my PC / Celeron N4000. I successed to build TensorFlow v1.14.0 without AVX instruction.
(Just build TensorFlow on CeleronN4000) I wrote the log on below.
https://github.com/naruai/wiki/blob/master/TensorFlow/BuildTensorFlowWOAVX.md In my case, used Python 3.6.8 .
I also tested with Python 2.7 .
About Python 3.5 , I not tested.
Maybe possible to use similar way, I think. The desired version of TensorFlow can be installed via a hack using anaconda.
First, go to the directory which has sufficient space and download anaconda there (Check the version you want to install). If you want to ensure the integrity of the Anaconda installed check it using SHA-256. Run the Anaconda Script: Output should be like: Now when you get the prompt:
Anaconda3 will be installed in this location: …. Enter the location where you want it to be installed or press enter to continue. Now as per your choice/requirement you can type yes/no for
“Do you wish the installer to initialize Anaconda3 by running conda init?” Now instead of using pip for installing tensorflow, we will use conda but for this we will have to first set the path using the vim ~/.bashrc file. Put your own path instead of /anaconda3/bin, like: /data/anaconda3/bin, or whatsoever. To make this effective, run: Now create a virtual environment. Now to install TensorFlow or Keras, run: or, if there is a particular version that you want to install say, version 1.14.0 for TensorFlow and 2.3.1 for Keras. You have to be in the same virtual environment as while installing Keras and/or TensorFlow for it to work properly. Tn this case tf_env by running source /anaconda3/bin/activate tf_env You can check the installation by runningI'm trying to understand using kfolds cross validation from the sklearn python module. I understand the basic flow: Where i'm confused is using sklearn kfolds with cross val score. As I understand it the cross_val_score function will fit the model and predict on the kfolds giving you an accuracy score for each fold. e.g. using code like this: So if I have a dataset with training and testing data, and I use the cross_val_score function with kfolds to determine the accuracy of the algorithm on my training data for each fold, is the model now fitted and ready for prediction on the testing data?
So in the case above using lr.predict No the model is not fitted. Looking at the source code for cross_val_score: As you can see, cross_val_score clones the estimator before fitting the fold training data to it. cross_val_score will give you output an array of scores which you can analyse to know how the estimator performs for different folds of the data to check if it overfits the data or not. You can know more about it here You need to fit the whole training data to the estimator once you are satisfied with the results of cross_val_score, before you can use it to predict on test data.Say I am loading MNIST from torchvision.datasets.MNIST, but I only want to load in 10000 images total, how would I slice the data to limit it to only some number of data points?  I understand that the DataLoader is a generator yielding data in the size of the specified batch size, but how do you slice datasets? You can use torch.utils.data.Subset() e.g. for the first 10,000 elements: Another quick way of slicing dataset is by using torch.utils.data.random_split() (supported in PyTorch v0.4.1+). It helps in randomly splitting a dataset into non-overlapping new datasets of given lengths. So we can have something like the following: here you can set tr_split_len and te_split_len as the required split lengths for training and testing datasets respectively. It is important to note that when you create the DataLoader object, it doesnt immediately load all of your data (its impractical for large datasets). It provides you an iterator that you can use to access each sample.  Unfortunately, DataLoader doesnt provide you with any way to control the number of samples you wish to extract. You will have to use the typical ways of slicing iterators. Simplest thing to do (without any libraries) would be to stop after the required number of samples is reached.  Or, you could use itertools.islice to get the first 10k samples. Like so.I'm learning how to create convolutional neural networks using Keras. I'm trying to get a high accuracy for the MNIST dataset. Apparently categorical_crossentropy is for more than 2 classes and binary_crossentropy is for 2 classes. Since there are 10 digits, I should be using categorical_crossentropy. However, after training and testing dozens of models, binary_crossentropy consistently outperforms categorical_crossentropy significantly. On Kaggle, I got 99+% accuracy using binary_crossentropy and 10 epochs. Meanwhile, I can't get above 97% using categorical_crossentropy, even using 30 epochs (which isn't much, but I don't have a GPU, so training takes forever). Here's what my model looks like now: Short answer: it is not.  To see that, simply try to calculate the accuracy "by hand", and you will see that it is different from the one reported by Keras with the model.evaluate method: The reason it seems to be so is a rather subtle issue at how Keras actually guesses which accuracy to use, depending on the loss function you have selected, when you include simply metrics=['accuracy'] in your model compilation. If you check the source code, Keras does not define a single accuracy metric, but several different ones, among them binary_accuracy and categorical_accuracy. What happens under the hood is that, since you have selected binary cross entropy as your loss function and have not specified a particular accuracy metric, Keras (wrongly...) infers that you are interested in the binary_accuracy, and this is what it returns. To avoid that, i.e. to use indeed binary cross entropy as your loss function (nothing wrong with this, in principle) while still getting the categorical accuracy required by the problem at hand (i.e. MNIST classification), you should ask explicitly for categorical_accuracy in the model compilation as follows: And after training, scoring, and predicting the test set as I show above, the two metrics now are the same, as they should be: (HT to this great answer to a similar problem, which helped me understand the issue...) UPDATE: After my post, I discovered that this issue had already been identified in this answer. First of all, binary_crossentropy is not when there are two classes. The "binary" name is because it is adapted for binary output, and each number of the softmax is aimed at being 0 or 1.
Here, it checks for each number of the output. It doesn't explain your result, since categorical_entropy exploits the fact that it is a classification problem. Are you sure that when you read your data there is one and only one class per sample? It's the only one explanation I can give.I am using bag of words to classify text. It's working well but I am wondering how to add a feature which is not a word.  Here is my sample code. Now it is clear that the text about London tends to be much longer than the text about New York. How would I add length of the text as a feature? 
Do I have to use another way of classification and then combine the two predictions? Is there any way of doing it along with the bag of words? 
Some sample code would be great -- I'm very new to machine learning and scikit learn.  As shown in the comments, this is a combination of a FunctionTransformer, a FeaturePipeline and a FeatureUnion. This will add the length of the text to the features used by the classifier. I assume that the new feature that you want to add is numeric. Here is my logic. First transform the text into sparse using TfidfTransformer or something similar. Then convert the sparse representation to a pandas DataFrame and add your new column which I assume is numeric. At the end, you may want to convert your data frame back to sparse matrix using scipy or any other module that you feel comfortable with. I assume that your data is in a pandas DataFrame called dataset containing a 'Text Column' and a 'Numeric Column'. Here is some code. Finally, you may want to; to ensure that the new column was successfully added. I hope this helps.Using Spark ML transformers I arrived at a DataFrame where each row looks like this: where text_features is a sparse vector of term weights, color_features is a small 20-element (one-hot-encoder) dense vector of colors, and type_features is also a one-hot-encoder dense vector of types. What would a good approach be (using Spark's facilities) to merge these features in one single, large array, so that I measure things like the cosine distance between any two objects? You can use VectorAssembler: For PySpark example see: Encode and assemble multiple features in PySparkI have a multi class classification problem and my dataset is skewed, I have 100 instances of a particular class and say 10 of some different class, so I want to split my dataset keeping ratio between classes, if I have 100 instances of a particular class and I want 30% of records to go in the training set I want to have there 30 instances of my 100 record represented class and 3 instances of my 10 record represented class and so on. You can use sklearn's StratifiedKFold, from the online docs: Stratified K-Folds cross validation iterator  Provides train/test
  indices to split data in train test sets.  This cross-validation object
  is a variation of KFold that returns stratified folds. The folds are
  made by preserving the percentage of samples for each class. This will preserve your class ratios so that the splits retain the class ratios, this will work fine with pandas dfs. As suggested by @Ali_m you could use StratifiedShuffledSplit which accepts a split ratio param:  sss = StratifiedShuffleSplit(y, 3, test_size=0.7, random_state=0) would produce a 70% split. As simple as :Below is my pipeline and it seems that I can't pass the parameters to my models by using the ModelTransformer class, which I take it from the link (http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html) The error message makes sense to me, but I don't know how to fix this. Any idea how to fix this? Thanks. Error Message:
ValueError: Invalid parameter n_estimators for estimator ModelTransformer. GridSearchCV has a special naming convention for nested objects. In your case ess__rfc__n_estimators stands for ess.rfc.n_estimators, and, according to the definition of the pipeline, it points to the property n_estimators of  Obviously, ModelTransformer instances don't have such property. The fix is easy: in order to access underlying object of ModelTransformer one needs to use model field. So, grid parameters become P.S. it's not the only problem with your code. In order to use multiple jobs in GridSearchCV, you need to make all objects you're using copy-able. This is achieved by implementing methods get_params and set_params, you can borrow them from BaseEstimator mixin.I wanna make a model with multiple inputs. So, I try to build a model like this. and the summary
:
_ But, when i try to train this model, the problem happens....
: Thanks for reading and hopefully helping me :) OOM stands for "out of memory". Your GPU is running out of memory, so it can't allocate memory for this tensor. There are a few things you can do: There is more useful information about this error: This is a weird shape. If you're working with images, you should normally have 3 or 1 channel. On top of that, it seems like you are passing your entire dataset at once; you should instead pass it in batches. From [800000,32,30,62] it seems your model put all the data in one batch. Try specified batch size like If it still OOM then try reduce the batch_size Happened to me as well.  You can try reducing trainable parameters by using some form of Transfer Learning - try freezing the initial few layers and use lower batch sizes.  I think the most common reason for this case to arise would be the absence of MaxPooling layers.
Use the same architecture, but add atleast one MaxPool layer after Conv2D layers. This might even improve the overall performance of the model.
You can even try reducing the depth of the model, i.e., remove the unnecessary layers and optimize.While applying min max scaling to normalize your features, do you apply min max scaling on the entire dataset before splitting it into training, validation and test data? Or do you split first and then apply min max on each set, using the min and max values from that specific set? Lastly , when making a prediction on a new input, should the features of that input be normalized using the min, max values from the training data before being fed into the network? Split it, then scale. Imagine it this way: you have no idea what real-world data looks like, so you couldn't scale the training data to it. Your test data is the surrogate for real-world data, so you should treat it the same way. To reiterate: Split, scale your training data, then use the scaling from your training data on the testing data.I am trying to calculate silhouette score as I find the optimal number of clusters to create, but get an error that says: I am unable to understand the reason for this. Here is the code, that I am using to cluster and calculate silhouette score.  I read the csv that contains the text to be clustered and run K-Means on the n cluster values. What could be the reason I am getting this error? The error is produced because you have a loop for different number of clusters n. During the first iteration, n_clusters  is 1 and this leads to all(km.labels_ == 0)to be True. In other words, you have only one cluster with label 0 (thus, np.unique(km.labels_) prints array([0], dtype=int32)). Example: We have 3 different clusters/cluster labels. The function works fine. Now, let's cause the error: From the documentation,  Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1 So one way to solve this problem is instead of using for k in range(1,15), try to start iteration from k = 2, which is for k in range(2,15). That works for me. Try changing min_samples and also algorithm and metric. for valid list of metrics and algoritms use.
sklearn.neighbors.VALID_METRICS Try to increase your eps value. I was also getting the same error but when I choose the higher eps value, the error is gone.I am having a tough time in figuring out how to use Kevin Murphy's
HMM toolbox Toolbox. It would be a great help if anyone who has an experience with it could clarify some conceptual questions. I have somehow understood the theory behind HMM but it's confusing how to actually implement it and mention all the parameter setting. There are 2 classes so we need 2 HMMs. 
Let say  the training vectors are :class1 O1={ 4 3 5 1 2} and class O_2={ 1 4 3 2 4}. 
Now,the system has to classify an unknown sequence O3={1 3 2 4 4} as either class1 or class2.  Instead of answering each individual question, let me illustrate how to use the HMM toolbox with an example -- the weather example which is usually used when introducing hidden markov models. Basically the states of the model are the three possible types of weather: sunny, rainy and foggy. At any given day, we assume the weather can be only one of these values. Thus the set of HMM states are: However in this example, we can't observe the weather directly (apparently we are locked in the basement!). Instead the only evidence we have is whether the person who checks on you every day is carrying an umbrella or not. In HMM terminology, these are the discrete observations: The HMM model is characterized by three things: Next we are either given the these probabilities, or we have to learn them from a training set. Once that's done, we can do reasoning like computing likelihood of an observation sequence with respect to an HMM model (or a bunch of models, and pick the most likely one)... Here is a sample code that shows how to fill existing probabilities to build the model: Then we can sample a bunch of sequences from this model: for example, the 5th example was: we can evaluate the log-likelihood of the sequence: or compute the Viterbi path (most probable state sequence):  Training is performed using the EM algorithm, and is best done with a set of observation sequences. Continuing on the same example, we can use the generated data above to train a new model and compare it to the original:  Keep in mind that the states order don't have to match. That's why we need to permute the states before comparing the two models. In this example, the trained model looks close to the original one: There are more things you can do with hidden markov models such as classification or pattern recognition. You would have different sets of obervation sequences belonging to different classes. You start by training a model for each set. Then given a new observation sequence, you could classify it by computing its likelihood with respect to each model, and predict the model with the highest log-likelihood. I do not use the toolbox that you mention, but I do use HTK. There is a book that describes the function of HTK very clearly, available for free http://htk.eng.cam.ac.uk/docs/docs.shtml The introductory chapters might help you understanding. I can have a quick attempt at answering #4 on your list. . . 
The number of emitting states is linked to the length and complexity of your feature vectors. However, it certainly does not have to equal the length of the array of feature vectors, as each emitting state can have a transition probability of going back into itself or even back to a previous state depending on the architecture. I'm also not sure if the value that you give includes the non-emitting states at the start and the end of the hmm, but these need to be considered also. Choosing the number of states often comes down to trial and error. Good luck!I am developping a segmentation neural network with only two classes, 0 and 1 (0 is the background and 1 the object that I want to find on the image). On each image, there are about 80% of 1 and 20% of 0. As you can see, the dataset is unbalanced and it makes the results wrong. My accuracy is 85% and my loss is low, but that is only because my model is good at finding the background ! I would like to base the optimizer on another metric, like precision or recall which is more usefull in this case. Does anyone know how to implement this ? You don't use precision or recall to be optimize. You just track them as valid scores to get the best weights. Do not mix loss, optimizer, metrics and other. They are not meant for the same thing. No. To do a 'gradient descent', you need to compute a gradient. For this the function need to be somehow smooth. Precision/recall or accuracy is not a smooth function, it has only sharp edges on which the gradient is infinity and flat places on which the gradient is zero. Hence you can not use any kind of numerical method to find a minimum of such a function - you would have to use some kind of combinatorial optimization and that would be NP-hard.  As others have stated, precision/recall is not directly usable as a loss function. However, better proxy loss functions have been found that help with a whole family of precision/recall related functions (e.g. ROC AUC, precision at fixed recall, etc.) The research paper Scalable Learning of Non-Decomposable Objectives covers this with a method to sidestep the combinatorial optimization by the use of certain calculated bounds, and some Tensorflow code by the authors is available at the tensorflow/models repository. Additionally, there is a followup question on StackOverflow that has an answer that adapts this into a usable Keras loss function. Special thanks to Francois Chollet and other participants on the Keras issue thread here that turned up that research paper. You may also find that thread provides other useful insights into the problem at hand. Having the same problem with an unbalanced dataset, I'd suggest you use the F1 score as the metric of your optimizer.
Andrew Ng teaches that having ONE metric for the model is the simplest (best?) way to train a model. If you have 2 metrics, like precision and recall - it's not clear which one is more important. Trying to set limits on one metric obviously impacts the other metric... F1 score is the prodigy of recall and precision - it is their harmonic mean. Keras that I'm using, unfortunately has no implementation of F1 score as a metric, like there is one for accuracy, or many other Keras metrics https://keras.io/api/metrics/. I found an implementation of the F1 score as a Keras metric, used at each epoch at:
https://medium.com/@aakashgoel12/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d I've implemented the simple function from the above article and the model trains now on F1 score as its Keras optimizer metric. Results on test: accuracy went down a bit and F1 score went up a lot. I have the same problem regarding an unbalanced dataset for binary classification and I want to increase the recall sensitivity too. I found out that there is a built-in function for recall  in tf.keras and can be used in the compile statement as follow: The recommended approach to deal with an unbalanced dataset like you have is to use class_weights or sample_weights. See the model fit API for details. Quote: class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to "pay more attention" to samples from an under-represented class. With weights that are inversely proportional to the class frequency the loss will avoid just predicting the background class. I understand that this is not how you formulated the question but imho it is the most practical approach to the issue you are facing. I think that the Callbacks and Early Stopping mechanisms provide one with techniques that can lead you as close as possible to what you want to achieve. Please read the following article by Jason Brownlee about Early Stopping (read to the end!): https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/How do you compute the true- and false- positive rates of a multi-class classification problem? Say, The confusion matrix is computed by metrics.confusion_matrix(y_true, y_prediction), but that just shifts the problem. EDIT after @seralouk's answer. Here, the class -1 is to be considered as the negatives, while 0 and 1 are variations of positives. Using your data, you can get all the metrics for all the classes at once: For a general case where we have a lot of classes, these metrics are represented graphically in the following image:  Another simple way is PyCM (by me), that supports multi-class confusion matrix analysis. Applied to your Problem : Since there are several ways to solve this, and none is really generic (see https://stats.stackexchange.com/questions/202336/true-positive-false-negative-true-negative-false-positive-definitions-for-mul?noredirect=1&lq=1 and
https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co#51301), here is the solution that seems to be used in the paper which I was unclear about: to count confusion between two foreground pages as false positive So the solution is to import numpy as np, use y_true and y_prediction as np.array, then:I am currently performing multi class SVM with linear kernel using python's scikit library. 
The sample training data and testing data are as given below: Model data: I want to plot the decision boundary and visualize the datasets. Can someone please help to plot this type of data. The data given above is just mock data so feel free to change the values.
It would be helpful if at least if you could suggest the steps that are to followed. 
Thanks in advance You have to choose only 2 features to do this. The reason is that you cannot plot a 7D plot. After selecting the 2 features use only these for the visualization of the decision surface. (I have also written an article about this here: https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=friends_link&sk=80f72ab272550d76a0cc3730d7c8af35) Now, the next question that you would ask: How can I choose these 2 features?. Well, there are a lot of ways. You could do a univariate F-value (feature ranking) test and see what features/variables are the most important. Then you could use these for the plot. Also, we could reduce the dimensionality from 7 to 2 using PCA for example. 2D plot for 2 features and using the iris dataset  EDIT: Apply PCA to reduce dimensionality.  EDIT 1 (April 15th, 2020):  You can use mlxtend. It's quite clean. First do a pip install mlxtend, and then: Where X is a two-dimensional data matrix, and y is the associated vector of training labels.I am using libSVM.
Say my feature values are in the following format:  I think there are two scaling can be applied. scale each instance vector such that each vector has zero mean and unit variance. scale each colum of the above matrix to a range. for example [-1, 1] According to my experiments with RBF kernel (libSVM) I found that the second scaling  (2) improves the results by about 10%. I did not understand the reason why (2) gives me a improved results. Could anybody explain me what is the reason for applying scaling and why the second option gives me improved results? The standard thing to do is to make each dimension (or attribute, or column (in your example)) have zero mean and unit variance. This brings each dimension of the SVM into the same magnitude. From http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf: The main advantage of scaling is to avoid attributes in greater numeric
  ranges dominating those in smaller numeric ranges. Another advantage is to avoid
  numerical diculties during the calculation. Because kernel values usually depend on
  the inner products of feature vectors, e.g. the linear kernel and the polynomial ker-
  nel, large attribute values might cause numerical problems. We recommend linearly
  scaling each attribute to the range [-1,+1] or [0,1]. I believe that it comes down to your original data a lot. If your original data has SOME extreme values for some columns, then in my opinion you lose some definition when scaling linearly, for example in the range [-1,1]. Let's say that you have a column where 90% of values are between 100-500 and in the remaining 10% the values are as low as -2000 and as high as +2500. If you scale this data linearly, then you'll have: You could argue that the discernibility between what was originally 100 and 500 is smaller in the scaled data in comparison to what it was in the original data. At the end, I believe it very much comes down to the specifics of your data and I believe the 10% improved performance is very coincidental, you will certainly not see a difference of this magnitude in every dataset you try both scaling methods on. At the same time, in the paper in the link listed in the other answer, you can clearly see that the authors recommend data to be scaled linearly. I hope someone finds this useful! The accepted answer speaks of "Standard Scaling", which is not efficient for high-dimensional data stored in sparse matrices (text data is a use-case); in such cases, you may resort to "Max Scaling" and its variants, which works with sparse matrices.I'm trying to get some hands on experience with Keras during the holidays, and I thought I'd start out with the textbook example of timeseries prediction on stock data. So what I'm trying to do is given the last 48 hours worth of average price changes (percent since previous), predict what the average price chanege of the coming hour is. However, when verifying against the test set (or even the training set) the amplitude of the predicted series is way off, and sometimes is shifted to be either always positive or always negative, i.e., shifted away from the 0% change, which I think would be correct for this kind of thing. I came up with the following minimal example to show the issue: As you can see, I create training and testing sequences, by selecting the last 48 hours, and the next step into a tuple, and then advancing 1 hour, repeating the procedure. The model is a very simple 1 LSTM and 1 dense layer. I would have expected the plot of individual predicted points to overlap pretty nicely the plot of training sequences (after all this is the same set they were trained on), and sort of match for the test sequences. However I get the following result on training data:  Any idea what might be going on? Did I misunderstand something? Update: to better show what I mean by shifted and squashed I also plotted the predicted values by shifting it back to match the real data and multiplied to match the amplitude.  As you can see the prediction nicely fits the real data, it's just squashed and offset somehow, and I can't figure out why. I presume you are overfitting, since the dimensionality of your data is 1, and a LSTM with 25 units seems rather complex for such a low-dimensional dataset. Here's a list of things that I would try: UPDATE. Let me summarize what we discussed in the comments section.  Just for clarification, the first plot doesn't show the predicted series for a validation set, but for the training set. Therefore, my first overfitting interpretation might be inaccurate. I think an appropriate question to ask would be: is it actually possible to predict the future price change from such a low-dimensional dataset? Machine learning algorithms aren't magical: they'll find patterns in the data only if they exist. If the past price change alone is indeed not very informative of the future price change then: If values at timesteps t and t+1 happened to be more correlated in general, then I presume that the model would be more confident about this correlation and the amplitude of the prediction would be bigger.  Try all of this and try to overfit (mse should be around zero on a real dataset). Then apply regularizations. UPDATE Let me explain you why did you get by a good fit. Ok, let we consider LSTM layer as black box and forget about it. It returns us 25 values - that's all.
This value goes forward to the Dense layer, where we apply to the vector of 25 values function: Here w and b - vectors that are defined by NN and in the beginning are usually near zero. x - your values after LSTM layer and y - target (single value). While you have just 1 epoch: w and b are not fitted at all to your data (they are around zero actually). But what if you apply to your predicted value? You (someway) apply to the target variable w and b. Now w and b are single values, not vectors, and they are applied to single value. But they do (almost) the same work as Dense layer. So, increase the number of epochs to get better fitting. UPDATE2
By the way, I see some outliers in the data. You can try also to use MAE as loss/accuracy metrics. I had the same problem this week and I found the solution. The only thing that worked for me was using window normalization method as described here: https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks (Check the part about sp500 forecasting) Have a good day :) Because I'm at this point aswell batch_size increase helps completly the SimpleRNN fixes the LSTM problems with relu, elu as activation, learning rate has to be increased from default, and try and cofigure some SmoteRNN soultion for more past data, or change the time period betweens rows to aquire more data for the model to find patterns with, seems to work for me, atm, still trying to push above 80% accuracy,
accuracy = (mean_abosule_error / test.mean()) * 100I am executing https://github.com/tensorflow/tensorflow this example of detecting objects in image. I want to get count of detected objects following is the code that gives me detected object drawn in an image. But I am not able to get count of detected objects. This is the block of code that gives actual object detection shown in below image:  How can I get the object count? You can use the TensorFlow Object Counting API that is an open source framework built on top of TensorFlow that makes it easy to develop object counting systems to count any objects! Moreover, it provides sample projects so you can adopt them to develop your own specific case studies!    See the TensorFlow Object Counting API for more info and please give a star that repo for showing your support to open source community if you find it useful! Solve it simply print length of boxes.shape You should check scores and count objects as manual.
Code is here: It's important to note that the number of boxes is always 100. If you look at the code that actually draws the boxes, i.e., the vis_util.visualize_boxes_and_labels_on_image_array function, you'll see that they're defining a threshold -- min_score_thresh=.5 -- to limit the boxes drawn to only those detections in which the score is > 0.5. You can think of this as only drawing boxes where the probability of accurate detection is >50%. You can adjust this threshold up or down to increase the number of boxes drawn. If you decrease it too low, however, you will get a lot of inaccurate boxes. add this part to count objects count is the number of objects detected this part will print count but will print it in continuous manner can it be used to print only once like final count = some value instead of printing it repeatedlyThis is very common use in Machine learning pipelines.
How does this work ? I have never understood the meaning of '-1' in reshape.  An exact question is this
But no solid explanation. Any answers pls ?  in numpy, creating a matrix of 100X100 items is like this: numpy internally stores all these 10000 items in an array of 10000 items regardless of the shape of this object, this allows us to change the shape of this array into any dimensions as long as the number of items on the array does not change for example, reshaping our object to 10X1000 is ok as we keep the 10000 items: reshaping to 10X2000 wont work as we does not have enough items on the list so back to the -1 question, what it does is the notation for unknown dimension,  meaning: 
let numpy fill the missing dimension with the correct value so my array remain with the same number of items. so this: is equivalent to this: internally what numpy does is just calculating 10000 / 10 to get the missing dimension. -1 can even be on the start of the array or in the middle.  the above two examples are equivalent to this: if we will try to mark two dimensions as unknown, numpy will raise an exception as it cannot know what we are meaning as there are more than one way to reshape the array. It means, that the size of the dimension, for which you passed -1, is being inferred. Thus, means, "reshape A so that its second dimension has a size of 28*28 and calculate the correct size of the first dimension". See documentation of reshape.This graph trains a simple signal identity encoder, and in fact shows that the weights are being evolved by the optimizer: However, when I uncomment the intermediate hidden layer and train the resulting network, I see that the weights are not evolving anymore: The evaluation of estimate0 converging quickly in some fixed value that becomes independient from the input signal. I have no idea why this is happening Question: Any idea what might be wrong with the second example? TL;DR: the deeper the neural network becomes, the more you should pay attention to the gradient flow (see this discussion of "vanishing gradients"). One particular case is variables initialization. I've added tensorboard summaries for the variables and gradients into both of your scripts and got the following: 2-layer network  3-layer network  The charts show the distributions of W:0 variable (the first layer) and how they are changed from 0 epoch to 1000 (clickable). Indeed, we can see, the rate of change is much higher in a 2-layer network. But I'd like to pay attention to the gradient distribution, which is much closer to 0 in a 3-layer network (first variance is around 0.005, the second one is around 0.000002, i.e. 1000 times smaller). This is the vanishing gradient problem. Here's the helper code if you're interested: All deep networks suffer from this to some extent and
there is no universal solution that will auto-magically fix any network. But there are some techniques that can push it in the right direction. Initialization is one of them. I replaced your normal initialization with: There are lots of tutorials on Xavier init, you can take a look at this one, for example.
Note that I set the bias init to be slightly positive to make sure that ReLu outputs are positive for the most of neurons, at least in the beginning. This changed the picture immediately:  The weights are still not moving quite as fast as before, but they are moving (note the scale of W:0 values) and the gradients distribution became much less peaked at 0, thus much better. Of course, it's not the end. To improve it further, you should implement the full autoencoder, because currently the loss is affected by the [0,0] element reconstruction, so most outputs aren't used in optimization. You can also play with different optimizers (Adam would be my choice) and the learning rates. That looks very exciting. Where exactly does this code belong? I've only recently discovered TensorBoard is this in callbacks somehow: is this after fiting:I am trying to make a simple proof-of-concept where I can see the probabilities of different classes for a given prediction. However, everything I try seems to only output the predicted class, even though I am using a softmax activation. I am new to machine learning, so I'm not sure if I am making a simple mistake or if this is a feature not available in Keras.  I'm using Keras + TensorFlow. I have adapted one of the basic examples given by Keras for classifying the MNIST dataset. My code below is exactly the same as the example, except for a few (commented) extra lines that exports the model to a local file. Then the second part of this is a simple script that should import the model, predict the class for some given data, and print out the probabilities for each class. (I am using the same mnist class included with the Keras codebase to make an example as simple as possible). If I run the first script to export the model, then the second script to classify some examples, I get the following output: This is great for seeing which class each is predicted to be, but what if I want to see the relative probabilities of each class for each example? I am looking for something more like this: In other words, I need to know how sure each prediction is, not just the prediction itself. I thought seeing the relative probabilities was a part of using a softmax activation in the model, but I can't seem to find anything in the Keras documentation that would give me probabilities instead of the predicted answer. Am I making some kind of silly mistake, or is this feature not available? So it turns out that the problem was I was not fully normalizing the data in the prediction script. My prediction script should have had the following lines: Because the data was not cast to float, and divided by 255 (so it would be between 0 and 1), it was just showing up as 1s and 0s. Keras predict indeed returns probabilities, and not classes. Cannot reproduce your issue with my system configuration: Here is my prediction output for your x_slice with the loaded model (trained for 20 epochs, as in your code): I suspect some rounding issue when printing (or you have trained for much more epochs, and your probabilities for the training set have gotten very close to 1)...  To convince yourself that you indeed get probabilities and not class predictions, I suggest to try getting predictions from your model trained for a single epoch; normally you should see much less 1.0's - here is the case here for a model trained for epochs=1:I have a dataset consisting of 70,000 numeric values representing distances ranging from 0 till 50, and I want to cluster these numbers; however, if I'm trying the classical clustering approach, then I would have to establish a 70,000X70,000 distance matrix representing the distances between each two numbers in my dataset, which won't fit in memory, so I was wondering if there is any smart way to solve this problem without the need to do stratified sampling?
I also tried bigmemory and big analytics libraries in R but still can't fit the data into memory 70000 is not large. It's not small, but it's also not particularly large... The problem is the limited scalability of matrix-oriented approaches. But there are plenty of clustering algorithms which do not use matrixes and do no need O(n^2) (or even worse, O(n^3)) runtime. You may want to try ELKI, which has great index support (try the R*-tree with SortTimeRecursive bulk loading). The index support makes it a lot lot lot faster. If you insist on using R, give at least kmeans a try and the fastcluster package. K-means has runtime complexity O(n*k*i) (where k is the parameter k, and i is the number of iterations); fastcluster has an O(n) memory and O(n^2) runtime implementation of single-linkage clustering comparable to the SLINK algorithm in ELKI. (The R "agnes" hierarchical clustering will use O(n^3) runtime and O(n^2) memory). Implementation matters. Often, implementations in R aren't the best IMHO, except for core R which usually at least has a competitive numerical precision. But R was built by statisticians, not by data miners. It's focus is on statistical expressiveness, not on scalability. So the authors aren't to blame. It's just the wrong tool for large data. Oh, and if your data is 1-dimensional, don't use clustering at all. Use kernel density estimation. 1 dimensional data is special: it's ordered. Any good algorithm for breaking 1-dimensional data into inverals should exploit that you can sort the data. You can use kmeans, which normally suitable for this amount of data, to calculate an important number of centers (1000, 2000, ...) and perform a hierarchical clustering approach on the coordinates of these centers.Like this the distance matrix will be smaller. Another non-matrix oriented approach, at least for visualizing cluster in big data, is the largeVis algorithm by Tang et al. (2016). The largeVis R package has unfortunately been orphaned on CRAN due to lacking package maintenance, but a (maintained?) version can still be compiled from its gitHub repository via (having installed Rtools), e.g., A python version of the package exists as well. The underlying algorithm uses segmentation trees and a neigbourhood refinement to find the K most similar instances for each observation and then projects the resulting neigbourhood network into dim lower dimensions. Its been implemented in C++ and uses OpenMP (if supported while compiling) for multi-processing; it has thus been sufficiently fast for clustering any larger data sets I have tested so far.I'm using python 2.7. I know this will be very basic, however I'm really confused and I would like to have a better understanding of seaborn. I have two numpy arrays X and y and I'd like to use Seaborn to plot them. Here is my X numpy array: And here is the y numpy array: How can I successfully plot my data points taking into account the class label from the y array?   Thank you, You can use seaborn functions to plot graphs. Do dir(sns) to see all the plots. Here is your output in sns.scatterplot. You can check the api docs here or example code with plots here Gives  
Is this the exact input output you wanted? You can do it with pyplot, just importing seaborn changes pyplot color and plot scheme Here is a stackoverflow post of doing the same with sns.lmplotSetting As already mentioned in the title, I got a problem with my custom loss function, when trying to load the saved model. My loss looks as follows: So during training, I used the weighted_loss function as loss function and everything worked well. When training is finished I save the model as .h5file with the standard model.save function from keras API. Problem When I am trying to load the model via I am getting a ValueError telling me that the loss is unknown. Error The error message looks as follows: Questions How can I fix this problem? May it be possible that the reason for that is my wrapped loss definition? So keras doesn't know, how to handle the weights variable? Your loss function's name is loss (i.e. def loss(y_true, y_pred):). Therefore, when loading back the model you need to specify 'loss' as its name: For full examples demonstrating saving and loading Keras models with custom loss functions or models, please have a look at the following GitHub gist files: Custom loss function defined using a wrapper:
https://gist.github.com/ashkan-abbasi66/a81fe4c4d588e2c187180d5bae734fde Custom loss function defined by subclassing:
https://gist.github.com/ashkan-abbasi66/327efe2dffcf9788847d26de934ef7bd Custom model:
https://gist.github.com/ashkan-abbasi66/d5a525d33600b220fa7b095f7762cb5b Note:
I tested the above examples on Python 3.8 with Tensorflow 2.5.I built a simple network from a tutorial and I got this error: RuntimeError: Expected object of type torch.cuda.FloatTensor but found
  type torch.FloatTensor for argument #4 'mat1' Any help? Thank you! TL;DR
This is the fix Why?!
There is a slight difference between torch.nn.Module.to() and torch.Tensor.to(): while Module.to() is an in-place operator, Tensor.to() is not. Therefore Changes net itself and moves it to device. On the other hand does not change inputs, but rather returns a copy of inputs that resides on device. To use that "on device" copy, you need to assign it into a variable, henceI'm new to Machine Learning and Tensorflow, since I don't know python so I decide to use there javascript version (maybe more like a wrapper).  The problem is I tried to build a model that process the Natural Language. So the first step is tokenizer the text in order to feed the data to model. I did a lot research, but most of them are using python version of tensorflow that use method like: tf.keras.preprocessing.text.Tokenizer which I can't find similar in tensorflow.js. I'm stuck in this step and don't know how can I transfer text to vector that can feed to model. Please help :) To transform text to vectors, there are lots of ways to do it, all depending on the use case. The most intuitive one, is the one using the term frequency, i.e , given the vocabulary of the corpus (all the words possible), all text document will be represented as a vector where each entry represents the occurrence of the word in text document. With this vocabulary : the following text:  will be transformed as this vector:  One of the disadvantage of this technique is that there might be lots of 0 in the vector which has the same size as the vocabulary of the corpus. That is why there are others techniques. However the bag of words is often referred to. And there is a slight different version of it using tf.idf  There is also the following module that might help to achieve what you want Well, I faced this issue and handled it by following below steps: I have created an npm module for this recently also if it helps anyone. https://github.com/rjmacarthy/string-tokeniserIs there an option to automatically detect the spoken language using Google Cloud Platform Machine Learning's Speech API?  https://cloud.google.com/speech/docs/languages indicates the list of the languages supported and user needs to be manually set this parameter to perform speech-to-text.  Thanks
Mahesh As of last month, Google added support for detection of spoken languages into its speech-to-text API. Google Cloud Speech v1p1beta1 It’s a bit limited though - you have to provide a list of probable language codes, up to 3 of them only, and it’s said to be supported only for voice command and voice search modes.  It’s useful if you have a clue what other languages may be in your audio.  From their docs: alternative_language_codes[]: string Optional A list of up to 3 additional BCP-47 language tags, listing
  possible alternative languages of the supplied audio. See Language
  Support for a list of the currently supported language codes. If
  alternative languages are listed, recognition result will contain
  recognition in the most likely language detected including the main
  language_code. The recognition result will include the language tag of
  the language detected in the audio. NOTE: This feature is only
  supported for Voice Command and Voice Search use cases and performance
  may vary for other use cases (e.g., phone call transcription).” Requests to Google Cloud Speech API require the following configuration parameters: encoding, sampleRateHertz and languageCode. 
https://cloud.google.com/speech/reference/rest/v1/RecognitionConfig Thus, it is not possible for the Google Cloud Speech API service to automatically detect the language used. The service will be configured by this parameter (languageCode) to start recognizing speech in that specific language. If you had in mind a parallel with Google Cloud Translation API, where the input language is automatically detected, please consider that automatically detecting the language used in an audio file requires much more bandwidth, storage space and processing power than in a text file. Also, Google Cloud Speech API offers Streaming Speech Recognition, a real-time speech-to-text service, where the languageCode parameter is especially required.I am having trouble fine tuning an Inception model with Keras.  I have managed to use tutorials and documentation to generate a model of fully connected top layers that classifies my dataset into their proper categories with an accuracy over 99% using bottleneck features from Inception. Epoch 50/50
  1424/1424 [==============================] - 17s 12ms/step - loss: 0.0398 - acc: 0.9909 I have also been able to stack this model on top of inception to create my full model and use that full model to successfully classify my training set. inspection of the results from processing on this full model match the accuracy of the bottleneck generated fully connected model.  Here is the problem:
When I take this full model and attempt to train it, Accuracy drops to 0 even though validation remains above 99%.  Epoch 1/50
  89/89 [==============================] - 388s 4s/step - loss: 13.5787 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.9937 and it gets worse as things progress Epoch 21/50
  89/89 [==============================] - 372s 4s/step - loss: 7.3850 - acc: 0.0035 - val_loss: 0.5813 - val_acc: 0.8272 The only thing I could think of is that somehow the training labels are getting improperly assigned on this last train, but I've successfully done this with similar code using VGG16 before. I have searched over the code trying to find a discrepancy to explain why a model making accurate predictions over 99% of the time drops its training accuracy while maintaining validation accuracy during fine tuning, but I can't figure it out. Any help would be appreciated. Information about the code and environment: Things that are going to stand out as weird, but are meant to be that way: I am using: I have checked out: but they appear unrelated. Note: Since your problem is a bit strange and difficult to debug without having your trained model and dataset, this answer is just a (best) guess after considering many things that may have could go wrong. Please provide your feedback and I will delete this answer if it does not work. Since the inception_V3 contains BatchNormalization layers, maybe the problem is due to (somehow ambiguous or unexpected) behavior of this layer when you set trainable parameter to False (1, 2, 3, 4). Now, let's see if this is the root of the problem: as suggested by @fchollet, set the learning phase when defining the model for fine-tuning: Side Note: This is not causing any problem in your case, but keep in mind that when you use top_model(base_model.output) the whole Sequential model (i.e. top_model) is stored as one layer of fullModel. You can verify this by either using fullModel.summary() or print(fullModel.layers[-1]). Hence when you used: you are actually not freezing the last layer of base_model as well. However, since it is a Concatenate layer, and therefore does not have trainable parameters, no problem occurs and it would behave as you intended. Like the previous reply, I'll try to share some thoughts to see whether it helps. There are a couple of things that called my attention (and maybe are worth reviewing). Note: some of them should have given you issues with the separate models as well. I hope that helps.I used caret for logistic regression in R: The default metric printed is accuracy and Cohen kappa. I want to extract the matching metrics like sensitivity, specificity, positive predictive value etc. but I cannot find an easy way to do it. The final model is provided but it is trained on all the data (as far as I can tell from documentation), so I cannot use it for predicting anew.  Confusion matrix calculates all required parameters, but passing it as a summary function doesn't work: Is there a way to extract this information in addition to accuracy and kappa, or somehow find it in the train_object returned by the caret train? Thanks in advance! Caret already has summary functions to output all the metrics you mention: defaultSummary outputs Accuracy and Kappa
twoClassSummary outputs AUC (area under the ROC curve - see last line of answer), sensitivity and specificity
prSummary outputs precision and recall in order to get combined metrics you can write your own summary function which combines the outputs of these three: lets try on the Sonar data set: when defining the train control it is important to set classProbs = TRUE since some of these metrics (ROC and prAUC) can not be calculated based on predicted class but based on the predicted probabilities. Now fit the model of your choice: in this output
ROC is in fact the area under the ROC curve - usually called AUC
and 
AUC is the area under the precision-recall curve across all cutoffs.I'm wondering, which is better to use with GridSearchCV( ..., n_jobs = ... ) to pick the best parameter set for a model, n_jobs = -1 or n_jobs with a big number, like n_jobs = 30 ? Based on Sklearn documentation: n_jobs = -1 means that the computation will be dispatched on all the
  CPUs of the computer. On my PC I have an Intel i3 CPU, which has 2 cores and 4 threads, so does that mean if I set n_jobs = -1, implicitly it will be equal to n_jobs = 2 ? ... does that mean if I set n_jobs = -1, implicitly it will be equal to n_jobs = 2 ? python ( scipy / joblib inside a GridSearchCV() ) used to detect the number of CPU-cores, that is reasonable to schedule concurrent ( independent ) processes, given a request was done with an n_jobs = -1 setting.  Funny to see a 3-CPU-core? In some virtualised-machine cases, that can synthetically emulate CPU / cores, the results are not as trivial as in your known Intel CPU / i3 case. If in doubts, one can test this with a trivialised case ( on an indeed small data-set, not the full-blown model-space search ... ) and let the story go on to prove this. A similar host-platform "self-detection" may report more details for different systems / settings: Or ... which is better to use with GridSearchCV to pick the best parameter set for a model, n_jobs = -1 or n_jobs with a big number like n_jobs = 30 ? The Scikit tools ( and many other followed this practice ) used to spawn, on n_jobs directive being used, a required amount of concurrent process-instances ( so as to escape from shared GIL-lock stepping - read more on this elsewhere if interested in details ). This process-instantiation is not cost-free ( both time-wise, i.e. spending a respectfull amount of the [TIME]-domain costs, but also space-wise, i.e. spending at least an n_jobs-times the RAM-allocations of the single python process-instance in [SPACE]-domain ). Given this, your fight is a battle against a dual-edged sword. An attempt to "underbook" CPU will let ( some ) CPU-cores possibly idling.
An attempt to "overbook" RAM-space will turn your performance worse than expected, as virtual-memory will turn operating system swapping, which turns your Machine Learning-scaled data-access times from ~ 10+[ns] more than 100,000 x slower ~ 10+ [ms] which is hardly what one will be pleased at. The overall effects of n_jobs = a_reasonable_amount_of_processes is subject of Amdahl's Law ( the re-formulated one, not an add-on overhead-naive version ), so there will be a practical optimality peak ( a maximum ) of how many CPU-cores will help to improve one's processing intentions, beyond of which the overhead-costs ( sketched for both the [TIME]- and [SPACE]-domains above ) will actually deteriorate any potential positive impact expectations. Having used RandomForestRegressor() on indeed large data-sets in production, I can tell you the [SPACE]-domain is your worse of the  enemies in trying to grow n_jobs any farther and none system-level tuning will ever overcome this boundary ( so more and more ultra-low latency RAM and more and more ( real ) CPU-cores is the only practical recipe for going into indeed any larger n_jobs computing plans ). An additional simpler answer by Prof. Kevyn Collins-Thompson, from course Applied Machine Learning in Python: If I have 4 cores in my system, n_jobs = 30 (30 as an example) will be the same as n_jobs = 4. So no additional effect So the maximum performance that can be obtained always is using n_jobs = -1I have a matrice of A(369x10) which I want to cluster in 19 clusters.
I use this method which yields
idx(369x1) and ctrs(19x10) I get the point up to here.All my rows in A is clustered in 19 clusters. Now I have an array B(49x10).I want to know where the rows of this B corresponds in the among given 19 clusters. How is it possible in MATLAB? Thank you in advance The following is a a complete example on clustering: I can't think of a better way to do it than what you described.  A built-in function would save one line, but I couldn't find one.  Here's the code I would use: I don't know if I get your meaning right, but if you want to know which cluster your points belong you can use KnnSearch function easily. It has two arguments and will search in first argument for the first one of them that is closest to argument two.  Assuming you're using squared euclidean distance metric, try this: predicted should then contain the index of the closest centroid, and distances should contain the distances to the closest centroid. Take a look inside the kmeans function, at the subfunction 'distfun'. This shows you how to do the above, and also contains the equivalents for other distance metrics. for small amount of data, you could do but this is somewhat obscure; the bsxfun with the permuted ctrs creates a 49 x 10 x 19 array of booleans, which is then 'all-ed' across the second dimension, permuted back and then the row ids are found. again, probably not practical for large amounts of data.In sklearn, a serial pipeline can be defined to get the best combination of hyperparameters for all consecutive parts of the pipeline. A serial pipeline can be implemented as follows: But what if I want to try different algorithms for each step of the pipeline? How can I e.g. gridsearch over  Principal Component Analysis OR Singular Value Decomposition AND
  Support Vector machines OR Random Forest This would require some kind of 2nd level or "meta-gridsearch", since the type of model would be one of the hyperparameters. Is that possible in sklearn?  Pipeline supports None in its steps(list of estimators) by which certain part of the pipeline can be toggled off. You can pass None parameter to the named_steps of the pipeline to not use that estimator by setting that in params passed to GridSearchCV. Lets assume you want to use PCA and TruncatedSVD. Add svd in pipeline and now just pass the pipeline object to gridsearchCV Calling grd.fit() will search the parameters over both the elements of the params_grid list, using all values from one at a time. If both estimators in your "OR" have same name of parameters as in this case, where PCA and TruncatedSVD has n_components (or you just want to search over this parameter, this can be simplified as: Generalization of this scheme We can make a function which can automatically populate our param_grid to be supplied to the GridSearchCV using appropriate values:- And use this function on any number of transformers and estimators Now initialize a pipeline object with names as used in above pipeline_steps Now, finally set out gridSearchCV object and fit dataI have a data matrix X (60x208) and a matrix of labels Y (1x208). I want to split my data matrix X into two random subsets of column vectors: training (which will be 70% of the data) and testing (which will be 30% of the data), but I need to still be able to identify which label from Y corresponds to each column vector. I couldn't find any function to do this, any ideas? EDIT: Thought I should add, there are only two labels in Y: 1 and 2 (not sure if this makes a difference) That's pretty easy to do.  Use randperm to generate a random permutation of indices from 1 up to as many points as you have... which is 208 in your case. Once you generate this sequence, simply use this and subset into your X and Y to extract the training and test data and labels.  As such, do something like this: The split_point determines how many points we need to place into our training set, and we will need to round it in case this calculation yields any decimal points.  I also didn't hard code 208 in there because your data set might grow and so this will work with any size data set you choose.  X_train and Y_train will contain your data and labels for your training set while X_test and Y_test will contain your data and labels for your test set. As such, the first column of X_train is your data point for the first element of your training set, with the first element of Y_train serving as the label for that particular point... and so on and so forth!I am trying to finetune/retrain InceptionV1 model here, on my own data. I was able to Convert the Image data to TFR format data using this. Pass the converted data to finetune_inception_v1_on_flowers  Complete the training and evaluation in according to the script file above, I am attaching the logs here. 4.The training process genertaed many checkpoints, two graph.pbtxt files. I used the latest checkpoint and the graph.pbtxt file in the freeze tool and generated a .pb file, according to the discussion here, I used following parameters --input_graph=/../../graph.pbtxt --output_node_names=InceptionV1/Logits/Predictions/Softmax   No Operation named [input] in the Graph" following are the changes I made toClassifierActivity.java private static final int INPUT_SIZE = 224;//224//299  private static final int IMAGE_MEAN = 117;//117//128 private static final float IMAGE_STD = 1;//1//128  private static final String INPUT_NAME ="input";//input  private static final String OUTPUT_NAME ="InceptionV1/Logits/Predictions/Softmax";//output private static final String MODEL_FILE ="file:///android_asset/frozen_1000_graph.pb";//tensorflow_inception_graph private static final String LABEL_FILE ="file:///android_asset/labels.txt";//imagenet_comp_graph_label_strings No inputs spotted. No variables spotted. Found 1 possible outputs:
  (name=InceptionV1/Logits/Predictions/Softmax, op=Softmax) Found
  5598451 (5.60M) const parameters, 0 (0) variable parameters, and 114
  control_edges Op types used: 472 Const, 230 Mul, 173 Add, 172 Sub, 116
  Identity, 114 Sum, 58 Reshape, 58 Conv2D, 57 Rsqrt, 57 Relu, 57
  Reciprocal, 57 Square, 57 SquaredDifference, 57 Mean, 57 StopGradient,
  13 MaxPool, 9 ConcatV2, 1 Squeeze, 1 RandomUniform, 1 Softmax, 1
  RealDiv, 1 QueueDequeueV2, 1 Floor, 1 FIFOQueueV2, 1 BiasAdd, 1
  AvgPool. Please help me understand, how I can fix this issue. Here are the input to the network created, so if you can add 
images = tf.identity(images, name='Inputs') to name the tensor to the network.Im new in spark and Machine learning in general.
I have followed with success some of the Mllib tutorials, i can't get this one working: i found the sample code here :
https://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression (section LinearRegressionWithSGD) here is the code: (that's exactly what's is on the website) The result is training Mean Squared Error = 6.2087803138063045 and gives My problem here is predictions looks totally random (and wrong), and since its the perfect copy of the website example, with the same input data (training set), i don't know where to look, am i missing something ? Please give me some advices or clue about where to search, i can read and experiment. Thanks As explained by zero323 here, setting the intercept to true will solve the problem. If not set to true, your regression line is forced to go through the origin, which is not appropriate in this case. (Not sure, why this is not included in the sample code) So, to fix your problem, change the following line in your code (Pyspark): to Although not mentioned explicitly, this is also why the code from 'selvinsource' in the above question is working. Changing the step size doesn't help much in this example.  Linear Regression is SGD based and requires tweaking the step size, see http://spark.apache.org/docs/latest/mllib-optimization.html for more details. In your example, if you set the step size to 0.1 you get better results (MSE = 0.5). For another example on a more realistic dataset, see  https://github.com/selvinsource/spark-pmml-exporter-validator/blob/master/src/main/resources/datasets/winequalityred_linearregression.md https://github.com/selvinsource/spark-pmml-exporter-validator/blob/master/src/main/resources/spark_shell_exporter/linearregression_winequalityred.scalaAs suggested in this answer, I tried to use joblib to train multiple scikit-learn models in parallel. However, the call to the last line takes ages without utilizing the CPU, in fact it just seems to block and never return anything. What is my mistake?  A test with a very small amount of data in XTrain suggests that copying of the numpy array across multiple processes is not the reason for the delay.  Check both the CPU% and also other resources' state figures across this node. Having read your profile was a stunning moment, Sir: I am a computer scientist specializing on algorithms and data analysis by training, and a generalist by nature. My skill set combines a strong scientific background with experience in software architecture and development, especially on solutions for the analysis of big data. I offer consulting and development services and I am looking for challenging projects in the area of data science. The problem IS deeply determined by a respect to elementary Computer Science + algorithm rules. The problem IS NOT demanding a strong scientific background, but a common sense. The problem IS NOT any especially Big Data but requires to smell how the things actually work.  Step #1:
Never hire or fire straight each and every Consultant, who does not respect facts ( the answer referred above did not suggest anything, the less granted any promises ). Ignoring facts might be a "successful sin" in PR / MARCOM / Advertisement / media businesses  ( in case The Customer tolerates such dishonesty and/or manipulative habit ) , but not in a scientifically fair quantitative domains. This is unforgivable. Step #2:
Never hire or fire straight each and every Consultant, who claimed having experience in software architecture, especially on solutions for ... big data but pays zero attention to the accumulated lumpsum of all the add-on overhead costs that are going to be introduced by each of the respective elements of the system architecture, once the processing started to go distributed across some pool of hardware and software resources. This is unforgivable. Step #3:
Never hire or fire straight each and every Consultant, who turns passive aggressive once facts do not fit her/his wishes and starts to accuse other knowledgeable person who have already delivered a helping hand to rather "improve ( their ) communication skills" instead of learning from mistake(s). Sure, skill may help to express the obvious mistakes in some other way, yet, the gigantic mistakes will remain gigantic mistakes and each and every scientist, being fair to her/his scientific title, should NEVER resort to attack on a helping colleague, but rather start searching for the root cause of the mistakes, one after the other. This --- @sascha ... May I suggest you take little a break from stackoverflow to cool off, work a little on your interpersonal communication skills  --- was nothing but a straight and intellectually unacceptable nasty foul to @sascha. The imperative form of a syntax-constructor ignites an immense amount of activities to start: To at least guess what happens, a scientifically fair approach would be to test several representative cases, benchmarking their actual execution, collect quantitatively supported facts and draw a hypothesis on a model of behaviour and its principal dependencies on CPU_core-count, on RAM-size, on <aFunction>-complexity and resources-allocation envelopes etc. Having collected representatively enough data on this NOP-case over a reasonably scaled 2D-landscape of [ RUNS_TO_RUN, JOBS_TO_SPAWN]-cartesian-space DataPoints, so as to generate at least some first-hand experience of the actual system costs of launching an actually intrinsically empty-processes' overhead workloads, related to the imperatively instructed joblib.Parallel(...)( joblib.delayed(...) )-syntax constructor, spawning into the system-scheduler just a few joblib-managed a_NOP_FUN() instances. Let's also agree that all the real-world problems, Machine Learning models included, are way more complex tools, that the just tested a_NOP_FUN(), while in both cases, you have to pay the already benchmarked overhead costs ( even if it was paid for getting literally zero product ). Thus a scientifically fair, rigorous work will follow from this simplest ever case, already showing the benchmarked costs of all the associated setup-overheads a smallest ever joblib.Parallel() penalty sine-qua-non forwards into a direction, where real world algorithms live - best with next adding some larger and larger "payload"-sizes into the testing loop: Again,
collect a representatively enough quantitative data about the costs of actual remote-process MEM-allocations, by running a a_NOP_FUN_WITH_JUST_A_MEM_ALLOCATOR() over some reasonable wide landscape of SIZE1D-scaling,
again
over a reasonably scaled 2D-landscape of [ RUNS_TO_RUN, JOBS_TO_SPAWN]-cartesian-space DataPoints, so as to touch a new dimension in the performance scaling, under an extended black-box PROCESS_under_TEST experimentation inside the joblib.Parallel() tool, leaving its magics yet left un-opened. One may soon notice, that not only the static-sizing matters, but also the MEM-transport BANDWIDTH ( hardware-hardwired ) will start cause problems, as moving data from/to CPU into/from MEM costs well ~ 100 .. 300 [ns], a way more, than any smart-shuffling of the few bytes "inside" the CPU_core, { CPU_core_private | CPU_core_shared | CPU_die_shared }-cache hierarchy-architecture alone ( and any non-local NUMA-transfer exhibits the same order of magnitude add-on pain ). So let's start to burn the oil! If all above was fine for starting to smell how the things under the hood actually work, this will grow to become ugly and dirty.  Still nothing extraordinary, compared to the common grade of payloads in the domain of a Machine Learning many-D-space, where all dimensions of the { aMlModelSPACE, aSetOfHyperParameterSPACE, aDataSET }-state-space impact the  scope of the processing required ( some having O( N ), some other O( N.logN ) complexity ), where almost immediately, where well engineered-in more than just one CPU_core soon gets harnessed even on a single "job"-being run. An indeed nasty smell starts, once a naive ( read resources-usage un-coordinated ) CPU-load mixtures get down the road and when mixes of task-related CPU-loads start to get mixed with naive ( read resources-usage un-coordinated ) O/S-scheduler processes happen to fight for common ( resorted to just a naive shared-use policy ) resources - i.e. MEM ( introducing SWAPs as HELL ), CPU ( introducing cache-misses and MEM re-fetches ( yes, with SWAPs penalties added ), not speaking about paying any kind of more than ~ 15+ [ms] latency-fees, if one forgets and lets a process to touch a fileIO-( 5 (!)-orders-of-magnitude slower + shared + being a pure-[SERIAL], by nature )-device. No prayers help here ( SSD included, just a few orders of magnitude less, but still a hell to share & running a device incredibly fast into its wear + tear grave ). Virtual memory paging and swaps start to literally deteriorate the rest of the so far somehow "just"-by-coincidence-( read: weakly-co-ordinated )-[CONCURRENTLY]-scheduled processing ( read: further-decreased individual PROCESS-under-TEST performance ). Again - fact matters: a light-weight resources-monitor class may help: Similarly a bit richer constructed resources-monitor may report a wider O/S context, to see where additional resource stealing / contention / race-conditions deteriorate the actually achieved process-flow: Besides the gradually built records of evidence, how the real-world system-deployment add-on overheads accumulate the costs, the recently re-formulated Amdahl's Law, extended so as to cover both the add-on overhead-costs plus the "process-atomicity" of the further indivisible parts' sizing, defines a maximum add-on costs threshold, that might be reasonable paid, if some distributed processing is to provide any above >= 1.00 computing process speedup. Dis-obeying the explicit logic of the re-formulated Amdahl's Law causes a process to proceed worse than if having been processed in a pure-[SERIAL] process-scheduling ( and sometimes the results of poor design and/or operations practices may look as if it were a case, when a joblib.Parallel()( joblib.delayed(...) ) method "blocks the process" ).My training images are downscaled versions of their associated HR image. Thus, the input and the output images aren't the same dimension. For now, I'm using a hand-crafted sample of 13 images, but eventually I would like to be able to use my 500-ish HR (high-resolution) images dataset. This dataset, however, does not have images of the same dimension, so I'm guessing I'll have to crop them in order to obtain a uniform dimension. I currently have this code set up: it takes a bunch of 512x512x3 images and applies a few transformations to augment the data (flips). I thus obtain a basic set of 39 images in their HR form, and then I downscale them by a factor of 4, thus obtaining my trainset which consits of 39 images of dimension 128x128x3. My question is: in my case, can I use the Preprocessing tool that Keras offers? I would ideally like to be able to input my varying sized images of high quality, crop them (not downsize them) to 512x512x3, and data augment them through flips and whatnot. Substracting the mean would also be part of what I'd like to achieve. That set would represent my validation set. Reusing the validation set, I want to downscale by a factor of 4 all the images, and that would generate my training set. Those two sets could then be split appropriately to obtain, ultimately, the famous X_train Y_train X_test Y_test. I'm just hesitant about throwing out all the work I've done so far to preprocess my mini sample, but I'm thinking if it can all be done with a single built-in function, maybe I should give that a go. This is my first ML project, hence me not understanding very well Keras, and the documentation isn't always the clearest. I'm thinking that the fact that I'm working with a X and Y that are different in size, maybe this function doesn't apply to my project. Thank you! :) Yes you can use keras preprocessing function. Below some snippets to help you... Christof Henkel's suggestion is very clean and nice. I would just like to offer another way to do it using imgaug, a convenient way to augment images in lots of different ways. It's usefull if you want more implemented augmentations or if you ever need to use some ML library other than Keras. It unfortunatly doesn't have a way to make crops that way but it allows implementing custom functions. Here is an example function for generating random crops of a set size from an image that's at least as big as the chosen crop size: You can then combine this function with any other builtin imgaug function, for example the flip functions that you're already using like this: This function could then generate lots of different crops from each image. An example image with some possible results (note that it would result in actual (128, 128, 3) images, they are just merged into one image here for visualization):   Your image set could then be generated by: It would also be simple to add new functions to be applied to the images, for example the remove mean functions you mentioned. Here's another way performing random and center crop before resizing using native ImageDataGenerator and flow_from_directory. You can add it as preprocess_crop.py module into your project. It first resizes image preserving aspect ratio and then performs crop. Resized image size is based on crop_fraction which is hardcoded but can be changed. See crop_fraction = 0.875 line where 0.875 appears to be the most common, e.g. 224px crop from 256px image. Note that the implementation has been done by monkey patching keras_preprocessing.image.utils.loag_img function as I couldn't find any other way to perform crop before resizing without rewriting many other classes above. Due to these limitations, the cropping method is enumerated into the interpolation field. Methods are delimited by : where the first part is interpolation and second is crop e.g. lanczos:random. Supported crop methods are none, center, random. When no crop method is specified, none is assumed. Just drop the preprocess_crop.py into your project to enable cropping. The example below shows how you can use random cropping for the training and center cropping for validation: Here's preprocess_crop.py file to include with your project:As an end result, I would like a computer program which can accept a list of inputs and outputs and then apply the same algorithm that went into those input/output's on another number, I.e: If given this list of input/output's It would realize that the algorithm would be (input * 2), or (output / 2) depending on what we wanted. So, if given the number 16, and asked to produce an output the program would respond with 32. And if given the number 10 and asked to produce an input, it would respond with 5. It would obviously be rather simple to 'hardcode' this into the program, although I'd like to learn how to have the program teach itself what the algorithm is. I understand that this will get rather complicated rather fast. you can not do this reliably for any type of input/output signal dependency instead you should support only some otherwise you need some kind of AI or very complex neural network + many functional generators with insane complexity and unknown reliability of the solution ... I would simplify this to dependencies like: polynomial up to some degree  exponential other Anyway I think just 3 input points will be not enough So at first you should determine which type of dependency it is and then try to find the coefficients of that particular function generator. For example: If you have mixed type signals then you need much more input points covering big enough range and probably would need some kind of approximation search of coefficients minimizing the distance between known inputs and generated output. If you have enough points you can normalize dataset and use correlation coefficient to compare it with supported function generators to simplify the decisioning [Notes] So you need to specify:I am new to Tensorflow and I can't get why the input placeholder is often dimensioned with the size of the batches used for training.  In this example I found here and in the Official Mnist tutorial it is not So what is the best and right way to dimension and create the model input and train it? Here you are specifying the model input. You want to leave the Batch size, to None, that means that you can run the model with a variable number of inputs (one or more). Batching is important to efficiently use your computing resources. The next important line is: Here you are sending 50 elements as input but you can also change that to just one  Without modifying the graph. If you specify the Batch size (some number instead of None in the first snippet), then you would have to change each time and that is not ideal, specially in production.I have a model which was trained on MNIST, but when I put in a handmade sample of an image it raises ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 784 but received input with shape (None, 1) I already checked the input of the model it is in the same shape as MNIST. x_train[0].shape (784,) and my image  arr.shape  (784,) Please help! ... ValueError: Input 0 of layer sequential is incompatible with the
layer: expected axis -1 of input shape to have value 784 but received
input with shape (None, 1) You need an extra dimension in here, arr.reshape(1, 784). Here is the full working code Model Eval  InferenceFollowing is my code: It generates tri gram by removing all the stopwords. What I want it to allow those TRIGRAM what have stopword in their middle ( not in start and end) Is there processor needs to be written for this.
Need suggestions. Yes, you need to supply your own analyzer function which will convert the documents to the features as per your requirements. According to the documentation: analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable In that custom callable you need to take care of first splitting the sentence into different parts, removing special chars like comma, braces, symbols etc, convert them to lower case, then convert them to n_grams. The default implementation works on a single sentences in the following order: You need to handle all this if you want to pass a custom callable to the analyzer param in the TfidfVectorizer. OR You can extend the TfidfVectorizer class and only override the last 2 steps. Something like this: Then, use it like:Edit: The answer of this questions is heavily discussed in: Sum in Spark gone bad In Compute Cost of Kmeans, we saw how one can compute the cost of his KMeans model. I was wondering if we are able to compute the Unbalanced factor? If there is no such functionality provide by Spark, is there any easy way to implement this? I was not able to find a ref for the Unbalanced factor, but it should be similar to Yael's unbalanced_factor (my comments): which I found here. So the idea is that tot (for total) will be equal to the number of points assigned to clusters (i.e. equal to the size of our dataset), while uf (for unbalanced factor) holds the square of the number of points assigned to a cluster. Finally he uses uf = uf * n / (tot * tot); to compute it. In python it could be something like:I am working in python and tensor flow but I miss 'units' argument and I do not know how to solve it, It looks like your post is mostly code; please add some more details.It looks like your post is mostly code; please add some more details. here the code Try changing this line: model.add(Dense(output_dim=NUM_CLASSES, activation='softmax')) to  model.add(Dense(NUM_CLASSES, activation='softmax')) I'm not experience in keras but I could not find a parameter called output_dim on the documentation page for Dense. I think you meant to provide units but labelled it as output_dim The Keras Dense layer documentation is as follows: Using the following : Will work as here the units means the output_dim saying that we need 6 neurons in the hidden layer. The weights are initialized with the uniform function and the input layer has 11 independent variables of the dataset (input_dim) to feed the above-hidden layer. I think it's a version issue. In updated version of keras for Dense there is no "output_dim" argument. You can see this documentation link for Dense arguments. https://keras.io/api/layers/core_layers/dense/ So the first argument is "units", Which is mandatory. instead of this line: use this: orI'm having trouble importing Machine Learning algorithms from scikit-learn.
 I have it installed but whenever I type for example "from sklearn.naive_bayes import GaussianNB" it says " 'from' is not recognized as an internal or external command, operable program or batch file.
 I'm using Anaconda on Windows 10. Is it compatibility issue? Am I missing something? Idk I'm still new to Python so I feel lost. Thanks That needs to be run in the Python REPL, not at a command line. Be sure to start one before typing Python statements. You need to write the Import command in the Python terminal. You can activate the Python terminal using the "Python" command.   I had the same issue. Internal or External command can be solved but just 3 steps:
(Before doing make sure you undo all hidden apps) It worked for meI am performing CNN in google colab notebook in the pro version. Though the x_train takes the shape  (60,000, 28,28). The model gets trained on only 1875 rows. Did any one faced this issue before? My model runs fine on local machine's jupyter notebook. It runs on all 60,000 rows     1875 is a number of batches. By default batches contain 32 samles.
60000 / 32 = 1875 If you use keras, instead if tensorflow.keras, the log will show: But both of them are internally the same, one is showing the number of samples to be trained (keras), another is showing the number of iterations (tf.keras). You can not probably train on all the 60000 samples at a time, we need to batch the inputs so that we are not out of GPU memory. You can try increasing your batch_size as much as you can, but after a point you'll get an error like OOMError, CUDA out of memory, etc.I have the tabular data. Some of the columns contain the text data. As you can see In the picture. After cleaning the text I converted text into matrixes and object data types into categories. Here I am using the tokenizer. The final result after making tokens

After converting in trainX  and trainY = label. Here I am lost How to implement the lstm in here?. I have seen many examples mostly they have one column other is target column. So I am confused here about how to adjust this data on lstm. Here is the data if you want to look at it: link to dataI am using the SVM implementation of OpenCV (based on LibSVM) on iOS. Is it possible to obtain the weight vector after training?  Thank you! After dealing with it I have been able to obtain the weights. For obtaining the weights one has to obtain first the support vectors and then add them multiplied by the alpha values. The only trick here is that the instance variable float *decision_function is protected on the opencv framework, so I had to change it in order to access it. A cursory glance of the doc and the source code (https://github.com/Itseez/opencv/blob/master/modules/ml/src/svm.cpp) tells me that on the surface the answer is "No". The hyperplane parameters seem to be tucked away into the CvSVMSolver class. CvSVM contains an object of this class called "solver". See if you can get to its members.I have implemented EM algorithm for GMM using this post GMMs and Maximum Likelihood Optimization Using NumPy unsuccessfully as follows: when I run the algorithm on a 1-D time-series dataset, for k equal to 3, it returns an output like the following: which I believe is working wrong since the outputs are two vectors which one of them represents means values and the other one represents variances values. The vague point which made me doubtful about implementation is it returns back 0.00000000e+000 for most of the outputs as it can be seen and it doesn't need really to visualize these outputs. BTW the input data are time-series data. I have checked everything and traced multiple times but no bug shows up. Here are my input data: I was wondering if there is an elegant way to implement it via numpy or SciKit-learn. Any helps will be appreciated. Update
Following is current output and expected output:
 As I mentioned in the comment, the critical point that I see is the means initialization. Following the default implementation of sklearn Gaussian Mixture, instead of random initialization, I switched to KMeans. This seems to yield the desired output much more consistently:  Finally we can see that the purely random initialization generates different results; let's see the resulting means: One can see how different these results are, in some cases the resulting means is constant, meaning that inizalization chose 3 similar values and didn't change much while iterating. Adding some print statements inside the EM_GMM will clarify that. Another problem, Also in the initialization of variances, With these changes, here is my implementation, And here is my result.I am trying to implement a weighted average between two tensors in TensorFlow, where the weight can be learned automatically. Following the advice on how to design a custom layer for a keras model here, my attempt is the following: Now the problem is that after training the model, saving, and loading it again, the value for w remains 0.5. Is it possible that the parameter does not receive any gradient updates? When printing the trainable variables of my model, the parameter is listed and should therefore be included when calling model.fit. Here is a possibility to implement a weighted average between two tensors, where the weight can be learned automatically. I also introduce the constrain that the weights must sum up to 1. To grant this we have to simply apply a softmax on our weights. In the dummy example below I combine with this method the output of two fully-connected branches but you can manage it in every other scenario here the custom layer: here the full example in a regression problem: in the end, you can also visualize the value of the weights in this way:Below is the train.Prototxt file that is used to train a pretrained model. Using the above prototxt file accuracy reported for test set at the end of the Training is 92%. For more details please see How to evaluate the accuracy and loss of a trained model is good or not in caffe? I took the model snapshot at the end of 13000 iteration and using below python script, i tried to construct the confusion matrix, Accuracy reported is 74%.  I am using the deploy.protxt  The command used to run the script is   My doubt is Why there is a difference in accuracy when i am using the same model and same test set. Am i doing anything wrong?. Thank you in advance.  There are differences between your validation step (TEST phase) and the python code you are running: You are using a different mean file for train and test (!): for phase: TRAIN you are using mean_file: "mean.binaryproto" while for phase: TEST you are using mean_file: "painmean.binaryproto". Your python evaluation code uses the training mean file and not the validation.
It is not a good practice to have different settings for train/validation. Your input images have new_height: 256 and copr_size: 224. This settings means caffe reads the image, scales it to 256x256 and then crops the center to size 224x224. Your python code seems to only scale the input to 224x224 without cropping: you feed your net with different inputs. Please verify that you do not have any other differences between your training prototxt and deploy prototxt.After spending days failing to use neural network for Q learning, I decided to go back to the basics and do a simple function approximation to see if everything was working correctly and see how some parameters affected the learning process.
Here is the code that I came up with The problem is that all my predictions are around 0 in value. As you can see I used an ExtraTreesRegressor from sklearn (commented lines) to check that the protocol is actually correct. So what is wrong with my neural network ? Why is it not working ? (The actual problem that I'm trying to solve is to compute the Q function for the mountain car problem using neural network. How is it different from this function approximator ?) With these changes: i.e. and the rest of your code unchanged, here is the result:  Tinker, again and again... A more concise version of your code that works: Changes made: replacing low layer activations with hyperbolic tangents, replacing the static dataset with a random generator, replacing sgd with adam. That said, there still are problems with other parts of your code that I haven't been able to locate yet (most likely your scaler and random process). I managed to get a good approximation by changing the architecture and the training as in the following code. It's a bit of an overkill but at least I know where the problem was coming from. However I'm still baffled that I found papers saying that they were using only two hidden layers of five neurons to approximate the Q function of the mountain car problem and training their network for only a few minutes and get good results. I will try changing my batch size in my original problem to see what results I can get but I'm not very optimisticI am trying to solve an optimization problem, that it's very similar to the knapsack problem but it can not be solved using the dynamic programming.
The problem I want to solve is very similar to this problem:   
 indeed you may solve this with CPLEX.
Let me show you that in OPL. The model (.mod)  The data .dat and this givesANN (Artificial Neural Networks) and SVM (Support Vector Machines) are two popular strategies for supervised machine learning and classification. It's not often clear which method is better for a particular project, and I'm certain the answer is always "it depends." Often, a combination of both along with Bayesian classification is used. These questions on Stackoverflow have already been asked regarding ANN vs SVM: ANN and SVM classification what the difference among ANN, SVM and KNN in my classification question Support Vector Machine or Artificial Neural Network for text processing? In this question, I'd like to know specifically what aspects of an ANN (specifically, a Multilayer Perceptron) might make it desirable to use over an SVM? The reason I ask is because it's easy to answer the opposite question: Support Vector Machines are often superior to ANNs because they avoid two major weaknesses of ANNs: (1) ANNs often converge on local minima rather than global minima, meaning that they are essentially "missing the big picture" sometimes (or missing the forest for the trees) (2) ANNs often overfit if training goes on too long, meaning that for any given pattern, an ANN might start to consider the noise as part of the pattern. SVMs don't suffer from either of these two problems. However, it's not readily apparent that SVMs are meant to be a total replacement for ANNs. So what specific advantage(s) does an ANN have over an SVM that might make it applicable for certain situations? I've listed specific advantages of an SVM over an ANN, now I'd like to see a list of ANN advantages (if any). Judging from the examples you provide, I'm assuming that by ANNs, you mean multilayer feed-forward networks (FF nets for short), such as multilayer perceptrons, because those are in direct competition with SVMs. One specific benefit that these models have over SVMs is that their size is fixed: they are parametric models, while SVMs are non-parametric. That is, in an ANN you have a bunch of hidden layers with sizes h1 through hn depending on the number of features, plus bias parameters, and those make up your model. By contrast, an SVM (at least a kernelized one) consists of a set of support vectors, selected from the training set, with a weight for each. In the worst case, the number of support vectors is exactly the number of training samples (though that mainly occurs with small training sets or in degenerate cases) and in general its model size scales linearly. In natural language processing, SVM classifiers with tens of thousands of support vectors, each having hundreds of thousands of features, is not unheard of. Also, online training of FF nets is very simple compared to online SVM fitting, and predicting can be quite a bit faster. EDIT: all of the above pertains to the general case of kernelized SVMs. Linear SVM are a special case in that they are parametric and allow online learning with simple algorithms such as stochastic gradient descent. One obvious advantage of artificial neural networks over support vector machines is that artificial neural networks may have any number of outputs, while support vector machines have only one. The most direct way to create an n-ary classifier with support vector machines is to create n support vector machines and train each of them one by one. On the other hand, an n-ary classifier with neural networks can be trained in one go. Additionally, the neural network will make more sense because it is one whole, whereas the support vector machines are isolated systems. This is especially useful if the outputs are inter-related. For example, if the goal was to classify hand-written digits, ten support vector machines would do. Each support vector machine would recognize exactly one digit, and fail to recognize all others. Since each handwritten digit cannot be meant to hold more information than just its class, it makes no sense to try to solve this with an artificial neural network. However, suppose the goal was to model a person's hormone balance (for several hormones) as a function of easily measured physiological factors such as time since last meal, heart rate, etc ... Since these factors are all inter-related, artificial neural network regression makes more sense than support vector machine regression. One thing to note is that the two are actually very related.  Linear SVMs are equivalent to single-layer NN's (i.e., perceptrons), and multi-layer NNs can be expressed in terms of SVMs.  See here for some details. If you want to use a kernel SVM you have to guess the kernel. However, ANNs are universal approximators with only guessing to be done is the width (approximation accuracy) and height (approximation efficiency). If you design the optimization problem correctly you do not over-fit (please see bibliography for over-fitting). It also depends on the training examples if they scan correctly and uniformly the search space. Width and depth discovery is the subject of integer programming. Suppose you have bounded functions f(.) and bounded universal approximators on I=[0,1] with range again I=[0,1] for example that are parametrized by a real sequence of compact support U(.,a) with the property that there exists a sequence of sequences with and you draw examples and tests (x,y) with a distribution D on IxI. For a prescribed support, what you do is to find the best a such that Let this a=aa which is a random variable!, the over-fitting is then average using D and D^{N} of ( y - U(x,aa) )^{2} Let me explain why, if you select aa such that the error is minimized, then for a rare set of values you have perfect fit. However, since they are rare the average is never 0. You want to minimize the second although you have a discrete approximation to D. And keep in mind that the support length is free. One answer I'm missing here:
Multi-layer perceptron is able to find relation between features. For example it is necessary in computer vision when a raw image is provided to the learning algorithm and now Sophisticated features are calculated.
Essentially the intermediate levels can calculate new unknown features. We should also consider that the SVM system can be applied directly to non-metric spaces, such as the set of labeled graphs or strings. In fact, the internal kernel function can be generalized properly to virtually any kind of input, provided that the positive definiteness requirement of the kernel is satisfied. On the other hand, to be able to use an ANN on a set of labeled graphs, explicit embedding procedures must be considered.I'm new to Tensorflow and would greatly benefit from some visualizations of what I'm doing. I understand that Tensorboard is a useful visualization tool, but how do I run it on my remote Ubuntu machine? Here is what I do to avoid the issues of making the remote server accept your local external IP: What it does is that everything on the port 6006 of the server (in 127.0.0.1:6006) will be forwarded to my machine on the port 16006. You can port-forward with another ssh command that need not be tied to how you are connecting to the server (as an alternative to the other answer). Thus, the ordering of the below steps is arbitrary. from your local machine, run ssh -N -f -L localhost:16006:localhost:6006 <user@remote> on the remote machine, run: tensorboard --logdir <path> --port 6006 Then, navigate to (in this example) http://localhost:16006 on your local machine. (explanation of ssh command: -N : no remote commands -f : put ssh in the background -L <machine1>:<portA>:<machine2>:<portB> : forward <machine1>:<portA> (local scope) to <machine2>:<portB> (remote scope) You don't need to do anything fancy. Just run: and connect with your server url and port. The --host 0.0.0.0 tells tensorflow to listen from connections on all IPv4 addresses on the local machine. Another option if you can't get it working for some reason is to simply mount a logdir directory on your filesystem with sshfs: sshfs user@host:/home/user/project/summary_logs ~/summary_logs and then run Tensorboard locally.  --bind_all option is useful. The port will be automatically selected from 6006 incrementally.(6006, 6007, 6008... ) You can directly run the following command on terminal of your remote server to run tensorboard: Or you can also start the tensorboard within your ipython notebook: If your remote server is open to traffic from your local IP address, you should be able to see your remote Tensorboard. Warning: if all internet traffic can access your system (if you haven't specified a single IP address that can access it), anyone may be able to view your TensorBoard results and runaway with creating SkyNet themselves. You have to create a ssh connection using port forwarding: Then you run the tensorboard command: Then you can easily access the tensorboard in your browser under: This is not a proper answer but a troubleshooter, hopefully helps other less seasoned networkers like me. In my case (firefox+ubuntu16) the browser was connecting, but showing a blank page (with the tensorboard logo on the tab), and no log activity at all was shown. I still don't know what could be the reason for that (didn't look much into it but if anybody knows please let know!), but I solved it switching to ubuntu's default browser. Here the exact steps, pretty much the same as in @Olivier Moindrot's answer: To check that the SSH tunnel is effectively working, a simple echo server like this python script can help: As I said, hope it helps!
Cheers,
Andres Another approach is to use a reverse proxy, which allows you to view Tensorboard from any internet connected device without SSHing. This approach can make it far easier / tractable to view Tensorboard on mobile devices, for example.  Steps: 1) Download reverse proxy Ngrok on your remote machine hosting Tensorboard. See https://ngrok.com/download for instructions (~5 minute setup). 2) Run ngrok http 6006 (assuming you're hosting Tensorboard on port 6006) 3) Save the URL that ngrok outputs:  4) Enter that into any browser to view TensorBoard:  Special thanks to Sam Kirkiles For anyone who must use the ssh keys (for a corporate server). Just add -i /.ssh/id_rsa at the end. $ ssh -N -f -L localhost:8211:localhost:6007 myname@servername -i /.ssh/id_rsa While running the tensorboard give one more option --host=ip of your system and then you can access it from other system using http://ip of your host system:6006Where is an explicit connection between the optimizer and the loss?  How does the optimizer know where to get the gradients of the loss without a call liks this optimizer.step(loss)? -More context- When I minimize the loss, I didn't have to pass the gradients to the optimizer. Without delving too deep into the internals of pytorch, I can offer a simplistic answer: Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are "stored" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values. More info on computational graphs and the additional "grad" information stored in pytorch tensors can be found in this answer. Referencing the parameters by the optimizer can sometimes cause troubles, e.g., when the model is moved to GPU after initializing the optimizer.
Make sure you are done setting up your model before constructing the optimizer. See this answer for more details. When you call loss.backward(), all it does is compute gradient of loss w.r.t all the parameters in loss that have requires_grad = True and store them in parameter.grad attribute for every parameter. optimizer.step() updates all the parameters based on parameter.grad Perhaps this will clarify a little the connection between loss.backward and optim.step (although the other answers are to the point). loss.backward() sets the grad attribute of all tensors with requires_grad=True
in the computational graph of which loss is the leaf (only x in this case). Optimizer just iterates through the list of parameters (tensors) it received on initialization and everywhere where a tensor has requires_grad=True, it subtracts the value of its gradient stored in its .grad property (simply multiplied by the learning rate in case of SGD). It doesn't need to know with respect to what loss the gradients were computed it just wants to access that .grad property so it can do x = x - lr * x.grad Note that if we were doing this in a train loop we would call optim.zero_grad() because in each train step we want to compute new gradients - we don't care about gradients from the previous batch. Not zeroing grads would lead to gradient accumulation across batches. Some answers explained well, but I'd like to give a specific example to explain the mechanism. Suppose we have a function : z = 3 x^2 + y^3.
The updating gradient formula of z w.r.t x and y is:  initial values are x=1 and y=2. Then calculating the gradient of x and y in current value (x=1, y=2)  Finally, using SGD optimizer to update the value of x and y according the formula:
 Let's say we defined a model: model, and loss function: criterion and we have the following sequence of steps: pred will have an grad_fn attribute, that references a function that created it, and ties it back to the model. Therefore, loss.backward() will have information about the model it is working with. Try removing grad_fn attribute, for example with: Then the model gradients will be None and consequently weights will not get updated. And the optimizer is tied to the model because we pass model.parameters() when we create the optimizer. Short answer: loss.backward() # do gradient of all parameters for  which we set required_grad= True. parameters could be any variable defined in code, like h2h or i2h. optimizer.step() # according to the optimizer function (defined previously in our code), we update those parameters to finally get the minimum loss(error).This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I'm following a tutorial about machine learning basics and there is mentioned that something can be a feature or a label.  From what I know, a feature is a property of data that is being used. I can't figure out what the label is, I know the meaning of the word, but I want to know what it means in the context of machine learning. Briefly, feature is input; label is output.  This applies to both classification and regression problems. A feature is one column of the data in your input set.  For instance, if you're trying to predict the type of pet someone will choose, your input features might include age, home region, family income, etc.  The label is the final choice, such as dog, fish, iguana, rock, etc. Once you've trained your model, you will give it sets of new input containing those features; it will return the predicted "label" (pet type) for that person. Feature:  In Machine Learning feature means property of your training data. Or you can say a column name in your training dataset. Suppose this is your training dataset  Then here Height, Sex and Age are the features. label:  The output you get from your model after training it is called a label.  Suppose you fed the above dataset to some algorithm and generates a model to predict gender as Male or Female, In the above model you pass features like age, height etc.  So after computing, it will return the gender as Male or Female. That's called a Label Here comes a more visual approach to explain the concept. Imagine you want to classify the animal shown in a photo. The possible classes of animals are e.g. cats or birds.
In that case the label would be the possible class associations e.g. cat or bird, that your machine learning algorithm will predict. The features are pattern, colors, forms that are part of your images e.g. furr, feathers, or more low-level interpretation, pixel values. 
Label: Bird
Features: Feathers  Label: Cat
Features: Furr Prerequisite: Basic Statistics and exposure to ML (Linear Regression) It can be answered in a sentence - They are alike but their definition changes according to the necessities. Let me explain my statement. Suppose that you have a dataset, for this purpose consider exercise.csv. Each column in the dataset are called as features. Gender, Age, Height, Heart Rate, Body_temp, and Calories might be one among various columns. Each column represents distinct features or property. To solidify the understanding and clear out the puzzle let us take two different problems (prediction case). CASE1: In this case we might consider using - Gender, Height, and Weight to predict the Calories burnt during exercise. That prediction(Y) Calories here is a Label. Calories is the column that you want to predict using various features like - x1: Gender, x2: Height and x3: Weight . CASE2: In the second case here we might want to predict the Heart_rate by using Gender and Weight as a feature. Here Heart_Rate is a Label predicted using features - x1: Gender and x2: Weight. Once you have understood the above explanation you won't really be confused with Label and Features anymore. Let's take an example where we want to detect the alphabet using handwritten photos. We feed these sample images in the program and the program classifies these images on the basis of the features they got. An example of a feature in this context is: the letter 'C' can be thought of like a concave facing right. A question now arises as to how to store these features. We need to name them. Here's the role of the label that comes into existence. A label is given to such features to distinguish them from other features.  Thus, we obtain labels as output when provided with features as input. Labels are not associated with unsupervised learning. A feature briefly explained would be the input you have fed to the system and the label would be the output you are expecting. For example, you have fed many features of a dog like his height, fur color, etc, so after computing, it will return the breed of the dog you want to know. Suppose you want to predict climate then features given to you would be historic climate data, current weather, temperature, wind speed, etc. and labels would be months.
The above combination can help you derive predictions.How to load a model from an HDF5 file in Keras? What I tried: The above code successfully saves the best model to a file named weights.hdf5. What I want to do is then load that model. The below code shows how I tried to do so: This is the error I get: If you stored the complete model, not only the weights, in the HDF5 file, then it is as simple as load_weights only sets the weights of your network. You still need to define its architecture before calling load_weights: See the following sample code on how to Build a basic Keras Neural Net Model, save Model (JSON) & Weights (HDF5) and load them: According to official documentation
 https://keras.io/getting-started/faq/#how-can-i-install-hdf5-or-h5py-to-save-my-models-in-keras you can do : first test if you have h5py installed by running the  if you dont have errors while importing h5py you are good to save: If you need to install h5py http://docs.h5py.org/en/latest/build.html I was struggling with this error for a bit, and then realized I was accidently using Whereas this syntax isnt intended to be used with this load model function, The normal way of just writing this, worked for me I done in this wayThis question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I know that there are a lot of explanations of what cross-entropy is, but I'm still confused. Is it only a method to describe the loss function? Can we use gradient descent algorithm to find the minimum using the loss function? Cross-entropy is commonly used to quantify the difference between two probability distributions. In the context of machine learning, it is a measure of error for categorical multi-class classification problems. Usually the "true" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution. For example, suppose for a specific training instance, the true label is B (out of the possible labels A, B, and C). The one-hot distribution for this training instance is therefore: You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C. Now, suppose your machine learning algorithm predicts the following probability distribution: How close is the predicted distribution to the true distribution? That is what the cross-entropy loss determines. Use this formula:  Where p(x) is the true probability distribution (one-hot) and q(x) is the predicted probability distribution. The sum is over the three classes A, B, and C. In this case the loss is 0.479 : Note that it does not matter what logarithm base you use as long as you consistently use the same one. As it happens, the Python Numpy log() function computes the natural log (log base e). Here is the above example expressed in Python using Numpy: So that is how "wrong" or "far away" your prediction is from the true distribution. A machine learning optimizer will attempt to minimize the loss (i.e. it will try to reduce the loss from 0.479 to 0.0). We see in the above example that the loss is 0.4797. Because we are using the natural log (log base e), the units are in nats, so we say that the loss is 0.4797 nats. If the log were instead log base 2, then the units are in bits. See this page for further explanation. To gain more intuition on what these loss values reflect, let's look at some extreme examples. Again, let's suppose the true (one-hot) distribution is: Now suppose your machine learning algorithm did a really great job and predicted class B with very high probability: When we compute the cross entropy loss, we can see that the loss is tiny, only 0.002: At the other extreme, suppose your ML algorithm did a terrible job and predicted class C with high probability instead. The resulting loss of 6.91 will reflect the larger error. Now, what happens in the middle of these two extremes? Suppose your ML algorithm can't make up its mind and predicts the three classes with nearly equal probability. The resulting loss is 1.10. Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss). These loss functions are typically written as J(theta) and can be used within gradient descent, which is an iterative algorithm to move the parameters (or coefficients) towards the optimum values. In the equation below, you would replace J(theta) with H(p, q). But note that you need to compute the derivative of H(p, q) with respect to the parameters first.  So to answer your original questions directly: Is it only a method to describe the loss function? Correct, cross-entropy describes the loss between two probability distributions. It is one of many possible loss functions. Then we can use, for example, gradient descent algorithm to find the
minimum. Yes, the cross-entropy loss function can be used as part of gradient descent. Further reading: one of my other answers related to TensorFlow. In short, cross-entropy(CE) is the measure of how far is your predicted value from the true label.  The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1). And the term entropy itself refers to randomness, so large value of it means your prediction is far off from real labels. So the weights are changed to reduce CE and thus finally leads to reduced difference between the prediction and true labels and thus better accuracy. Adding to the above posts, the simplest form of cross-entropy loss is known as binary-cross-entropy (used as loss function for binary classification, e.g., with logistic regression), whereas the generalized version is categorical-cross-entropy (used as loss function for multi-class classification problems, e.g., with neural networks). The idea remains the same: when the model-computed (softmax) class-probability becomes close to 1 for the target label for a training instance (represented with one-hot-encoding, e.g.,), the corresponding CCE loss decreases to zero otherwise it increases as the predicted probability corresponding to the target class becomes smaller. The following figure demonstrates the concept (notice from the figure that BCE becomes low when both of y and p are high or both of them are low simultaneously, i.e., there is an agreement):  Cross-entropy is closely related to relative entropy or KL-divergence that computes distance between two probability distributions. For example, in between two discrete pmfs, the relation between them is shown in the following figure:I am newbie in convolutional neural networks and just have idea about feature maps and how convolution is done on images to extract features. I would be glad to know some details on applying batch normalisation in CNN. I read this paper https://arxiv.org/pdf/1502.03167v3.pdf and could understand the BN algorithm applied on a data but in the end they mentioned that a slight modification is required when applied to CNN: For convolutional layers, we additionally want the normalization to obey the convolutional property – so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a mini- batch, over all locations. In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations – so for a mini-batch of size m and feature maps of size p × q, we use the effec- tive mini-batch of size m′ = |B| = m · pq. We learn a pair of parameters γ(k) and β(k) per feature map, rather than per activation. Alg. 2 is modified similarly, so that during inference the BN transform applies the same linear transformation to each activation in a given feature map. I am total confused when they say
"so that different elements of the same feature map, at different locations, are normalized in the same way" I know what feature maps mean and different elements are the weights in every feature map. But I could not understand what location or spatial location means. I could not understand the below sentence at all
"In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations" I would be glad if someone cold elaborate and explain me in much simpler terms  Let's start with the terms. Remember that the output of the convolutional layer is a 4-rank tensor [B, H, W, C], where B is the batch size, (H, W) is the feature map size, C is the number of channels. An index (x, y) where 0 <= x < H and 0 <= y < W is a spatial location. Now, here's how the batchnorm is applied in a usual way (in pseudo-code): Basically, it computes H*W*C means and H*W*C standard deviations across B elements. You may notice that different elements at different spatial locations have their own mean and variance and gather only B values. This way is totally possible. But the convolutional layer has a special property: filter weights are shared across the input image (you can read it in detail in this post). That's why it's reasonable to normalize the output in the same way, so that each output value takes the mean and variance of B*H*W values, at different locations. Here's how the code looks like in this case (again pseudo-code): In total, there are only C means and standard deviations and each one of them is computed over B*H*W values. That's what they mean when they say "effective mini-batch": the difference between the two is only in axis selection (or equivalently "mini-batch selection"). Some clarification on Maxim's answer.  I was puzzled by seeing in Keras that the axis you specify is the channels axis, as it doesn't make sense to normalize over the channels - as every channel in a conv-net is considered a different "feature". I.e. normalizing over all channels is equivalent to normalizing number of bedrooms with size in square feet (multivariate regression example from Andrew's ML course). This is usually not what you want - what you do is normalize every feature by itself. I.e. you normalize the number of bedrooms across all examples to be with mu=0 and std=1, and you normalize the the square feet across all examples to be with mu=0 and std=1. This is why you want C means and stds, because you want a mean and std per channel/feature. After checking and testing it myself I realized the issue: there's a bit of a confusion/misconception here. The axis you specify in Keras is actually the axis which is not in the calculations. i.e. you get average over every axis except the one specified by this argument. This is confusing, as it is exactly the opposite behavior of how NumPy works, where the specified axis is the one you do the operation on (e.g. np.mean, np.std, etc.). I actually built a toy model with only BN, and then calculated the BN manually - took the mean, std across all the 3 first dimensions [m, n_W, n_H] and got n_C results, calculated (X-mu)/std (using broadcasting) and got identical results to the Keras results. Hope this helps anyone who was confused as I was. I'm only 70% sure of what I say, so if it does not make sense, please edit or mention it before downvoting.  About location or spatial location: they mean the position of pixels in an image or feature map. A feature map is comparable to a sparse modified version of image where concepts are represented.  About so that different elements of the same feature map, at different locations, are normalized in the same way: 
some normalisation algorithms are local, so they are dependent of their close surrounding (location) and not the things far apart in the image. They probably mean that every pixel, regardless of their location, is treated just like the element of a set, independently of it's direct special surrounding.  About In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations: They get a flat list of every values of every training example in the minibatch, and this list combines things whatever their location is on the feature map.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 8 years ago. I'm looking for an open source implementation, preferably in python, of Textual Sentiment Analysis (http://en.wikipedia.org/wiki/Sentiment_analysis). Is anyone familiar with such open source implementation I can use? I'm writing an application that searches twitter for some search term, say "youtube", and counts "happy" tweets vs. "sad" tweets. 
I'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.
I haven't been able to find such sentiment analyzer so far, specifically not in python. 
Are you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python. Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts. BTW, twitter does support the ":)" and ":(" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself. Thanks! BTW, an early demo is here and the code I have so far is here and I'd love to opensource it with any interested developer. Good luck with that. Sentiment is enormously contextual, and tweeting culture makes the problem worse because you aren't given the context for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared "real world" context to pack meaningful communication in a very short message. If they say the video is bad, does that mean bad, or bad? A linguistics professor was lecturing
  to her class one day. "In English,"
  she said, "A double negative forms a
  positive. In some languages, though,
  such as Russian, a double negative is
  still a negative. However, there is no
  language wherein a double positive can
  form a negative." A voice from the back of the room
  piped up, "Yeah . . .right." With most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it. You may be interested in The Toolkit for Advanced Discriminative Modeling, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task. The University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for Computational Linguistics II to get an idea of how to make it work and what previous applications it has served. Another great tool which works in the same vein is Mallet.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  Weka is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production. Good luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion. Thanks everyone for your suggestions, they were indeed very useful!
I ended up using a Naive Bayesian classifier, which I borrowed from here. 
I started by feeding it with a list of good/bad keywords and then added a "learn" feature by employing user feedback. It turned out to work pretty nice. The full details of my work as in a blog post. Again, your help was very useful, so thank you! I have constructed a word list labeled with sentiment. You can access it from here: http://www2.compute.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip You will find a short Python program on my blog:  http://finnaarupnielsen.wordpress.com/2011/06/20/simplest-sentiment-analysis-in-python-with-af/ This post displays how to use the word list with single sentences as well as with Twitter. Word lists approaches have their limitations. You will find a investigation of the limitations of my word list in the article "A new ANEW: Evaluation of a word list for sentiment analysis in microblogs". That article is available from my homepage. Please note a unicode(s, 'utf-8') is missing from the code (for paedagogic reasons). A lot of research papers indicate that a good starting point for sentiment analysis is looking at adjectives, e.g., are they positive adjectives or negative adjectives. For a short block of text this is pretty much your only option... There are papers that look at entire documents, or sentence level analysis, but as you say tweets are quite short... There is no real magic approach to understanding the sentiment of a sentence, so I think your best bet would be hunting down one of these research papers and trying to get their data-set of positively/negatively oriented adjectives. Now, this having been said, sentiment is domain specific, and you might find it difficult to get a high-level of accuracy with a general purpose data-set. Good luck. I think you may find it difficult to find what you're after. The closest thing that I know of is LingPipe, which has some sentiment analysis functionality and is available under a limited kind of open-source licence, but is written in Java. Also, sentiment analysis systems are usually developed by training a system on product/movie review data which is significantly different from the average tweet. They are going to be optimised for text with several sentences, all about the same topic. I suspect you would do better coming up with a rule-based system yourself, perhaps based on a lexicon of sentiment terms like the one the University of Pittsburgh provide. Check out We Feel Fine for an implementation of similar idea with a really beautiful interface (and twitrratr). Take a look at Twitter sentiment analysis tool. It's written in python, and it uses Naive Bayes classifier with semi-supervised machine learning. The source can be found here. Maybe TextBlob (based on NLTK and pattern) is the right sentiment analysis tool for you. I came across Natural Language Toolkit a while ago. You could probably use it as a starting point. It also has a lot of modules and addons, so maybe they already have something similar. Somewhat wacky thought: you could try using the Twitter API to download a large set of tweets, and then classifying a subset of that set using emoticons: one positive group for  ":)", ":]", ":D", etc, and another negative group with ":(", etc. Once you have that crude classification, you could search for more clues with frequency or ngram analysis or something along those lines. It may seem silly, but serious research has been done on this (search for "sentiment analysis" and emoticon). Worth a look.   There's a Twitter Sentiment API by TweetFeel that does advanced linguistic analysis of tweets, and can retrieve positive/negative tweets. See http://www.webservius.com/corp/docs/tweetfeel_sentiment.htm For those interested in coding Twitter Sentiment Analyis from scratch, there is a Coursera course "Data Science" with python code on GitHub (as part of assignment 1 - link). The sentiments are part of the AFINN-111. You can find working solutions, for example here. In addition to the AFINN-111 sentiment list, there is a simple implementation of builing a dynamic term list based on frequency of terms in tweets that have a pos/neg score (see here).I have trouble understanding the difference (if there is one) between roc_auc_score() and auc() in scikit-learn. Im tying to predict a binary output with imbalanced classes (around 1.5% for Y=1). and Somebody can explain this difference ? I thought both were just calculating the area under the ROC curve. Might be because of the imbalanced dataset but I could not figure out why. Thanks! AUC is not always area under the curve of a ROC curve. Area Under the Curve is an (abstract) area under some curve, so it is a more general thing than AUROC. With imbalanced classes, it may be better to find AUC for a precision-recall curve. See sklearn source for roc_auc_score: As you can see, this first gets a roc curve, and then calls auc() to get the area. I guess your problem is the predict_proba() call. For a normal predict() the outputs are always the same: If you change the above for this, you'll sometimes get different outputs: predict returns only one class or the other. Then you compute a ROC with the results of predict on a classifier, there are only three thresholds (trial all one class, trivial all the other class, and in between).  Your ROC curve looks like this: Meanwhile, predict_proba() returns an entire range of probabilities, so now you can put more than three thresholds on your data. Hence different areas. When you use the y_pred (class labels), you already decided on
the threshold. When you use y_prob (positive class probability)
you are open to the threshold, and the ROC Curve should help
you decide the threshold. For the first case you are using the probabilities: When you do that, you're considering the AUC 'before' taking
a decision on the threshold you'll be using. In the second case, you are using the prediction (not the probabilities),
in that case, use 'predict' instead of 'predict_proba' for both and you
should get the same result.I was wondering if there are classifiers that handle nan/null values in scikit-learn.  I thought random forest regressor handles this but I got an error when I call predict. Can I not call predict with any scikit-learn algorithm with missing values? Edit.
Now that I think about this, it makes sense.  It's not an issue during training but when you predict how do you branch when the variable is null?  maybe you could just split both ways and average the result?  It seems like k-NN should work fine as long as the distance function ignores nulls though. Edit 2 (older and wiser me)
Some gbm libraries (such as xgboost) use a ternary tree instead of a binary tree precisely for this purpose: 2 children for the yes/no decision and 1 child for the missing decision. sklearn is using a binary tree Sometimes missing values are simply not applicable. Imputing them is meaningless. In these cases you should use a model that can handle missing values. Scitkit-learn's models cannot handle missing values. XGBoost can. As mentioned in this article, scikit-learn's decision trees and KNN algorithms are not (yet) robust enough to work with missing values. If imputation doesn't make sense, don't do it. keep in mind this is a made-up example Consider a dataset with rows of cars ("Danho Diesel", "Estal Electric", "Hesproc Hybrid") and columns with their properties (Weight, Top speed, Acceleration, Power output, Sulfur Dioxide Emission, Range). Electric cars do not produce exhaust fumes - so the Sulfur dioxide emission of the Estal Electric should be a NaN-value (missing). You could argue that it should be set to 0 - but electric cars cannot produce sulfur dioxide. Imputing the value will ruin your predictions. As mentioned in this article, scikit-learn's decision trees and KNN algorithms are not (yet) robust enough to work with missing values. If imputation doesn't make sense, don't do it. I made an example that contains both missing values in training and the test sets I just picked a strategy to replace missing data with the mean, using the SimpleImputer class. There are other strategies. If you are using DataFrame, you could use fillna. Here I replaced the missing data with the mean of that column. For NoData located at the edge of a GeoTIFF image (which can obviously not be interpolated using the average of the values of neighbouring pixels), I masked it in a few lines of code. Please note that this was performed on one band (VH band of a Sentinel 1 image, which was first converted into an array). After I performed a Random Forest classification on my initial image, I did the following: When saving it, do not forget to assign a NoData value:I want to write a *.txt file with the neural network hyperparameters and the model architecture. Is it possible to write the object model.summary() to my output file? It happens that I'm getting "None" as you can see below. Any idea how to deal with that? With my version of Keras (2.0.6) and Python (3.5.0), this works for me: This outputs the following lines to the file: For me, this worked to just get the model summary as a string: And if you want to write to a log: I understand the OP has already accepted winni2k's answer, but since the question title actually implies saving the outputs of model.summary() to a string, not a file, the following code might help others who come to this page looking for that (like I did). The code below was run using TensorFlow 1.12.0, which comes with Keras 2.1.6-tf on Python 3.6.2. Which yields (as a string): I stumbled upon the same problem as well!
There are two possible workarounds: Use to_json() method of the model This is your case above. Otherwise use the ascii method from keras_diagram It's not the best way to do it but one thing you can do is to redirect stdout: see "How to redirect 'print' output to a file using python?" One option, although not an exact replacement for model.summary, is to export a model's configuration using model.get_config(). From the docs: model.get_config(): returns a dictionary containing the configuration of the model. The model can be reinstantiated from its config via: I had this same problem. @Pasa's answer was very useful but I thought I would post a more minimal example: it's a reasonable assumption that you already have a Keras model at this point. An example of when having this string is useful: if you have a matplotlib plot. You can then use: To write the summary of your model to your performance chart, for quick reference:
 As I came here to find a way to log the summary, I wanted to share this little twist to @ajb answer to avoid the INFO: at every line in the log file by using @FAnders answer: which produces a log file as:I'm looking through the Apple's Vision API documentation and I see a couple of classes that relate to text detection in UIImages: 1) class VNDetectTextRectanglesRequest 2) class VNTextObservation It looks like they can detect characters, but I don't see a means to do anything with the characters. Once you've got characters detected, how would you go about turning them into something that can be interpreted by NSLinguisticTagger? Here's a post that is a brief overview of Vision. Thank you for reading.  This is how to do it ...  You'll find the complete project here included is the trained model ! SwiftOCR I just got SwiftOCR to work with small sets of text. https://github.com/garnele007/SwiftOCR uses https://github.com/Swift-AI/Swift-AI which uses NeuralNet-MNIST model for text recognition. TODO : VNTextObservation > SwiftOCR Will post example of it using VNTextObservation once I have it one connected to the other. OpenCV + Tesseract OCR I tried to use OpenCV + Tesseract but got compile errors then found SwiftOCR. SEE ALSO : Google Vision iOS Note Google Vision Text Recognition - Android sdk has text detection but also has iOS cocoapod. So keep an eye on it as should add text recognition to the iOS eventually. https://developers.google.com/vision/text-overview //Correction: just tried it but only Android version of the sdk supports text detection. https://developers.google.com/vision/text-overview If you subscribe to releases: 
https://libraries.io/cocoapods/GoogleMobileVision Click SUBSCRIBE TO RELEASES
you can see when TextDetection is added to the iOS part of the Cocoapod Apple finally updated Vision to do OCR. Open a playground and dump a couple of test images in the Resources folder. In my case, I called them "demoDocument.jpg" and "demoLicensePlate.jpg". The new class is called VNRecognizeTextRequest. Dump this in a playground and give it a whirl: There's an in-depth discussion of this from WWDC19 Adding my own progress on this, if anyone have a better solution:  I've successfully drawn the region box and character boxes on screen. The vision API of Apple is actually very performant. You have to transform each frame of your video to an image and feed it to the recogniser. It's much more accurate than feeding directly the pixel buffer from the camera.  Now I'm trying to actually reconize the text. Apple doesn't provide any built in OCR model. And I want to use CoreML to do that, so I'm trying to convert a Tesseract trained data model to CoreML. You can find Tesseract models here: https://github.com/tesseract-ocr/tessdata and I think the next step is to write a coremltools converter that support those type of input and output a .coreML file.  Or, you can link to TesseractiOS directly and try to feed it with your region boxes and character boxes you get from the Vision API.  Thanks to a GitHub user, you can test an example: https://gist.github.com/Koze/e59fa3098388265e578dee6b3ce89dd8 The thing is, the result is an array of bounding boxes for each detected character. From what I gathered from Vision's session, I think you are supposed to use CoreML to detect the actual chars. Recommended WWDC 2017 talk: Vision Framework: Building on Core ML (haven't finished watching it either), have a look at 25:50 for a similar example called MNISTVision Here's another nifty app demonstrating the use of Keras (Tensorflow) for the training of a MNIST model for handwriting recognition using CoreML: Github I'm using Google's Tesseract OCR engine to convert the images into actual strings. You'll have to add it to your Xcode project using cocoapods. Although Tesseract will perform OCR even if you simply feed the image containing texts to it, the way to make it perform better/faster is to use the detected text rectangles to feed pieces of the image that actually contain text, which is where Apple's Vision Framework comes in handy.
Here's a link to the engine: 
Tesseract OCR
And here's a link to the current stage of my project that has text detection + OCR already implemented:
Out Loud - Camera to Speech
Hope these can be of some use. Good luck! For those still looking for a solution I wrote a quick library to do this. It uses both the Vision API and Tesseract and can be used to achieve the task the question describes with one single method: This method will look for text in your image, return the string found and a slice of the original image showing where the text was found Firebase ML Kit does it for iOS (and Android) with their on-device Vision API and it outperforms Tesseract and SwiftOCR.The introductory documentation, which I am reading (TOC here) uses the term "batch" (for instance here) without having defined it. Let's say you want to do digit recognition (MNIST) and you have defined your architecture of the network (CNNs). Now, you can start feeding the images from the training data one by one to the network, get the prediction (till this step it's called as doing inference), compute the loss, compute the gradient, and then update the parameters of your network (i.e. weights and biases) and then proceed with the next image ... This way of training the model is sometimes called as online learning. But, you want the training to be faster, the gradients to be less noisy, and also take advantage of the power of GPUs which are efficient at doing array operations (nD-arrays to be specific). So, what you instead do is feed in say 100 images at a time (the choice of this size is up to you (i.e. it's a hyperparameter) and depends on your problem too). For instance, take a look at the below picture, (Author: Martin Gorner)  Here, since you're feeding in 100 images(28x28) at a time (instead of 1 as in the online training case), the batch size is 100. Oftentimes this is called as mini-batch size or simply mini-batch. Also the below picture: (Author: Martin Gorner)  Now, the matrix multiplication will all just work out perfectly fine and you will also be taking advantage of the highly optimized array operations and hence achieve faster training time. If you observe the above picture, it doesn't matter that much whether you give 100 or 256 or 2048 or 10000 (batch size) images as long as it fits in the memory of your (GPU) hardware. You'll simply get that many predictions. But, please keep in mind that this batch size influences the training time, the error that you achieve, the gradient shifts etc., There is no general rule of thumb as to which batch size works out best. Just try a few sizes and pick the one which works best for you. But try not to use large batch sizes since it will overfit the data. People commonly use mini-batch sizes of 32, 64, 128, 256, 512, 1024, 2048. Bonus: To get a good grasp of how crazy you can go with this batch size, please give this paper a read: weird trick for parallelizing CNNsI am using RandomForestClassifier implemented in python sklearn package to build a binary classification model. The below is the results of cross validations: I am using "Price" feature to predict "quality" which is a ordinal value. In each cross validation, there are 163 training examples and 41 test examples.  Apparently, overfitting occurs here. So is there any parameters provided by sklearn can be used to overcome this problem? I found some parameters here, e.g. min_samples_split and min_sample_leaf, but I do not quite understand how to tune them. Thanks in advance! I would agree with @Falcon w.r.t. the dataset size. It's likely that the main problem is the small size of the dataset. If possible, the best thing you can do is get more data, the more data (generally) the less likely it is to overfit, as random patterns that appear predictive start to get drowned out as the dataset size increases. That said, I would look at the following params: Note when doing this work to be scientific. Use 3 datasets, a training set, a separate 'development' dataset to tweak your parameters, and a test set that tests the final model, with the optimal parameters. Only change one parameter at a time and evaluate the result. Or experiment with the sklearn gridsearch algorithm to search across these parameters all at once. Adding this late comment in case it helps others. In addition to the parameters mentioned above (n_estimators, max_features, max_depth, and min_samples_leaf) consider setting 'min_impurity_decrease'. Doing this manually is cumbersome. So use sklearn.model_selection.GridSearchCV to test a range of parameters (parameter grid) and find the optimal parameters. You can use 'gini' or 'entropy' for the Criterion, however, I recommend sticking with 'gini', the default. In the majority of cases, they produce the same result but 'entropy' is more computational expensive to compute. Max depth works well and is an intuitive way to stop a tree from growing, however, just because a node is less than the max depth doesn't always mean it should split. If the information gained from splitting only addresses a single/few misclassification(s) then splitting that node may be supporting overfitting. You may or may not find this parameter useful, depending on the size of your dataset and/or your feature space size and complexity, but it is worth considering while tuning your parameters.I trained a model to classify images from 2 classes and saved it using model.save(). Here is the code I used: It successfully trained with 0.98 accuracy which is pretty good. To load and test this model on new images, I used the below code: It outputs: [[0]] Why wouldn't it give out the actual name of the class and why [[0]]? If someone is still struggling to make predictions on images, here is the optimized code to load the saved model and make predictions: You can use model.predict() to predict the class of a single image as follows [doc]: In this example, a image is loaded as a numpy array with shape (1, height, width, channels). Then, we load it into the model and predict its class, returned as a real value in the range [0, 1] (binary classification in this example). keras predict_classes (docs) outputs A numpy array of class predictions. Which in your model case, the index of neuron of highest activation from your last(softmax) layer. [[0]] means that your model predicted that your test data is class 0. (usually you will be passing multiple image, and the result will look like [[0], [1], [1], [0]] ) You must convert your actual label (e.g. 'cancer', 'not cancer') into binary encoding (0 for 'cancer', 1 for 'not cancer') for binary classification. Then you will interpret your sequence output of [[0]] as having class label 'cancer' That's because you're getting the numeric value associated with the class. For example if you have two classes cats and dogs, Keras will associate them numeric values 0 and 1. To get the mapping between your classes and their associated numeric value, you can use Now you know the mapping between your classes and indices. So now what you can do is 
if classes[0][0] == 1:
    prediction = 'dog'
else:
    prediction = 'cat'
 Forwarding the example by @ritiek, I'm a beginner in ML too, maybe this kind of formatting will help see the name instead of just class number.Given a vector of scores and a vector of actual class labels, how do you calculate a single-number AUC metric for a binary classifier in the R language or in simple English?  Page 9 of "AUC: a Better Measure..." seems to require knowing the class labels, and here is an example in MATLAB where I don't understand  Because R (not to be confused with the R language) is defined a vector but used as a function? With the package pROC you can use the function auc() like this example from the help page: The ROCR package will calculate the AUC among other statistics: As mentioned by others, you can compute the AUC using the ROCR package.  With the ROCR package you can also plot the ROC curve, lift curve and other model selection measures.  You can compute the AUC directly without using any package by using the fact that the AUC is equal to the probability that a true positive is scored greater than a true negative. For example, if pos.scores is a vector containing a score of the positive examples, and neg.scores is a vector containing the negative examples then the AUC is approximated by: will give an approximation of the AUC.  You can also estimate the variance of the AUC by bootstrapping: Without any additional packages:  I found some of the solutions here to be slow and/or confusing (and some of them don't handle ties correctly) so I wrote my own data.table based function auc_roc() in my R package mltools. You can learn more about AUROC in this blog post by Miron Kursa: https://mbq.me/blog/augh-roc/ He provides a fast function for AUROC: Let's test it: auroc() is 100 times faster than pROC::auc() and computeAUC(). auroc() is 10 times faster than mltools::auc_roc() and ROCR::performance(). Combining code from ISL 9.6.3 ROC Curves, along with @J. Won.'s answer to this question and a few more places, the following plots the ROC curve and prints the AUC in the bottom right on the plot. Below probs is a numeric vector of predicted probabilities for binary classification and test$label contains the true labels of the test data. This gives a plot like this:  I usually use the function ROC from the DiagnosisMed package. I like the graph it produces. AUC is returned along with it's confidence interval and it is also mentioned on the graph. Along the lines of erik's response, you should also be able to calculate the ROC directly by comparing all possible pairs of values from pos.scores and neg.scores: Certainly less efficient than the sample approach or the pROC::auc, but more stable than the former and requiring less installation than the latter. Related: when I tried this it gave similar results to pROC's value, but not exactly the same (off by 0.02 or so); the result was closer to the sample approach with very high N. If anyone has ideas why that might be I'd be interested. Currently top voted answer is incorrect, because it disregards ties. When positive and negative scores are equal, then AUC should be 0.5. Below is corrected example. Calculating AUC with Metrics  package is very easy and straightforward:The docs for an Embedding Layer in Keras say: Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] I believe this could also be achieved by encoding the inputs as one-hot vectors of length vocabulary_size, and feeding them into a Dense Layer. Is an Embedding Layer merely a convenience for this two-step process, or is something fancier going on under the hood? An embedding layer is faster, because it is essentially the equivalent of a dense layer that makes simplifying assumptions. Imagine a word-to-embedding layer with these weights: A Dense layer will treat these like actual weights with which to perform matrix multiplication. An embedding layer will simply treat these weights as a list of vectors, each vector representing one word; the 0th word in the vocabulary is w[0], 1st is w[1], etc. For an example, use the weights above and this sentence: A naive Dense-based net needs to convert that sentence to a 1-hot encoding then do a matrix multiplication = However, an Embedding layer simply looks at [0, 2, 1, 2] and takes the weights of the layer at indices zero, two, one, and two to immediately get = So it's the same result, just obtained in a hopefully faster way. The Embedding layer does have limitations: However, none of those limitations should matter if you just want to convert an integer-encoded word into an embedding. Mathematically, the difference is this: An embedding layer performs select operation. In keras, this layer is equivalent to: A dense layer performs dot-product operation, plus an optional activation: You can emulate an embedding layer with fully-connected layer via one-hot encoding, but the whole point of dense embedding is to avoid one-hot representation. In NLP, the word vocabulary size can be of the order 100k (sometimes even a million). On top of that, it's often needed to process the sequences of words in a batch. Processing the batch of sequences of word indices would be much more efficient than the batch of sequences of one-hot vectors. In addition, gather operation itself is faster than matrix dot-product, both in forward and backward pass. Here I want to improve the voted answer by providing more details: When we use embedding layer, it is generally to reduce one-hot input vectors (sparse) to denser representations. Embedding layer is much like a table lookup. When the table is small, it is fast. When the table is large, table lookup is much slower. In practice, we would use dense layer as a dimension reducer to reduce the one-hot input instead of embedding layer in this case.I've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work. My question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic? A classic paper by Peter Turney (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words excellent and poor as a seed set. Turney uses the mutual information of other words with these two adjectives to achieve an accuracy of 74%. I haven't tried doing untrained sentiment analysis such as you are describing, but off the top of my head I'd say you're oversimplifying the problem.  Simply analyzing adjectives is not enough to get a good grasp of the sentiment of a text; for example, consider the word 'stupid.'  Alone, you would classify that as negative, but if a product review were to have '... [x] product makes their competitors look stupid for not thinking of this feature first...' then the sentiment in there would definitely be positive. The greater context in which words appear definitely matters in something like this.  This is why an untrained bag-of-words approach alone (let alone an even more limited bag-of-adjectives) is not enough to tackle this problem adequately. The pre-classified data ('training data') helps in that the problem shifts from trying to determine whether a text is of positive or negative sentiment from scratch, to trying to determine if the text is more similar to positive texts or negative texts, and classify it that way.  The other big point is that textual analyses such as sentiment analysis are often affected greatly by the differences of the characteristics of texts depending on domain.  This is why having a good set of data to train on (that is, accurate data from within the domain in which you are working, and is hopefully representative of the texts you are going to have to classify) is as important as building a good system to classify with. Not exactly an article, but hope that helps. The paper of Turney (2002) mentioned by larsmans is a good basic one. In a newer research, Li and He [2009] introduce an approach using Latent Dirichlet Allocation (LDA) to train a model that can classify an article's overall sentiment and topic simultaneously in a totally unsupervised manner. The accuracy they achieve is 84.6%. I tried several methods of Sentiment Analysis for opinion mining in Reviews. 
What worked the best for me is the method described in Liu book: http://www.cs.uic.edu/~liub/WebMiningBook.html In this Book Liu and others, compared many strategies and discussed different papers on Sentiment Analysis and Opinion Mining. Although my main goal was to extract features in the opinions, I implemented a sentiment classifier to detect positive and negative classification of this features.  I used NLTK for the pre-processing (Word tokenization, POS tagging) and the trigrams creation. Then also I used the Bayesian Classifiers inside this tookit to compare with other strategies Liu was pinpointing.  One of the methods relies on tagging as pos/neg every trigrram expressing this information, and using some classifier on this data. 
Other method I tried, and worked better (around 85% accuracy in my dataset), was calculating  the sum of scores of PMI (punctual mutual information) for every word in the sentence and the words excellent/poor as seeds of pos/neg class.  I tried spotting keywords using a dictionary of affect to predict the sentiment label at sentence level. Given the generality of the vocabulary (non domain dependent), the results were just about 61%. The paper is available in my homepage. In a somewhat improved version, negation adverbs were considered. The whole system, named EmoLib, is available for demo: http://dtminredis.housing.salle.url.edu:8080/EmoLib/ Regards, David,  I'm not sure if this helps but you may want to look into Jacob Perkin's blog post on using NLTK for sentiment analysis. There are no magic "shortcuts" in sentiment analysis, as with any other sort of text analysis that seeks to discover the underlying "aboutness," of a chunk of text. Attempting to short cut proven text analysis methods through simplistic "adjective" checking or similar approaches leads to ambiguity, incorrect classification, etc., that at the end of the day give you a poor accuracy read on sentiment. The more terse the source (e.g. Twitter), the more difficult the problem.I run a python program that calls sklearn.metrics's methods to calculate precision and F1 score. Here is the output when there is no predicted sample: When there is no predicted sample, it means that TP+FP is 0, so  In my case, sklearn.metrics also returns the accuracy as 0.8, and recall as 0. So FN is not zero. But why does scikilearn says F1 is ill-defined? What is the definition of F1 used by Scikilearn? https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/classification.py F1 = 2 * (precision * recall) / (precision + recall) precision = TP/(TP+FP) as you've just said if predictor doesn't predicts positive class at all - precision is 0. recall = TP/(TP+FN), in case if predictor doesn't predict positive class - TP is 0 - recall is 0. So now you are dividing 0/0. Precision, Recall, F1-score and Accuracy calculation   Wikipedia referenceHow can I calculate in python the Cumulative Distribution Function (CDF)? I want to calculate it from an array of points I have (discrete distribution), not with the continuous distributions that, for example, scipy has. (It is possible that my interpretation of the question is wrong. If the question is how to get from a discrete PDF into a discrete CDF, then np.cumsum divided by a suitable constant will do if the samples are equispaced. If the array is not equispaced, then np.cumsum of the array multiplied by the distances between the points will do.) If you have a discrete array of samples, and you would like to know the CDF of the sample, then you can just sort the array. If you look at the sorted result, you'll realize that the smallest value represents 0% , and largest value represents 100 %. If you want to know the value at 50 % of the distribution, just look at the array element which is in the middle of the sorted array. Let us have a closer look at this with a simple example: This gives the following plot where the right-hand-side plot is the traditional cumulative distribution function. It should reflect the CDF of the process behind the points, but naturally, it is not as long as the number of points is finite.  This function is easy to invert, and it depends on your application which form you need. Assuming you know how your data is distributed (i.e. you know the pdf of your data), then scipy does support discrete data when calculating cdf's  We can even print the first few values of the cdf to show they are discrete The same method to calculate the cdf also works for multiple dimensions: we use 2d data below to illustrate In the above examples, I had prior knowledge that my data was normally distributed, which is why I used scipy.stats.norm() - there are multiple distributions scipy supports. But again, you need to know how your data is distributed beforehand to use such functions. If you don't know how your data is distributed and you just use any distribution to calculate the cdf, you most likely will get incorrect results. The empirical cumulative distribution function is a CDF that jumps exactly at the values in your data set. It is the CDF for a discrete distribution that places a mass at each of your values, where the mass is proportional to the frequency of the value. Since the sum of the masses must be 1, these constraints determine the location and height of each jump in the empirical CDF. Given an array a of values, you compute the empirical CDF by first obtaining the frequencies of the values. The numpy function unique() is helpful here because it returns not only the frequencies, but also the values in sorted order. To calculate the cumulative distribution, use the cumsum() function, and divide by the total sum. The following function returns the values in sorted order and the corresponding cumulative distribution: To plot the empirical CDF you can use matplotlib's plot() function. The option drawstyle='steps-post' ensures that jumps occur at the right place. However, you need to force a jump at the smallest data value, so it's necessary to insert an additional element in front of x and y. Example usages: with output:  For calculating CDF for array of discerete numbers: Note that the return array pdf has the length of bins (500 here) and bin_edges has the length of bins+1 (501 here). So, to calculate the CDF which is nothing but the area below the PDF distribution curve, we can simply calculate the cumulative sum of bin widths (np.diff(bins_edges)) times pdf using Numpy cumsum function Here's an alternative pandas solution to calculating the empirical CDF, using pd.cut to sort the data into evenly spaced bins first, and then cumsum to compute the distribution. Below is an example use of the function to discretize the distribution of 10000 datapoints into 100 evenly spaced bins:My code is follow the class of machine learning of google.The two code are same.I don't know why it show error.May be the type of variable is error.But google's code is same to me.Who has ever had this problem? This is error This is code I think you are using newer version of python. Please try with pydotplus. This should do it. pydot.graph_from_dot_data() returns a list, so try: I had exactly the same issue. Turned out that I hadn't installed graphviz. Once i did that it started to work.  @Alex Sokolov, for my case in window, i downloaded and install / unzip the following to a folder then setup the PATH in Windows environment variables. re-run the py code works for me. hope is helpful to you. I install scikit-learn via conda and all of about not work.
Firstly, I have to install libtool Then I follow this sklearn guide
Then change the python file to this code  Finally convert to png in terminal I tried the previous answers and still got a error when running the script Therefore,
I just used pydotplus and install the "graphviz" by using: Then it worked for me, and I added Thanks to the previous contributors. It works as the following on Python3.7 but don't forget to install pydot using Anaconda prompt: I use Anaconda. Here's what worked for me: 
run from terminal: Then run Then from the terminal: To add all graphs for the number of your n_estimators you can do: you could also switch the line for this one  and it would still work. I hope this helps, I was having a similar issue. I decided not to use pydot / pydotplus, but rather graphviz. I modified (barely) the code and it works wonders! :)I have two numpy arrays: I want to load these into TensorFlow so I can classify them using a neural network. How can this be done? What shape do the numpy arrays need to have?  Additional Info - My images are 60 (height) by 160 (width) pixels each and each of them have 5 alphanumeric characters. Here is a sample image:  Each label is a 5 by 62 array.  You can use tf.convert_to_tensor(): Here's a link to the documentation for this method: https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor You can use tf.pack (tf.stack in TensorFlow 1.0.0) method for this purpose. Here is how to pack a random image of type numpy.ndarray into a Tensor: UPDATE: to convert a Python object to a Tensor you can use tf.convert_to_tensor function. You can use placeholders and feed_dict. Suppose we have numpy arrays like these: You can declare two placeholders: Then, use these placeholders (X, and Y) in your model, cost, etc.:
    model = tf.mul(X, w) ... Y ...
    ... Finally, when you run the model/cost, feed the numpy arrays using feed_dict:I am trying to produce a CNN using Keras, and wrote the following code: I want to use Keras's LeakyReLU activation layer instead of using Activation('relu'). However, I tried using LeakyReLU(alpha=0.1) in place, but this is an activation layer in Keras, and I get an error about using an activation layer and not an activation function. How can I use LeakyReLU in this example? All advanced activations in Keras, including LeakyReLU, are available as layers, and not as activations; therefore, you should use it as such: Sometimes you just want a drop-in replacement for a built-in activation layer, and not having to add extra activation layers just for this purpose. For that, you can use the fact that the activation argument can be a callable object. Since a Layer is also a callable object, you could also simply use which now works in TF2. This is a better solution as this avoids the need to use a custom_object during loading as @ChristophorusReyhan mentionned. you can import the function to make the code cleaner and then use it like any other activation. if you choose not to define alpha, don't forget to add brackets "LeakyReLU()"I'm trying to build a LSTM autoencoder with the goal of getting a fixed sized vector from a sequence, which represents the sequence as good as possible. This autoencoder consists of two parts: So, in the end, the encoder is a many to one LSTM and the decoder is a one to many LSTM. 
Image source: Andrej Karpathy On a high level the coding looks like this (similar as described here): The shape (number of training examples, sequence length, input dimension) of the data array is (1200, 10, 5) and looks like this: Problem: I am not sure how to proceed, especially how to integrate LSTM to Model and how to get the decoder to generate a sequence from a vector. I am using keras with tensorflow backend. EDIT: If someone wants to try out, here is my procedure to generate random sequences with moving ones (including padding): Models can be any way you want. If I understood it right, you just want to know how to create models with LSTM? Using LSTMs Well, first, you have to define what your encoded vector looks like. Suppose you want it to be an array of 20 elements, a 1-dimension vector. So, shape (None,20). The size of it is up to you, and there is no clear rule to know the ideal one.  And your input must be three-dimensional, such as your (1200,10,5). In keras summaries and error messages, it will be shown as (None,10,5), as "None" represents the batch size, which can vary each time you train/predict.  There are many ways to do this, but, suppose you want only one LSTM layer: This is enough for a very very simple encoder resulting in an array with 20 elements (but you can stack more layers if you want). Let's create the model: Now, for the decoder, it gets obscure. You don't have an actual sequence anymore, but a static meaningful vector. You may want to use LTSMs still, they will suppose the vector is a sequence.  But here, since the input has shape (None,20), you must first reshape it to some 3-dimensional array in order to attach an LSTM layer next. The way you will reshape it is entirely up to you. 20 steps of 1 element? 1 step of 20 elements? 10 steps of 2 elements? Who knows? It's important to notice that if you don't have 10 steps anymore, you won't be able to just enable "return_sequences" and have the output you want. You'll have to work a little. Acually, it's not necessary to use "return_sequences" or even to use LSTMs, but you may do that.  Since in my reshape I have 10 timesteps (intentionally), it will be ok to use "return_sequences", because the result will have 10 timesteps (as the initial input) You could work in many other ways, such as simply creating a 50 cell LSTM without returning sequences and then reshaping the result: And our model goes: After that, you unite the models with your code and train the autoencoder. 
All three models will have the same weights, so you can make the encoder bring results just by using its predict method. What I often see about LSTMs for generating sequences is something like predicting the next element. You take just a few elements of the sequence and try to find the next element. And you take another segment one step forward and so on. This may be helpful in generating sequences.  You can find a simple of sequence to sequence autoencoder here: https://blog.keras.io/building-autoencoders-in-keras.html Here is an example  Let's create a synthetic data consisting of a few sequence. The idea is looking into these sequences through the lens of an autoencoder. In other words, lowering the dimension or summarizing them into a fixed length. Let's device a simple LSTM                                                   Lets build the autoencoder And here is the representation of the sequencesI'm using a Scikit-Learn custom pipeline (sklearn.pipeline.Pipeline) in conjunction with RandomizedSearchCV for hyper-parameter optimization. This works great. Now I would like to insert a Keras model as a first step into the pipeline. Parameters of the model should be optimized. The computed (fitted) Keras model should then be used later on in the pipeline by other steps, so I think I have to store the model as a global variable so that the other pipeline steps can use it. Is this right? I know that Keras offers some wrappers for the Scikit-Learn API but the problem is that these wrappers already do classification / regression but I only want to compute the Keras model and nothing else. How can this be done? For example I have a method which returns the model: The method needs some fixed parameters like a file path etc. but X and y is not needed (or can be ignored). The parameters of the model should be optimized (number of layers etc.). You need to wrap your Keras model as a Scikit learn model first, and then just proceed as normal.  Here's a quick example (I've omitted the imports for brevity) Here is a full blog post with this one and many other examples: Scikit-learn Pipeline Examples This is a modification of the RBM example in sklearn documentation (http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py) but the neural network implemented in keras with tensorflow backendI can't find how Keras defines "accuracy" and "loss".  I know I can specify different metrics (e.g. mse, cross entropy) - but keras prints out a standard "accuracy".  How is that defined? Likewise for loss: I know I can specify different types of regularization -- are those in the loss? Ideally, I'd like to print out the equation used to define it; if not, I'll settle for an answer here. Have a look at metrics.py, there you can find definition of all available metrics including different types of accuracy. Accuracy is not printed unless you add it to the list of desired metrics when you compile your model. Regularizers are by definition added to the loss. For example, see add_loss method of the Layerclass. Update The type of accuracy is determined based on the objective function, see training.py. The default choice is categorical_accuracy. Other types like binary_accuracy and sparse_categorical_accuracy are selected when the objective function is either binary or sparse.I am using the  LogisticRegression() method in scikit-learn on a highly unbalanced data set. I have even turned the class_weight feature to auto. I know that in Logistic Regression it should be possible to know what is the threshold value for a particular pair of classes.  Is it possible to know what the threshold value is in each of the One-vs-All classes the LogisticRegression() method designs? I did not find anything in the documentation page. Does it by default apply the 0.5 value as threshold for all the classes regardless of the parameter values? There is a little trick that I use, instead of using model.predict(test_data) use model.predict_proba(test_data). Then use a range of values for thresholds to analyze the effects on the prediction; Best! Yes, Sci-Kit learn is using a threshold of P>=0.5 for binary classifications. I am going to build on some of the answers already posted with two options to check this: One simple option is to extract the probabilities of each classification using the output from model.predict_proba(test_x) segment of the code below along with class predictions (output from model.predict(test_x) segment of code below).  Then, append class predictions and their probabilities to your test dataframe as a check. As another option, one can graphically view precision vs. recall at various thresholds using the following code. Logistic regression chooses the class that has the biggest probability. In case of 2 classes, the threshold is 0.5: if P(Y=0) > 0.5 then obviously P(Y=0) > P(Y=1). The same stands for the multiclass setting: again, it chooses the class with the biggest probability (see e.g. Ng's lectures, the bottom lines). Introducing special thresholds only affects in the proportion of false positives/false negatives (and thus in precision/recall tradeoff), but it is not the parameter of the LR model. See also the similar question. we can use a wrapper as follows:I think it would be immensely helpful to the Tensorflow community if there was a well-documented solution to the crucial task of testing a single new image against the model created by the convnet in the CIFAR-10 tutorial.  I may be wrong, but this critical step that makes the trained model usable in practice seems to be lacking. There is a "missing link" in that tutorial—a script that would directly load a single image (as array or binary), compare it against the trained model, and return a classification. Prior answers give partial solutions that explain the overall approach, but none of which I've been able to implement successfully. Other bits and pieces can be found here and there, but unfortunately haven't added up to a working solution. Kindly consider the research I've done, before tagging this as duplicate or already answered. Tensorflow: how to save/restore a model? Restoring TensorFlow model Unable to restore models in tensorflow v0.8 https://gist.github.com/nikitakit/6ef3b72be67b86cb7868 The most popular answer is the first, in which @RyanSepassi and @YaroslavBulatov describe the problem and an approach: one needs to "manually construct a graph with identical node names, and use Saver to load the weights into it". Although both answers are helpful, it is not apparent how one would go about plugging this into the CIFAR-10 project. A fully functional solution would be highly desirable so we could port it to other single image classification problems. There are several questions on SO in this regard that ask for this, but still no full answer (for example Load checkpoint and evaluate single image with tensorflow DNN). I hope we can converge on a working script that everyone could use. The below script is not yet functional, and I'd be happy to hear from you on how this can be improved to provide a solution for single-image classification using the CIFAR-10 TF tutorial trained model. Assume all variables, file names etc. are untouched from the original tutorial. New file: cifar10_eval_single.py There are two methods to feed a single new image to the cifar10 model. The first method is a cleaner approach but requires modification in the main file, hence will require retraining. The second method is applicable when a user does not want to modify the model files and instead wants to use the existing check-point/meta-graph files. The code for the first approach is as follows: The script requires that a user creates two placeholders and a conditional execution statement for it to work. The placeholders and conditional execution statement are added in cifar10_train.py as shown below: The inputs in cifar10 model are connected to queue runner object which is a multistage queue that can prefetch data from files in parallel. See a nice animation of queue runner here While queue runners are efficient in prefetching large dataset for training, they are an overkill for inference/testing where only a single file is needed to be classified, also they are a bit more involved to modify/maintain.
For that reason, I have added a placeholder "is_training", which is set to False while training as shown below: Another placeholder "imgs" holds a tensor of shape (1,32,32,3) for the image that will be fed during inference -- the first dimension is the batch size which is one in this case. I have modified cifar model to accept 32x32 images instead of 24x24 as the original cifar10 images are 32x32. Finally, the conditional statement feeds the placeholder or queue runner output to the graph. The "is_training" placeholder is set to False during inference and "img" placeholder is fed a numpy array -- the numpy array is reshaped from 3 to 4 dimensional vector to conform to the input tensor to inference function in the model. That is all there is to it. Any model can be inferred with a single/user defined test data like shown in the script above. Essentially read the graph, feed data to the graph nodes and run the graph to get the final output. Now the second method. The other approach is to hack cifar10.py and cifar10_eval.py to change batch size to one and replace the data coming from the queue runner with the one read from a file. Set batch size to 1: Call inference with an image file read. Then pass logits to eval_once and modify eval once to evaluate logits: There is no separate script to run this method of inference, just run cifar10_eval.py which will now read a file from the user defined location with a batch size of one.  Here's how I ran a single image at a time.  I'll admit it seems a bit hacky with the reuse of getting the scope.   This is a helper function Here is the main part of the code that runs a single image at a time within the for loop.   Here is an alternative implementation to the above using place holders it's a bit cleaner in my opinion. but I'll leave the above example for historical reasons. got it working with this output I don't have working code for you I'm afraid, but here's how we often tackle this problem in production: Save out the GraphDef to disk, using something like write_graph. Use freeze_graph to load the GraphDef and checkpoints, and save out a GraphDef with the Variables converted into Constants. Load the GraphDef in something like label_image or classify_image. For your example this is overkill, but I would at least suggest serializing the graph in the original example as a GraphDef, and then loading it in your script (so you don't have to duplicate the code generating the graph). With the same graph created, you should be able to populate it from a SaverDef, and the freeze_graph script may help as an example.Working with Sklearn stratified kfold split, and when I attempt to split using multi-class, I received on error (see below).  When I tried and split using binary, it works no problem. keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; from the split method docs: split(X, y, groups=None) [...] y : array-like, shape (n_samples,) The target variable for supervised learning problems. Stratification is done based on the y labels. i.e. your y must be a 1-D array of your class labels. Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards. Call to split() like this: If your target variable is continuous then use simple KFold cross validation instead of StratifiedKFold. I bumped into the same problem and found out that you can check the type of the target with this util function: From its docstring: With LabelEncoder you can transform your classes into an 1d array of numbers (given your target labels are in an 1d array of categoricals/object): In my case, x was a 2D matrix, and y was also a 2d matrix, i.e. indeed a multi-class multi-output case. I just passed a dummy np.zeros(shape=(n,1)) for the y and the x as usual. Full code example: Complementing what @desertnaut said, in order to convert your one-hot-encoding back to 1-D array you will only need to do is: This will convert back to the initial representation of your classes.TensorFlow MNIST example not running with fully_connected_feed.py I checked this out and realized that input_data was not built-in.  So I downloaded the whole folder from here. How can I start the tutorial: I'm using iPython (Jupyter) so do I need to change my working directory to this folder I downloaded? or can I add this to my tensorflow directory? If so, where do I add the files? I installed tensorflow with pip (on my OSX) and the current location is ~/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py Are these files meant to be accessed directly through tensorflow like sklearn datasets? or am I just supposed to cd into the directory and work from there? The example is not clear.  EDIT: This post is very out-dated So let's assume that you are in the directory: /somePath/tensorflow/tutorial (and this is your working directory). All you need to do is to download the input_data.py file and place it like this. Let's assume that the file name you invoke: is main.py and it is also in the same directory. Once this is done, you can just start running main.py which will start downloading the files and will put them in the MNIST_data folder (once they are there the script will not be downloading them next time). The old tutorial said, to import the MNIST data, use: This will cause the error.
The new tutorial uses the following code to do so: And this works well. I am using different version - following Install on Windows with Docker here - and had similar problem. An easy workaround I've found was: 1.Into the Linux command line, figure out where is the input_data.py on my Docker image (in your case you mentionned that you had to download it manually. In my case, it was already here). I used the follwing linux command: I've got the files & path 2.launch Python and type the following command using SYS: you will get the existing paths. 4.add the path of inputa_data.py: Hope that it can help. If you found better option, let me know. :) How can I start the tutorial  I didn't download the folder you did but I installed tensorflow by pip and then I had similar problem. My workaround was to replace  import tensorflow.examples.tutorials.mnist.input_data with import tensorflow.examples.tutorials.mnist.input_data as input_data If you're using Tensorflow 2.0 or higher, you need to install tensorflow_datasets first: or if you're using an Anaconda distribution: from the command line. If you're using a Jupyter Notebook you will need to install and enable ipywidgets. According to the docs (https://ipywidgets.readthedocs.io/en/stable/user_install.html) using pip: If you're using an Anaconda distribution, install ipywidgets from the command line like such: With the Anaconda distribution there is no need to enable the extension, conda handles this for you. Then import into your code: You should be able to use it without error if you follow these instructions. I might be kinda late, but for tensorflow version 0.12.1, you might wanna use input_data.read_data_sets instead. Basically using this function to load the data from your local drive that you had downloaded from http://yann.lecun.com/exdb/mnist/. from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('data_set/') For TensorFlow API 2.0 the mnist data changed place to: tf.keras.datasets.mnist.load_data  There's now a much easier way to load MNIST data into tensorflow without having to download the data by using Tensorflow 2 and Tensorflow Datasets To get started, make sure you import Tensorflow and specify the 2nd version: Then load the data into a dictionary using the following code: and Then split the data into train and test: Now you can use these data generators however you like.  Remove the lines: and the line below will suffice: Note that if the dataset is not available in the examples built-in to the keras, this will download the dataset and solve the problem. :) MNIST input_data was built-in, it's just not a individual module, it's inside Tensorflow module, try  MNIST data set included as a part of tensorflow examples tutorial, If we want to use this : As TensorFlow official website shown, All MNIST data is hosted on http://yann.lecun.com/exdb/mnist/  For Tensorflow API above 2.0, to use MNIST dataset following command can be used, The following steps work perfectly in my Notebook: step 1 : get Python files from github :
!git clone https://github.com/tensorflow/tensorflow.git step 2 : append these files in my Python path :  import sys sys.path.append('/content/tensorflow/tensorflow/examples/tutorials/mnist') step 3 : load the MNIST data with 'input_data' fonction import input_data mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) That's all !While constructing each tree in the random forest using bootstrapped samples, for each terminal node, we select m variables at random from p variables to find the best split (p is the total number of features in your data). My questions (for RandomForestRegressor) are: 1) What does max_features correspond to (m or p or something else)? 2) Are m variables selected at random from max_features variables (what is the value of m)? 3) If max_features corresponds to m, then why would I want to set it equal to p for regression (the default)? Where is the randomness with this setting (i.e., how is it different from bagging)? Thanks. Straight from the documentation:  [max_features] is the size of the random subsets of features to consider when splitting a node. So max_features is what you call m. When max_features="auto", m = p and no feature subset selection is performed in the trees, so the "random forest" is actually a bagged ensemble of ordinary regression trees. The docs go on to say that Empirical good default values are max_features=n_features for regression problems, and max_features=sqrt(n_features) for classification tasks By setting max_features differently, you'll get a "true" random forest. @lynnyi, max_features is the number of features that are considered on a per-split level, rather than on the entire decision tree construction. More clear, during the construction of each decision tree, RF will still use all the features (n_features), but it only consider number of "max_features" features for node splitting. And the "max_features" features are randomly selected from the entire features. You could confirm this by plotting one decision tree from a RF with max_features=1, and check all the nodes of that tree to count the number of features involved. max_features is basically the number of features selected at random and without replacement at split. Suppose you have 10 independent columns or features, then max_features=5 will select at random and without replacement 5 features at every split.I am working on a seq2seq keras/tensorflow 2.0 model. Every time the user inputs something, my model prints the response perfectly fine. However on the last line of each response I get this: You: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least steps_per_epoch * epochs batches (in this case, 2 batches). You may need to use the repeat() function when building your dataset. The "You:" is my last output, before the user is supposed to type something new in. The model works totally fine, but I guess no error is ever good, but I don't quite get this error. It says "interrupting training", however I am not training anything, this program loads an already trained model. I guess this is why the error is not stopping the program? In case it helps, my model looks like this: To make sure that you have "at least steps_per_epoch * epochs batches", set the steps_per_epoch to You can see the maximum number of batches that model.fit() can take by the progress bar when the training interrupts: Here, the maximum would be 5230 - 1 Importantly, keep in mind that by default, batch_size is 32 in model.fit(). If you're using a tf.data.Dataset, you can also add the repeat() method, but be careful: it will loop indefinitely (unless you specify a number). I have also had a number of models crash with the same warnings while trying to train them.  The training dataset if created using the tf.keras.preprocessing.image_dataset_from_directory() and split 80/20.  I have created a variable to try and not run out of image.  Using ResNet50 with my own images..... but it still does. BATCH_SIZE is set to 32 so i am taking 80% of the number of images and dividing by 32 then taking away 1 to have surplus.....or so i thought. Error after 3 hours processing a a single successful Epoch is: Epoch 1/25 374/374 [==============================] - 8133s 22s/step -
  loss: 7.0126 - accuracy: 0.0028 - val_loss: 6.8585 - val_accuracy:
  0.0000e+00 Epoch 2/25   1/374 [..............................] - ETA: 0s - loss: 6.0445 - accuracy: 0.0000e+00WARNING:tensorflow:Your input
  ran out of data; interrupting training. Make sure that your dataset or
  generator can generate at least steps_per_epoch * epochs batches (in
  this case, 9350.0 batches). You may need to use the repeat() function
  when building your dataset. this might help.... Solution which worked for me was to set drop_remainder=True while generating the dataset. This automatically handles any extra data that is left over. For example: I had same problem and decreasing validation_steps from 50 to 10 solved the issue. If you create a dataset with image_dataset_from_directory, remove  steps_per_epoch and validation_steps parameters from model.fit. The reason is steps has been initiated when batch_size passed into image_dataset_from_directory, and you can trying get the steps number with len. I had the same problem in TF 2.1. It has something to do with the shape/ type of the input, namely the query. In my case, I solved the problem as follows: (My model takes 3 inputs) Output: WARNING:tensorflow:Your input ran out of data; interrupting training.
  Make sure that your dataset or generator can generate at least
  steps_per_epoch * epochs batches (in this case, 7 batches). You may
  need to use the repeat() function when building your dataset. array([[2.053718]], dtype=float32) Solution: Output: array([[2.053718]], dtype=float32) I understand that it is completely fine. Firstly. it is a warning not an error. Secondly, the situation is similar to one data is trained during one epoch, next epoch trains next data and you have set epochs value too high e.g.500 (assuming your data size is not fixed but will approximately be <=500). Suppose data size is 480. Now, the remaining epoch don't have any data left to process, hence the warning. As a result, it returns to the recent state when the last data was trained.
I hope this helps. Do let me know if the concept is misunderstood. Thanks! Try reducing the steps_per_epoch value below the value you have currently set. This helped me solve the problem I am seeing this issue with TF-2.9 and a custom ImageDataGenerator. The core issue appears to be that TF/Keras is not selecting the correct data adapter: select_data_adapter() was selecting a GeneratorDataAdapter when it should be a KerasSequenceAdapter. I updated the following file to work around the issue: A better solution is using : here .5 is float value which determines how much of the data you want to fit it.
This approach is better than the ones with BATCH_SIZE, but this will always fit the whole dataset, so change the data_amount value to adjust it  I also got this while training a model in google colab, and the reason was that there is not enough memory/RAM to store the amount of data per batch (if you are using batch), so after I lower the batch_size it all ran perfectlyI'm using scickit-learn to tune a model hyper-parameters. I'm using a pipeline to have chain the preprocessing with the estimator. A simple version of my problem would look like this: In my case the preprocessing (what would be StandardScale() in the toy example) is time consuming, and I'm not tuning any parameter of it. So, when I execute the example, the StandardScaler is executed 12 times. 2 fit/predict * 2 cv * 3 parameters. But every time StandardScaler is executed for a different value of the parameter C, it returns the same output, so it'd be much more efficient, to compute it once, and then just run the estimator part of the pipeline. I can manually split the pipeline between the preprocessing (no hyper parameters tuned) and the estimator. But to apply the preprocessing to the data, I should provide the training set only. So, I would have to implement the splits manually, and not use GridSearchCV at all. Is there a simple/standard way to avoid repeating the preprocessing while using GridSearchCV? Update:
Ideally, the answer below should not be used as it leads to data leakage as discussed in comments. In this answer, GridSearchCV will tune the hyperparameters on the data already preprocessed by StandardScaler, which is not correct. In most conditions that should not matter much, but algorithms which are too sensitive to scaling will give wrong results. Essentially, GridSearchCV is also an estimator, implementing fit() and predict() methods, used by the pipeline. So instead of: Do this: What it will do is, call the StandardScalar() only once, for one call to clf.fit() instead of multiple calls as you described.  Edit: Changed refit to True, when GridSearchCV is used inside a pipeline. As mentioned in documentation: refit : boolean, default=True
      Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance
  after fitting. If refit=False, clf.fit() will have no effect because the GridSearchCV object inside the pipeline will be reinitialized after fit().
When refit=True, the GridSearchCV will be refitted with the best scoring parameter combination on the whole data that is passed in fit().  So if you want to make the pipeline, just to see the scores of the grid search, only then the refit=False is appropriate. If you want to call the clf.predict() method, refit=True must be used, else Not Fitted error will be thrown. For those who stumbled upon a little bit different problem, that I had as well. Suppose you have this pipeline: Then, when specifying parameters you need to include this 'clf_' name that you used for your estimator. So the parameters grid is going to be: It is not possible to do this in the current version of scikit-learn (0.18.1). A fix has been proposed on the github project: https://github.com/scikit-learn/scikit-learn/issues/8830 https://github.com/scikit-learn/scikit-learn/pull/8322I am having trouble with the Keras backend functions for setting values.  I am trying to convert a model from PyTorch to Keras and am trying to set the weights of the Keras model, but the weights do not appear to be getting set.  Note: I am not actually setting with np.ones just using that for an example. I have tried... Loading an existing model Creating a simple model Then using set_weights or set_value or... afterwards I call either one of the following: And None of the weights appear to have been set.  The same values as before are returned. How do I set the weights of a layer in Keras with a numpy array of values? What is keras_layer in your code? You can set weights these ways: Where model is an instance of an existing model. 
You can see the expected length of the list and its array shapes using the method get_weights() from the same instances above. The set_weights() method of keras accepts a list of numpy arrays, what you have passed to the method seems like a single array.
The shape of this should be the same as the shape of the output of get_weights() on the same layer.
Here's the code: This worked for me and it returns the updated weights on calling get_weights(). If you are trying to convert Pytorch model to Keras model, you can also try a Pytorch2Keras converter. It supports base layers like Conv2d, Linear, Activations, some element-wise operations etc. You can follow pytorch2keras/layers.py for layer convertion functions.I have a dataset where the classes are unbalanced.  The classes are either '1' or '0' where the ratio of class '1':'0' is 5:1.  How do you calculate the prediction error for each class and the rebalance weights accordingly in sklearn with Random Forest, kind of like in the following link:  http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#balance You can pass sample weights argument to Random Forest fit method Sample weights. If None, then samples are equally weighted. Splits
    that would create child nodes with net zero or negative weight are
    ignored while searching for a split in each node. In the case of
    classification, splits are also ignored if they would result in any
    single class carrying a negative weight in either child node. In older version there were a preprocessing.balance_weights method to generate balance weights for given samples, such that classes become uniformly distributed. It is still there, in internal but still usable preprocessing._weights module, but is deprecated and will be removed in future versions. Don't know exact reasons for this. Update Some clarification, as you seems to be confused. sample_weight usage is straightforward, once you remember that its purpose is to balance target classes in training dataset. That is, if you have X as observations and y as classes (labels), then len(X) == len(y) == len(sample_wight), and each element of sample witght 1-d array represent weight for a corresponding (observation, label) pair. For your case, if 1 class is represented 5 times as 0 class is, and you balance classes distributions, you could use simple  assigning weight of 5 to all 0 instances and weight of 1 to all 1 instances. See link above for a bit more crafty balance_weights weights evaluation function. This is really a shame that sklearn's "fit" method does not allow specifying a performance measure to be optimized. No one around seem to understand or question or be interested in what's actually going on when one calls fit method on data sample when solving a classification task. We (users of the scikit learn package) are silently left with suggestion to indirectly use crossvalidated grid search with specific scoring method suitable for unbalanced datasets in hope to stumble upon a parameters/metaparameters set which produces appropriate AUC or F1 score. But think about it: looks like "fit" method called under the hood each time always optimizes accuracy. So in end effect, if we aim to maximize F1 score, GridSearchCV gives us "model with best F1 from all modesl with best accuracy". Is that not silly? Would not it be better to directly optimize model's parameters for maximal F1 score? 
Remember old good Matlab ANNs package, where you can set desired performance metric to RMSE, MAE, and whatever you want given that gradient calculating algo is defined. Why is choosing of performance metric silently omitted from sklearn? At least, why there is no simple option to assign class instances weights automatically to remedy unbalanced datasets issues? Why do we have to calculate wights manually? Besides, in many machine learning books/articles I saw authors praising sklearn's manual as awesome if not the best sources of information on topic. No, really? Why is unbalanced datasets problem (which is obviously of utter importance to data scientists) not even covered nowhere in the docs then?
I address these questions to contributors of sklearn, should they read this. Or anyone knowing reasons for doing that welcome to comment and clear things out. UPDATE 
Since scikit-learn 0.17, there is class_weight='balanced' option which you can pass at least to some classifiers:
 The “balanced” mode uses the values of y to automatically adjust
  weights inversely proportional to class frequencies in the input data
  as n_samples / (n_classes * np.bincount(y)). Use the parameter class_weight='balanced'  From sklearn documentation: The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) If the majority class is 1, and the minority class is 0, and they are in the ratio 5:1, the sample_weight array should be: Note that you do not invert the ratios.This also applies to class_weights. The larger number is associated with the majority class.I have a scenario where I have several thousand instances of data. The data itself is represented as a single integer value. I want to be able to detect when an instance is an extreme outlier.  For example, with the following example data: d is clearly an anomaly, and I would want to perform a specific action based on this. I was tempted to just try an use my knowledge of the particular domain to detect anomalies. For instance, figure out a distance from the mean value that is useful, and check for that, based on heuristics. However, I think it's probably better if I investigate more general, robust anomaly detection techniques, which have some theory behind them. Since my working knowledge of mathematics is limited, I'm hoping to find a technique which is simple, such as using standard deviation. Hopefully the single-dimensioned nature of the data will make this quite a common problem, but if more information for the scenario is required please leave a comment and I will give more info. Edit: thought I'd add more information about the data and what I've tried in case it makes one answer more correct than another. The values are all positive and non-zero. I expect that the values will form a normal distribution. This expectation is based on an intuition of the domain rather than through analysis, if this is not a bad thing to assume, please let me know. In terms of clustering, unless there's also standard algorithms to choose a k-value, I would find it hard to provide this value to a k-Means algorithm. The action I want to take for an outlier/anomaly is to present it to the user, and recommend that the data point is basically removed from the data set (I won't get in to how they would do that, but it makes sense for my domain), thus it will not be used as input to another function. So far I have tried three-sigma, and the IQR outlier test on my limited data set. IQR flags values which are not extreme enough, three-sigma points out instances which better fit with my intuition of the domain. Information on algorithms, techniques or links to resources to learn about this specific scenario are valid and welcome answers. What is a recommended anomaly detection technique for simple, one-dimensional data? Check out the three-sigma rule: An alternative method is the IQR outlier test: this test is usually employed by Box plots (indicated by the whiskers):  EDIT: For your case (simple 1D univariate data), I think my first answer is well suited.
That however isn't applicable to multivariate data. @smaclell suggested using K-means to find the outliers. Beside the fact that it is mainly a clustering algorithm (not really an outlier detection technique), the problem with k-means is that it requires knowing in advance a good value for the number of clusters K. A better suited technique is the DBSCAN: a density-based clustering algorithm. Basically it grows regions with sufficiently high density into clusters which will be maximal set of density-connected points.  DBSCAN requires two parameters: epsilon and minPoints. It starts with an arbitrary point that has not been visited. It then finds all the neighbor points within distance epsilon of the starting point. If the number of neighbors is greater than or equal to minPoints, a cluster is formed. The starting point and its neighbors are added to this cluster and the starting point is marked as visited. The algorithm then repeats the evaluation process for all the neighbors recursively. If the number of neighbors is less than minPoints, the point is marked as noise. If a cluster is fully expanded (all points within reach are visited) then the algorithm proceeds to iterate through the remaining unvisited points until they are depleted. Finally the set of all points marked as noise are considered outliers. There are a variety of clustering techniques you could use to try to identify central tendencies within your data. One such algorithm we used heavily in my pattern recognition course was K-Means. This would allow you to identify whether there are more than one related sets of data, such as a bimodal distribution. This does require you having some knowledge of how many clusters to expect but is fairly efficient and easy to implement. After you have the means you could then try to find out if any point is far from any of the means. You can define 'far' however you want but I would recommend the suggestions by @Amro as a good starting point. For a more in-depth discussion of clustering algorithms refer to the wikipedia entry on clustering. This is an old topic but still it lacks some information. Evidently, this can be seen as a case of univariate outlier detection. The approaches presented above have several pros and cons. Here are some weak spots: I think this problem can be solved in a few lines of python code like this: Subsequently you reject values above a certain threshold (97.5 percentile of the distribution of data), in case of an assumed normal distribution the threshold is 2.24. Here it translates to: or the 467 entry being rejected. Of course, one could argue, that the MAD (as presented) also assumes a normal dist. Therefore, why is it that argument 2 above (small sample) does not apply here? The answer is that MAD has a very high breakdown point. It is easy to choose different threshold points from different distributions and come to the same conclusion: 467 is the outlier. Both three-sigma rule and IQR test are often used, and there are a couple of simple algorithms to detect anomalies. The IQR test should be:Want to improve this question? Update the question so it can be answered with facts and citations by editing this post. Closed 4 years ago. What does number of hidden layers in a multilayer perceptron neural network do to the way neural network behaves? Same question for number of nodes in hidden layers? Let's say I want to use a neural network for hand written character recognition. In this case I put pixel colour intensity values as input nodes, and character classes as output nodes.  How would I choose number of hidden layers and nodes to solve such problem? Note: this answer was correct at the time it was made, but has since become outdated. It is rare to have more than two hidden layers in a neural network. The number of layers will usually not be a parameter of your network you will worry much about. Although multi-layer neural networks with many layers can represent
  deep circuits, training deep networks has always been seen as somewhat
  of a challenge. Until very recently, empirical studies often found
  that deep networks generally performed no better, and often worse,
  than neural networks with one or two hidden layers. Bengio, Y. & LeCun, Y., 2007. Scaling learning algorithms towards AI. Large-Scale Kernel Machines, (1), pp.1-41. The cited paper is a good reference for learning about the effect of network depth, recent progress in teaching deep networks, and deep learning in general. The general answer is to for picking hyperparameters is to cross-validate.  Hold out some data, train the networks with different configurations, and use the one that performs best on the held out set. Most of the problems I have seen were solved with 1-2 hidden layers. It is proven that MLPs with only one hidden layer are universal function approximators (Hornik et. al.). More hidden layers can make the problem easier or harder. You usually have to try different topologies. I heard that you cannot add an arbitrary number of hidden layers if you want to train your MLP with backprop because the gradient will become too small in the first layers (I have no reference for that). But there are some applications where people used up to nine layers. Maybe you are interested in a standard benchmark problem which is solved by different classifiers and MLP topologies. Besides the fact that cross-validation on different model configurations(no. of hidden layers OR neurons per layer) will lead you to choose better configuration. One approach is training a model, as big and deep as possible and use dropout regularization to turn off some neurons and reduce overfitting. the reference to this approach can be seen in this paper.
https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf All the above answers are of course correct but just to add some more ideas:
Some general rules are the following based on this paper: 'Approximating Number of Hidden layer neurons in Multiple Hidden Layer BPNN Architecture' by Saurabh Karsoliya In general: Keep always in mind that you need to explore and try a lot of different combinations. Also, using GridSearch you could find the "best model and parameters".  E.g. we can do a GridSearch in order to determine the "best" size of the hidden layer.Using Keras from Tensorflow 1.4.1, how does one copy weights from one model to another? As some background, I'm trying to implement a deep-q network (DQN) for Atari games following the DQN publication by DeepMind.  My understanding is that the implementation uses two networks, Q and Q'.  The weights of Q are trained using gradient descent, and then the weights are copied periodically to Q'. Here's how I build Q and Q': I call that twice to get Q and Q'. I have an updateTargetModel method below that is my attempt at copying weights.  The code runs fine, but my overall DQN implementation is failing.  I'm really just trying to verify if this is a valid way of copying weights from one network to another. There's another question here that discusses saving and loading weights to and from disk (Tensorflow Copy Weights Issue), but there's no accepted answer.  There is also a question about loading weights from individual layers (Copying weights from one Conv2D layer to another), but I'm wanting to copy the entire model's weights. Actually what you've done is much more than simply copying weights. You made these two models identical all the time. Every time you update one model - the second one is also updated - as both models have the same weights variables.  If you want to just copy weights - the simplest way is by this command:I am following this tutorial for learning TensorFlow Slim but upon running the following code for Inception: I seem to be getting this set of errors: This is strange because all of this code is from their official guide. I am new to TF and any help would be appreciated. I got the same problem when using the 1.0 released and I could make it work without having to roll back on a previous version. The problem is caused by change in the api. That discussion helped me to find the solution: Google group > 
Recent API Changes in TensorFlow  You just have to update all the line with tf.concat for example  should be changed to  Note: I was able to use the models without problem. But I still got error afterward when wanting to load the pretrained weight.
Seems that the slim module got several changed since they made the checkpoint file. The graph created by the code and the one present in the checkpoint file were different. Note2: I was able to use the pretrain weights for inception_resnet_v2 by adding to all conv2d layer biases_initializer=None explicitly writing the name of the arguments solves the problem. instead of  use I got same error when I did the work. I found that The output is shape=(10, 64, 64). The code want concat outputs[0] to outputs[9] => get a new shape(640,64). But the "tf.concat" API may not allow to do this. (train_labels same to this) So I write to  It can run! I found most people answering wrong way. Its just due to the change in the tf.concat.
It works in the following way.  net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3]) use the following net = tf.concat(values=[branch_0, branch_1, branch_2, branch_3],axis=3,) Remember while passing the keyword arguments should be before the others.I have the following data set I am trying to make a plot of Date vs WearRate and color by Wheel. The code is as follows: It works but I want to put actual date labels. How do I do it? Edit The plot currently looks as shown here. However, I want to see "Aug 2015", "Sep 2015" etc on X axis and I want to display all the ticks.  The easiest way would be to use scale_x_date %b: Abbreviated month name
%y: Year without century
For a description of the full possibilities see ?strftime()I am using sklearn on Python to do some clustering. I've trained 200,000 data, and code below works well. But when I have new testing content, I'd like to cluster it to existed clusters I'd trained. So I'm wondering how to save IDF result, so that I can do TFIDF for the new testing content and make sure the result for new testing content have same array length. Thanks in advance. UPDATE I may need to save "transformer" or "tfidf" variable to file(txt or others), if one of them contains the trained IDF result. UPDATE For example. I have the training data: And do TFIDF, the result will contains 4 features(a,b,c,d) When I TEST: to see which cluster(already made by k-means) it belongs to. TFIDF will only give the result with 3 features(a,c,d), so the clustering in k-means will fall. (If I test ["a", "b", "e"], there may have other problems.) So how to store the features list for testing data (even more, store it in file)? UPDATE Solved, see answers below. I successfully saved the feature list by saving vectorizer.vocabulary_, and reuse by CountVectorizer(decode_error="replace",vocabulary=vectorizer.vocabulary_) Codes below: That works. tfidf will have same feature length as trained data. Instead of using the CountVectorizer for storing the vocabulary, the vocabulary of the tfidfvectorizer can be used directly. Training phase: The fit_transform works here as we are using the old vocabulary. If you were not storing the tfidf, you would have just used transform on the test data. Even when you are doing a transform there, the new documents from the test data are being "fit" to the vocabulary of the vectorizer of the train. That is exactly what we are doing here. The only thing we can store and re-use for a tfidf vectorizer is the vocabulary. If you want to store features list for testing data for use in future, you can do this: a simpler solution, just use joblib libarary as document said: you can do the vectorization and tfidf transformation in one stage: then fit and transform on the training data and use the tfidf model to transformI am trying to train a simple 2 layer Fully Connected neural net for Binary Classification in Tensorflow keras. I have split my data into Training and Validation sets with a 80-20 split using sklearn's train_test_split(). When I call model.fit(X_train, y_train, validation_data=[X_val, y_val]), it shows 0 validation loss and accuracy for all epochs, but it trains just fine.  Also, when I try to evaluate it on the validation set, the output is non-zero.  Can someone please explain why I am facing this 0 loss 0 accuracy error on validation. Thanks for your help. Here is the complete sample code (MCVE) for this error: https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing If you use keras instead of tf.keras everything works fine. With tf.keras, I even tried validation_data = [X_train, y_train], this also gives zero accuracy. Here is a demonstration: So, definitely there is some issue with tensorflow implementation of fit. I dug up the source, and it seems the part responsible for validation_data: internally calls model.evaluate, as we have already established evaluate works fine, I realized the only culprit could be unpack_x_y_sample_weight. So, I looked into the implementation: It's crazy, but if you just pass a tuple instead of a list, everything works fine due to the check inside unpack_x_y_sample_weight. (Your labels are missing after this step and somehow the data is getting fixed inside evaluate, so you're training with no reasonable labels, this seems like a bug but the documentation clearly states to pass tuple) The following code gives correct validation accuracy and loss: So, as this seems to be a bug, I have just opened a relevant issue at Tensorflow Github repo: https://github.com/tensorflow/tensorflow/issues/39370 Try changing the loss in your model.fit from loss="categorical_crossentropy" to loss="binary_crossentropy" I Had the SAME problem, and tried the answer above, but this is what worked for me. The thing is that I have a binary classification model, with only 1 output node, not a multi-classification model with multiple output nodes, so loss="binary_crossentropy" is the appropriate loss function in this case. ğŸ˜ŠI have the following data: I need to split the dataset into a training and testing set based on the "Group_ID" so that 80% of the data goes into a training set and 20% into a test set.  That is, I need my training set to look something like: And test set: What would be the simplest way to do this? As far as I know, the standard test_train_split function in sklearn does not support splitting by groups in a way where I can also indicate the size of the split (e.g. 80/20). I figured out the answer. This seems to work:I'd have expected t.word_index to have just the top 3 words. What am I doing wrong? There is nothing wrong in what you are doing. word_index is computed the same way no matter how many most frequent words you will use later (as you may see here). So when you will call any transformative method - Tokenizer will use only three most common words and at the same time, it will keep the counter of all words - even when it's obvious that it will not use it later. Just a add on Marcin's answer ("it will keep the counter of all words - even when it's obvious that it will not use it later."). The reason it keeps counter on all words is that you can call fit_on_texts multiple times. Each time it will update the internal counters, and when transformations are called, it will use the top words based on the updated counters. Hope it helps. Limiting num_words to a small number (eg, 3) has no effect on fit_on_texts outputs such as word_index, word_counts, word_docs. It does have effect on texts_to_matrix. The resulting matrix will have num_words (3) columns. Just to add a little bit to farid khafizov's answer,
words at sequence of num_words and above are removed from the results of texts_to_sequences (4 in 1st, 5 in 2nd and 6 in 3rd sentence disappeared respectively)I encoded my categorical data using sklearn.OneHotEncoder and fed them to a random forest classifier. Everything seems to work and I got my predicted output back. Is there a way to reverse the encoding and convert my output back to its original state? A good systematic way to figure this out is to start with some test data and work through the sklearn.OneHotEncoder source with it. If you don't much care about how it works and simply want a quick answer, skip to the bottom. Lines 1763-1786 determine the n_values_ parameter. This will be determined automatically if you set n_values='auto' (the default). Alternatively you can specify a maximum value for all features (int) or a maximum value per feature (array). Let's assume that we're using the default. So the following lines execute: Next the feature_indices_ parameter is calculated. So feature_indices_ is merely the cumulative sum of n_values_ with a 0 prepended. Next, a scipy.sparse.coo_matrix is constructed from the data. It is initialized from three arrays: the sparse data (all ones), the row indices, and the column indices. Note that the coo_matrix is immediately converted to a scipy.sparse.csr_matrix. The coo_matrix is used as an intermediate format because it "facilitates fast conversion among sparse formats." Now, if n_values='auto', the sparse csr matrix is compressed down to only the columns with active features. The sparse csr_matrix is returned if sparse=True, otherwise it is densified before returning. Now let's work in reverse. We'd like to know how to recover X given the sparse matrix that is returned along with the OneHotEncoder features detailed above. Let's assume we actually ran the code above by instantiating a new OneHotEncoder and running fit_transform on our data X. The key insight to solving this problem is understanding the relationship between active_features_ and out.indices. For a csr_matrix, the indices array contains the column numbers for each data point. However, these column numbers are not guaranteed to be sorted. To sort them, we can use the sorted_indices method. We can see that before sorting, the indices are actually reversed along the rows. In other words, they are ordered with the last column first and the first column last. This is evident from the first two elements: [12, 0]. 0 corresponds to the 3 in the first column of X, since 3 is the minimum element it was assigned to the first active column. 12 corresponds to the 5 in the second column of X. Since the first row occupies 10 distinct columns, the minimum element of the second column (1) gets index 10. The next smallest (3) gets index 11, and the third smallest (5) gets index 12. After sorting, the indices are ordered as we would expect. Next we look at active_features_: Notice that there are 19 elements, which corresponds to the number of distinct elements in our data (one element, 8, was repeated once). Notice also that these are arranged in order. The features that were in the first column of X are the same, and the features in the second column have simply been summed with 100, which corresponds to ohc.feature_indices_[1]. Looking back at out.indices, we can see that the maximum column number is 18, which is one minus the 19 active features in our encoding. A little thought about the relationship here shows that the indices of ohc.active_features_ correspond to the column numbers in ohc.indices. With this, we can decode: This gives us: And we can get back to the original feature values by subtracting off the offsets from ohc.feature_indices_: Note that you will need to have the original shape of X, which is simply (n_samples, n_features). Given the sklearn.OneHotEncoder instance called ohc, the encoded data (scipy.sparse.csr_matrix) output from ohc.fit_transform or ohc.transform called out, and the shape of the original data (n_samples, n_feature), recover the original data X with: Just compute dot-product of the encoded values with ohe.active_features_. It works both for sparse and dense representation. Example: The key insight is that the active_features_ attribute of the OHE model represents the original values for each binary column. Thus we can decode the binary-encoded number by simply computing a dot-product with active_features_. For each data point there's just a single 1 the position of the original value. Use numpy.argmax() with axis = 1. Example: Since version 0.20 of scikit-learn, the active_features_ attribute of the OneHotEncoder class has been deprecated, so I suggest to rely on the categories_ attribute instead. The below function can help you recover the original data from a matrix that has been one-hot encoded: To test it, I have created a small data set that includes the ratings that users have given to users If we are building a prediction model, we have to remember to delete the dependent variable (in this case the rating) from the DataFrame before we encode it. Then we proceed to do the encoding Which results in: After encoding the we can reverse using the reverse_one_hot function we defined above, like this: Which gives us: If the features are dense, like [1,2,4,5,6], with several number missed. Then, we can mapping them to corresponding positions. This is a compromised and simple method, but works and easy to reverse by argmax(), e.g.: See https://stackoverflow.com/a/42874726/562769 gives: The short answer is "no". The encoder takes your categorical data and automagically transforms it to a reasonable set of numbers. The longer answer is "not automatically". If you provide an explicit mapping using the n_values parameter, though, you can probably implement own decoding at the other side. See the documentation for some hints on how that might be done. That said, this is a fairly strange question. You may want to, instead, use a DictVectorizer Pandas approach : 
To convert categorical variables to binary variables, pd.get_dummies  does that and to convert them back, you can find the index of the value where there is 1 using pd.Series.idxmax(). Then you can map to a list(index in according to original data) or dictionary.Pretty common situation, I'd wager. You have a blog or news site and you have plenty of articles or blags or whatever you call them, and you want to, at the bottom of each, suggest others that seem to be related. Let's assume very little metadata about each item. That is, no tags, categories. Treat as one big blob of text, including the title and author name. How do you go about finding the possibly related documents? I'm rather interested in the actual algorithm, not ready solutions, although I'd be ok with taking a look at something implemented in ruby or python, or relying on mysql or pgsql. edit: the current answer is pretty good but I'd like to see more. Maybe some really bare example code for a thing or two. This is a pretty big topic -- in addition to the answers people come up with here, I recommend tracking down the syllabi for a couple of information retrieval classes and checking out the textbooks and papers assigned for them. That said, here's a brief overview from my own grad-school days: The simplest approach is called a bag of words. Each document is reduced to a sparse vector of {word: wordcount} pairs, and you can throw a NaiveBayes (or some other) classifier at the set of vectors that represents your set of documents, or compute similarity scores between each bag and every other bag (this is called k-nearest-neighbour classification). KNN is fast for lookup, but requires O(n^2) storage for the score matrix; however, for a blog, n isn't very large. For something the size of a large newspaper, KNN rapidly becomes impractical, so an on-the-fly classification algorithm is sometimes better. In that case, you might consider a ranking support vector machine. SVMs are neat because they don't constrain you to linear similarity measures, and are still quite fast. Stemming is a common preprocessing step for bag-of-words techniques; this involves reducing morphologically related words, such as "cat" and "cats", "Bob" and "Bob's", or "similar" and "similarly", down to their roots before computing the bag of words. There are a bunch of different stemming algorithms out there; the Wikipedia page has links to several implementations. If bag-of-words similarity isn't good enough, you can abstract it up a layer to bag-of-N-grams similarity, where you create the vector that represents a document based on pairs or triples of words. (You can use 4-tuples or even larger tuples, but in practice this doesn't help much.) This has the disadvantage of producing much larger vectors, and classification will accordingly take more work, but the matches you get will be much closer syntactically. OTOH, you probably don't need this for semantic similarity; it's better for stuff like plagiarism detection. Chunking, or reducing a document down to lightweight parse trees, can also be used (there are classification algorithms for trees), but this is more useful for things like the authorship problem ("given a document of unknown origin, who wrote it?"). Perhaps more useful for your use case is concept mining, which involves mapping words to concepts (using a thesaurus such as WordNet), then classifying documents based on similarity between concepts used. This often ends up being more efficient than word-based similarity classification, since the mapping from words to concepts is reductive, but the preprocessing step can be rather time-consuming. Finally, there's discourse parsing, which involves parsing documents for their semantic structure; you can run similarity classifiers on discourse trees the same way you can on chunked documents. These pretty much all involve generating metadata from unstructured text; doing direct comparisons between raw blocks of text is intractable, so people preprocess documents into metadata first. You should read the book "Programming Collective Intelligence: Building Smart Web 2.0 Applications" (ISBN 0596529325)! For some method and code: First ask yourself, whether you want to find direct similarities based on word matches, or whether you want to show similar articles that may not directly relate to the current one, but belong to the same cluster of articles. See Cluster analysis / Partitional clustering. A very simple (but theoretical and slow) method for finding direct similarities would be: Preprocess: Find similar articles: This is a typical case of Document Classification which is studied in every class of Machine Learning. If you like statistics, mathematics and computer science, I recommend that you have a look at the unsupervised methods like kmeans++, Bayesian methods and LDA. In particular, Bayesian methods are pretty good at what are you looking for, their only problem is being slow (but unless you run a very large site, that shouldn't bother you much). On a more practical and less theoretical approach, I recommend that you have a look a this and this other great code examples. A small vector-space-model search engine in Ruby. The basic idea is that two documents are related if they contain the same words. So we count the occurrence of words in each document and then compute the cosine between these vectors (each terms has a fixed index, if it appears there is a 1 at that index, if not a zero). Cosine will be 1.0 if two documents have all terms common, and 0.0 if they have no common terms. You can directly translate that to % values. the definition of Array#cosine is left as an exercise to the reader (should deal with nil values and different lengths, but well for that we got Array#zip right?) BTW, the example documents are taken from the SVD paper by Deerwester etal :) Some time ago I implemented something similiar. Maybe this idea is now outdated, but I hope it can help. I ran a ASP 3.0 website for programming common tasks and started from this principle: user have a doubt and will stay on website as long he/she can find interesting content on that subject. When an user arrived, I started an ASP 3.0 Session object and recorded all user navigation, just like a linked list. At Session.OnEnd event, I take first link, look for next link and incremented a counter column like: So, to check related articles I just had to list top n NextPage entities, ordered by counter column descending.I wonder if we can set up an "optional" step in sklearn.pipeline. For example, for a classification problem, I may want to try an ExtraTreesClassifier with AND without a PCA transformation ahead of it. In practice, it might be a pipeline with an extra parameter specifying the toggle of the PCA step, so that I can optimize on it via GridSearch and etc. I don't see such an implementation in sklearn source, but is there any work-around? Furthermore, since the possible parameter values of a following step in pipeline might depend on the parameters in a previous step (e.g., valid values of ExtraTreesClassifier.max_features depend on PCA.n_components), is it possible to specify such a conditional dependency in sklearn.pipeline and sklearn.grid_search? Thank you!   From the docs: Individual steps may also be replaced as parameters, and non-final
  steps may be ignored by setting them to None: Pipeline steps cannot currently be made optional in a grid search but you could wrap the PCA class into your own OptionalPCA component with a boolean parameter to turn off PCA when requested as a quick workaround. You might want to have a look at hyperopt to setup more complex search spaces. I think it has good sklearn integration to support this kind of patterns by default but I cannot find the doc anymore. Maybe have a look at this talk. For the dependent parameters problem, GridSearchCV supports trees of parameters to handle this case as demonstrated in the documentation.How are we supposed to use the dictionary output from lightgbm.cv to improve our predictions? Here's an example - we train our cv model using the code below: How can we use the parameters found from the best iteration of the above code to predict an output? In this case, cv_mod has no "predict" method like lightgbm.train, and the dictionary output from lightgbm.cvthrows an error when used in lightgbm.train.predict(..., pred_parameters = cv_mod). Am I missing an important transformation step? In general, the purpose of CV is NOT to do hyperparameter optimisation. The purpose is to evaluate performance of model-building procedure.  A basic train/test split is conceptually identical to a 1-fold CV (with a custom size of the split in contrast to the 1/K train size in the k-fold CV). The advantage of doing more splits (i.e. k>1 CV) is to get more information about the estimate of generalisation error. There is more info in a sense of getting the error + stat uncertainty. There is an excellent discussion on CrossValidated (start with the links added to the question,  which cover the same question, but formulated in a different way). It covers  nested cross validation and is absolutely not straightforward. But if you will wrap your head around the concept in general, this will help you in various non-trivial situations. The idea that you have to take away is: The purpose of CV is to evaluate performance of model-building procedure.  Keeping that idea in mind, how does one approach hyperparameter estimation in general (not only in LightGBM)?  Then you can make a step further and say that you had an additional hold-out set, that was separated before hyperparameter optimisation was started. This way you can evaluate the chosen best model on that set to measure the final generalisation error. However, you can make even step further and instead of having a single test sample you can have an outer CV loop, which brings us to nested cross validation. Technically, lightbgm.cv() allows you only to evaluate performance on a k-fold split with fixed model parameters. For hyper-parameter tuning you will need to run it in a loop providing different parameters and recoding averaged performance to choose the best parameter set. after the loop is complete. This interface is different from sklearn, which provides you with complete functionality to do hyperparameter optimisation in a CV loop. Personally, I would recommend to use the sklearn-API of lightgbm. It is just a wrapper around the native lightgbm.train() functionality, thus it is not slower. But it allows you to use the full stack of sklearn toolkit, thich makes your life MUCH easier. If you're happy with your CV results, you just use those parameters to call the 'lightgbm.train' method. Like @pho said, CV is usually just for param tuning. You don't use the actual CV object for predictions. You should use CV for parameter optimization. If your model performs well on all folds use these parameters to train on the whole training set.
Then evaluate that model on the external test set.I'm using the scikit-learn machine learning library (Python) for a machine learning project. One of the algorithms I'm using is the Gaussian Naive Bayes implementation. One of the attributes of the GaussianNB() function is the following: I want to alter the class prior manually since the data I use is very skewed and the recall of one of the classes is very important. By assigning a high prior probability to that class the recall should increase. However, I can't figure out how to set the attribute correctly. I've read the below topics already but their answers don't work for me. How can the prior probabilities manually set for the Naive Bayes clf in scikit-learn? How do I know what prior's I'm giving to sci-kit learn? (Naive-bayes classifiers.) This is my code: I figured this was the correct syntax and I could find out which class belongs to which place in the array by playing with the values but the results remain unchanged. Also no errors were given. What is the correct way of setting the attributes of the GaussianNB algorithm from scikit-learn library? Link to the scikit documentation of GaussianNB @Jianxun Li: there is in fact a way to set prior probabilities in GaussianNB. It's called 'priors' and its available as a parameter. See documentation:
"Parameters:    priors : array-like, shape (n_classes,)
    Prior probabilities of the classes. If specified the priors are not adjusted according to the data."
So let me give you an example: But if you changed the prior probabilities, it will give a different answer which is what you are looking for I believe. The GaussianNB() implemented in scikit-learn does not allow you to set class prior. If you read the online documentation, you see .class_prior_ is an attribute rather than parameters. Once you fit the GaussianNB(), you can get access to class_prior_ attribute. It is calculated by simply counting the number of different labels in your training sample. You see the estimator is smart enough to take into account the unbalanced weight issue. So you don't have to manually specify the priors.I have a DataFrame with a few time series: I want to decompose the first time series divida in a way that I can separate its trend from its seasonal and residual components. I found an answer here, and am trying to use the following code: However I keep getting this error: How can I proceed? Works fine when you convert your index to DateTimeIndex: Access the components via: Statsmodel will decompose the series only if you provide frequency. Usually all time series index will contain frequency eg: Daywise, Business days, weekly So it shows error. You can remove this error by two ways: It depends on the index format. You can have DateTimeIndex or you can have PeriodIndex. Stefan presented the example for DateTimeIndex. Here is my example for PeriodIndex. 
My original DataFrame has a MultiIndex index with year in first level and month in second level. Here is how I convert it to PeriodIndex: Now it is ready to be used by seasonal_decompose. Make it simple: Follow three steps:
1. if not done, make the column in yyyy-mm-dd or dd-mm-yyyy( using excel).
2. Then using pandas convert it into date format as:
df['Date'] = pd.to_datetime(df['Date'])
3. decompose it using: And finally:    Try parsing the date column using parse_dates , and later mention the index column .In my classification scheme, there are several steps including: The main parameters to be tuned in the scheme above are percentile (2.) and hyperparameters for SVC (4.) and I want to go through grid search for tuning. The current solution builds a "partial" pipeline including step 3 and 4 in the scheme clf = Pipeline([('normal',preprocessing.StandardScaler()),('svc',svm.SVC(class_weight='auto'))])
and breaks the scheme into two parts: Tune the percentile of features to keep through the first grid search The f1 scores will be stored and then be averaged through all fold partitions for all percentiles, and the percentile with the best CV score is returned. The purpose of putting 'percentile for loop' as the inner loop is to allow fair competition as we have the same training data (including synthesized data) across all fold partitions for all percentiles. After determining the percentile, tune the hyperparameters by second grid search It is done in the very similar way, except we tune the hyperparamter for SVC rather than percentile of features to select. My questions are: In the current solution, I only involve 3. and 4. in the clf and do 1. and 2. kinda "manually" in two nested loop as described above. Is there any way to include all four steps in a pipeline and do the whole process at once? If it is okay to keep the first nested loop, then is it possible (and how) to simplify the next nested loop using a single pipeline and simply use GridSearchCV(clf_all, parameter_comb) for tuning? Please note that both SMOTE and Fisher (ranking criteria) have to be done only for the training data in each fold partition. It would be so much appreciated for any comment. SMOTE and Fisher are shown below: SMOTE is from https://github.com/blacklab/nyan/blob/master/shared_modules/smote.py, it returns the synthesized data. I modified it to return the original input data stacked with the synthesized data along with its labels and synthesized ones. scikit created a FunctionTransformer as part of the preprocessing class in version 0.17. It can be used in a similar manner as David's implementation of the class Fisher in the answer above - but with less flexibility. If the input/output of the function is configured properly, the transformer can implement the fit/transform/fit_transform methods for the function and thus allow it to be used in the scikit pipeline. For example, if the input to a pipeline is a series, the transformer would be as follows: where vect is a tf_idf transformer, clf is a classifier and train is the training dataset. "train.desc" is the series text input to the pipeline. I don't know where your SMOTE() and Fisher() functions are coming from, but the answer is yes you can definitely do this. In order to do so you will need to write a wrapper class around those functions though. The easiest way to this is inherit sklearn's BaseEstimator and TransformerMixin classes, see this for an example: http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html If this isn't making sense to you, post the details of at least one of your functions (the library it comes from or your code if you wrote it yourself) and we can go from there. EDIT: I apologize, I didn't look at your functions closely enough to realize that they transform your target in addition to your training data (i.e. both X and y). Pipeline does not support transformations to your target so you will have do them prior as you originally were. For your reference, here is what it would look like to write your custom class for your Fisher process which would work if the function itself did not need to affect your target variable. You actually can put all of these functions into a single pipeline! In the accepted answer, @David wrote that your functions transform your target in addition to your training data (i.e. both X and y). Pipeline does not support transformations to your target so you will have do them prior as you originally were. It is true that sklearn's pipeline does not support this. However imblearn's pipeline here supports this. The imblearn pipeline is just like that of sklearn but it allows you to call transformations separately on the training and testing data via sample methods. Moreover, these sample methods are actually designed so that you can change both the data X and the labels y. This is important because many times you want to include smote in your pipeline but you want to smote just the training data, not the testing data. And with the imblearn pipeline, you can call smote in the pipeline to transform just X_train and y_train and not X_test and y_test. So you can create an imblearn pipeline that has a smote sampler, pre-processing step, and svc. For more details check out this stack overflow post here and machine learning mastery article here.I have an issue with tf.callbacks.ModelChekpoint. As you can see in my log file, the warning comes always before the last iteration where the val_acc is calculated. Therefore, Modelcheckpoint never finds the val_acc This is my code for training the CNN. I know how frustrating these things can be sometimes..but tensorflow requires that you explicitly write out the name of metric you are wanting to calculate You will need to actually say 'val_accuracy' Hope this helps =) To add to the accepted answer as I just struggled with this. Not only do you have to use the full the metric name, it must match for your model.compile, ModelCheckpoint, and EarlyStopping. I had one set to accuracy and the other two set to val_accuracy and it did not work. Print the metrics after training for one epoch like below. This will print the metrics defined for your model. Now replace them in your metrics. It will work like charm. This hack was given by the gentleman in the below link. Thanks to him!!
https://github.com/tensorflow/tensorflow/issues/33163#issuecomment-540451749 I had the same issue as even after mentioning the metric=val_accuracy it did not work. So I just changed it to  metric=val_acc and it worked. If you are using validation_steps or steps per epochs in model.fit() function. Remove that parameter. The validation losses and accuracy will start appearing. Just include a few parameters as possible: If you are using ModelCheckpoint and EarlyStopping then in that case both the "momitor" metric should be same like 'accuracy'. Also, EarlyStopping doesn't support all metrics in some tensorflow versions so you have to choose an metrics that's common in both and what best suits your model. I still had the issue even after changing the argument from monitor='val_acc' to monitor='val_accuracy'. You can check this link from Keras and make sure you keep the arguments and the values you are passing as it is. I removed extra arguments I was passing and it worked for me! Before After You have to write what the name appears when you run it. You are probably using a different metric instead of 'accuracy' in the metric section. BinaryAccuracy, SparseAccuracy, CategoricalAccuracy etc. For example, when you use BinaryAccuracy, 'binary_accuracy' is written instead of 'accuracy' in the run section. This is how you should write in the monitor section. monitor='val_loss' in both  Checkpointing and Earlystopping callbacks worked for me. You may also find your model metrics having an incrementing number appended to them after the first run. E.g. If that is the case, you can reset the session before each run so that the numbers aren't appended.I'm implementing a Multilayer Perceptron in Keras and using scikit-learn to perform cross-validation. For this, I was inspired by the code found in the issue Cross Validation in Keras In my studies on neural networks, I learned that the knowledge representation of the neural network is in the synaptic weights and during the network tracing process, the weights that are updated to thereby reduce the network error rate and improve its performance. (In my case, I'm using Supervised Learning) For better training and assessment of neural network performance, a common method of being used is cross-validation that returns partitions of the data set for training and evaluation of the model. My doubt is... In this code snippet: We define, train and evaluate a new neural net for each of the generated partitions? If my goal is to fine-tune the network for the entire dataset, why is it not correct to define a single neural network and train it with the generated partitions? That is, why is this piece of code like this? and not so? Is my understanding of how the code works wrong? Or my theory? If my goal is to fine-tune the network for the entire dataset It is not clear what you mean by "fine-tune", or even what exactly is your purpose for performing cross-validation (CV); in general, CV serves one of the following purposes: Since you don't define any search grid for hyperparameter selection in your code, it would seem that you are using CV in order to get the expected performance of your model (error, accuracy etc). Anyway, for whatever reason you are using CV, the first snippet is the correct one; your second snippet will train your model sequentially over the different partitions (i.e. train on partition #1, then continue training on partition #2 etc), which essentially is just training on your whole data set, and it is certainly not cross-validation... That said, a final step after the CV which is often only implied (and frequently missed by beginners) is that, after you are satisfied with your chosen hyperparameters and/or model performance as given by your CV procedure, you go back and train again your model, this time with the entire available data. You can use wrappers of the Scikit-Learn API with Keras models. Given inputs x and y, here's an example of repeated 5-fold cross-validation: I think many of your questions will be answered if you read about nested cross-validation. This is a good way to "fine tune" the hyper parameters of your model.  There's a thread here: https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection The biggest issue to be aware of is "peeking" or circular logic.  Essentially - you want to make sure that none of data used to assess model accuracy is seen during training. One example where this might be problematic is if you are running something like PCA or ICA for feature extraction. If doing something like this, you must be sure to run PCA on your training set, and then apply the transformation matrix from the training set to the test set. The main idea of testing your model performance is to perform the following steps: So basically - the data you should finally test your model should mimic the first data portion you'll get from your client/application to apply your model on. So that's why cross-validation is so powerful - it makes every data point in your whole dataset to be used as a simulation of new data. And now - to answer your question - every cross-validation should follow the following pattern: because after all, you'll first train your model and then use it on a new data. In your second approach - you cannot treat it as a mimicry of a training process because e.g. in second fold your model would have information kept from the first fold - which is not equivalent to your training procedure. Of course - you could apply a training procedure which uses 10 folds of consecutive training in order to finetune network. But this is not cross-validation then - you'll need to evaluate this procedure using some kind of schema above. The commented out functions make this a little less obvious, but the idea is to keep track of your model performance as you iterate through your folds and at the end provide either those lower level performance metrics or an averaged global performance. For example: The train_evaluate function ideally would output some accuracy score for each split, which could be combined at the end. So yes you do want to create a new model for each fold as the purpose of this exercise is to determine how your model as it is designed performs on all segments of the data, not just one particular segment that may or may not allow the model to perform well. This type of approach becomes particularly powerful when applied along with a grid search over hyperparameters. In this approach you train a model with varying hyperparameters using the cross validation splits and keep track of the performance on splits and overall. In the end you will be able to get a much better idea of which hyperparameters allow the model to perform best. For a much more in depth explanation see sklearn Model Selection and pay particular attention to the sections of Cross Validation and Grid Search.I am trying to run the following code for a brief machine learning algorithm: In this piece of code, I am trying to read the dataset 'MNIST Original' present at mldata.org via sklearn. This results in the following error(there are more lines of code but I am getting error at this particular line): I have tried researching on internet but there is hardly any help available. Any expert help related to solving this error will be much appreciated. TIA. As of version 0.20, sklearn deprecates fetch_mldata function and adds fetch_openml instead. Download MNIST dataset with the following code: There are some changes to the format though. For instance, mnist['target'] is an array of string category labels (not floats as before). Looks like the cached data are corrupted. Try removing them and download again (it takes a moment). If not specified differently the data for 'MINST original' should be in I downloaded the dataset from this link https://github.com/amplab/datascience-sp14/blob/master/lab7/mldata/mnist-original.mat then I typed these lines *** the path is (your working directory)/files/mldata/mnist-original.mat I hope you get it , it worked well for me Here is some sample code how to get MNIST data ready to use for sklearn: I experienced the same issue and found different file size of mnist-original.mat at different times while I use my poor WiFi. I switched to LAN and it works fine. It maybe the issue of networking. Try it like this: This worked for me. Since you used the from ... import ... syntax, you shouldn't prepend datasets when you use it I was also getting a fetch_mldata() "IOError: could not read bytes" error. Here is the solution; the relevant lines of code are ... be sure to change 'data_home' for your preferred location (directory). Here is a script: If you didn't give the data_home, program look the ${yourprojectpath}/mldata/minist-original.mat  you can download the program and put the file the correct path I also had this problem in the past. It is due to the dataset is quite large (about 55.4 mb), I run the "fetch_mldata" but because of the internet connection, it took awhile to download them all. I did not know and interrupt the process. The dataset is corrupted and that why the error happened. Apart from what @szymon has mentioned you can alternatively load dataset using: That's 'MNIST original'. With a lowercase on "o".I'm training a YOLO model, I have the bounding boxes in this format:- I need to convert it to YOLO format to be something like:- I already calculated the center point X, Y, the height H, and the weight W.
But still need a away to convert them to floating numbers as mentioned. for those looking for the reverse of the question (yolo format to normal bbox format) Here's code snipet in python to convert x,y coordinates to yolo format Check my sample program to convert from LabelMe annotation tool format to Yolo format https://github.com/ivder/LabelMeYoloConverter There is a more straight-forward way to do those stuff with pybboxes. Install with, use it as below, Note that, converting to YOLO format requires the image width and height for scaling. YOLO normalises the image space to run from 0 to 1 in both x and y directions. To convert between your (x, y) coordinates and yolo (u, v) coordinates you need to transform your data as u = x / XMAX and  y = y / YMAX where XMAX, YMAX are the maximum coordinates for the image array you are using. This all depends on the image arrays being oriented the same way. Here is a C function to perform the conversion There are two potential solutions. First of all you have to understand if your first bounding box is in the format of Coco or Pascal_VOC. Otherwise you can't do the right math. Here is the formatting; Coco Format: [x_min, y_min, width, height]
Pascal_VOC Format: [x_min, y_min, x_max, y_max] Here are some Python Code how you can do the conversion: Converting Coco to Yolo Converting Pascal_voc to Yolo If need additional conversions you can check my article at Medium: https://christianbernecker.medium.com/convert-bounding-boxes-from-coco-to-pascal-voc-to-yolo-and-back-660dc6178742 For yolo format to x1,y1, x2,y2 format There are two things you need to do: If you're using PyTorch, Torchvision provides a function that you can use for the conversion: Just reading the answers I am also looking for this but find this more informative to know what happening at the backend.
Form Here: Source Assuming x/ymin and x/ymax are your bounding corners, top left and bottom right respectively. Then: You then need to normalize these, which means give them as a proportion of the whole image, so simple divide each value by its respective size from the values above: This assumes a top-left origin, you will have to apply a shift factor if this is not the case. So the answerI made a simple module that should figure out the relationship between input and output numbers, in this case, x and x squared. The code in Python: I tried a different number of units, and adding more layers, and even using the relu activation function, but the results were always wrong.
It works with other relationships like x and 2x. What is the problem here? You are making two very basic mistakes: It is certainly understood that neural networks need to be of some complexity if they are to solve problems even as "simple" as x*x; and where they really shine is when fed with large training datasets. The methodology when trying to solve such function approximations is not to just list the (few possible) inputs and then fed to the model, along with the desired outputs; remember, NNs learn through examples, and not through symbolic reasoning. And the more examples the better. What we usually do in similar cases is to generate a large number of examples, which we subsequently feed to the model for training. Having said that, here is a rather simple demonstration of a 3-layer neural network in Keras for approximating the function x*x, using as input 10,000 random numbers generated in [-50, 50]: Well, not that bad! Remember that NNs are function approximators: we should expect them neither to exactly reproduce the functional relationship nor to "know" that the results for 4 and -4 should be identical. Let's generate some new random data in [-50,50] (remember, for all practical purposes, these are unseen data for the model) and plot them, along with the original ones, to get a more general picture: Result:  Well, it arguably does look like a good approximation indeed... You could also take a look at this thread for a sine approximation. The last thing to keep in mind is that, although we did get a decent approximation even with our relatively simple model, what we should not expect is extrapolation, i.e. good performance outside [-50, 50]; for details, see my answer in Is deep learning bad at fitting simple non linear functions outside training scope? The problem is that x*x is a very different beast than a*x. Please note what a usual "neural network" does: it stacks y = f(W*x + b) a few times, never multiplying x with itself. Therefore, you'll never get perfect reconstruction of x*x. Unless you set f(x) = x*x or similar. What you can get is an approximation in the range of values presented during training (and perhaps a very little bit of extrapolation). Anyway, I'd recommend you to work with a smaller range of values, it will be easier to optimize the problem. And on a philosophical note: In machine learning, I find it more useful to think of good/bad, rather than correct/wrong. Especially with regression, you cannot get the result "right" unless you have the exact model. In which case there is nothing to learn. There actually are some NN architectures multiplying f(x) with g(x), most notably LSTMs and Highway networks. But even these have one or both of f(x), g(s) bounded (by logistic sigmoid or tanh), thus are unable to model x*x fully. Since there is some misunderstanding expressed in comments, let me emphasize a few points: As an example, here is a result from a model with a single hidden layer of 10 units with tanh activation, trained by SGD with learning rate 1e-3 for 15k iterations to minimize the MSE of your data. Best of five runs:  Here is the full code to reproduce the result. Unfortunately, I cannot install Keras/TF in my current environment, but I hope that the PyTorch code is accessible :-) My answer is a bit different. For the trivial case x*x, you can just write your own activation function that takes in x and outputs x*x. This answers the question above, "how to build a NN that calcuates x*x?". But this may violate the "spirit" of the question.

I mention this because sometimes you want to perform a non-trivial operation like
(x --> exp[A * x*x] * sinh[ 1/sqrt( log(k * x)) ] ).\
You could write an activation function for this, but the back propagation operation would be hellish and impenetrable to another developer.

AND suppose you also want the function
(x --> exp[A * x*x] * cosh[ 1/sqrt( log(k * x) ) ]).
Writing another stand-alone activation function would just be wasteful.

For this reason, you might want to build a library of activation functions with atomic operations like, z*z, exp(z), sinh(z), cosh(z), sqrt(z), log(z). These activation functions would be applied one at a time with the help of auxiliary network layers consisting of passthrough (i.e. no-op) nodes.I have a spark dataframe 'mydataframe' with many columns. I am trying to run kmeans on only two columns: lat and long (latitude & longitude) using them as simple values). I want to extract 7 clusters based on just those 2 columns and then I want to attach the cluster asignment to my original dataframe. I've tried: But I am getting an error after a while:  org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5191.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5191.0 (TID 260738, 10.19.211.69, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last) I've tried to detach and re-attach the cluster. Same result. What am I doing wrong? Since, based on another recent question of yours, I guess you are in your very first steps with Spark clustering (you are even importing sqrt & array, without ever using them, probably because it is like that in the docs example), let me offer advice in a more general level rather than in the specific question you are asking here (hopefully also saving you from subsequently opening 3-4 more questions, trying to get your cluster assignments back into your dataframe)... Since  you have your data already in a dataframe  you want to attach the cluster membership back into your initial
dataframe you have no reason to revert to an RDD and use the (soon to be deprecated) MLlib package; you will do your job much more easily, elegantly, and efficiently using the (now recommended) ML package, which works directly with dataframes. Step 0 - make some toy data resembling yours: Step 1 - assemble your features In contrast to most ML packages out there, Spark ML requires your input features to be gathered in a single column of your dataframe, usually named features; and it provides a specific method for doing this, VectorAssembler: As perhaps already guessed, the argument inputCols serves to tell VectoeAssembler which particular columns in our dataframe are to be used as features.  Step 2 - fit your KMeans model select('features') here serves to tell the algorithm which column of the dataframe to use for clustering - remember that, after Step 1 above, your original lat & long features are no more directly used. Step 3 - transform your initial dataframe to include cluster assignments The last column of the transformed dataframe, prediction, shows the cluster assignment - in my toy case, I have ended up with 4 records in cluster #0 and 1 record in cluster #1. You can further manipulate the transformed dataframe with select statements, or even drop the features column (which has now fulfilled its function and may be no longer necessary)... Hopefully you are much closer now to what you actually wanted to achieve in the first place. For extracting cluster statistics etc., another recent answer of mine might be helpful... Despite my other general answer, and in case you, for whatever reason, must stick with MLlib & RDDs, here is what causes your error using the same toy df. When you select columns from a dataframe to convert to RDD, as you do, the result is an RDD of Rows: which is not suitable as an input to MLlib KMeans. You'll need a map operation for this to work: So, your code should be like this:I have the following code running inside a Jupyter notebook: The code collects epochs history, then displays the progress history.  Q: How can I make the chart change while training so I can see the changes in real time? There is livelossplot Python package for live training loss plots in Jupyter Notebook for Keras (disclaimer: I am the author). To see how does it work, look at its source, especially this file: https://github.com/stared/livelossplot/blob/master/livelossplot/outputs/matplotlib_plot.py (from IPython.display import clear_output and clear_output(wait=True)). A fair disclaimer: it does interfere with Keras output.  Keras comes with a callback for TensorBoard. You can easily add this behaviour to your model and then just run tensorboard on top of the logging data. And then on your shell: If you need it in your notebook, you can also write your own callback to get metrics while training: This would get the training accuracy at the end of the current epoch and print it. There's some good documentation around it on the official keras site. this gives you an idea of the simplest codes. [ Sample ]: [ Output ]: ...I am working on a medical dataset where I am trying to have as less false negatives as possible. A prediction of "disease when actually no disease" is okay for me but a prediction "no disease when actually a disease" is not. That is, I am okay with FP but not FN. After doing some research, I found out ways like Keeping higher learning rate for one class, using class weights,ensemble learning with specificity/sensitivity etc. I achieved the near desired result using class weights like class_weight = {0 : 0.3,1: 0.7} and then calling the model.fit(class_weights=class_weight). This gave me very low FN but a pretty high FP. I am trying to reduce FP as much as possible keeping FN very low. I am struggling to write a custom loss function using Keras which will help me to penalize the false negatives. Thanks for the help. I'll briefly introduce the concepts we're trying to tackle.  From all that were positive, how many did our model predict as positive? All that were positive =  What our model said were positive =   Since recall is inversely proportional to FN, improving it decreases FN. From all that were negative, how many did our model predict as negative? All that were negative =  What our model said were negative =   Since specificity is inversely proportional to FP, improving it decreases FP. In your next searches, or whatever classification-related activity you perform, knowing these is going to give you an extra edge in communication and understanding. So. These two concepts, as you mas have figured out already, are opposites. This means that increasing one is likely to decrease the other. Since you want priority on recall, but don't want to loose too much in specificity, you can combine both of those and attribute weights. Following what's clearly explained in this answer: Notice recall_weight and spec_weight? They're weights we're attributing to each of the metrics. For distribution convention, they should always add to 1.0¹, e.g. recall_weight=0.9, specificity_weight=0.1. The intention here is for you to see what proportion best suits your needs. But Keras' loss functions must only receive (y_true, y_pred) as arguments, so let's define a wrapper: And onto using it, we'd have ¹ The weights, added, must total 1.0, because in case both recall=1.0 and specificity=1.0 (the perfect score), the formula  Shall give us, for example,  Clearly, if we got the perfect score, we'd want our loss to equal 0.I am currently performing multi class SVM with linear kernel using python's scikit library. 
The sample training data and testing data are as given below: Model data: I want to plot the decision boundary and visualize the datasets. Can someone please help to plot this type of data. The data given above is just mock data so feel free to change the values.
It would be helpful if at least if you could suggest the steps that are to followed. 
Thanks in advance You have to choose only 2 features to do this. The reason is that you cannot plot a 7D plot. After selecting the 2 features use only these for the visualization of the decision surface. (I have also written an article about this here: https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=friends_link&sk=80f72ab272550d76a0cc3730d7c8af35) Now, the next question that you would ask: How can I choose these 2 features?. Well, there are a lot of ways. You could do a univariate F-value (feature ranking) test and see what features/variables are the most important. Then you could use these for the plot. Also, we could reduce the dimensionality from 7 to 2 using PCA for example. 2D plot for 2 features and using the iris dataset  EDIT: Apply PCA to reduce dimensionality.  EDIT 1 (April 15th, 2020):  You can use mlxtend. It's quite clean. First do a pip install mlxtend, and then: Where X is a two-dimensional data matrix, and y is the associated vector of training labels.I am using recursive feature elimination with cross validation (rfecv) as a feature selector for randomforest classifier as follows.  I am also performing GridSearchCV as follows to tune the hyperparameters of RandomForestClassifier as follows. However, I am not clear how to merge feature selection (rfecv) with GridSearchCV. EDIT:  When I run the answer suggested by @Gambit I got the following error:  I could resolve the above issue by using estimator__ in the param_grid parameter list. My question now is How to use the selected features and parameters in x_test to verify if the model works fine with unseen data. How can I obtain the best features and train it with the optimal hyperparameters? I am happy to provide more details if needed. Basically you want to fine tune the hyper parameter of your classifier (with Cross validation) after feature selection using recursive feature elimination (with Cross validation). Pipeline object is exactly meant for this purpose of assembling the data transformation and applying estimator. May be you could use a different model (GradientBoostingClassifier, etc. ) for your final classification. It would be possible with the following approach: Now, you can apply this pipeline (Including feature selection) for test data. You can do what you want by prefixing the names of the parameters you want to pass to the estimator with 'estimator__'. Output on fake data I made: You just need to pass the Recursive Feature Elimination Estimator directly into the GridSearchCV object. Something like this should workLet's say I have network with following params: Now, I know that the loss is calculated in the following manner: binary cross entropy is applied to each pixel in the image with regards to each class. So essentially, each pixel will have 5 loss values  What happens after this step? When I train my network, it prints only a single loss value for an epoch.
There are many levels of loss accumulation that need to happen to produce a single value and how it happens is not clear at all in the docs/code. To state it in a different way: how are the losses for different classes combined to produce a single loss value for an image? This is not explained in the docs at all and would be very helpful for people doing multi-class predictions on keras, regardless of the type of network. Here is the link to the start of keras code where one first passes in the loss function. The closest thing I could find to an explanation is  loss: String (name of objective function) or objective function. See losses. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses from keras. So does this mean that the losses for each class in the image is simply summed? Example code here for someone to try it out. Here's a basic implementation borrowed from Kaggle and modified for multi-label prediction: The actual BCE-DICE loss function can be found here. Motivation for the question: Based on the above code, the total validation loss of the network after 20 epochs is ~1%; however, the mean intersection over union scores for the first 4 classes are above 95% each, but for the last class its 23%. Clearly indicating that the 5th class isn't doing well at all. However, this loss in accuracy isn't being reflected at all in the loss. Hence, that means the individual losses for the sample are being combined in a way that completely negates the huge loss we see for the 5th class. And, so when the per sample losses are being combined over batch, it's still really low. I'm not sure how to reconcile this information.  Although I have already mentioned part of this answer in a related answer, but let's inspect the source code step-by-step with more details to find the answer concretely. First, Let's feedforward(!): there is a call to weighted_loss function which takes y_true, y_pred, sample_weight and mask as inputs: weighted_loss is actually an element of a list which contains all the (augmented) loss functions passed to fit method: The "augmented" word I mentioned is important here. That's because, as you can see above, the actual loss function is wrapped by another function called weighted_masked_objective which has been defined as follows: So, there is a nested function, weighted, that actually calls the real loss function fn in the line score_array = fn(y_true, y_pred). Now, to be concrete, in case of the example the OP provided, the fn (i.e. loss function) is binary_crossentropy. Therefore we need to take a look at the definition of binary_crossentropy() in Keras: which in turn, calls the backend function K.binary_crossentropy(). In case of using Tensorflow as the backend, the definition of K.binary_crossentropy() is as follows: The tf.nn.sigmoid_cross_entropy_with_logits returns: A Tensor of the same shape as logits with the componentwise logistic losses. Now, let's backpropagate(!): considering the above note, the output shape of K.binray_crossentropy would be the same as y_pred (or y_true). As the OP mentioned, y_true has a shape of (batch_size, img_dim, img_dim, num_classes). Therefore, the K.mean(..., axis=-1) is applied over a tensor of shape (batch_size, img_dim, img_dim, num_classes) which results in an output tensor of shape (batch_size, img_dim, img_dim). So the loss values of all classes are averaged for each pixel in the image. Hence, the shape of score_array in weighted function mentioned above would be (batch_size, img_dim, img_dim). There is one more step: the return statement in weighted function takes the mean again i.e. return K.mean(score_array). So how does it compute the mean? If you take a look at the definition of mean backend function you would find out that the axis argument is None by default: And it calls the tf.reduce_mean() which given an axis=None argument, takes the mean over all the axes of input tensor and return one single value. Therefore, the mean of the whole tensor of shape (batch_size, img_dim, img_dim) is computed, which translates to taking the average over all the labels in the batch and over all their pixels, and is returned as one single scalar value which represents the loss value. Then, this loss value is reported back by Keras and is used for optimization. Bonus: what if our model has multiple output layers and therefore multiple loss functions are used? Remember the first piece of code I mentioned in this answer: As you can see there is an i variable which is used for indexing the array. You may have guessed correctly: it is actually part of a loop which computes the loss value for each output layer using its designated loss function and then takes the (weighted) sum of all these loss values to compute the total loss: then all the pixels in the image or (2)all the pixels in the image for
  each individual class, then all the class losses are combined?
      2) How exactly are these different pixel combinations happening - where is it being summed / where is it being averaged? My answer for (1):
When training a batch of images, an array consisting of pixel values is trained by calculating the non-linear function, loss and optimizing (updating the weights). The loss is not calculated for each pixel value; rather, it is done for each image.  The pixel values (X_train), weights and bias (b) are used in a sigmoid (for the simplest example of non-linearity) to calculate the predicted y value. This, along with the y_train (a batch at a time) is used to calculate the loss, which is optimized using one of the optimization methods like SGD, momentum, Adam, etc to update the weights and biases. My answer for (2):
During the non-linearity operation, the pixel values (X_train) are combined with the weights (through a dot product) and added to bias to form a predicted target value.  In a batch, there may be training examples belonging to different classes. The corresponding target values (for each class) are compared with the corresponding predicted values to compute the loss. These are Therefore, it is perfectly fine to sum all the losses.  It really doesn't matter if they belong to one class or multiple classes as long as you compare it with a corresponding target of the correct class. Make sense?I need to build a classifier for text, and now I'm using TfidfVectorizer and SelectKBest to selection the features, as following: I want to print out selected features name(text) after select k best features, is there any way to do that? I just need to print out selected feature names, maybe I should use CountVectorizer instead? The following should work: To expand on @ogrisel's answer, the returned list of features is in the same order when they've been vectorized. The code below will give you a list of top ranked features sorted according to their Chi-2 scores in descending order (along with the corresponding p-values):I need to fit multivariate gaussian distribution i.e obtain mean vector and covariance matrix of the nearest multivariate gaussian for a given dataset of audio features in python. The audio features (MFCC coefficients) are a N X 13 matrix where N is around 4K. Can someone please outline the packages and technique to fit the gaussian for this data in python?  Use the numpy package. numpy.mean and numpy.cov will give you the Gaussian parameter estimates. Assuming that you have 13 attributes and N is the number of observations, you will need to set rowvar=0 when calling numpy.cov for your N x 13 matrix (or pass the transpose of your matrix as the function argument). If your data are in numpy array data:It is described in Mahout in Action that normalization can slightly improve the accuracy.
Can anyone explain the reason, thanks! Normalization is not always required, but it rarely hurts.  Some examples: K-means: K-means clustering is "isotropic" in all directions of space and
  therefore tends to produce more or less round (rather than elongated)
  clusters. In this situation leaving variances unequal is equivalent to
  putting more weight on variables with smaller variance. Example in Matlab:   (FYI: How can I detect if my dataset is clustered or unclustered (i.e. forming one single cluster) Distributed clustering: The comparative analysis shows that the distributed clustering results
  depend on the type of normalization procedure. Artificial neural network (inputs): If the input variables are combined linearly, as in an MLP, then it is
  rarely strictly necessary to standardize the inputs, at least in
  theory. The reason is that any rescaling of an input vector can be
  effectively undone by changing the corresponding weights and biases,
  leaving you with the exact same outputs as you had before. However,
  there are a variety of practical reasons why standardizing the inputs
  can make training faster and reduce the chances of getting stuck in
  local optima. Also, weight decay and Bayesian estimation can be done
  more conveniently with standardized inputs. Artificial neural network (inputs/outputs) Should you do any of these things to your data? The answer is, it
  depends. Standardizing either input or target variables tends to make the training
  process better behaved by improving the numerical condition (see 
  ftp://ftp.sas.com/pub/neural/illcond/illcond.html) of the optimization
  problem and ensuring that various default values involved in
  initialization and termination are appropriate. Standardizing targets
  can also affect the objective function.  Standardization of cases should be approached with caution because it
  discards information. If that information is irrelevant, then
  standardizing cases can be quite helpful. If that information is
  important, then standardizing cases can be disastrous. Interestingly, changing the measurement units may even lead one to see a very different clustering structure: Kaufman, Leonard, and Peter J. Rousseeuw.. "Finding groups in data: An introduction to cluster analysis." (2005). In some applications, changing the measurement units may even lead one
  to see a very different clustering structure. For example, the age (in
  years) and height (in centimeters) of four imaginary people are given
  in Table 3 and plotted in Figure 3. It appears that {A, B ) and { C,
  0) are two well-separated clusters. On the other hand, when height is
  expressed in feet one obtains Table 4 and Figure 4, where the obvious
  clusters are now {A, C} and { B, D}. This partition is completely
  different from the first because each subject has received another
  companion. (Figure 4 would have been flattened even more if age had
  been measured in days.) To avoid this dependence on the choice of measurement units, one has
  the option of  standardizing the data. This converts the original
  measurements to unitless variables. 
 Kaufman et al. continues with some interesting considerations (page 11): From a philosophical point of view, standardization does not really
  solve the problem. Indeed, the choice of measurement units gives rise
  to relative weights of the variables. Expressing a variable in smaller
  units will lead to a larger range for that variable, which will then
  have a large effect on the resulting structure. On the other hand, by
  standardizing one attempts to give all variables an equal weight, in
  the hope of achieving objectivity. As such, it may be used by a
  practitioner who possesses no prior knowledge. However, it may well be
  that some variables are intrinsically more important than others in a
  particular application, and then the assignment of weights should be
  based on subject-matter knowledge (see, e.g., Abrahamowicz, 1985). On
  the other hand, there have been attempts to devise clustering
  techniques that are independent of the scale of the variables
  (Friedman and Rubin, 1967). The proposal of Hardy and Rasson (1982) is
  to search for a partition that minimizes the total volume of the
  convex hulls of the clusters. In principle such a method is invariant
  with respect to linear transformations of the data, but unfortunately
  no algorithm exists for its implementation (except for an
  approximation that is restricted to two dimensions). Therefore, the
  dilemma of standardization appears unavoidable at present and the
  programs described in this book leave the choice up to the user. the reason behind it is that sometimes the measurements of the different variables are different in nature so the variance of the results is adjusted by normalizing.
for instance in an age(x) vs weight (y) comparison for a set of children, the age can go from one to 10 and the weight can go from 10 pounds to 100.
if you dont normalize the graphic will produce a two very weird long oval shapes to the right of your graph since both scales need to go form one to 100.
normalizing would give both axis a 1 to 100 scale hecnce the graphic will show more meaningful clusters. As clustering makes use of distance measure(like euclidean) while forming clusters, standardize/normalization of inputs is performed to ensure that important inputs with small magnitude don't loose their significance midway the clustering process. √(3-1)^2+(1000-900)^2 ≈ √(1000-900)^2 

Here, (3-1) contributes hardly a thing to the result and hence the input corresponding to these values is considered futile by the model. Similarly, most(not all) classifiers also make use of distance measure for classification. Hence, it's a good practice to normalize input data in these classifiers. Normalization really helps when intuitively important parameters have small values.I build a simple recommendation system for the MovieLens DB inspired by https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html. I also have problems with explicit training like here: Apache Spark ALS collaborative filtering results. They don't make sense 
Using implicit training (on both explicit and implicit data) gives me reasonable results, but explicit training doesn't. While this is ok for me by now, im curious on how to update a model. While my current solution works like  I want to have a flow like this: Therefore I must update my model, without completely recompute it. Is there any chance to do so? While the first way is good for batch processing (like generating recommendations in nightly batches) the second way would be good for nearly-live generating of recommendations. Edit: the following worked for me because I had implicit feedback ratings and was only interesting in ranking the products for a new user.
More details here You can actually get predictions for new users using the trained model (without updating it): To get predictions for a user in the model, you use its latent representation (vector u of size f (number of factors)), which is multiplied by the product latent factor matrix (matrix made of the latent representations of all products, a bunch of vectors of size f) and gives you a score for each product. For new users, the problem is that you don't have access to their latent representation (you only have the full representation of size M (number of different products), but what you can do is use a similarity function to compute a similar latent representation for this new user by multiplying it by the transpose of the product matrix. i.e. if you user latent matrix is u and your product latent matrix is v, for user i in the model, you get scores by doing: u_i * v
for a new user, you don't have a latent representation, so take the full representation full_u and do: full_u  * v^t * v
This will approximate the latent factors for the new users and should give reasonable recommendations (if the model already gives reasonable recommendations for existing users) To answer the question of training, this allows you to compute predictions for new users without having to do the heavy computation of the model which you can now do only once in a while. So you have you batch processing at night and can still make prediction for new user during the day. Note: MLLIB gives you access to the matrix u and v It seems like you want to be doing some kind of online learning. That's the notion that you're actually updating the model while receiving data. Spark MLLib has limited streaming machine learning options. There's a streaming linear regression and a streaming K-Means. Many machine learning problems work just fine with batch solutions, perhaps retraining the model every few hours or days. There are probably strategies for solving this. One option could be an ensemble model where you combine the results of your ALS with another model that helps make predictions about unseen movies. If you expect to see a lot of previously unseen movies though, collaborative filtering probably doesn't do what you want. If those new movies aren't in the model at all, there's no way for the model to know what other people who watched those liked. A better option might be to take a different strategy and try some kind of latent semantic analysis on the movies and model concepts of what a movie is (like genre, themes, etc...), that way new movies with various properties and fit into an existing model, and ratings affect how strongly those properties interact together.How to calculate Levenshtein Distance matrix of strings in Python ? Using the distance function, we can calculate distance between 2 words.
In my case, I have one list containing N number of strings.
The desired result is to calculate the distance matrix and after that,do the clustering of words. Just use the pdist version that accepts a custom metric. and for the levensthein then you can use the implementation of rosettacode as suggested by Tanu If you want a full squared matrix just use squareform on the result: Here is my code You could do something like thisI have a question about the kkmeans function in the kernlab package of R.  I am new to this package and please forgive me if I'm missing something obvious here. I would like to assign a new data point to a cluster in a set of clusters that were created using kernel k-means with the function 'kkmeans'.  With regular clustering, one would do this by calculating the Euclidian distance between the new data point and the cluster centroids, and choose the cluster with the closest centroid.  In kernel k-means, one must do this in the feature space.   Take the example used in the kkmeans description: Say that I have a new data point here, which I would like to assign to the closest cluster created above in sc. Any tips on how to do this? Your help is very appreciated.  Kernel K-means uses the Kernel function to calculate similarity of objects. In the simple k-means you loop through all centroids and select the one which minimizes the distance (under used metric) to the given data point. In case of kernel method (default kernel function in kkmeans is radial basis function), you simply loop through centroids and select the one that maximizes the kernel function value (in case of RBF) or minimizes the kernel induced distance (for any kernel). Detailed description of converting kernel to distance measure is provided here - in general distance induced by kernel K can be calculated through d^2(a,b) = K(a,a)+K(b,b)-2K(a,b), but as in case of RBF, K(x,x)=1 for all x, you can just maximize the K(a,b) instead of minimizing the whole K(a,a)+K(b,b)-2K(a,b). To get the kernel function from kkmeans object you can use kernelf function So for your example the closest centroid is c[3,]=5.032692 3.401923 1.598077 0.3115385 in the sense of used kernel function.While training model I got this warning "UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.)", after showing this warning, model starts training. What does this warning means? Is it something that will affect my training and I need to worry about? This is just a user warning that will be usually thrown when you try to fetch the inputs,targets during training. This is because a timeout is set for the queuing mechanism which will be specified inside the data_utils.py. For more details you can refer the data_utils.py file which will be inside the keras/utils folder. https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py I got the same warning when training a model in Google Colab. The problem was that I tried to fetch the data from my Google Drive that I had mounted to the Colab session. The solution was to move the data into Colab's working directory and use it from there. This can be done simply via !cp -r path/to/google_drive_data_dir/ path/to/colab_data_dir in the notebook. Note that you will have to do this each time when a new Colab session is created. This may or may not be the problem that Rahul was asking, but I think this might be helpful to others who face the issue. If you are running the training in GPU, the Warning will occur. You have to know that there are two running progress during the fit_generator running. While, they are parallel tasks. So if CPU's compute is lower than GPUs', the Warning occurs. Just set your batch_size smaller or upgrade your CPU config. make sure the path of data set you have given is correct only..this definitely helps
example:train_data_dir="/content/drive/My Drive/Colab Notebooks/dataset" I faced the same issue while training a deep neural network on my machine using keras, and it took me a while to figure it out. 
The images I was loading using the  from were of a lower resolution, say 100*100 and I was trying to convert them into 256*256, and apparently there is no inbuilt support provided for this.  As soon as I fixed the output shape of the image returned by the ImageDataGenerator, the warning vanished.  //Note: the figures 100*100 and 255*255 are just for explanation.  You can reduce the number of workers and max_queue_size to solve problems. I got this warning when I was training on the amount of data samples that was smaller than the batch size. (The training would actually seem to have started, but then get stuck before even showing the progress bar for the first epoch.) I faced the same issue, when I change my Keras version from 2.3.1 to 2.2.4, the warning is disappeared, And my CUDA and cuDNN can also work normally. If still not resolved, please additional references：https://github.com/keras-team/keras/issues/13878 Now my System information: OS : Win10 TensorFlow version: 1.15.0 Keras version: 2.2.4 Python version: 3.6 CUDA version: 10.0.130 cuDNN version: 7.6.5If I want to implement a classifier using the sklearn library. Is there a way to save the model or convert the file into a saved tensorflow file in order to convert it to tensorflow lite later? If you replicate the architecture in TensorFlow, which will be pretty easy given that scikit-learn models are usually rather simple, you can explicitly assign the parameters from the learned scikit-learn models to TensorFlow layers. Here is an example with logistic regression turned into a single dense layer: It is not always easy to replicate a scikit model in tensorflow. For instance scitik has a lot of on the fly imputation libraries which will be a bit tricky to implement in tensorflowSome months ago, I used the tf.contrib.learn.DNNRegressor API from TensorFlow, which I found very convenient to use. I didn't keep up with the development of TensorFlow the last few months. Now I have a project where I want to use a Regressor again, but with more control over the actual model as provided by DNNRegressor. As far as I can see, this is supported by the Estimator API using the model_fn parameter. But there are two Estimators in the TensorFlow API: Both provide a similar API, but are nevertheless slightly different in their usage. Why are there two different implementations and are there reasons to prefer one? Unfortunately, I can't find any differences in the TensorFlow documentation or a guide when to use which of both. Actually, working through the TensorFlow tutorials produced a lot of warnings as some of the interfaces apparently have changed (instead of the x,y parameter, the input_fn parameter et cetera). I wondered the same and cannot give a definitive answer, but I have a few educated guesses that might help you:  It seems that tf.estimator.Estimator together with a model function that returns tf.estimator.EstimatorSpec is the most current one that is used in the newer examples and the one to be used in new code. My guess now is that the tf.contrib.learn.Estimator is an early prototype that got replaced by the tf.estimator.Estimator. According to the docs everything in tf.contrib is unstable API that may change at any time and it looks like the tf.estimator module is the stable API that “evolved” from the tf.contrib.learn module. I assume that the authors just forgot to mark tf.contrib.learn.Estimator as deprecated and that it wasn't removed yet so existing code won't break. Now there is this explicit statement in the docs: Note: TensorFlow also includes a deprecated Estimator class at tf.contrib.learn.Estimator, which you should not use.
 https://www.tensorflow.org/programmers_guide/estimators  For some reason it's not marked Deprecated in code. To add to Christoph's answer. The distinction between these packages has been specifically mentioned at Tensorflow Dev Summit 2017 by Martin Wicke: The distinction between core and contrib is really in core things
  don't change. Things are backward compatible until release 2.0, and nobody's thinking about that right now. If you have something in core, it's stable, you should use it. If you have something in contrib, the API may change and depending on your needs
  you may or may not want to use it. So you can think of tf.contrib package as "experimental" or "early preview". For classes that are already in both tf.estimator and tf.contrib, you should definitely use tf.estimator version, because tf.contrib class gets deprecated automatically (even if it's not stated explicitly in the documentation) and can be dropped in the next release. As of tensorflow 1.4 the list of "graduated" classes includes: Estimator
DNNClassifier, DNNRegressor, LinearClassifier, LinearRegressor, DNNLinearCombinedClassifier, DNNLinearCombinedRegressor. These should be ported to tf.estimator. I had the same question about to ask. I guess tf.estimator.Estimator is high level interface and recommended usage while tf.contrib.learn.Estimator is so said not high level interface (but it is indeed).  As Christoph mentioned, tf.contrib is unstable so tf.contrib.learn.Estimator is vulnerable to changes. It is changed from 0.x versions to 1.1 version and changed again in 2016.12.  The problem is, the usage of them seems different. You can use tf.contrib.learn.SKCompat to wrap tf.contrib.learn.Estimator while for tf.estimator.Estimator, you can't do the same thing. And model_fn requirement/parameter is different if you check error messages. The conclusion is that this two Estimator is different thing! Anyway, I think tf doc did very bad on this topic since tf.estimator is in their tutorial page which means they are very serious about this...I have a series like: I'm using scikit's LabelEncoder to convert it to numerical values to be fed into the RandomForestClassifier. During the training, I'm doing as follows: But, now for testing/prediction, when I pass in new data, I want to transform the 'ID' from this data based on le_id i.e., if same values are present then transform it according to the above label encoder, otherwise assign a new numerical value. In the test file, I was doing as follows: But, I'm getting the following error: ValueError: y contains new labels How do I fix this?? Thanks! UPDATE: So the task I have is to use the below (for example) as training data and predict the 'High', 'Mod', 'Low' values for new BankNum, ID combinations. The model should learn the characteristics where a 'High' is given, where a 'Low' is given from the training dataset. For example, below a 'High' is given when there are multiple entries with same BankNum and different IDs. And then predict it on something like: I'm doing something like this: I think the error message is very clear: Your test dataset contains ID labels which have not been included in your training data set. For this items, the LabelEncoder can not find a suitable numeric value to represent. There are a few ways to solve this problem. You can either try to balance your data set, so that you are sure that each label is not only present in your test but also in your training data. Otherwise, you can try to follow one of the ideas presented here.  One of the possibles solutions is, that you search through your data set at the beginning, get a list of all unique ID values, train the LabelEncoder on this list, and keep the rest of your code just as it is at the moment. An other possible solution is, to check that the test data have only labels which have been seen in the training process. If there is a new label, you have to set it to some fallback value like unknown_id (or something like this). Doin this, you put all new, unknown IDs in one class; for this items the prediction will then fail, but you can use the rest of your code as it is now. you can try solution from "sklearn.LabelEncoder with never seen before values" https://stackoverflow.com/a/48169252/9043549
The thing is to create dictionary with classes, than map column and fill new classes with some "known value" If your data are pd.DataFrame I suggest you this simple solution... I build a custom transformer that integer maps each categorical feature. When fitted you can transform all the data you want. You can compute also simple label encoding or onehot encoding. If new unseen categories or NaNs are present in new data: 1] For label encoding, 0 is a special token reserved for mapping these cases. 2] For onehot encoding, all the onehot columns are zeros in these cases. Usage: This is in fact a known bug on LabelEncoder : BUG for fit_transform ... basically you have to fit it and then transform. It will work fine ! A suggestion is to keep a dictionary of your encoders to each and every column so that in the inverse transform you are able to retrieve the original categorical values. I'm able to mentally process operations better when dealing in DataFrames. The approach below fits and transforms the LabelEncoder() using the training data, then uses a series of pd.merge joins to map the trained fit/transform encoder values to the test data. When there is a test data value not seen in the training data, the code defaults to the max trained encoder value + 1. I found an easy hack around this issue. Assuming X is the dataframe of features, First, we need to create a list of dicts which would have the key as the iterable starting from 0 and the corresponding value pair would be the categorical column name. We easily accomplish this using enum. cat_cols_enum = list(enumerate(X.select_dtypes(include = ['O']).columns)) Then the idea is to create a list of label encoders whose dimension is equal to the number of qualitative(categorical) columns present in the dataframe X. le = [LabelEncoder() for i in range(len(cat_cols_enum))] Next and the last part would be fitting each of the label encoders present in the list of encoders with the unique values of each of the categorical columns present in the list of dicts respectively. for i in cat_cols_enum: le[i[0]].fit(X[i[1]].value_counts().index) Now, we can transform the labels to their respective encodings using This error comes when transform function getting any new value for which LabelEncoder try to encode and because in training samples, when you are using fit_transform, that specific value did not present in the corpus. So there is a hack, whether use all the unique values with fit_transform function if you are sure that no new value will come further, or try some different encoding method which suits on the problem statement like HashingEncoder. Here is the example if no further new values will come in testing I also encountered the exact same error and was able to fix it. When I used the following code, the error occurred The reason for the error was that the true value into array was of string type
And I set boolean. i changed bool type to string Thus, the error was fixed I hope my answer be helpful I hope this helps someone as it's more recent. sklearn uses the fit_transform to perform the fit function and transform function directing on label encoding.
To solve the problem for Y label throwing error for unseen values, use: This solves it! I used  and I was able to resolve the issue. It does fit and transform both. we dont need to worry about unknown values in the test splitSuppose there is a matrix B, where its size is a 500*1000 double(Here, 500 represents the number of observations and 1000 represents the number of features). sigma is the covariance matrix of B, and D is a diagonal matrix whose diagonal elements are the eigenvalues of sigma. Assume A is the eigenvectors of the covariance matrix sigma. I have the following questions: I need to select the first k = 800 eigenvectors corresponding to the eigenvalues with the largest magnitude to rank the selected features. The final matrix named Aq. How can I do this in MATLAB? What is the meaning of these selected eigenvectors? It seems the size of the final matrix Aq is 1000*800 double once I calculate Aq. The time points/observation information of 500 has disappeared. For the final matrix Aq, what does the value 1000 in matrix Aq represent now? Also, what does the value 800 in matrix Aq represent now? I'm assuming you determined the eigenvectors from the eig function.  What I would recommend to you in the future is to use the eigs function.  This not only computes the eigenvalues and eigenvectors for you, but it will compute the k largest eigenvalues with their associated eigenvectors for you.  This may save computational overhead where you don't have to compute all of the eigenvalues and associated eigenvectors of your matrix as you only want a subset.  You simply supply the covariance matrix of your data to eigs and it returns the k largest eigenvalues and eigenvectors for you. Now, back to your problem, what you are describing is ultimately Principal Component Analysis.  The mechanics behind this would be to compute the covariance matrix of your data and find the eigenvalues and eigenvectors of the computed result.  It has been known that doing it this way is not recommended due to numerical instability with computing the eigenvalues and eigenvectors for large matrices.  The most canonical way to do this now is via Singular Value Decomposition.  Concretely, the columns of the V matrix give you the eigenvectors of the covariance matrix, or the principal components, and the associated eigenvalues are the square root of the singular values produced in the diagonals of the matrix S. See this informative post on Cross Validated as to why this is preferred: https://stats.stackexchange.com/questions/79043/why-pca-of-data-by-means-of-svd-of-the-data I'll throw in another link as well that talks about the theory behind why the Singular Value Decomposition is used in Principal Component Analysis: https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca Now let's answer your question one at a time. MATLAB generates the eigenvalues and the corresponding ordering of the eigenvectors in such a way where they are unsorted.  If you wish to select out the largest k eigenvalues and associated eigenvectors given the output of eig (800 in your example), you'll need to sort the eigenvalues in descending order, then rearrange the columns of the eigenvector matrix produced from eig then select out the first k values. I should also note that using eigs will not guarantee sorted order, so you will have to explicitly sort these too when it comes down to it. In MATLAB, doing what we described above would look something like this: It's a good thing to note that you do the sorting on the absolute value of the eigenvalues because scaled eigenvalues are also eigenvalues themselves.  These scales also include negatives.  This means that if we had a component whose eigenvalue was, say -10000, this is a very good indication that this component has some significant meaning to your data, and if we sorted purely on the numbers themselves, this gets placed near the lower ranks.   The first line of code finds the covariance matrix of B, even though you said it's already stored in sigma, but let's make this reproducible.  Next, we find the eigenvalues of your covariance matrix and the associated eigenvectors.  Take note that each column of the eigenvector matrix A represents one eigenvector.  Specifically, the ith column / eigenvector of A corresponds to the ith eigenvalue seen in D.   However, the eigenvalues are in a diagonal matrix, so we extract out the diagonals with the diag command, sort them and figure out their ordering, then rearrange A to respect this ordering.  I use the second output of sort because it tells you the position of where each value in the unsorted result would appear in the sorted result.  This is the ordering we need to rearrange the columns of the eigenvector matrix A.  It's imperative that you choose 'descend' as the flag so that the largest eigenvalue and associated eigenvector appear first, just like we talked about before. You can then pluck out the first k largest vectors and values via: It's a well known fact that the eigenvectors of the covariance matrix are equal to the principal components.  Concretely, the first principal component (i.e. the largest eigenvector and associated largest eigenvalue) gives you the direction of the maximum variability in your data.  Each principal component after that gives you variability of a decreasing nature.  It's also good to note that each principal component is orthogonal to each other. Here's a good example from Wikipedia for two dimensional data:  I pulled the above image from the Wikipedia article on Principal Component Analysis, which I linked you to above. This is a scatter plot of samples that are distributed according to a bivariate Gaussian distribution centred at (1,3) with a standard deviation of 3 in roughly the (0.878, 0.478) direction and of 1 in the orthogonal direction.  The component with a standard deviation of 3 is the first principal component while the one that is orthogonal is the second component.  The vectors shown are the eigenvectors of the covariance matrix scaled by the square root of the corresponding eigenvalue, and shifted so their tails are at the mean. Now let's get back to your question.  The reason why we take a look at the k largest eigenvalues is a way of performing dimensionality reduction.  Essentially, you would be performing a data compression where you would take your higher dimensional data and project them onto a lower dimensional space.  The more principal components you include in your projection, the more it will resemble the original data.  It actually begins to taper off at a certain point, but the first few principal components allow you to faithfully reconstruct your data for the most part. A great visual example of performing PCA (or SVD rather) and data reconstruction is found by this great Quora post I stumbled upon in the past. http://qr.ae/RAEU8a You would use this matrix to reproject your higher dimensional data onto a lower dimensional space.  The number of rows being 1000 is still there, which means that there were originally 1000 features in your dataset.  The 800 is what the reduced dimensionality of your data would be.  Consider this matrix as a transformation from the original dimensionality of a feature (1000) down to its reduced dimensionality (800).   You would then use this matrix in conjunction with reconstructing what the original data was.  Concretely, this would give you an approximation of what the original data looked like with the least amount of error.  In this case, you don't need to use all of the principal components (i.e. just the k largest vectors) and you can create an approximation of your data with less information than what you had before. How you reconstruct your data is very simple. Let's talk about the forward and reverse operations first with the full data.  The forward operation is to take your original data and reproject it but instead of the lower dimensionality, we will use all of the components.  You first need to have your original data but mean subtracted: Bm will produce a matrix where each feature of every sample is mean subtracted. bsxfun allows the subtraction of two matrices in unequal dimension provided that you can broadcast the dimensions so that they can both match up.  Specifically, what will happen in this case is that the mean of each column / feature of B will be computed and a temporary replicated matrix will be produced that is as large as B.  When you subtract your original data with this replicated matrix, the effect will subtract every data point with their respective feature means, thus decentralizing your data so that the mean of each feature is 0. Once you do this, the operation to project is simply: The above operation is quite simple.  What you are doing is expressing each sample's feature as a linear combination of principal components.  For example, given the first sample or first row of the decentralized data, the first sample's feature in the projected domain is a dot product of the row vector that pertains to the entire sample and the first principal component which is a column vector..  The first sample's second feature in the projected domain is a weighted sum of the entire sample and the second component.  You would repeat this for all samples and all principal components.  In effect, you are reprojecting the data so that it is with respect to the principal components - which are orthogonal basis vectors that transform your data from one representation to another. A better description of what I just talked about can be found here.  Look at Amro's answer:  Matlab Principal Component Analysis (eigenvalues order) Now to go backwards, you simply do the inverse operation, but a special property with the eigenvector matrix is that if you transpose this, you get the inverse.  To get the original data back, you undo the operation above and add the means back to the problem: You want to get the original data back, so you're solving for Bm with respect to the previous operation that I did.  However, the inverse of Asort is just the transpose here.  What's happening after you perform this operation is that you are getting the original data back, but the data is still decentralized.  To get the original data back, you must add the means of each feature back into the data matrix to get the final result.  That's why we're using another bsxfun call here so that you can do this for each sample's feature values. You should be able to go back and forth from the original domain and projected domain with the above two lines of code.  Now where the dimensionality reduction (or the approximation of the original data) comes into play is the reverse operation.  What you need to do first is project the data onto the bases of the principal components (i.e. the forward operation), but now to go back to the original domain where we are trying to reconstruct the data with a reduced number of principal components, you simply replace Asort in the above code with Aq and also reduce the amount of features you're using in Bproject.  Concretely: Doing Bproject(:,1:k) selects out the k features in the projected domain of your data, corresponding to the k largest eigenvectors.  Interestingly, if you just want the representation of the data with regards to a reduced dimensionality, you can just use Bproject(:,1:k) and that'll be enough.  However, if you want to go forward and compute an approximation of the original data, we need to compute the reverse step.  The above code is simply what we had before with the full dimensionality of your data, but we use Aq as well as selecting out the k features in Bproject.  This will give you the original data that is represented by the k largest eigenvectors / eigenvalues in your matrix. If you'd like to see an awesome example, I'll mimic the Quora post that I linked to you but using another image.  Consider doing this with a grayscale image where each row is a "sample" and each column is a feature.  Let's take the cameraman image that's part of the image processing toolbox: We get this image:  This is a 256 x 256 image, which means that we have 256 data points and each point has 256 features.  What I'm going to do is convert the image to double for precision in computing the covariance matrix.  Now what I'm going to do is repeat the above code, but incrementally increasing k at each go from 3, 11, 15, 25, 45, 65 and 125.  Therefore, for each k, we are introducing more principal components and we should slowly start to get a reconstruction of our data. Here's some runnable code that illustrates my point: As you can see, the majority of the code is the same from what we have seen.  What's different is that I loop over all values of k, project back onto the original space (i.e. computing the approximation) with the k highest eigenvectors, then show the image. We get this nice figure:  As you can see, starting with k=3 doesn't really do us any favours... we can see some general structure, but it wouldn't hurt to add more in.  As we start increasing the number of components, we start to get a clearer picture of what the original data looks like.  At k=25, we actually can see what the cameraman looks like perfectly, and we don't need components 26 and beyond to see what's happening.  This is what I was talking about with regards to data compression where you don't need to work on all of the principal components to get a clear picture of what's going on. I'd like to end this note by referring you to Chris Taylor's wonderful exposition on the topic of Principal Components Analysis, with code, graphs and a great explanation to boot!  This is where I got started on PCA, but the Quora post is what solidified my knowledge. Matlab - PCA analysis and reconstruction of multi dimensional dataI have a DataFrame in pandas that contain training examples, for example: which I generated using: As you can see, the training set is imbalanced (8 samples have class 0, while only 2 samples have class 1). I would like to oversample the training set. Specifically, I would like to duplicating training samples with class 1 so that the training set is balanced (i.e., where the number of samples with class 0 is approximately the same as the number of samples with class 1). How can I do so?  Ideally I would like a solution that may generalize to a multiclass setting (i.e., the integer in the class column may be more than 1). You can find the maximum size a group has with  In your example, this equals 8. For each group, you can sample with replacement max_size - len(group_size) elements. This way if you concat these to the original DataFrame, their sizes will be the same and you'll keep the original rows. You can play with max_size-len(group) and maybe add some noise to it because this will make all group sizes equal.I saw the help in Matlab, but they have provided an example without explaining how to use the parameters in the 'classregtree' function. Any help to explain the use of 'classregtree' with its parameters will be appreciated. The documentation page of the function classregtree is self-explanatory... Lets go over some of the most common parameters of the classification tree model: A complete example to illustrate the process:  The above classregtree class was made obsolete, and is superseded by ClassificationTree and RegressionTree classes in R2011a (see the fitctree and fitrtree functions, new in R2014a). Here is the updated example, using the new functions/classes:I know that LIBSVM only allows one-vs-one classification when it comes to multi-class SVM. However, I would like to tweak it a bit to perform one-against-all classification. I have tried to perform one-against-all below. Is this the correct approach? The code: I might have done some mistakes. I would like to hear some feedback. Thanks. Second Part: 
As grapeot said : 
I need to do Sum-pooling (or voting as a simplified solution) to come up with the final answer. I am not sure how to do it. I need some help on it; I saw the python file but still not very sure. I need some help. From the code I can see you are trying to first turn the labels into "some class" vs "not this class", and then invoke LibSVM to do training and testing. Some questions and suggestions: Instead of probability estimates, you can also use the decision values as follows to achieve the same purpose.I just finished implementing a kd-tree for doing fast nearest neighbor searches. I'm interested in playing around with different distance metrics other than the Euclidean distance. My understanding of the kd-tree is that the speedy kd-tree search is not guaranteed to give exact searches if the metric is non-Euclidean, which means that I might need to implement a new data structure and search algorithm if I want to try out new metrics for my search. I have two questions: The nearest-neighbour search procedure described on the Wikipedia page you linked to can certainly be generalised to other distance metrics, provided you replace "hypersphere" with the equivalent geometrical object for the given metric, and test each hyperplane for crossings with this object. Example: if you are using the Manhattan distance instead (i.e. the sum of the absolute values of all differences in vector components), your hypersphere would become a (multidimensional) diamond.  (This is easiest to visualise in 2D -- if your current nearest neighbour is at distance x from the query point p, then any closer neighbour behind a different hyperplane must intersect a diamond shape that has width and height 2x and is centred on p). This might make the hyperplane-crossing test more difficult to code or slower to run, however the general principle still applies. I don't think you're tied to euclidean distance - as j_random_hacker says, you can probably use Manhattan distance - but I'm pretty sure you're tied to geometries that can be represented in cartesian coordinates. So you couldn't use a kd-tree to index a metric space, for example.More specifically I have a simple fprop that is a composition of tf operations.
I want to override the tensorflow gradient computation with my own gradient method using RegisterGradient. What's wrong with this code? I want to see all zeros in the print, but instead I am getting: You need to define the op within the scope of with g.gradient_override_map({'Myop': 'MyopGrad'}) Also, you need to map Identity rather than the name Myop to your new gradient. Here is the full code: Output: If you want to use tf.RegisterGradient() for this purpose, I'm not sure if it is a proper solution. Because in the official documents https://www.tensorflow.org/api_docs/python/tf/RegisterGradient , it says: This decorator is only used when defining a new op type. which means you need to define a new op written in C++ or wrapped  in py_func. I'm not totally sure if it can apply on the group of "tf op" you said.  However, You can also refer to the "trick" methods mentioned in this thread: How Can I Define Only the Gradient for a Tensorflow Subgraph? where you could combine tf.stop_gradient() and tfgradient_override_map() together to re-define the gradients for groups of operations See this answer (note that different questions might be satisfactorily answered by the same answer).Can anyone help me in this.? I am getting below error. I use Google Colab. How to Solve this error.? size mismatch, m1: [64 x 100], m2: [784 x 128] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:2070 Below Code I am trying to Run. All you have to care is b=c and you are done: m1 is [a x b] which is [batch size x in features] m2 is [c x d] which is [in features x out features] You have a size mismatch!
Your first layer of model expects a 784-dim input (I assume you got this value by 28x28=784, the size of MNIST digits).
However, your trainset applies transforms.CenterCrop(10) - that is it crops a 10x10 region from the center of the image, and thus your input dimension is actually 100. To summarize:For Weka Explorer (GUI), when we do a 10-fold CV for any given ARFF file, then what Weka Explorer provides (as far as I can see) is the average result for all the 10 folds. Q. Is there any way to get the results of each fold? For instance, I need the error rates (incorrectly identified instances) for each fold.  Help appreciated. I think this is possible using Weka's GUI. You need to use the Experimenter though instead of the Explorer. Here are the steps: Weka Explorer does not have an option to give the results for individual folds when using the crossvalidation option, there are some workarounds. If you explicitly don't want to change any code, you need to do some manual fiddling, but I think this gives more or less what you want This is not exactly equivalent to 10-fold crossvalidation though, since the pseudo-folds you make this way might overlap. An alternative that is equivalent to crossvalidation, but more cumbersome, would be to make 10 folds manually by using the unsupervised instance filter RemoveFolds or RemoveRange.
Generate and save 10 training sets and 10 test sets. Then for every fold, load the training set, select Supplied test set in the classify tab, and select the appropriate test fold.Given two simple sets of data:   I want to fit a linear regression line on the training data, and use that line (or the coefficients) to calculate the "test MSE" or Mean Squared Error of the Residuals on the test data once that line is fit there. In this case, it is more precise to call it MSPE (mean squared prediction error): This is a more useful measure as all models aim at prediction. We want a model with minimal MSPE. In practice, if we do have a spare test data set, we can directly compute MSPE as above. However, very often we don't have spare data. In statistics, the leave-one-out cross-validation is an estimate of MSPE from the training dataset. There are also several other statistics for assessing prediction error, like Mallows's statistic and AIC.Please add a minimum comment on your thoughts so that I can improve my query. Thank you. -) I'm trying to train a tf.keras model with Gradient Accumulation (GA). But I don't want to use it in the custom training loop (like) but customize the .fit() method by overriding the train_step.Is it possible? How to accomplish this?  The reason is if we want to get the benefit of keras built-in functionality like fit, callbacks, we don't want to use the custom training loop but at the same time if we want to override train_step for some reason (like GA or else) we can customize the fit method and still get the leverage of using those built-in functions. And also, I know the pros of using GA but what are the major cons of using it? Why does it's not come as a default but an optional feature with the framework? Please, run and check with the following toy setup. I've found that some others also tried to achieve this and ended up with the same issue. One has got some workaround, here, but it's too messy and I think there should be some better approach. The accepted answer (by Mr.For Example) is fine and works well in single strategy. Now, I like to start 2nd bounty to extend it to support multi-gpu, tpu, and with mixed-precision techniques. There are some complications, see details. Yes it is possible to customize the .fit() method by overriding the train_step without a custom training loop, following simple example will show you how to train a simple mnist classifier with gradient accumulation: Outputs: Gradient accumulation is a mechanism to split the batch of samples —
used for training a neural network — into several mini-batches of
samples that will be run sequentially  Because GA calculates the loss and gradients after each mini-batch, but instead of updating the model parameters, it waits and accumulates the gradients over consecutive batches, so it can overcoming memory constraints, i.e using less memory to training the model like it using large batch size. Example: If you run a gradient accumulation with steps of 5 and batch
size of 4 images, it serves almost the same purpose of running with a
batch size of 20 images. We could also parallel the training when using GA, i.e aggregate gradients from multiple machines. This technique is working so well so it is widely used, there few things to consider before using it that I don't think it should be called cons, after all, all GA does is turning 4 + 4 to 2 + 2 + 2 + 2. If your machine has sufficient memory for the batch size that already large enough then there no need to use it, because it is well known that too large of a batch size will lead to poor generalization, and it will certainly run slower if you using GA to achieve the same batch size that your machine's memory already can handle. Reference: What is Gradient Accumulation in Deep Learning? Thanks to @Mr.For Example for his convenient answer. Usually, I also observed that using Gradient Accumulation, won't speed up training since we are doing n_gradients times forward pass and compute all the gradients. But it will speed up the convergence of our model. And I found that using the mixed_precision technique here can be really helpful here. Details here. Here is a complete gist.I've created an xgboost classifier in Python: train is a pandas dataframe with 100k rows and 50 features as columns.
target is a pandas series However, after training, when I use this classifier to predict values the entire results array is the same number. Any idea why this would be happening? Data clarification:
~50 numerical features with a numerical target I've also tried RandomForestRegressor from sklearn with the same data and it does give realistic predictions. Perhaps a legitimate bug in the xgboost implementation? This question has received several responses including on this thread as well as here and here. I was having a similar issue with both XGBoost and LGBM.  For me, the solution was to increase the size of the training dataset.   I was training on a local machine using a random sample (~0.5%) of a large sparse dataset (200,000 rows and 7000 columns) because I did not have enough local memory for the algorithm.  It turned out that for me, the array of predicted values was just an array of the average values of the target variable.  This suggests to me that the model may have been underfitting.  One solution to an underfitting model is to train your model on more data, so I tried my analysis on a machine with more memory and the issue was resolved:  my prediction array was no longer an array of average target values.  On the other hand, the issue could simply have been that the slice of predicted values I was looking at were predicted from training data with very little information (e.g. 0's and nan's).  For training data with very little information, it seems reasonable to predict the average value of the target feature. None of the other suggested solutions I came across were helpful for me.  To summarize some of the suggested solutions included:
1) check if gamma is too high
2) make sure your target labels are not included in your training dataset
3) max_depth may be too small. One of the reasons for the same is that you're providing a high penalty through parameter gamma. Compare the mean value of your training response variable and check if the prediction is close to this. If yes then the model is restricting too much on the prediction to keep train-rmse and val-rmse as close as possible. Your prediction is the simplest with higher value of gamma. So you'll get the simplest model prediction like mean of training set as prediction or naive prediction. Won't the max_depth =3 too smaller, try to get it bigger,the default value is 7 if i remember it correctly. and set silent to be 1, then you can monitor what's the error each epochs You need to post a reproducible example for any real investigation. It's entirely likely that your response target is highly unbalanced and that your training data is not super predictive, thus you always (or almost always) get one class predicted. Have you looked at the predicted probabilities at all to see if there is any variance? Is it just an issue of not using the proper cut-off for classification labels? Since you said that a RF gave reasonable predictions it would useful to see your training parameters for that. At a glance, it's curious why you're using a regression objective function in your xgboost call though -- that could easily be why you are seeing such poor performance. Trying changing your objective to: 'binary:logistic. You should check there are no inf values in your target. Try to increase (significantly) min_child_weight in XGBoost or min_data_in_leaf in LightGBM: Actually, it may be a case of overfitting masking as underfitting. It happens for instance for zero-inflated targets in case of insurance claims frequency models. One solution would be to increase the representation/coverage of rare target levels (e.g. non-zero insurance claims) in each tree leaf, by increasing the hyperparameter controlling minimum leaf size to some rather large values, such as those specified in the example above. I just had this problem and managed to fix it. The problem was I was training on tree_method='gpu_hist' which gave all the same predictions. If I set tree_method='auto' it works properly but wayy longer runtimes. So then if I set tree_method='gpu_hist' along with base_score=0 it worked. I think base_score should be about the mean of your predicted variable. I have tried all solutions on this page, but none worked. As I was grouping time series, certain frequencies created gaps in data.
I solved this issue by filling all NaN's. Probably the hyper-parameters you use cause errors. Try using default values. In my case, this problem was solved by removing subsample and min_child_weight hyper-parameters from params.I am searching for a hyperparameter tune package for code written directly in Tensorflow (not Keras or Tflearn). Could you make some suggestion? Usually you don't need to have your hyperparameter optimisation logic coupled with the optimised model (unless your hyperparemeter optimisation logic is specific to the kind of model that you are training, in which case you would need to tell us a bit more). There are several tools and packages available for the task. Here is a good paper on the topic, and here is a more practical blog post with examples. Out of these, I have only really (that is, with a real problem) used hyperopt with TensorFlow, and it didn't took too much effort. The API is a bit weird at some points and the documentation is not terribly thorough, but it does work and seems to be under active development, with more optimization algorithms and adaptations (e.g. specifically for neural networks) possibly coming. However, as suggested in the previously linked blog post, Scikit-Optimize is probably as good, and SigOpt looks quite easy to use if it fits you. I'd like to add one more library to @jdehesa's list, which I have applied in my research particularly with tensorflow. It's hyper-engine, Apache 2.0 licensed. It also implements Gaussian Process Bayesian optimization and some other techniques, like learning curve prediction, which save a lot of time. You can try out Ray Tune, a simple library for scaling hyperparameter search. I mainly use it for Tensorflow model training, but it's agnostic to the framework - works seamlessly with PyTorch, Keras, etc. Here's the docs page - ray.readthedocs.io/en/latest/tune.html You can use it to run distributed versions of state-of-the-art algorithms such as HyperBand or Bayesian Optimization in about 10 lines of code. As an example to run 4 parallel evaluations at a time: You also don't need to change your code if you want to run this script on a cluster. Disclaimer: I work on this project - let me know if you have any feedback! I'm not sure if this is also the parameters that you want but you mentioned TensorFlow hyperparameters so I guess I can suggest some.  Try to clone this repository for having the needed scripts; git clone https://github.com/googlecodelabs/tensorflow-for-poets-2 and in the Master folder, invoke your command prompt and run this line; python -m scripts.retrain -h to get the list of optional arguments. Source: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#6 I found sci-kit optimize very simple to use for bayesian optimization of hyperameters, and it works with any tensorflow API (estimator, custom estimator, core, keras, etc.) https://stackoverflow.com/a/53582472/2218905 You could use variational inference (bayesian) as a point cloud over the optimization space; hyperparameter tuning would be much better. Tensorflow probability would be an approach.I'm working in a project that isolate vocal parts from an audio. I'm using the DSD100 dataset, but for doing tests I'm using the DSD100subset dataset from I only use the mixtures and the vocals. I'm basing this work on this article First I process the audios to extract a spectrogram and put it on a list, with all the audios forming four lists (trainMixed, trainVocals, testMixed, testVocals). Like this: Next i build the model: And run the model: But I'm getting this result: I am new to this topic, thanks for the help provided in advance. It's probably an issue with specifying input data to Keras' fit() function. I would recommend using a tf.data.Dataset as input to fit() like so: You can then also use functions like shuffle() and batch() on the TF datasets. EDIT: It also seems like your input shapes are incorrect. The input_shape you specified for the first conv layer is (513, 25, 1), so the input should be a batch tensor of shape (batch_size, 513, 25, 1), whereas you're inputting the shape (batch_size, 2584). So you'll need to reshape and probably cut your inputs to the specified shape, or specify a new shape. Basically, no matter what you define the shape of Conv2D is 2D, 3D,... it requires 4D when you feeding input X to it, where X.shape is look like this (batch,row,col,channel). The below example here is the clarify about Conv2D TL;DR Now let's elaborating above codes Line 1 input_layer was defined with the shape of 3D, but at line no.4 X was reshaped to 4D shape which is not matching the shape at all. However, in order to feed any input X to input_layer or Conv2D must pass with 4D shape.Is there anyway to compute AUC within Vowpal Wabbit? One of the reasons I am using Vowpal Wabbit is the large size of the data file.
I can calculate the AUC outside of the Vowpal Wabbit environment using the output of Vowpal Wabbit but this might be problematic if the data file is large. Currently, VW cannot report AUC. What is worse, it cannot optimize directly for AUC. Optimizing for AUC is not compatible with online learning, but there are some approximations of AUC suitable for optimizing. Concerning your question, you don't need to store the intermediate file with raw predictions on disk. You can pipe it directly to the external evaluation tool (perf in this case): Edit:
John Langford confirmed that AUC can generally be optimized by changing the ratio of false positive and false negative loss. In VW, this means setting a different importance weight for positive and negative examples. You need to tune the optimal weight using a hold out set (or cross validation, or progressive validation loss for one-pass learning).I am trying to use multi-layer neural network to predict nth square. I have the following training data containing the first 99 squares This is the code: Which prints 1 for both 100th and 101st squares: What is the right way to do this? Following Filip Malczak's and Seanny123's suggestions and comments, I implemented a neural network in tensorflow to check what happens when we try to teach it to predict (and interpolate) the 2-nd square.  Training on continuous interval  I trained the network on the interval [-7,7] (taking 300 points inside this interval, to make it continuous), and then tested it on the interval [-30,30]. The activation functions are ReLu, and the network has 3 hidden layers, each one is of size 50. epochs=500. The result is depicted in the figure below. 
 So basically, inside (and also close to) the interval [-7,7], the fit is quite perfect, and then it continues more or less linearly outside. It is nice to see that at least initially, the slope of the network's output tries to "match" the slope of x^2. If we increase the test interval, the two graphs diverge quite a lot, as one can see in the figure below:  Training on even numbers Finally, if instead I train the network on the set of all even integers in the interval [-100,100], and apply it on the set of all integers (even and odd) in this interval, I get: When training the network to produce the image above, I increased the epochs to 2500 to get a better accuracy. The rest of the parameters stayed unchanged. So it seems that interpolating "inside" the training interval works quite well (maybe except of the area around 0, where the fit is a bit worse). Here is the code that I used for the first figure: Checked the docs for neurolab - newff creates NN with sigmoid transfer function in all neurons by default. Sigmoid value is always in (-1; 1) range, so your output will never leave this range.  Second square (4) is already out of this range, so your code doesn't match your problem at all.  Try using other functions (I'd propose SoftPlus or ReLU). They work quite well with feed-forward networks, allow for backpropagation training (as they are derivable in whole domain) and have values in range (0, ∞), just as you need.  Also: first param to newff defines ranges for input data - you're using [0, 99] which matches all the training data, but doesn't match values that you've tried while testing (since 100 and 101 are bigger than 99). Change this value to something way bigger, so the values you test on are not "special" (meaning "on the end of the range") - I'd propose something like [-300, 300]. Besides, as stated by Seanny123 in a comment, I don't think it's gonna work at all, but with current setup I can be sure of that. Good luck. Let me know (for example in comments) if you succeeded. Last, but not least - what you're trying to do is extrapolation (figuring out values out of some range based on values in that range). NN are better suited for interpolation (figuring out values in the range based on samples from that range), as they are supposed to generalize data used in training. Try teaching it squares of, for example, every 3rd square (so 1, 16, 49, ...) and then testing by asking for squares of the rest (for example asking for square of 2 or 8).I want to train multiple LinearSVC models with different random states but I prefer to do it in parallel. Is there an mechanism supporting this in sklearn? I know Gridsearch or some ensemble methods are doing in implicitly but what is the thing under the hood? The "thing" under the hood is the library joblib, which powers for example the multi-processing in GridSearchCV and some ensemble methods. It's Parallel helper class is a very handy Swiss knife for embarrassingly parallel for loops.  This is an example to train multiple LinearSVC models with different random states in parallel with 4 processes using joblib:I am building an image processing classifier. This line is giving me an error: input_img_resize=cv2.resize(input_img,(128,128)) The error: ('error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3483: error: (-215) ssize.width > 0 && ssize.height > 0 in function resize') My code: Well, obviously this line
input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )
returns an empty array. You should check whether the image exists first before reading. And it is better not to use string combination to join file paths, use python os.path.join instead. It is because of one image.  To find the image I added a line of code that prints the name of the image before it enters the cv2.resize and another line that prints the name after it is resized. It will automatically stop at the image with fault.  Make sure the input_img's resolution is not zero, i.e, (0,0,number_of_channels) which means it is not finding any image.
So add the following checking before performing operations on it: If you are working with several images (for example, 1000 images), it may be difficult for you to identify which image is giving you problems. It may also be that, there are several others that are corrupt. In such a case, the code below can be useful.  where filenames is a list which contains names of the images. You are facing this problem because the image might not have been read properly while scanning. So do make sure tat image is loaded.    This skips the current image and then continue the scan. This breaks into two segments results as follows: 
 Hope this helps :)  This is because the region of image is not properly identified. Here's one way you can try: This will let you to manually select the region of image in each pic if its not detected. For this error, you can simply use this code. It definitely runs and resolves your problem. I was also facing this error and fixed it with this code.Suppose I have a confusion matrix as like as below. How can I calculate precision and recall?   first, your matrix is arranged upside down. 
You want to arrange your labels so that true positives are set on the diagonal [(0,0),(1,1),(2,2)] this is the arrangement that you're going to find with confusion matrices generated from sklearn and other packages.  Once we have things sorted in the right direction, we can take a page from this answer and say that:  \  Then we take some formulas from sklearn docs for precision and recall. 
And put it all into code: Since we remove the true positives to define false_positives/negatives only to add them back... we can simplify further by skipping a couple of steps:  I don't think you need summation at last. Without summation, your method is correct; it gives precision and recall for each class.  If you intend to calculate average precision and recall, then you have two options: micro and macro-average.  Read more here http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html For the sake of completeness for future reference, given a list of grounth (gt) and prediction (pd). The following code snippet computes confusion matrix and then calculates precision and recall.   hypothetical confusion matrix (cm) precision and recall for each class using map() to calculate list division. please note: 10 classes, 10 precisions and 10 recalls. Agreeing with gruangly and EuWern, I modified PabTorre's solution accordingly to generate precision and recall per class. Also, given my use case (NER) where a model could: I wrap the array with a numpy.nan_to_num() to convert any nan to zero. This is not a mathematical decision, but a per use-case, functional decision in how to handle never-predicted, or never-occuring classes.I have a SimpleRNN like: The model summary says: I am curious about the parameter number 120 for simple_rnn_1. Could you someone answer my question? When you look at the headline of the table you see the title Param:    This number represents the number of trainable parameters (weights and biases) in the respective layer, in this case your SimpleRNN. Edit: The formula for calculating the weights is as follows: recurrent_weights + input_weights + biases *resp: (num_features + num_units)* num_units + num_units Explanation: num_units = equals the number of units in the RNN num_features = equals the number features of your input Now you have two things happening in your RNN. First you have the recurrent loop, where the state is fed recurrently into the model to generate the next step. Weights for the recurrent step are: recurrent_weights =  num_units*num_units The secondly you have new input of your sequence at each step. input_weights = num_features*num_units (Usually both last RNN state and new input are concatenated and then multiplied with one single weight matrix, nevertheless inputs and last RNN state use different weights) So now we have the weights, whats missing are the biases - for every unit one bias: biases = num_units*1 So finally we have the formula: recurrent_weights + input_weights + biases or num_units* num_units + num_features* num_units + biases  =  (num_features + num_units)* num_units + biases In your cases this means the trainable parameters are: 10*10 + 1*10 + 10 = 120 I hope this is understandable, if not just tell me - so I can edit it to make it more clear. It might be easier to understand visually with a simple network like this:  The number of weights is 16 (4 * 4) + 12 (3 * 4) = 28 and the number of biases is 4. where 4 is the number of units and 3 is the number of input dimensions, so the formula is just like in the first answer: num_units ^ 2 + num_units * input_dim + num_units or simply num_units * (num_units + input_dim + 1), which yields 10 * (10 + 1 + 1) = 120 for the parameters given in the question. I visualize the SimpleRNN you add, I think the figure can explain a lot. SimpleRNN layer, I'm a newbie here, can't post images directly, so you need to click the link. From the unrolled version of SimpleRNN layer，it can be seen as a dense layer. And the previous layer is a concatenation of input and the current layer(previous step) itself. So the number of parameters of SimpleRNN can be computed as a dense layer: num_para = units_pre * units + num_bias where: units_pre is the sum of input neurons（1 in your settings） and units(see below), units is the number of neurons（10 in your settings） in the current layer, num_bias is the number of bias term in the current layer, which is the same as the units. Plugging in your settings, we achieve the num_para = (1 + 10) * 10 + 10 = 120.Is Session.run(fetches, feed_dict) guaranteed to execute its fetches arguments in-order? The documentation doesn't seem to mention it. For example, if you run the order of execution matters: train_op will update parameters affecting accuracy. No. By default, Tensorflow is free to evaluate operators in any order. Because of concurrency, that order may even change between runs. This is usually a good thing because it means that Tensorflow may make optimal use of the available hardware. It can be problematic if your code mutates state such as Variables. However, if for some reason you do wish to control the order of evaluation, in general you can use control dependencies to enforce an order between operators. Control dependencies are documented here: https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies Hope that helps! After posting this, and during the discussion in Is it possible to get the objective function value during each training step? I noticed that the execution order is undefined. For example, consider this code: With TensorFlow 1.1, if the environment variable CUDA_VISIBLE_DEVICES is set to one of the GPUs, this prints and if it is set to "", this code prints Unfortunately, I don't see anything in the documentation specifying the execution order or warning users that it's undefined.I am trying to apply decision tree here. Decision tree takes care of splitting at each node itself. But at first node I want to split my tree on the basis of "Age". How do I force that.? There is no built-in option to do that in ctree(). The easiest method to do this "by hand" is simply: Learn a tree with only Age as explanatory variable and maxdepth = 1 so that this only creates a single split. Split your data using the tree from step 1 and create a subtree for the left branch. Split your data using the tree from step 1 and create a subtree for the right branch. This does what you want (although I typically wouldn't recommend to do so...). If you use the ctree() implementation from partykit you can also merge these three trees into a single tree again for visualizations and predictions etc. It requires a bit of hacking but is still feasible. I will illustrate this using the iris data and I will force a split in the variable Sepal.Length which otherwise wouldn't be used in the tree. Learning the three trees above is easy: Note, however, that it is important to use the formula with Sepal.Length + . to assure that the variables in the model frame are ordered in exactly the same way in all trees. Next comes the most technical step: We need do extract the raw node structure from all three trees, fix-up the node ids so that they are in a proper sequence and then integrate everything into a single node: And finally we set up a joint model frame containing all data and combine that with the new joint tree. Some information on fitted nodes and the response is added to be able to turn the tree into a constparty for nice visualization and predictions. See vignette("partykit", package = "partykit") for the background on this: And then we're done and can visualize our combined tree with the forced first split:  At every iteration, a decision tree will choose the best variable for splitting (either based on information gain / gini index, for CART, or based on chi-square test as for conditional inference tree). If you have better predictor variable that separates the classes more than that can be done by the predictor Age, then that variable will be chosen first. I think based on your requirement, you can do the following couple of things: (1) Unsupervised: Discretize the Age variable (create bins e.g., 0-20, 20-40, 40-60 etc., as per your domain knowledge) and subset the data for each of the age bins, then train a separate decision tree on each of these segments. (2) Supervised: Keep on dropping the other predictor variables until Age is chosen first. Now, you will get a decision tree where Age is chosen as the first variable. Use the rules for Age (e.g., Age > 36 & Age <= 36) created by the decision tree to subset the data into 2 parts. On each of the parts learn a full decision tree with all the variables separately. (3) Supervised Ensemble: you can use Randomforest classifier to see how important your Age variable is. You can use rpart and partykit combination to achieve such operation. Notice that if you use ctree to train DT then use data_party function to extract data from different node, the only variables included in the extracted data set would be the training variables only, in your case Age. We have to use rpart in the first step to train the model with selected variable because there is a way using rpart to train DT such that you can keep all your variables in the extracted data set without putting those variables as training variables: Using this method, your only training variable would be Age, and you can convert your rpart tree to partykit and extract data from different node and train them desperately: Now you have two dataset split based on Age with all the variables you want to use to train DT in the future, you can build DT based on those subsets however you see fit, use rpart or ctree. Later you can use partynode and partysplit combo to construct the tree based on training rules you achieved. Hope this is what you are looking for.I want to leverage machine learning to model a user's intent and potentially automate commonly performed tasks. To do this I would like to have access to a fire-hose of information about user actions and the machine state. To this end, it is my current thinking that getting access to the stream of windows messages is probably the way forward.  I would like to have as much information as is possible, filtering the information to that which is pertinent I would like to leave to the machine learning tool. How would this be accomplished? (Preferably in C#). Please assume that I know how to manage and use this large influx of data. Any help would be gratefully appreciated. You can use SetWindowsHookEx to set low level hooks to catch (specific) windows messages.
Specifically these hook-ids might be interesting for monitoring:  WH_CALLWNDPROC (4) Installs a hook procedure that monitors messages
  before the system sends them to the destination window procedure. For
  more information, see the CallWndProc hook procedure. WH_CALLWNDPROCRET(12) Installs a hook procedure that monitors
  messages after they have been processed by the destination window
  procedure. For more information, see the CallWndRetProc hook
  procedure. It's been a while since I've implemented it, but as an example I've posted the base class I use to hook specific messages. (For example, I've used it in a global mousewheel trapper, that makes sure my winforms apps behave the same as internet explorer: scroll the control underneath the cursor, instead of the active control).I am using this code to compare performance of a number of models: I can use 'accuracy' and 'recall' as scoring and these will give accuracy and sensitivity. How can I create a scorer that gives me 'specificity' Specificity= TN/(TN+FP) where TN, and FP are true negative and false positive values in the confusion matrix I have tried this and then  but it will not work for n_split >=10
I get this error for calculation of my_scorer  IndexError: index 1 is out of bounds for axis 1 with size 1 If you change the recall_score parameters for a binary classifier to pos_label=0 you get specificity (default is sensitivity, pos_label=1) You cannot get specificity in scikit but what you can actually get is fpr which is:  So for getting specificity, you just need to subtract fpr from 1. fpr can be calculated using roc_curve.  But for the above to work you need to calculate the y_pred by training the model. If you want to use this inside the cross_val_score, you can make a custom scorer like this: And then: NOTE: The above codes will only give correct results for binary y.The model.eval() method modifies certain modules (layers) which are required to behave differently during training and inference. Some examples are listed in the docs: This has [an] effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. Is there an exhaustive list of which modules are affected? In addition to info provided by @iacob: Searching site:https://pytorch.org/docs/stable/generated/torch.nn. "during evaluation" on google, it would appear the following modules are affected:I have built a TensorFlow model that uses a DNNClassifier to classify input into two categories. My problem is that Outcome 1 occurs upwards of 90-95% of the time. Therefore, TensorFlow is giving me the same probabilities for all of my predictions. I am trying to predict the other outcome (e.g. having a false positive for Outcome 2 is preferable to missing a possible occurrence of Outcome 2). I know that in machine learning in general, in this case it would be worthwhile to try to upweight Outcome 2. However, I don't know how to do this in TensorFlow. The documentation alludes to it being possible, but I can't find any examples of what it would actually look like. Has anyone has successfully done this, or does anyone know where I could find some example code or a thorough explanation (I'm using Python)? Note: I have seen exposed weights being manipulated when someone is using the more fundamental parts of TensorFlow and not an estimator. For maintenance reasons, I need to do this using an estimator. tf.estimator.DNNClassifier constructor has weight_column argument: weight_column: A string or a _NumericColumn created by
  tf.feature_column.numeric_column defining feature column representing
  weights. It is used to down weight or boost examples during training.
  It will be multiplied by the loss of the example. If it is a string,
  it is used as a key to fetch weight tensor from the features. If it is
  a _NumericColumn, raw tensor is fetched by key weight_column.key, then
  weight_column.normalizer_fn is applied on it to get weight tensor. So just add a new column and fill it with some weight for the rare class: [Update] Here's a complete working example:When using random forests in R I came across the following situation: Output: and Output: In the first example I set importance = FALSE and in the second example TRUE. From my understanding this should not affect the resulting prediction. There's also no indication for that behavior in the documentation. According to the Cross Validated thread Do proximity or importance influence predictions by a random forest?, the importance flag should not influence the predictions, but it clearly does in the example above. So why is the importance parameter of the randomForest method influencing the performance of the model? This is a nice example for demonstrating the limits of reproducibility; in short: Let's see why this is so here... All reproducibility studies are based on a strong implicit assumption: all other being equal; if a change between two experiments invalidates this assumption, we cannot expect reproducibility in the deterministic sense you are seeking here (we may of course still expect reproducibility in the statistical sense, but this is not the issue here). The difference between the two cases you present here (calculating feature importance or not) is really subtle; to see why it actually violates the principle stated above, we have to dig a little, both in the documentation as well as in the source code. The documentation of the RF importance function already provides a strong hint (emphasis mine): Here are the definitions of the variable importance measures. The first measure is computed from permuting OOB data [...] Then the same is done after permuting each predictor variable. You may have already started becoming suspicious; such data permutations are normally performed in a random sense, hence potentially invoking the random number generator (RNG) for one extra process when we use importance=TRUE, which process is absent in the importance=FALSE case. In other words: if in the importance=TRUE case the RNG is involved in a way that is absent from the importance=FALSE case, then from the first time such a thing happens in the program the two cases stop being deterministically comparable, irrespectively of the common random seed. At this point, this may be a strong hint, but it is still only a speculation; after all, and in principle, permutations can be performed deterministically, i.e. without involving any random processes (and hence the RNG). Where is the smoking gun? Turns out that the smoking gun exists indeed, buried in the underlying C source code used by the randomForest R package; here is the relevant part of the C function permuteOOB: where we can clearly see the function unif_rand() (i.e. the RNG) being invoked at line #4 of the snippet (here in the source code), in a method called only when we ask for importance=TRUE, and not in the opposite case. Arguably, and given the fact that RF is an algorithm where randomness (and hence RNG use) enters from too many points, this should be enough evidence as to why the two cases you present are indeed not expected to be identical, since the different use of the RNG makes the actual results diverge. On the other hand, a difference of one single misclassification among 150 samples should provide enough reassurance that the two cases are still statistically similar. It should be also apparent that the subtle implementation issue here (i.e. the RNG involvement) does not violate the expectation that the two results should be equal in theory.Running some experiments with TensorFlow, want to look at the implementation of some functions just to see exactly how some things are done, started with the simple case of tf.train.GradientDescentOptimizer. Downloaded the zip of the full source code from github, ran some searches over the source tree, got to: Okay, so presumably the actual code is in apply_gradient_descent, searched for that... not there. Only three occurrences in the entire source tree, all of which are uses, not definitions. What about training_ops? There does exist a source file with a suggestive name: ... the above is the entire content of that file. Hmm. I did find this file: which seems to confirm such and such other files are object code, generated in the build process - but where is the source code they are generated from? So this is the point at which I give up and ask for help. Can anyone familiar with the TensorFlow code base point me to where the relevant source code is? The implementation further goes to the native c++ code. Here's ApplyGradientDescent GPU implementation (core/kernels/training_ops_gpu.cu.cc): CPU implementation is here (core/kernels/training_ops.cc):I'm curious what the value field is in the nodes of the decision tree produced by Graphviz when used for regression. I understand that this is the number of samples in each class that are separated by a split when using decision tree classification but I'm not sure what it means for regression. My data has a 2 dimensional input and a 10 dimensional output. Here is an example of what a tree looks like for my regression problem:  produced using this code & visualized with webgraphviz What a regression tree actually returns as output is the mean value of the dependent variable (here Y) of the training samples that end up in the respective terminal nodes (leaves); these mean values are shown as lists named value in the picture, which are all of length 10 here, since your Y is 10-dimensional. In other words, and using the leftmost terminal node (leaf) of your tree as an example: You can confirm that this is the case by predicting some samples (from your training or test set - it doesn't matter) and checking that your 10-dimensional result is one of the 4 value lists depicted in the terminal leaves above. Additionally, you can confirm that, for each element in value, the weighted averages of the children nodes are equal to the respective element of the parent node. Again, using the first element of your 2 leftmost terminal nodes (leaves), we get: i.e. the value[0] element of their parent node (the leftmost node in the intermediate level). One more example, this time for the first value elements of your 2 intermediate nodes: which again agrees with the -0.0 first value element of your root node. Judging from the value list of your root node, it seems that the mean values of all elements of your 10-dimensional Y are almost zero, which you can (and should) verify manually, as a final confirmation. So, to wrap-up:I am trying to implement a simple sequence-to-sequence model using Keras. However, I keep seeing the following ValueError: Other questions like this or looking at this issue on Github suggests that this might have something to do with the cross-entropy loss function; but I fail to see what I am doing wrong here. I do not think that this is the problem, but I want to mention that I am on a nightly build of TensorFlow, tf-nightly==2.2.0.dev20200410 to be precise. This following code is a standalone example and should reproduce the exception from above: There are two different sets of problems in your code, which could be categorized as syntactical and architectural problems. The error raised (i.e. No gradients provided for any variable) is related to the syntactical problems which I would mostly address below, but I would try to give you some pointers about the architectural problems after that as well. The main cause of syntactical problems is about using named inputs and outputs for the model. Named inputs and outputs in Keras is mostly useful when the model has multiple input and/or output layers. However, your model has only one input and one output layer. Therefore, it may not be very useful to use named inputs and outputs here, but if that's your decision I would explain how it could be done properly. First of all, you should keep in mind that when using Keras models, the data generated from any input pipeline (whether it's a Python generator or tf.data.Dataset) should be provided as a tuple i.e. (input_batch, output_batch) or (input_batch, output_batch, sample_weights). And, as I said, this is the expected format everywhere in Keras when dealing with input pipelines, even when we are using named inputs and outputs as dictionaries.  For example, if I want to use inputs/outputs naming and my model has two input layers named as "words" and "importance", and also two output layers named as "output1" and "output2", they should be formatted like this: So as you can see above, it's a tuple where each element of the tuple is a dictionary; the first element corresponds to inputs of the model and the second element corresponds to outputs of the model. Now, according to this point, let's see what modifications should be done to your code: In sample_generator we should return a tuple of dicts, not a dict. So: In make_dataset function, the input arguments of tf.data.Dataset should respect this: The signature of prepare_example and its body should be modified as well: And finally, the call method of subclassed model: And one more thing: we should also put these names on the corresponding input and output layers using the name argument when constructing the layers (like Dense(..., name='output'); however, since we are using the Model sub-classing here to define our model, that's not necessary to do. All right, these would resolve the input/output problems and the error related to gradients would be gone; however, if you run the code after applying the above modifications, you would still get an error regarding incompatible shapes. As I said earlier, there are architectural issues in your model which I would briefly address below. As you mentioned, this is supposed to be a seq-to-seq model. Therefore, the output is a sequence of one-hot encoded vectors, where the length of each vector is equal to (target sequences) vocabulary size. As a result, the softmax classifier should have as much units as vocabulary size, like this (Note: never in any model or problem use a softmax layer with only one unit; that's all wrong! Think about why it's wrong!): The next thing to consider is the fact that we are dealing with 1D sequences (i.e. a sequence of tokens/words). Therefore using 2D-convolution and 2D-pooling layers does not make sense here. You can either use their 1D counterparts or replace them with something else like RNN layers. As a result of this, the Lambda layer should be removed as well. Also, if you want to use convolution and pooling, you should adjust the number of filters in each layer as well as the pool size properly (i.e. one conv filter, Conv1D(1,...) is not probably optimal, and pool size of 1 does not make sense). Further, that Dense layer before the last layer which has only one unit could severely limit the representational capacity of the model (i.e. it is essentially the bottleneck of your model). Either increase its number of units, or remove it. The other thing is that there is no reason for not one-hot encoding the labels of dev set. Rather, they should be one-hot encoded like the labels of training set. Therefore, either the training argument of make_generator should be removed entirely or, if you have some other use case for it, the dev dataset should be created with training=True argument passed to make_dataset function. Finally, after all these changes your model might work and start fitting on data; but after a few batches passed, you might get incompatible shapes error again. That's because you are generating input data with unknown dimension and also use a relaxed padding approach to pad each batch as much as needed (i.e. by using (None,) for padded_shapes). To resolve this you should decide on a fixed input/output dimension (e.g. by considering a fixed length for input/output sequences), and then adjust the architecture or hyper-parameters of the model (e.g. conv kernel size, conv padding, pooling size, adding more layers, etc.) as well as the padded_shapes argument accordingly. Even if you would like your model to support input/output sequences of variable length instead, then you should consider it in model's architecture and hyper-parameters and also the  padded_shapes argument. Since this the solution depends on the task and desired design in your mind and there is no one-fits-all solutions, I would not comment further on that and leave it to you to figure it out. But here is a working solution (which may not be, and probably isn't, optimal at all) just to give you an idea:I have a audio data set and each of them has different length. There are some events in these audios, that I want to train and test but these events are placed randomly, plus the lengths are different, it is really hard to build a machine learning system with using that dataset. I thought fixing a default size of length and build a multilayer NN however, the length's of events are also different. Then I thought about using CNN, like it is used to recognise patterns or multiple humans on an image. The problem for that one is I am really struggling when I try to understand the audio file. So, my questions, Is there anyone who can give me some tips about building a machine learning system that classifies different types of defined events with training itself on a dataset that has these events randomly(1 data contains more than 1 events and they are different from each other.) and each of them has different lenghts?  I will be so appreciated if anyone helps. First, you need to annotate your events in the sound streams, i.e. specify bounds and labels for them. Then, convert your sounds into sequences of feature vectors using signal framing. Typical choices are MFCCs or log-mel filtebank features (the latter corresponds to a spectrogram of a sound). Having done this, you will convert your sounds into sequences of fixed-size feature vectors that can be fed into a classifier. See this. for better explanation. Since typical sounds have a longer duration than an analysis frame, you probably need to stack several contiguous feature vectors using sliding window and use these stacked frames as input to your NN.  Now you have a) input data and b) annotations for each window of analysis. So, you can try to train a DNN or a CNN or a RNN to predict a sound class for each window. This task is known as spotting. I suggest you to read Sainath, T. N., & Parada, C. (2015). Convolutional Neural Networks for Small-footprint Keyword Spotting. In Proceedings INTERSPEECH (pp. 1478–1482) and to follow its references for more details. You can use a recurrent neural network (RNN). https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html The input data is a sequence and you can put a label in every sample of the time series. For example a LSTM (a kind of RNN) is available in libraries like tensorflow.Getting an error when using glmnet in Caret Example below
Load Libraries  Load churn data set from library C50 create x and y variables Use createFolds() to create 5 CV folds on churn_y, the target variable Create trainControl object: myControl Fit glmnet model: model_glmnet Im getting the following error Error in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
  NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning message:
In lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  :
  NAs introduced by coercion I have checked and there are no missing values in the churn_x variables  Does anyone know the answer? The problem is in the model specification. If you use the caret train formula interface the training will work: However when you specify x and y it will not work because glmnet takes the x in the form of a model matrix, When you supply the formula to caret it will take care of model.matrix creation but if you just specify the x and y then it will assume x is a model.matrix and will pass it to glmnet. For instance this works: model.matrix is needed only when there are factor features If you want to use glmnet and get the same error do this! Short answer: using data.matrix() fixed my issue! Initially, I was doing: This was fixed by: Longer answer(not long at all): I had the same problem, I was passing my X matrix using as.matrix() which turns all elements of a data frame into a coercible type for all columns, if you happen to have factors in your data frame, as.matrix() turns everything into a character. Usingdata.matrix() fixed it for me. data.matrix() can handle factors and ordered factor where as.matrix is more basic.this question is asked here before What is a good strategy to group similar words? but no clear answer is given on how to "group" items. The solution based on difflib is basically search, for given item, difflib can return the most similar word out of a list. But how can this be used for grouping?  I would like to reduce  to  or One idea I tried was, for each item, iterate through the list, if get_close_matches returns more than one match, use it, if not keep the word as is. This partly worked, but it can suggest apple for appel, then appel for apple, these words would simply switch places and nothing would change.  I would appreciate any pointers, names of libraries, etc. Note: also in terms of performance, we have a list of 300,000 items, and get_close_matches seems a bit slow. Does anyone know of a C/++ based solution out there?  Thanks,  Note: Further investigation revealed kmedoid is the right algorithm (as well as hierarchical clustering), since kmedoid does not require "centers", it takes / uses data points themselves as centers (these points are called medoids, hence the name). In word grouping case, the medoid would be the representative element of that group / cluster. You need to normalize the groups. In each group, pick one word or coding that represents the group. Then group the words by their representative. Some possible ways: Grouping the words could be difficult, though. If A is similar to B, and B is similar to C, A and C is not necessarily similar to each other. If B is the representative, both A and C could be included in the group. But if A or C is the representative, the other could not be included. Going by the first alternative (first encountered word): Example: Output: You have to decide in closed matches words, which words you want to use. May be get the first element from the list which get_close_matches is returning, or just use random function on that list and get one element from closed matches.  There must be some sort of rule, for it.. Now remove c from the initial list, thats it...
For c++, you can use Levenshtein_distance Here is another version using Affinity Propagation algorithm.   Distances had to be converted to similarities, I did that by taking the negative of distance. The output is Another method could be using matrix factorization, using SVD. First we create word distance matrix, for 100 words this would be 100 x 100 matrix representating the distance from each word to all other words. Then, SVD is ran on this matrix, the u in the resulting u,s,v can be seen as membership strength to each cluster.  Code The result The selection of k for number of clusters is important, k=25 gives much better results than k=20 for instance.  The code also selects a representative word for each cluster by picking the word whose u[..] coordinate is closest to the cluster centroid.  Here is an approach based on medoids. First install MlPy. On Ubuntu Then The output is The bigger word list and using k=10I am sorry for my naivety, but I don't understand why word embeddings that are the result of NN training process (word2vec) are actually vectors. Embedding is the process of dimension reduction, during the training process NN reduces the 1/0 arrays of words into smaller size arrays, the process does nothing that applies vector arithmetic. So as result we got just arrays and not the vectors. Why should I think of these arrays as vectors? Even though, we got vectors, why does everyone depict them as vectors coming from the origin (0,0)? Again, I am sorry if my question looks stupid. What are embeddings? Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension. (Source: https://en.wikipedia.org/wiki/Word_embedding) What is Word2Vec? Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space. (Source: https://en.wikipedia.org/wiki/Word2vec) What's an array? In computer science, an array data structure, or simply an array, is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored so that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array. What's a vector / vector space? A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below. (Source: https://en.wikipedia.org/wiki/Vector_space) What's the difference between vectors and arrays? Firstly, the vector in word embeddings is not exactly the programming language data structure (so it's not Arrays vs Vectors: Introductory Similarities and Differences). Programmatically, a word embedding vector IS some sort of an array (data structure) of real numbers (i.e. scalars) Mathematically, any element with one or more dimension populated with real numbers is a tensor. And a vector is a single dimension of scalars. To answer the OP question: Why are word embedding actually vectors? By definition, word embeddings are vectors (see above) Why do we represent words as vectors of real numbers? To learn the differences between words, we have to quantify the difference in some manner. Imagine, if we assign theses "smart" numbers to the words: We see that the distance between fruit and apple is close but samsung and apple isn't. In this case, the single numerical "feature" of the word is capable of capturing some information about the word meanings but not fully. Imagine the we have two real number values for each word (i.e. vector): To compute the difference, we could have done: That's not very informative, it returns a vector and we can't get a definitive measure of distance between the words, so we can try some vectorial tricks and compute the distance between the vectors, e.g. euclidean distance: Now, we can see more "information" that apple can be closer to samsung than orange to samsung. Possibly that's because apple co-occurs in the corpus more frequently with samsung than orange. The big question comes, "How do we get these real numbers to represent the vector of the words?". That's where the Word2Vec / embedding training algorithms (originally conceived by Bengio 2003) comes in. Since adding more real numbers to the vector representing the words is more informative then why don't we just add a lot more dimensions (i.e. numbers of columns in each word vector)? Traditionally, we compute the differences between words by computing the word-by-word matrices in the field of distributional semantics/distributed lexical semantics, but the matrices become really sparse with many zero values if the words don't co-occur with another. Thus a lot of effort has been put into dimensionality reduction after computing the word co-occurrence matrix. IMHO, it's like a top-down view of how global relations between words are and then compressing the matrix to get a smaller vector to represent each word. So the "deep learning" word embedding creation comes from the another school of thought and starts with a randomly (sometimes not-so random) initialized a layer of vectors for each word and learning the parameters/weights for these vectors and optimizing these parameters/weights by minimizing some loss function based on some defined properties. It sounds a little vague but concretely, if we look at the Word2Vec learning technique, it'll be clearer, see Here's more resources to read-up on word embeddings: https://github.com/keon/awesome-nlp#word-vectors the process does nothing that applies vector arithmetic The training process has nothing to do with vector arithmetic, but when the arrays are produced, it turns out they have pretty nice properties, so that one can think of "word linear space". For example, what words have embeddings closest to a given word in this space?  Put it differently, words with similar meaning form a cloud. Here's a 2-D t-SNE representation:  Another example, the distance between "man" and "woman" is very close to the distance between "uncle" and "aunt":  As a result, you have pretty much reasonable arithmetic: So it's not far fetched to call them vectors. All pictures are from this wonderful post that I very much recommend to read. Each word is mapped to a point in d-dimension space (d is usually 300 or 600 though not necessary), thus its called a vector (each point in d-dim space is nothing but a vector in that d-dim space).  The points have some nice properties (words with similar meanings tend to occur closer to each other) [proximity is measured using cosine distance between 2 word vectors] Famous Word2Vec implementation is CBOW + Skip-Gram Your input for CBOW is your input word vector (each is a vector of length N; N = size of vocabulary). All these input word vectors together are an array of size M x N; M=length of words). Now what is interesting in the graphic below is the projection step, where we force an NN to learn a lower dimensional representation of our input space to predict the output correctly. The desired output is our original input.  This lower dimensional representation P consists of abstract features describing words e.g. location, adjective, etc. (in reality these learned features are not really clear). Now these features represent one view on these words.  And like with all features, we can see them as high-dimensional vectors.
If you want you can use dimensionality reduction techniques to display them in 2 or 3 dimensional space.   More details and source of graphic: https://arxiv.org/pdf/1301.3781.pdfThis is NOT MY code by here is the line, where it shows a problem: (As I am thinking now, it is very possible this code uses an older version of TF, because 'epochs' was written as 'nb_epoch'). The last update of the code is from: Jan 11, 2017! I have tried everything from the internet (which is not so much), including looking inside the source code of tensorflow/keras for some hints. Just to make it clear that I don't have a variable, called 'batch_index' inside the code. So far I have looked inside different versions of TF (tensorflow/tensorflow/python/keras/engine/training_arrays.py). It appears that all are from 2018 copyright, but some start with the function fit_loop, and other with model_iteration (which is probably an update of fit_loop). So, this 'batch_index' variable can be seen only in the first function. I wonder if I am going in the right direction at all??! There is no point in showing the code, because, as I explained, there is no such variable in the first place inside the code. but, here is some code of the function 'stock_prediction', that gives the error: A little clarification: I tried to see my version of tf/keras and here is it: 2.2.4-tf 2.2.5 1.14.0 Why keras shows different versions?? I checked in the training_arrays.py (here) the function in which you got the error and, as I can see, I think the problem could be in these statements (from lines 177 - 205): If batches is an empty list, you could get this error. Could be that your training set has some problem? The reason for the problem is that the list of batches is empty! batches ==[] The reason it is blank is because the number of samples for training data is too small to be divided by batch_size You should check your data, number of samples or
You should reduce batch_size to a point that allows you to divide the number of samples by batch size with a real result.. I had to import the correct libraries (Tensorflow and not Keras directly): Apparently this is related to the different version issue of Keras. This error is because there is empty training data. whether you import from keras directly or from tensorflow there will be error if you not pass proper data, error message might be different as per import or version. Also make sure your are passing couple of records in data. If you import Keras from tensorflow and use the the error will be "raise ValueError('Empty training data.')
ValueError: Empty training data." If directly then the message will be the error message given in question.I have one question though. I heard from someone that in R, you can use extra packages to extract the decision rules implemented in RF, I try to google the same thing in python but without luck, if there is any help on how to achieve that. 
thanks in advance! Assuming that you use sklearn RandomForestClassifier you can find the invididual decision trees as .estimators_. Each tree stores the decision nodes as a number of NumPy arrays under tree_. Here is some example code which just prints each node in order of the array. In a typical application one would instead traverse by following the children. Example outout: We use something similar in emlearn to compile a Random Forest to C code.I'm trying to create a new string Attribute using Weka's Java API... Reading through the API javadocs, it appears that the way to do so is to use this constructor:  but I'm stuck as to what I should pass into the attributeValues parameter... when I put in null, Java complains about protected objects
when I put in Null, it's syntax error
when I put in new FastVector(), it becomes a nominal attribute that is empty rather than a string attribute...
when I create a new object:  and then pass fv into the argument, it returns a null pointer exception... what exactly should I put into the attributeValues argument so that it becomes a string attribute?  You have to cast the null to FastVector. Otherwise more methods would apply to the method signature: Here is a good resource for creating Instances on the fly: https://waikato.github.io/weka-wiki/formats_and_processing/creating_arff_file/ Easy way to build STRING Attribute in WEKA is this: Main problem is WEKA's definition of NULL value, or NULL vector in new type of Java editors with imported weka.jar and throw exceptions mode.I am training a neural network with Keras using EarlyStopping based on val_acc and patience=0. EarlyStopping stops the training as soon as val_acc decreases. However the final model that I obtain is not the best model, namely the one with the highest val_acc. But I rather have the model corresponding to the epoch after, namely the one corresponding to a val_acc just a bit lower than the best one and that caused the early stopping! How do I get the best one?  I tried to use the save the best model using the call back:  But I get the same results. In Keras 2.2.3, a new argument called restore_best_weights have been introduced for EarlyStopping callback that if set to True (defaults to False), it would restore the weights from the epoch with the best monitored quantity: restore_best_weights: whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the
model weights obtained at the last step of training are used. If you would like to save the highest accuracy then you should set the checkpoint monitor='val_acc' it will automatically save on highest. Lowest loss might not necessarily correspond to highest accuracy. You can also set verbose=1 to see which model is being saved and why.I am trying to use keras to fit a CNN model to classify 2 classes of data . I have imbalanced dataset I want to balance the data. I don't know can I use class_weight in model.fit_generator . I wonder if I used class_weight="balanced"  in  model.fit_generator The main code: If you don't want to change your data creation process, you can use class_weight in your fit generator. You can use dictionary to set your class_weight and observe with fine tuning. For instance when class_weight is not used, and you have 50 examples for class0 and 100 examples for class1. Then, loss function calculate loss uniformly. It means that class1 will be a problem. But, when you set: It means that loss function will give 2 times weight to your class 0 now. Therefore, misclassification of underrepresented data will take 2 times more punishment than before. Thus, model can handle imbalanced data. If you use class_weight='balanced' model can make that setting automatically. But my suggestion is that, create a dictionary like class_weight = {0:a1 , 1:a2} and try different values for a1 and a2, so you can understand difference. Also, you can use undersampling methods for imbalanced data instead of using class_weight. Check Bootstrapping methods for that purpose.I'm learning deep learning with keras and trying to compare the results (accuracy) with machine learning algorithms (sklearn) (i.e random forest, k_neighbors) It seems that with keras I'm getting the worst results.
I'm working on simple classification problem: iris dataset
My keras code looks: I have tired to add layers and/or change number of units per layer and/or change the activation function (to relu) by it seems that the result are not higher than 0.85. With sklearn random forest or k_neighbors I'm getting result (on same dataset) above 0.95. What am I missing ? With sklearn I did little effort and got good results, and with keras, I had a lot of upgrades but not as good as sklearn results. why is that ? How can I get same results with keras ? In short, you need: In detail: The first issue here is that nowadays we never use activation='tanh' for the intermediate network layers. In such problems, we practically always use activation='relu'. The second issue is that you have build quite a large Keras model, and it might very well be the case that with only 100 iris samples in your training set you have too few data to effectively train such a large model. Try reducing drastically both the number of layers and the number of nodes per layer. Start simpler. Large neural networks really thrive when we have lots of data, but in cases of small datasets, like here, their expressiveness and flexibility may become a liability instead, compared with simpler algorithms, like RF or k-nn. The third issue is that, in contrast to tree-based models, like Random Forests, neural networks generally require normalizing the data, which you don't do. Truth is that knn also requires normalized data, but in this special case, since all iris features are in the same scale, it does not affect the performance negatively. Last but not least, you seem to run your Keras model for only one epoch (the default value if you don't specify anything in model.fit); this is somewhat equivalent to building a random forest with a single tree (which, BTW, is still much better than a single decision tree). All in all, with the following changes in your code: and everything else as is, we get: We can do better: use slightly more training data and stratify them, i.e. And with the same model & training epochs you can get a perfect accuracy of 1.0 in the test set: (Details might differ due to some randomness imposed by default in such experiments). Adding some dropout might help you improve accuracy. See Tensorflow's documentation for more information. Essentially how you add a Dropout layer is just very similar to how you added those Dense() layers. Note: The parameter '0.2 implies that 20% of the connections in the layer is randomly omitted to reduce the interdependencies between them, which reduces overfitting.I'm trying to do transfer learning, using a pretrained Xception model with a newly added classifier. This is the model: The dataset I'm using is oxford_flowers102 taken directly from tensorflow datasets.
This is a dataset page. I have a problem with selecting some parameters - either training accuracy shows suspiciously low values, or there's an error. I need help with specifying this parameter, for this (oxford_flowers102) dataset: I tried: I'm not sure whether it should be SparseCategoricalCrossentropy or CategoricalCrossentropy, and what about from_logits parameter? I'm also not sure whether should I choose for metricskeras.metrics.Accuracy() or keras.metrics.CategoricalAccuracy() I am definitely lacking some theoretical knowledge, but right now I just need this to work. Looking forward to your answers! The dataset is divided into a training set, a validation set, and a test set. The training set and validation set each consist of 10 images per class (totaling 1020 images each). The test set consists of the remaining 6149 images (minimum 20 per class). If we check, we'll see So, it has 102 categories or classes and the target comes with an integer with different shapes input. First, if you keep this integer target or label, you should use sparse_categorical_accuracy for accuracy and sparse_categorical_crossentropy for loss function. But if you transform your integer label to a one-hot encoded vector, then you should use categorical_accuracy for accuracy, and categorical_crossentropy for loss function. As these data set have integer labels, you can choose sparse_categorical or you can transform the label to one-hot in order to use categorical. Second, if you set outputs = keras.layers.Dense(102, activation='softmax')(x) to the last layer, you will get probabilities score. But if you set outputs = keras.layers.Dense(102)(x), then you will get logits. So, if you set activations='softmax', then you should not use from_logit = True. For example in your above code you should do as follows (here's some theory for you): Third, keras uses string identifier such as metrics=['acc'] , optimizer='adam'. But in your case, you need to be a bit more specific as you mention loss function specific. So, instead of keras.metrics.Accuracy(), you should choose keras.metrics.SparseCategoricalAccuracy() if you target are integer or keras.metrics.CategoricalAccuracy() if your target are one-hot encoded vector. Here is an end-to-end example. Note, I will transform integer labels to a one-hot encoded vector (right now, it's a matter of preference to me). Also, I want probabilities (not logits) from the last layer which means from_logits = False. And for all of these, I need to choose the following parameters in my training: Let's complete the whole code. Preprocess and Augmentation Model Okay, additionally, here I like to use two metrics to compute top-1 and top-3 accuracy. EvaluateIn the docs, the predict_proba(self, x, batch_size=32, verbose=1) is  Generates class probability predictions for the input samples batch by batch. and returns  A Numpy array of probability predictions. Suppose my model is binary classification model, does the output is [a, b], for a is probability of class_0, and b is the probability of class_1? Here the situation is different and somehow misleading, especially when you are comparing predict_proba method to sklearn methods with the same name. In Keras (not sklearn wrappers) a method predict_proba is exactly the same as a predict method. You can even check it here: So - in a binary classification case - the output which you get depends on the design of your network: This second method is rarely used and there are some theorethical advantages of using the first method - but I wanted to inform you - just in case. It depends on how you specify output of your model and your targets. It can be both. Usually when one is doing binary classification the output is a single value which is a probability of the positive prediction. One minus the output is probability of the negative prediction.I am trying to create an autoencoder for: How to split train it and split with the trained weights? Make encoder: Make decoder: Make autoencoder: Now you can use any of them any way you want to.I already referred the posts here, here and here. Don't mark it as duplicate. I am working on a binary classification problem where my dataset has categorical and numerical columns. However, some of the categorical columns has a mix of numeric and string values. Nontheless, they only indicate the category name. For instance, I have a column called biz_category which has values like A,B,C,4,5 etc. I guess the below error is thrown due to values like 4 and 5. Therefore, I tried the belowm to convert them into category datatype. (but still it doesn't work) And my data info looks like below So, after dtype conversion, when I try the below SMOTENC, I get an error This results in an error as shown below --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call
last)
~\AppData\Roaming\Python\Python39\site-packages\sklearn\utils_encode.py
in _unique_python(values, return_inverse)
134
--> 135         uniques = sorted(uniques_set)
136         uniques.extend(missing_values.to_list()) TypeError: '<' not supported between instances of 'str' and 'int' During handling of the above exception, another exception occurred: TypeError                                 Traceback (most recent call
last)
C:\Users\SATHAP~1\AppData\Local\Temp/ipykernel_31168/1931674352.py in

6 from imblearn.over_sampling import SMOTE, SMOTENC
7 sm = SMOTENC(categorical_features=cat_index,random_state = 2,sampling_strategy = 'minority')
----> 8 X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
9
10 print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) ~\AppData\Roaming\Python\Python39\site-packages\imblearn\base.py in
fit_resample(self, X, y)
81         )
82
---> 83         output = self.fit_resample(X, y)
84
85         y = ( ~\AppData\Roaming\Python\Python39\site-packages\imblearn\over_sampling_smote\base.py
in fit_resample(self, X, y)
511
512         # the input of the OneHotEncoder needs to be dense
--> 513         X_ohe = self.ohe.fit_transform(
514             X_categorical.toarray() if sparse.issparse(X_categorical) else X_categorical
515         ) ~\AppData\Roaming\Python\Python39\site-packages\sklearn\preprocessing_encoders.py
in fit_transform(self, X, y)
486         """
487         self._validate_keywords()
--> 488         return super().fit_transform(X, y)
489
490     def transform(self, X): ~\AppData\Roaming\Python\Python39\site-packages\sklearn\base.py in
fit_transform(self, X, y, **fit_params)
850         if y is None:
851             # fit method of arity 1 (unsupervised transformation)
--> 852             return self.fit(X, **fit_params).transform(X)
853         else:
854             # fit method of arity 2 (supervised transformation) ~\AppData\Roaming\Python\Python39\site-packages\sklearn\preprocessing_encoders.py
in fit(self, X, y)
459         """
460         self._validate_keywords()
--> 461         self.fit(X, handle_unknown=self.handle_unknown, force_all_finite="allow-nan")
462         self.drop_idx = self._compute_drop_idx()
463         return self ~\AppData\Roaming\Python\Python39\site-packages\sklearn\preprocessing_encoders.py
in _fit(self, X, handle_unknown, force_all_finite)
92             Xi = X_list[i]
93             if self.categories == "auto":
---> 94                 cats = _unique(Xi)
95             else:
96                 cats = np.array(self.categories[i], dtype=Xi.dtype) ~\AppData\Roaming\Python\Python39\site-packages\sklearn\utils_encode.py
in _unique(values, return_inverse)
29     """
30     if values.dtype == object:
---> 31         return _unique_python(values, return_inverse=return_inverse)
32     # numerical
33     out = np.unique(values, return_inverse=return_inverse) ~\AppData\Roaming\Python\Python39\site-packages\sklearn\utils_encode.py
in _unique_python(values, return_inverse)
138     except TypeError:
139         types = sorted(t.qualname for t in set(type(v) for v in values))
--> 140         raise TypeError(
141             "Encoders require their input to be uniformly "
142             f"strings or numbers. Got {types}" TypeError: Encoders require their input to be uniformly strings or
numbers. Got ['int', 'str'] Should I transform y_train into categorical as well? Currently, it is int64. Help please SMOTE requires the values in each categorical/numerical column to have uniform datatype. Essentially you can not have mixed datatypes in any of the column in this case your biz_category column. Also merely casting the column to categorical  type does not necessarily mean that the values in that column will have uniform datatype. One possible solution to this problem is to re-encode the values in those columns which have mixed data types for example you could use lableencoder but I think in your case simply changing the dtype to string would also work.I'm getting a few errors here but I think it's due to pandas not importing as it's greyed out. If that is the problem, how would I fix this? C:\Anaconda\python.exe
  C:/Users/nickd/Documents/SKLEARN-STOCKS/stock-mach.py Traceback (most
  recent call last):   File
  "C:/Users/nickd/Documents/SKLEARN-STOCKS/stock-mach.py", line 38, in
  
      Key_Stats()   File "C:/Users/nickd/Documents/SKLEARN-STOCKS/stock-mach.py", line 12, in
  Key_Stats
      df = pandas.DataFrame(columns = ['Date','Unix','Ticker','DE Ratio']) NameError: global name 'pandas' is not defined Process finished with exit code 1 You have imported it as  and calling  You could either change  edit: error is due to missing ) You imported pandas as pd. Either refer to it as such throughout, or remove the as pd from the import. (Plus, never ever ever do except Exception... pass. You'll swallow all sorts of errors and never know what they were. If you aren't going to handle them, don't catch them at all.)This is code for feature scaling in which i am using fit_transform() and transform(): fit means to fit the pre-processor to the data being provided. This is where the pre-processor "learns" from the data. transform means to transform the data (produce outputs) according to the fitted pre-processor; it is normally used on the test data, and unseen data in general (e.g. in new data that come after deploying a model). fit_transform means to do both - Fit the pre-processor to the data, then transform the data according to the fitted pre-processor. Calling fit_transform is a convenience to avoid needing to call fit and transform sequentially on the same input, but of course this is only applicable to the training data (calling again fit_transform in test or unseen data is unfortunately a common rookie mistake).I've followed the TensorFlow for Poets tutorial and replaced the stock flower_photos with a few classes of my own. Now I've got my labels.txt file and my graph.pb saved on my local machine. Is there a way for me to deploy this pre-trained model to Google Cloud Platform? I've been reading the docs and all I can find are instructions on how to create, train, and deploy models from within their ML Engine. But I don't want to spend money training my model on Google's servers when I only need them to host my model so I can call it for predictions. Anyone else run into the same problem? Deploying a locally trained model is a supported use case; the instructions are essentially the same regardless of where you trained it: To deploy a model version you'll need: A TensorFlow SavedModel saved on Google Cloud Storage. You can get a
  model by: Following the Cloud ML Engine training steps to train in the
  cloud. Training elsewhere and exporting to a SavedModel. Unfortunately, TensorFlow for Poets does not show how to export a SavedModel (I've filed a feature request to address that). In the meantime, you can write a "converter" script like the following (you could alternatively do this at the end of training instead of saving out graph.pb and reading it back in): (Untested code based on this codelab and this SO post). If you want the output to use string labels instead of integer indices, make the following change: Partial answer only, unfortunately, but I have been able to accomplish this...but with some ongoing issues that I have not yet resolved. I ported over the trained pb and txt files to my server, installed Tensorflow, and am calling the trained model via HTTP request. It works perfectly...on the first run. Then fails every other time. tensorflow deployment on openshift, errors with gunicorn and mod_wsgi Surprised there are not more people out there trying to go after this general issue.I am working on a binary classification using random forest and trying out SHAP to explain the model predictions. However, I would like to convert the SHAP local explanation plots with values into a pandas dataframe for each instance. Is there any one here who can help me with exporting SHAP local explanations to pandas dataframe for each instance? I know that SHAPASH has .to_pandas() method but couldn't find anything like that in SHAP I tried something like below based on the SO post here but it doesn't help I expect my output something like below. Here, negative sign indicates feature contribution for class 0 and positive values indicates feature contribution for class 1 If you have a model like this: you can decompose your results like this: Which is exactly equal to: If you want to put results to Pandas df: Alternatively, if you wish everything arranged row-wise:I am trying to apply the idea of sklearn ROC extension to multiclass to my dataset. My per-class ROC curve looks find of a straight line each, unline the sklearn's example showing curve's fluctuating. I give an MWE below to show what I mean: The following function then plots the ROC curve: Output:  Kind of straight line bending once. I would like to see the model performance at different thresholds, not just one, a figure similar to sklearn's illustration for 3-classes shown below:  Point is that you're using predict() rather than predict_proba()/decision_function() to define your y_hat. This means - considering that the threshold vector is defined by the number of distinct values in y_hat (see here for reference), that you'll have few thresholds per class only on which tpr and fpr are computed (which in turn implies that your curves are evaluated at few points only). Indeed, consider what the doc says to pass to y_scores in roc_curve(), either prob estimates or decision values. In the example from sklearn, decision values are used to compute the scores. Given that you're considering a RandomForestClassifier(), considering probability estimates in your y_hat should be the way to go. What's the point then of label-binarizing the output? The standard definition for ROC is in terms of binary classification. To pass to a multiclass problem, you have to convert your problem into binary by using OneVsAll approach, so that you'll have n_class number of ROC curves. (Observe, indeed, that as SVC() handles multiclass problems in a OvO fashion by default, in the example they had to force to use OvA by applying OneVsRestClassifier constructor; with a RandomForestClassifier you don't have such problem as that's inherently multiclass, see here for reference). In these terms, once you switch to predict_proba() you'll see there's no much sense in label binarizing predictions. Eventually, consider that roc_curve() has also a drop_intermediate parameter meant for dropping suboptimal thresholds (it might be useful to know). Just to update on @amiola answer: I had an issue with non-monotonic classes which lead to very strange fuzzy results. In this case a little modification to the function above will work very well: Use this in the label_binarize line: And then when you need a range in the function, just use: Thanks to @dx2-66 for the answer. You can check for more details here.I want to perform time-series prediction of future events using SVR module from scikit-learn. Here is my source code I am trying to work with: My dataset is very huge and so a portion of my csv dataset is included at the end. I am interested in the 7th column. I wanted to predict when the values in the 7th column increase or when it decreases. Is it possible to look into the 7th column ONLY and do the time-series prediction?  Any help with this would be so much appreciated? Thanks! Ok, the svm function below has a problem:  The second line, win = ... is unused, and will cause an error.  Delete it.  Second, I don't know why there is an entire function for reading a csv.  Ignore it and use pandas. Here is a sample code that will work:  The intermediate steps, where you clean up the imported data, turn it from a dataframe to a numpy array, copy your 7th column as the regression to fit, delete it from your training data, and rebuild a new array must be done before fitting to the SVR.  Let me know if these worked.  I just took a few lines from your data table above.I tried to create a neural network to estimate y = x ^ 2. So I created a fitting neural network and gave it some samples for input and output. I tried to build this network in C++. But the result is different than I expected. With the following inputs: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 -1
-2 -3 -4 -5 -6 -7 -8 -9 -10 -11 -12 -13 -14 -15 -16 -17 -18 -19 -20 -21 -22 -23 -24 -25 -26 -27 -28 -29 -30 -31 -32 -33 -34 -35 -36 -37 -38 -39 -40 -41 -42 -43 -44 -45 -46 -47 -48 -49 -50 -51 -52 -53 -54 -55 -56 -57 -58 -59 -60 -61 -62 -63 -64 -65 -66 -67 -68 -69 -70 -71 and the following outputs: 0 1 4 9 16 25 36 49 64 81 100 121 144 169 196 225 256 289 324 361 400
441 484 529 576 625 676 729 784 841 900 961 1024 1089 1156 1225 1296
1369 1444 1521 1600 1681 1764 1849 1936 2025 2116 2209 2304 2401 2500
2601 2704 2809 2916 3025 3136 3249 3364 3481 3600 3721 3844 3969 4096
4225 4356 4489 4624 4761 4900 5041 1 4 9 16 25 36 49 64 81 100 121 144
169 196 225 256 289 324 361 400 441 484 529 576 625 676 729 784 841
900 961 1024 1089 1156 1225 1296 1369 1444 1521 1600 1681 1764 1849
1936 2025 2116 2209 2304 2401 2500 2601 2704 2809 2916 3025 3136 3249
3364 3481 3600 3721 3844 3969 4096 4225 4356 4489 4624 4761 4900 5041 I used fitting tool network. with matrix rows. Training is 70%, validation is 15% and testing is 15% as well. The number of hidden neurons is two. Then in command lines I wrote this: Other information : My net.b[1] is: -1.16610230053776 1.16667147712026 My net.b[2] is: 51.3266249426358 And net.IW(1) is: 0.344272596370387 0.344111217766824 net.LW(2) is: 31.7635369693519    -31.8082184881063 When my inputTest is 3, the result of this command is 16, while it should be about 9. Have I made an error somewhere? I found the Stack Overflow post Neural network in MATLAB that contains a problem like my problem, but there is a little difference, and the differences is in that problem the ranges of input and output are same, but in my problem is no. That solution says I need to scale out the results, but how can I scale out my result? You are right about scaling. As was mentioned in the linked answer, the neural network by default scales the input and output to the range [-1,1]. This can be seen in the network processing functions configuration: The second preprocessing function applied to both input/output is mapminmax with the following parameters: to map both into the range [-1,1] (prior to training). This means that the trained network expects input values in this range, and outputs values also in the same range. If you want to manually feed input to the network, and compute the output yourself, you have to scale the data at input, and reverse the mapping at the output. One last thing to remember is that each time you train the ANN, you will get different weights. If you want reproducible results, you need to fix the state of the random number generator (initialize it with the same seed each time). Read the documentation on functions like rng and RandStream. You also have to pay attention that if you are dividing the data into training/testing/validation sets, you must use the same split each time (probably also affected by the randomness aspect I mentioned). Here is an example to illustrate the idea (adapted from another post of mine): I opted to use the mapminmax function, but you could have done that manually as well. The formula is a pretty simply linear mapping:I have been using OpenCV for a quite time. I decided to check its power for Machine Learning lately. So I ended up with implementing a neural network for face recognition. To summarize my strategy for face recognition : Predict the test data using the trained network. So everything was OK until the prediction stage. I was using the max responsed output unit to classify the face. So normally OpenCV's sigmoid implementation should give values in range of -1 to 1 which is stated at the docs. 1 is the max closure to class. After I got nearly 0 accuracy I checked the output responses for each class for each test data. I was suprised with the values : 14.53, -1.7 , #IND . If sigmoid was applied, how could i get these values ? Where am i doing wrong ? To help you understand the matter and for the ones wondering how to apply PCA and use it with NN I m sharing my code : Reading csv: Rolling images row by row : Converting vector of labels to Mat of labels MAIN NOTE: I used AT&T 's first 20 class for my dataset.  Thanks to Canberk Baci's comment I managed to overcome sigmoid output discrepancy. Problem seems to be at default parameters of mlp 's create function which takes alpha and beta 0 as default. When they both are given as 1, sigmoid function works as it was stated in the docs and neural network can predict something but with errors of course. And for the results of Neural Network : By modifying some parameters like momentum etc, and without any illumunation correction algorithm, I got %72 accuracy  on the dataset of (randomly sampled 936 train, 262 test images ) first 20 classes of CroppedYaleB from opencv tutorials. For the other factors to increase accuracy; when I applied PCA, I directly gave the reduced dimension size as 500. This may also reduce accuracy because retained variance may be below %95 or worse. So when I have free time I will apply these to increase accuracy : I shared these so that someone may wonder how to increase the classification accuracy of NN. I hope it helps. By the way you can track the issue about this here:
http://code.opencv.org/issues/3583I recently discovered this amazing library for ML interpretability. I decided to build a simple xgboost classifier using a toy dataset from sklearn and to draw a force_plot. To understand the plot the library says: The above explanation shows features each contributing to push the
model output from the base value (the average model output over the
training dataset we passed) to the model output. Features pushing the
prediction higher are shown in red, those pushing the prediction lower
are in blue (these force plots are introduced in our Nature BME
paper). So it looks to me as the base_value should be the same as clf.predict(X_train).mean()which equals 0.637. However this is not the case when looking at the plot, the number is actually not even within [0,1]. I tried doing the log in different basis (10, e, 2) assuming it would be some kind of monotonic transformation... but still not luck. How can I get to this base_value? To get base_value in raw space (when link="identity") you need to unwind class labels --> to probabilities --> to raw scores. Note, the default loss is "deviance", so the raw is inverse sigmoid: The relevant plot for 0th data point in raw space:  Should you wish to switch to sigmoid probability space (link="logit"): The relevant plot for 0th data point in probability space:  Note, the probability base_value from shap's perspective, what they call a baseline probability if no data is available, is not what a reasonable person would define by having no independent variables (0.6373626373626373 in this case) Full reproducible example: Output:I'm trying to implement a loss function by using the representations of the intermediate layers. As far as I know, the Keras backend custom loss function only accepts two input arguments(y_ture, and y-pred). How can I define a loss function with @tf.function and use it for a model that has been defined via Keras?
any help would be appreciated. this a simple workaround to pass additional variables to your loss function. in our case, we pass the hidden output of one of our layers (x1). this output can be used to do something inside the loss function (I do a dummy operation)I'm working with a really simple dataset. It has some missing values, both in categorical and numeric features. Because of this, I'm trying to use sklearn.preprocessing.KNNImpute to get the most accurate imputation I can. However, when I run the following code: I get the error: ValueError: could not convert string to float: 'Private' That makes sense, it obviously can't handle categorical data. But when I try to run OneHotEncoder with: It throws the error: ValueError: Input contains NaN I'd prefer to use KNNImpute even with the categorical data as I feel like I'd be losing some accuracy if I just use a ColumnTransform and impute with numeric and categorical data seperately. Is there any way to get OneHotEncoder to ignore these missing values? If not, is using ColumnTransform or a simpler imputer a better way of tackling this problem? Thanks in advance There are open issues/PRs to handle missing values on OneHotEncoder, but it's not clear yet what the options would be.  In the interim, here's a manual approach.I want to implement a AdaBoost model using scikit-learn (sklearn). My question is similar to another question but it is not totally the same. As far as I understand, the random_state variable described in the documentation is for randomly splitting the training and testing sets, according to the previous link. So if I understand correctly, my classification results should not be dependent on the seeds, is it correct? Should I be worried if my classification results turn out to be dependent on the random_state variable? Your classification scores will depend on random_state. As @Ujjwal rightly said, it is used for splitting the data into training and test test. Not just that, a lot of algorithms in scikit-learn use the random_state to select the subset of features, subsets of samples, and determine the initial weights etc. For eg.  Tree based estimators will use the random_state for random selections of features and samples (like DecisionTreeClassifier, RandomForestClassifier). In clustering estimators like Kmeans, random_state is used to initialize centers of clusters. SVMs use it for initial probability estimation Its mentioned in the documentation that: If your code relies on a random number generator, it should never use functions like numpy.random.random or numpy.random.normal. This approach can lead to repeatability issues in tests. Instead, a numpy.random.RandomState object should be used, which is built from a random_state argument passed to the class or function. Do read the following questions and answers for better understanding: It does matter. When your training set differs then your trained state also changes. For a different subset of data you can end up with a classifier which is little different from the one trained with some other subset. Hence, you should use a constant seed like 0 or another integer, so that your results are reproducible.I'm extremely new to python and I'm recently trying to understand more about Machine Learning and Neural Nets I know this is a trivial question but I seem to be having problem importing data_utils on jupyter notebook. Can anyone please help  Note: I am not using Keras, and I am following the tutorial in this video. I was also following the video and went through the same issue, after searching for a while; Here is the link from Github tensorflow_chatbot_required_files. You can download from here and copy it to your working directory (python file directory).  Now you will be able to import both. Based on the link that you provided for the video you are using, go to
this link and download both files into your working directory. The files you need to download are data_utils and seq2seq_model. But before doing this tutorial try the tensorflow tutorials found on the tensorflow website, to get you started with this library.  Also, if you are very new to python, I recommend this tutorial first. Since you mentioned about Machine Learning and Neural Nets, I'll assume you're referring to Keras. I'll also assume that you installed it via In that case, you need to uninstall it first by running Then, clone it from Github, cd into its directory and run More information here I'm assuming you are using code from tensorflow's github, in that case you need to download https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/data_utils.py into your folder also.  what you are trying to do is importing a self-defined module, in order to do that do as follow: in your case the module is called 'data_util' which contains functions that will be called later as data_util.'function name'. let say that the data_util which is a python file (.py) is in this directory (C:/Users/xxx/modules), so all what you have to do is to run this line of code in order for python to find your modul when you call import data_util: I too was using same files.
Just open seq2seq_model.py and in line 35 remove from TensorFlow and keep it just import dat_I currently have a (1631160,78) np array as my input to a neural network. I would like to try something with LSTM which requires a 3D structure as input data. I'm currently using the following code to generate the 3D structure needed but it is super slow (ETA > 1day). Is there a better way to do this with numpy?  My current code to generate data: Here's an approach using NumPy strides to vectorize the creation of output_x  - Sample run - This creates a view into the input array and as such memory-wise we are being efficient. In most cases, this should translate to benefits on performance too with further operations involving it. Let's verify that its a view indeed - Another sure-shot way to verify would be to set some values into output and check the input -In neural nets, regularization (e.g. L2, dropout) is commonly used to reduce overfitting. For example, the plot below shows typical loss vs epoch, with and without dropout. Solid lines = Train, dashed = Validation, blue = baseline (no dropout), orange = with dropout. Plot courtesy of Tensorflow tutorials.

Weight regularization behaves similarly. Regularization delays the epoch at which validation loss starts to increase, but regularization apparently does not decrease the minimum value of validation loss (at least in my models and the tutorial from which the above plot is taken). If we use early stopping to stop training when validation loss is minimum (to avoid overfitting) and if regularization is only delaying the minimum validation loss point (vs. decreasing the minimum validation loss value) then it seems that regularization does not result in a network with greater generalization but rather just slows down training. How can regularization be used to reduce the minimum validation loss (to improve model generalization) as opposed to just delaying it? If regularization is only delaying minimum validation loss and not reducing it, then why use it? Over-generalizing from a single tutorial plot is arguably not a good idea; here is a relevant plot from the original dropout paper:  Clearly, if the effect of dropout was to delay convergence it would not be of much use. But of course it does not work always (as your plot clearly suggests), hence it should not be used by default (which is arguably the lesson here)...I want to predict the kind of 2 diseases but I get results as binary (like 1.0 and 0.0). How can I get accuracy of these (like 0.7213)? Training code: Single prediction code: The file structures is like: test_set training set   Training set structure is also the same as test set. Update: As you clarified in the comments, you are looking for the probabilities of each class given one single test sample. Therefore you can use predict method. However, note that you must first preprocess the image the same way you have done in the training phase: The result would be the probability of the given image belonging to class one (i.e. positive class). If you have a generator for test data, then you can use evaluate_generator() to get the loss as well as the accuracy (or any other metric you have set) of the model on the test data. For example, right after fitting the model, i.e. using fit_generator, you can use evaluate_generator on your test data generator, i.e. test_set:Is there a way to reload the weights from a certain epoch or the best weights from the model checkpoint files created by ModelCheckpoint once the training is over? I have trained that trained for 10 epochs and created a checkpoint that only saved weights after each epoch. The final epoch's val_categorical_accuracy is a bit lower than epoch no. 5. I know I should have set save_best_only=True but I missed that. What are my options here? Thanks for your help in advance. Below is my implementation: If the filepath doesn't contain formatting options like {epoch} then filepath will be overwritten by each new better model. In your case, that's why you can't get the weight at a specific epoch (e.g epoch 5). Your option here, however, is to choose the formatting option in the ModelCheckpoint callback during training time. Such as This will save the model weight (in .h5 format) at each epoch, in a different but convenient way. Additionally, if we choose save_best_only to True, it will save best weights in the same way. Code Example Here is one end-to-end working example for reference. We will save model weights at each epoch in a convenient way with a formatting option that we will define the filepath parameter as follows: It will save the model weight at each epoch. And I will find every weight in my local disk. However, note that I used save_best_only = False, but If we set it to True, you then only get the best weight in the same way. Something like this:I have the following data (X) that is stored in a numpy array: The array is much larger, and it gets fed into this neural network: Y is binary ones and zeroes. Where 1 means that will ride a taxi or 0 that will not ride a taxi. My question stems from here if Sigmoid is an activation layer that performs binary classification or these probability outputs? Because I was expecting 1's and 0's I eventually assuming that these are probability outputs I created the following: Much of my confusion lies in binary classification how does a neural network handle them, and how does one get 1's and 0's. When you pass some input for prediction to your binary classifier (sigmoid activation in its last layer), it will give you matrices in which each row represents the probability of those inputs to be in class 1. In your case: Here, each score represents the possibility of each sample in X_test[:5] to be in class 1. From this point, in order to get class labels (e.g. 1 and 0), by default API uses the 0.5 threshold to consider each score belong to class 1 and class 0; more specifically, score greater than 0.5 are considered to class 1. But of course, we can tweak the threshold. Here is one dummy example Probabilities Class labelsI'm working on a binary classification problem. I had this situation that I used the logistic regression and support vector machine model imported from sklearn. These two models were fit with the same , imbalanced training data and class weights were adjusted. And they have achieved comparable performances. When I used these two pre-trained models to predict a new dataset. The LR model and the SVM models predicted similar number of instances as positives. And the predicted instances share a big overlap. However, when I looked at the probability scores of being classified as positives, the distribution by LR is from 0.5 to 1 while the SVM starts from around 0.1. I called the function model.predict(prediction_data) to find out the instances predicted as each class and the function
model.predict_proba(prediction_data) to give the probability scores of being classified as 0(neg) and 1(pos), and assume they all have a default threshold 0.5. There is no error in my code and I have no idea why the SVM predicted instances with probability scores < 0.5 as positives as well. Any thoughts on how to interpret this situation? That's a known fact in sklearn when it comes to binary classification problems with SVC(), which is reported, for instance, in these github issues
(here and here). Moreover, it is also
reported in the User guide where it is said that: In addition, the probability estimates may be inconsistent with the scores:
the “argmax” of the scores may not be the argmax of the probabilities; in binary classification, a sample may be labeled by predict as belonging to the positive class even if the output of predict_proba is less than 0.5; and similarly, it could be labeled as negative even if the output of predict_proba is more than 0.5. or directly within libsvm faq, where it is said that Let's just consider two-class classification here. After probability information is obtained in training, we do not have prob > = 0.5 if and only if decision value >= 0. All in all, the point is that: on one side, predictions are based on decision_function values: if the decision value computed on a new instance is positive, the predicted class is the positive class and viceversa. on the other side, as stated within one of the github issues, np.argmax(self.predict_proba(X), axis=1) != self.predict(X) which is where the inconsistency comes from. In other terms, in order to always have consistency on binary classification problems you would need a classifier whose predictions are based on the output of predict_proba() (which is btw what you'll get when considering calibrators), like so: I'd also suggest this post on the topic.I want all special tokens to always be available. How do I do this? My first attempt to give it to my tokenizer: but feels hacky. refs: seems useful: https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings I want to add standard tokens by adding the right "standard tokens" the solution provided didn't work for me since the .bos_token is still None. See: code: I do not entirely understand what you're trying to accomplish, but here are some notes that might help: T5 documentation shows that T5 has only three special tokens (</s>, <unk> and <pad>). You can also see this in the T5Tokenizer class definition. I am confident this is because the original T5 model was trained only with these special tokens (no BOS, no MASK, no CLS). Running, e.g., will show you these three tokens as well as the <extra_id_*> tokens. Is there a reason you want the other tokens like BOS? (Edit - to answer your comments):
(I really think you would benefit from reading the linked documentation at huggingface. The point of a pretrained model is to take advantage of what has already been done. T5 does not use BOS nor CLS in the way you seem to be imagining. Maybe you can get it to work, but IMO it makes more sense to adapt the task you want to solve to the T5 approach) </s> is the sep token and is already available. As I understand, for the T5 model, masking (for the sake of ignoring loss) is implemented using attention_mask. On the other hand, if you want to "fill in the blank" then <extra_id> is used to indicate to the model that it should predict the missing token (this is how semi-supervised pretraining is done). See the section on training in the documentation. BOS is similar - T5 is not trained to use a BOS token. (E.g. (again from documentation), Note that T5 uses the pad_token_id as the decoder_start_token_id, so
when doing generation without using generate(), make sure you start it
with the pad_token_id. t5 does not use the CLS token. If you want to do classification, you should finetune a new task (or find a corresponding one done in pretraining), finetuning the model to generate a word (or words) that correspond to the classifications you want.
(again from documentation:) Build model inputs from a sequence or a pair of sequence for sequence
classification tasks by concatenating and adding special tokens. A
sequence has the following format: I think this is correct. Please correct me if I'm wrong: left comment from doc that inspired my answer: it was in hf's tokenization_utils_base.py I think the right answer is here: https://stackoverflow.com/a/73361984/1601580 Links can be bad answers so here is the code: Feedback is always welcomed.I am trying to create a simple CNN to classify images in MNIST dataset. The model achieved an acceptable accuracy but I noticed that the model is trained only on 1875 images in each epoch. What could be the cause of it? How can it be fixed? model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) model.fit(train_images,train_labels,epochs=5) screenshot of model in colab screenshot of trained model There's no problem with the training. Model is being trained on 1875 batches of 32 images each, not 1875 images. 1875*32 = 60000 imagesCan I use AdaBoost with random forest as a base classifier? I searched on the internet and I didn't find anyone who does it. Like in the following code; I try to run it but it takes a lot of time: I tried with the GridSearchCV, I added the RF classifier into the AdaBoost parameters.
if I use it would the accuracy increase? No wonder you have not actually seen anyone doing it - it is an absurd and bad idea. You are trying to build an ensemble (Adaboost) which in itself consists of ensemble base classifiers (RFs) - essentially an "ensemble-squared"; so, no wonder about the high computation time. But even if it was practical, there are good theoretical reasons not to do it; quoting from my own answer in Execution time of AdaBoost with SVM base classifier: Adaboost (and similar ensemble methods) were conceived using decision trees as base classifiers (more specifically, decision stumps, i.e. DTs with a depth of only 1); there is good reason why still today, if you don't specify explicitly the base_classifier argument, it assumes a value of DecisionTreeClassifier(max_depth=1). DTs are suitable for such ensembling because they are essentially unstable classifiers, which is not the case with SVMs, hence the latter are not expected to offer much when used as base classifiers. On top of this, SVMs are computationally much more expensive than decision trees (let alone decision stumps), which is the reason for the long processing times you have observed. The argument holds for RFs, too - they are not unstable classifiers, hence there is not any reason to actually expect performance improvements when using them as base classifiers for boosting algorithms, like Adaboost. Short answer:
It's not impossible.
I don't know if there's anything wrong with doing so in theory, but I tried this once and the accuracy increased. Long answer: I tried it on a typical dataset with n rows of p real-valued features, and a label list of length n. In case it matters, they are embeddings of nodes in a graph obtained by the DeepWalk algorithm, and the nodes are categorized into two classes. I trained a few classification models on this data using 5-fold cross validation, and measured common evaluation metrics for them (precision, recall, AUC etc.). The models I have used are SVM, logistic regression, random Forest, 2-layer perceptron and Adaboost with random forest classifiers. The last model, Adaboost with random forest classifiers, yielded the best results (95% AUC compared to multilayer perceptron's 89% and random forest's 88%). Sure, now the runtime has increased by a factor of, let's say, 100, but it's still about 20 mins, so it's not a constraint to me. Here's what I thought: Firstly, I'm using cross validation, so there's probably no overfitting flying under the radar. Secondly, both are ensemble learning methods, but random forest is a bagging method, wheras Adaboost is a boosting technique. Perhaps they're still different enough for their combination to make sense?I am playing around a bit bit with Microsofts ELL library/compiler to deploy a simple learning algorithm to a micro controller. But my knowledge regarding embedded development has been better in the past. The Problem is the following: ELL creates an LLVM IR file and a C Header file from a CNTK machine learning model (There are no pure c/c++ files). So far so good. Now I can use the IR to tell llc to make an assembler or object file for the desired target from it (ARM Cortex M4 in my case). So I end up with a header file model.h and an assembler file model.s or an object file model.o.
Now I want to include this model with the header and the precompiled model in my embedded project.
For developing, I use the Bosch XDK, the IDE is basically Eclipse.
So, is there a way, that I can include the precompiled model in my code? When yes, how? And how do I correctly include it in Eclipse? Or do I have to do further steps? I also thought about making a static library out of the object file, but I do not have any experience on this and my tries did not end successfully so far. Thanks for your kind help. If you make a static library from the object file, the linker will simply extract the object file and link it.  That is an unnecessary step, you can add the object file to the linker command line directly.  Alternatively add the .s source file to your project - the default build rules should identify it as an assembly language file and invoke the assembler rather then the compiler.I have a very large amount of data in the form of matrix.I have already clustered it using k-means clustering in MATLAB R2013a. I want the exact coordinates of the centroid of each cluster formed.. Is it possible using any  formula or anything else?  I want to find out the centroid of each cluster so that whenever some new data arrives in matrix, i can compute its distance from each centroid so as to find out the cluster to which new data will belong My data is heterogeneous in nature.So,its difficult to find out average of data of each cluster.So, i am trying to write some code for printing the centroid location automatically. In MATLAB, use  instead of  As per the documentation: [idx,C] = kmeans(..) returns the k cluster centroid locations in the k-by-p matrix C. The centroid is simply evaluated as the average value of all the points' coordinates that are assigned to that cluster. If you have the assignments {point;cluster} you can easily evaluate the centroid: let's say you have a given cluster with n points assigned to it and these points are a1,a2,...,an. You can evaluate the centroid for such cluster by using: Obviously you can run this process in a loop, depending on how your data structure (i.e. the assignment point/centroid) is organized.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. In Computer Vision and Object Detection, a common evaluation method is mAP.
What is it and how is it calculated? Quotes are from the above mentioned Zisserman paper - 4.2 Evaluation of Results (Page 11): First an "overlap criterion" is defined as an intersection-over-union greater than 0.5. (e.g. if a predicted box satisfies this criterion with respect to a ground-truth box, it is considered a detection). Then a matching is made between the GT boxes and the predicted boxes using this "greedy" approach: Detections output by a method were assigned to ground truth objects
  satisfying the overlap criterion in order ranked by the (decreasing)
  confidence output. Multiple detections of the same object in an image
  were considered false detections e.g. 5 detections of a single object
  counted as 1 correct detection and 4 false detections Hence each predicted box is either True-Positive or False-Positive.
Each ground-truth box is True-Positive.
There are no True-Negatives. Then the average precision is computed by averaging the precision values on the precision-recall curve where the recall is in the range [0, 0.1, ..., 1] (e.g. average of 11 precision values). To be more precise, we consider a slightly corrected PR curve, where for each curve point (p, r), if there is a different curve point (p', r') such that p' > p and r' >= r, we replace p with maximum p' of those points. What is still unclear to me is what is done with those GT boxes that are never detected (even if the confidence is 0). This means that there are certain recall values that the precision-recall curve will never reach, and this makes the average precision computation above undefined. Edit: Short answer: in the region where the recall is unreachable, the precision drops to 0. One way to explain this is to assume that when the threshold for the confidence approaches 0, an infinite number of predicted bounding boxes light up all over the image.  The precision then immediately goes to 0 (since there is only a finite number of GT boxes) and the recall keeps growing on this flat curve until we reach 100%. mAP is Mean Average Precision. Its use is different in the field of Information Retrieval (Reference [1] [2] )and Multi-Class classification (Object Detection) settings. To calculate it for Object Detection, you calculate the average precision for each class in your data based on your model predictions. Average precision is related to the area under the precision-recall curve for a class. Then Taking the mean of these average individual-class-precision gives you the Mean Average Precision.  To calculate Average Precision, see [3] For detection, a common way to determine if one object proposal was
  right is Intersection over Union (IoU, IU). This takes the set A
  of proposed object pixels and the set of true object pixels B and
  calculates:  Commonly, IoU > 0.5 means that it was a hit, otherwise it was a fail. For each class, one can calculate the The mAP (mean average precision) is then:  Note:
If one wants better proposals, one does increase the IoU from 0.5 to a higher value (up to 1.0 which would be perfect). One can denote this with mAP@p, where p \in (0, 1) is the IoU. mAP@[.5:.95] means that the mAP is calculated over multiple thresholds and then again being averaged Edit: 
For more detailed Information see the COCO Evaluation metrics I think the important part here is linking how object detection can be considered the same as the standard information retrieval problems for which there exists at least one excellent description of average precision. The output of some object detection algorithm is a set of proposed bounding boxes, and for each one, a confidence and classification scores (one score per class). Let's ignore the classification scores for now, and use the confidence as input to a threshold binary classification. Intuitively, the average precision is an aggregation over all choices for the threshold/cut-off value. But wait; in order to calculate precision, we need to know if a box is correct! This is where it gets confusing/difficult; as opposed to typical information retrieval problems, we actually have an extra level of classification here. That is, we can't do exact matching between boxes, so we need to classify if a bounding box is correct or not. The solution is to essentially do a hard-coded classification on the box dimensions; we check if it sufficiently overlaps with any ground truth to be considered 'correct'. The threshold for this part is chosen by common sense. The dataset you are working on will likely define what this threshold for a 'correct' bounding box is. Most datasets just set it at 0.5 IoU and leave it at that (I recommend doing a few manual IoU calculations [they're not hard] to get a feel for how strict IoU of 0.5 actually is). Now that we have actually defined what it means to be 'correct', we can just use the same process as information retrieval. To find mean average precision (mAP), you just stratify your proposed boxes based on the maximum of the classification scores associated with those boxes, then average (take the mean) of the average precision (AP) over the classes. TLDR; make the distinction between determining if a bounding box prediction is 'correct' (extra level of classification) and evaluating how well the box confidence informs you of a 'correct' bounding box prediction (completely analogous to information retrieval case) and the typical descriptions of mAP will make sense. It's worth noting that Area under the Precision/Recall curve is the same thing as average precision, and we are essentially approximating this area with the trapezoidal or right-hand rule for approximating integrals. Definition: mAP → mean Average Precision In most of the object detection contests, there are many categories to detect, and the evaluation of the model is performed on one specific category each time, the eval result is the AP of that category. When every category is evaluated, the mean of all APs is calculated as the final result of the model, which is mAP.  Intersection Over Union (IOU) is measure based on Jaccard Index that evaluates the overlap between two bounding boxes. It requires a ground truth bounding box and a predicted bounding box By applying the IOU we can tell if a detection is valid (True Positive) or not (False Positive).IOU is given by the overlapping area between the predicted bounding box and the ground truth bounding box divided by the area of union between them.Suppose I have a Tensorflow tensor. How do I get the dimensions (shape) of the tensor as integer values? I know there are two methods, tensor.get_shape() and tf.shape(tensor), but I can't get the shape values as integer int32 values. For example, below I've created a 2-D tensor, and I need to get the number of rows and columns as int32 so that I can call reshape() to create a tensor of shape (num_rows * num_cols, 1). However, the method tensor.get_shape() returns values as Dimension type, not int32. To get the shape as a list of ints, do tensor.get_shape().as_list(). To complete your tf.shape() call, try tensor2 = tf.reshape(tensor, tf.TensorShape([num_rows*num_cols, 1])). Or you can directly do tensor2 = tf.reshape(tensor, tf.TensorShape([-1, 1])) where its first dimension can be inferred. Another way to solve this is like this: This will return the int value of the Dimension object. 2.0 Compatible Answer: In Tensorflow 2.x (2.1), you can get the dimensions (shape) of the tensor as integer values, as shown in the Code below: Method 1 (using tf.shape): Method 2 (using tf.get_shape()): for a 2-D tensor, you can get the number of rows and columns as int32 using the following code: Another simple solution is to use map() as follows: This converts all the Dimension objects to int In later versions (tested with TensorFlow 1.14) there's a more numpy-like way to get the shape of a tensor. You can use tensor.shape to get the shape of the tensor.This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed 2 years ago. In an LSTM network (Understanding LSTMs), why does the input gate and output gate use tanh? What is the intuition behind this? It is just a nonlinear transformation? If it is, can I change both to another activation function (e.g., ReLU)? Sigmoid specifically, is used as the gating function for the three gates (in, out, and forget) in LSTM, since it outputs a value between 0 and 1, and it can either let no flow or complete flow of information throughout the gates. On the other hand, to overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero. Tanh is a good function with the above property. A good neuron unit should be bounded, easily differentiable, monotonic (good for convex optimization) and easy to handle. If you consider these qualities, then I believe you can use ReLU in place of the tanh function since they are very good alternatives of each other. But before making a choice for activation functions, you must know what the advantages and disadvantages of your choice over others are. I am shortly describing some of the activation functions and their advantages. Sigmoid Mathematical expression: sigmoid(z) = 1 / (1 + exp(-z)) First-order derivative: sigmoid'(z) = -exp(-z) / 1 + exp(-z)^2 Advantages: Tanh Mathematical expression: tanh(z) = [exp(z) - exp(-z)] / [exp(z) + exp(-z)] First-order derivative: tanh'(z) = 1 - ([exp(z) - exp(-z)] / [exp(z) + exp(-z)])^2 = 1 - tanh^2(z) Advantages: Hard Tanh Mathematical expression: hardtanh(z) = -1 if z < -1; z if -1 <= z <= 1; 1 if z > 1 First-order derivative: hardtanh'(z) = 1 if -1 <= z <= 1; 0 otherwise Advantages: ReLU Mathematical expression: relu(z) = max(z, 0) First-order derivative: relu'(z) = 1 if z > 0; 0 otherwise Advantages: Leaky ReLU Mathematical expression: leaky(z) = max(z, k dot z) where 0 < k < 1 First-order derivative: relu'(z) = 1 if z > 0; k otherwise Advantages: This paper explains some fun activation function. You may consider to read it. LSTMs manage an internal state vector whose values should be able to increase or decrease when we add the output of some function. Sigmoid output is always non-negative; values in the state would only increase. The output from tanh can be positive or negative, allowing for increases and decreases in the state. That's why tanh is used to determine candidate values to get added to the internal state. The GRU cousin of the LSTM doesn't have a second tanh, so in a sense the second one is not necessary. Check out the diagrams and explanations in Chris Olah's Understanding LSTM Networks for more. The related question, "Why are sigmoids used in LSTMs where they are?" is also answered based on the possible outputs of the function: "gating" is achieved by multiplying by a number between zero and one, and that's what sigmoids output. There aren't really meaningful differences between the derivatives of sigmoid and tanh; tanh is just a rescaled and shifted sigmoid: see Richard Socher's Neural Tips and Tricks. If second derivatives are relevant, I'd like to know how.My training set has 970 samples and validation set has 243 samples. How big should batch size and number of epochs be when fitting a model to optimize the val_acc? Is there any sort of rule of thumb to use based on data input size? Since you have a pretty small dataset (~ 1000 samples), you would probably be safe using a batch size of 32, which is pretty standard. It won't make a huge difference for your problem unless you're training on hundreds of thousands or millions of observations.  To answer your questions on Batch Size and Epochs: In general: Larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster. It's definitely problem dependent.  In general, the models improve with more epochs of training, to a point. They'll start to plateau in accuracy as they converge. Try something like 50 and plot number of epochs (x axis) vs. accuracy (y axis). You'll see where it levels out.  What is the type and/or shape of your data? Are these images, or just tabular data? This is an important detail.  Great answers above. Everyone gave good inputs. Ideally, this is the sequence of the batch sizes that should be used:  I use Keras to perform non-linear regression on speech data. Each of my speech files gives me features that are 25000 rows in a text file, with each row containing 257 real valued numbers. I use a batch size of 100, epoch 50 to train  Sequential model in Keras with 1 hidden layer. After 50 epochs of training, it converges quite well to a low val_loss.  I used Keras to perform non linear regression for market mix modelling. I got best results with a batch size of 32 and epochs = 100 while training a Sequential model in Keras with 3 hidden layers. Generally batch size of 32 or 25 is good, with epochs = 100 unless you have large dataset. in case of large dataset you can go with batch size of 10 with epochs b/w 50 to 100.
Again the above mentioned figures have worked fine for me. Well I haven't seen the answer I was looking for so I made a research myself.
 In this article this is said: In this paper, they were trying 256,512,1024 batch sizes and the performance of all models were in the standard deviation of each other. This means that the batch size didn't have any significant influence on performance. Final word: If you find this post useful, please up-vote & comment. Took the time to share it with you. Thanks With Keras you can make use of tf.keras.callbacks.EarlyStopping which automatically stops training if the monitored loss has stopped improving. You can allow epochs with no improvement using the parameter patience. It helps to find the plateau from which you can go on refining the number of epochs or may even suffice to reach your goal without having to deal with epochs at all. From one study, a rule of thumb is that batch size and learning_rates have a high correlation, to achieve good performance. High learning rate in the study below means 0.001, small learning rate is 0.0001. In my case, I usually have a high batch size of 1024 to 2048 for a dataset of a million records for example, with learning rate at 0.001 (default of Adam optimizer).  However, i also use a cyclical learning rate scheduler which changes this value during fitting, which is another topic. 'In this paper, we compared the performance of CNN using different batch sizes and different learning rates. According to our results, we can conclude that the learning rate and the batch size have a significant impact on the performance of the network. There is a high correlation between the learning rate and the batch size, when the learning rates are high, the large batch size performs better than with small learning rates. We recommend choosing small batch size with low learning rate. In practical terms, to determine the optimum batch size, we recommend trying smaller batch sizes first(usually 32 or 64), also keeping in mind that small batch sizes require small learning rates. The number of batch sizes should be a power of 2 to take full advantage of the GPUs processing. Subsequently, it is possible to increase the batch size value till satisfactory results are obtained.' - https://www.sciencedirect.com/science/article/pii/S2405959519303455 Epochs is up to your wish, depending upon when validation loss stops improving further. This much should be batch size:What is the difference between sparse_categorical_crossentropy and categorical_crossentropy? When should one loss be used as opposed to the other? For example, are these losses suitable for linear regression? Simply: Consider a classification problem with 5 categories (or classes). In the case of cce, the one-hot target may be [0, 1, 0, 0, 0] and the model may predict [.2, .5, .1, .1, .1] (probably right) In the case of scce, the target index may be [1] and the model may predict: [.5]. Consider now a classification problem with 3 classes. Many categorical models produce scce output because you save space, but lose A LOT of information (for example, in the 2nd example, index 2 was also very close.)  I generally prefer cce output for model reliability. There are a number of situations to use scce, including: 220405: response to "one-hot encoding" comments: one-hot encoding is used for a category feature INPUT to select a specific category (e.g. male versus female).  This encoding allows the model to train more efficiently: training weight is a product of category, which is 0 for all categories except for the given one. cce and scce are a model OUTPUT. cce is a probability array of each category, totally 1.0. scce shows the MOST LIKELY category, totally 1.0. scce is technically a one-hot array, just like a hammer used as a door stop is still a hammer, but its purpose is different. cce is NOT one-hot. I was also confused with this one. Fortunately, the excellent keras documentation came to the rescue. Both have the same loss function and are ultimately doing the same thing, only difference is in the representation of the true labels. Use this crossentropy loss function when there are two or more label
classes. We expect labels to be provided in a one_hot representation. Use this crossentropy loss function when there are two or more label
classes. We expect labels to be provided as integers. One good example of the sparse-categorical-cross-entropy is the fasion-mnist dataset. From the TensorFlow source code, the sparse_categorical_crossentropy is defined as categorical crossentropy with integer targets: From the TensorFlow source code, the categorical_crossentropy is defined as categorical cross-entropy between an output tensor and a target tensor. The meaning of integer targets is that the target labels should be in the form of an integer list that shows the index of class, for example: For sparse_categorical_crossentropy, For class 1 and class 2 targets, in a 5-class classification problem, the list should be [1,2]. Basically, the targets should be in integer form in order to call sparse_categorical_crossentropy. This is called sparse since the target representation requires much less space than one-hot encoding. For example, a batch with b targets and k classes needs b * k space to be represented in one-hot, whereas a batch with b targets and k classes needs b space to be represented in integer form.   For categorical_crossentropy, for class 1 and class 2 targets, in a 5-class classification problem, the list should be [[0,1,0,0,0], [0,0,1,0,0]]. Basically, the targets should be in one-hot form in order to call categorical_crossentropy. The representation of the targets are the only difference, the results should be the same since they are both calculating categorical crossentropy.In a slide within the introductory lecture on machine learning by Stanford's Andrew Ng at Coursera, he gives the following one line Octave solution to the cocktail party problem given the audio sources are recorded by two spatially separated microphones: At the bottom of the slide is "source: Sam Roweis, Yair Weiss, Eero Simoncelli" and at the bottom of an earlier slide is "Audio clips courtesy of Te-Won Lee". In the video, Professor Ng says, "So you might look at unsupervised learning like this and ask, 'How complicated is it to implement this?' It seems like in order to build this application, it seems like to do this audio processing, you would write a ton of code, or maybe link  into a bunch of C++ or Java libraries that process audio. It seems like it would be a really complicated program to do this audio: separating out audio and so on. It turns out the algorithm to do what you just heard, that can be done with just one line of code ... shown right here. It did take researchers a long time to come up with this line of code. So I'm not saying this is an easy problem. But it turns out that when you use the right programming environment many learning algorithms will be really short programs." The separated audio results played in the video lecture are not perfect but, in my opinion, amazing. Does anyone have any insight on how that one line of code performs so well? In particular, does anyone know of a reference that explains the work of Te-Won Lee, Sam Roweis, Yair Weiss, and Eero Simoncelli with respect to that one line of code? UPDATE To demonstrate the algorithm's sensitivity to microphone separation distance, the following simulation (in Octave) separates the tones from two spatially separated tone generators. After about 10 minutes of execution on my laptop computer, the simulation generates the following three figures illustrating the two isolated tones have the correct frequencies. 

 However, setting the microphone separation distance to zero (i.e., dMic = 0) causes the simulation to instead generate the following three figures illustrating the simulation could not isolate a second tone (confirmed by the single significant diagonal term returned in svd's s matrix). 

 I was hoping the microphone separation distance on a smartphone would be large enough to produce good results but setting the microphone separation distance to 5.25 inches (i.e., dMic = 0.1333 meters) causes the simulation to generate the following, less than encouraging, figures illustrating higher frequency components in the first isolated tone. 

 I was trying to figure this out as well, 2 years later. But I got my answers; hopefully it'll help someone.  You need 2 audio recordings. You can get audio examples from http://research.ics.aalto.fi/ica/cocktail/cocktail_en.cgi.  reference for implementation is http://www.cs.nyu.edu/~roweis/kica.html ok, here's code -  x(t) is the original voice from one channel/microphone.  X = repmat(sum(x.*x,1),size(x,1),1).*x)*x' is an estimation of the power spectrum of x(t). Although X' = X, the intervals between rows and columns are not the same at all. Each row represents the time of the signal, while each column is frequency. I guess this is an estimation and simplification of a more strict expression called spectrogram.  Singular Value Decomposition on spectrogram is used to factorize the signal into different components based on spectrum information. Diagonal values in s are the magnitude of different spectrum components. The rows in u and columns in v' are the orthogonal vectors that map the frequency component with the corresponding magnitude to X space. I don't have voice data to test, but in my understanding, by means of SVD, the components fall into the similar orthogonal vectors are hopefully be clustered with the help of unsupervised learning. Say, if the first 2 diagonal magnitudes from s are clustered, then u*s_new*v' will form the one-person-voice, where s_new is the same of s except all the elements at (3:end,3:end) are eliminated. Two articles about the sound-formed matrix and SVD are for your reference.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I am using LibSVM to classify some documents. The documents seem to be a bit difficult to classify as the final results show. However, I have noticed something while training my models. and that is: If my training set is for example 1000 around 800 of them are selected as support vectors.
I have looked everywhere to find if this is a good thing or bad. I mean is there a relation between the number of support vectors and the classifiers performance?
I have read this previous post but I am performing a parameter selection and also I am sure that the attributes in the feature vectors are all ordered.
I just need to know the relation.
Thanks.
p.s: I use a linear kernel. Support Vector Machines are an optimization problem. They are attempting to find a hyperplane that divides the two classes with the largest margin.  The support vectors are the points which fall within this margin.  It's easiest to understand if you build it up from simple to more complex. Hard Margin Linear SVM In a training set where the data is linearly separable, and you are using a hard margin (no slack allowed), the support vectors are the points which lie along the supporting hyperplanes (the hyperplanes parallel to the dividing hyperplane at the edges of the margin)  All of the support vectors lie exactly on the margin.  Regardless of the number of dimensions or size of data set, the number of support vectors could be as little as 2. Soft-Margin Linear SVM But what if our dataset isn't linearly separable? We introduce soft margin SVM. We no longer require that our datapoints lie outside the margin, we allow some amount of them to stray over the line into the margin. We use the slack parameter C to control this. (nu in nu-SVM) This gives us a wider margin and greater error on the training dataset, but improves generalization and/or allows us to find a linear separation of data that is not linearly separable.  Now, the number of support vectors depends on how much slack we allow and the distribution of the data. If we allow a large amount of slack, we will have a large number of support vectors.  If we allow very little slack, we will have very few support vectors. The accuracy depends on finding the right level of slack for the data being analyzed. Some data it will not be possible to get a high level of accuracy, we must simply find the best fit we can. Non-Linear SVM This brings us to non-linear SVM. We are still trying to linearly divide the data, but we are now trying to do it in a higher dimensional space.  This is done via a kernel function, which of course has its own set of parameters. When we translate this back to the original feature space, the result is non-linear:  Now, the number of support vectors still depends on how much slack we allow, but it also depends on the complexity of our model. Each twist and turn in the final model in our input space requires one or more support vectors to define.  Ultimately, the output of an SVM is the support vectors and an alpha, which in essence is defining how much influence that specific support vector has on the final decision. Here, accuracy depends on the trade-off between a high-complexity model which may over-fit the data and a large-margin which will incorrectly classify some of the training data in the interest of better generalization.  The number of support vectors can range from very few to every single data point if you completely over-fit your data.  This tradeoff is controlled via C and through the choice of kernel and kernel parameters. I assume when you said performance you were referring to accuracy, but I thought I would also speak to performance in terms of computational complexity.  In order to test a data point using an SVM model, you need to compute the dot product of each support vector with the test point. Therefore the computational complexity of the model is linear in the number of support vectors.  Fewer support vectors means faster classification of test points. A good resource:
A Tutorial on Support Vector Machines for Pattern Recognition 800 out of 1000 basically tells you that the SVM needs to use almost every single training sample to encode the training set.  That basically tells you that there isn't much regularity in your data.   Sounds like you have major issues with not enough training data.  Also, maybe think about some specific features that separate this data better. Both number of samples and number of attributes may influence the number of support vectors, making model more complex. I believe you use words or even ngrams as attributes, so there are quite many of them, and natural language models are very complex themselves. So, 800 support vectors of 1000 samples seem to be ok. (Also pay attention to @karenu's comments about C/nu parameters that also have large effect on SVs number). To get intuition about this recall SVM main idea. SVM works in a multidimensional feature space and tries to find hyperplane that separates all given samples. If you have a lot of samples and only 2 features (2 dimensions), the data and hyperplane may look like this:   Here there are only 3 support vectors, all the others are behind them and thus don't play any role. Note, that these support vectors are defined by only 2 coordinates. Now imagine that you have 3 dimensional space and thus support vectors are defined by 3 coordinates.   This means that there's one more parameter (coordinate) to be adjusted, and this adjustment may need more samples to find optimal hyperplane. In other words, in worst case SVM finds only 1 hyperplane coordinate per sample.  When the data is well-structured (i.e. holds patterns quite well) only several support vectors may be needed - all the others will stay behind those. But text is very, very bad structured data. SVM does its best, trying to fit sample as well as possible, and thus takes as support vectors even more samples than drops. With increasing number of samples this "anomaly" is reduced (more insignificant samples appear), but absolute number of support vectors stays very high.  SVM classification is linear in the number of support vectors (SVs). The number of SVs is in the worst case equal to the number of training samples, so 800/1000 is not yet the worst case, but it's still pretty bad. Then again, 1000 training documents is a small training set. You should check what happens when you scale up to 10000s or more documents. If things don't improve, consider using linear SVMs, trained with LibLinear, for document classification; those scale up much better (model size and classification time are linear in the number of features and independent of the number of training samples). There is some confusion between sources. In the textbook ISLR 6th Ed, for instance, C is described as a "boundary violation budget" from where it follows that higher C will allow for more boundary violations and more support vectors.
But in svm implementations in R and python the parameter C is implemented as "violation penalty" which is the opposite and then you will observe that for higher values of C there are fewer support vectors.Given a linearly separable dataset, is it necessarily better to use a a hard margin SVM over a soft-margin SVM? I would expect soft-margin SVM to be better even when training dataset is linearly separable. The reason is that in a hard-margin SVM, a single outlier can determine the boundary, which makes the classifier overly sensitive to noise in the data. In the diagram below, a single red outlier essentially determines the boundary, which is the hallmark of overfitting  To get a sense of what soft-margin SVM is doing, it's better to look at it in the dual formulation, where you can see that it has the same margin-maximizing objective (margin could be negative) as the hard-margin SVM, but with an additional constraint that each lagrange multiplier associated with support vector is bounded by C. Essentially this bounds the influence of any single point on the decision boundary, for derivation, see Proposition 6.12 in Cristianini/Shaw-Taylor's "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods". The result is that soft-margin SVM could choose decision boundary that has non-zero training error even if dataset is linearly separable, and is less likely to overfit. Here's an example using libSVM on a synthetic problem. Circled points show support vectors. You can see that decreasing C causes classifier to sacrifice linear separability in order to gain stability, in a sense that influence of any single datapoint is now bounded by C.  Meaning of support vectors: For hard margin SVM, support vectors are the points which are "on the margin". In the picture above, C=1000 is pretty close to hard-margin SVM, and you can see the circled points are the ones that will touch the margin (margin is almost 0 in that picture, so it's essentially the same as the separating hyperplane) For soft-margin SVM, it's easer to explain them in terms of dual variables. Your support vector predictor in terms of dual variables is the following function.  Here, alphas and b are parameters that are found during training procedure, xi's, yi's are your training set and x is the new datapoint. Support vectors are datapoints from training set which are are included in the predictor, ie, the ones with non-zero alpha parameter. In my opinion, Hard Margin SVM overfits to a particular dataset and thus can not generalize. Even in a linearly separable dataset (as shown in the above diagram), outliers well within the boundaries can influence the margin. Soft Margin SVM has more versatility because we have control over choosing the support vectors by tweaking the C.I am applying transfer-learning on a pre-trained network using the GPU version of keras. I don't understand how to define the parameters max_queue_size, workers, and use_multiprocessing. If I change these parameters (primarily to speed-up learning), I am unsure whether all data is still seen per epoch. max_queue_size: maximum size of the internal training queue which is used to "precache" samples from the generator  Question: Does this refer to how many batches are prepared on CPU? How is it related to workers? How to define it optimally? workers:  number of threads generating batches in parallel. Batches are computed in parallel on the CPU and passed on the fly onto the GPU for neural network computations  Question: How do I find out how many batches my CPU can/should generate in parallel? use_multiprocessing:  whether to use process-based threading Question: Do I have to set this parameter to true if I change workers? Does it relate to CPU usage? Related questions can be found here: What does worker mean in fit_generator in Keras? What is the parameter “max_q_size” used for in “model.fit_generator”? A detailed example of how to use data generators with Keras. I am using fit_generator() as follows: The specs of my machine are: Q_0:  Question: Does this refer to how many batches are prepared on CPU? How is it related to workers? How to define it optimally? From the link you posted, you can learn that your CPU keeps creating batches until the queue is at the maximum queue size or reaches the stop. You want to have batches ready for your GPU to "take" so that the GPU doesn't have to wait for the CPU. 
An ideal value for the queue size would be to make it large enough that your GPU is always running near the maximum and never has to wait for the CPU to prepare new batches.  Q_1: Question: How do I find out how many batches my CPU can/should generate in parallel? If you see that your GPU is idling and waiting for batches, try to increase the amount of workers and perhaps also the queue size. Q_2: Do I have to set this parameter to true if I change workers? Does it relate to CPU usage? Here is a practical analysis of what happens when you set it to True or False. Here is a recommendation to set it to False to prevent freezing (in my setup True works fine without freezing). Perhaps someone else can increase our understanding of the topic. Try not to have a sequential setup, try to enable the CPU to provide enough data for the GPU.
 Also: You could (should?) create several questions the next time, so that it is easier to answer them.While training a tensorflow seq2seq model I see the following messages : What does it mean , does it mean I am having some resource allocation issues? Am running on Titan X 3500+ CUDA ,12 GB GPU TensorFlow has multiple memory allocators, for memory that will be used in different ways.  Their behavior has some adaptive aspects. In your particular case, since you're using a GPU, there is a PoolAllocator for CPU memory that is pre-registered with the GPU for fast DMA.  A tensor that is expected to be transferred from CPU to GPU, e.g., will be allocated from this pool. The PoolAllocators attempt to amortize the cost of calling a more expensive underlying allocator by keeping around a pool of allocated then freed chunks that are eligible for immediate reuse.  Their default behavior is to grow slowly until the eviction rate drops below some constant.  (The eviction rate is the proportion of free calls where we return an unused chunk from the pool to the underlying pool in order not to exceed the size limit.)  In the log messages above, you see "Raising pool_size_limit_" lines that show the pool size growing.  Assuming that your program actually has a steady state behavior with a maximum size collection of chunks it needs, the pool will grow to accommodate it, and then grow no more.  It behaves this way rather than simply retaining all chunks ever allocated so that sizes needed only rarely, or only during program startup, are less likely to be retained in the pool. These messages should only be a cause for concern if you run out of memory.  In such a case the log messages may help diagnose the problem.  Note also that peak execution speed may only be attained after the memory pools have grown to the proper size. tl;dr You are OOM.. 
try reducing the size of your network or training fewer models simultaneously or play around with batch size etc.I have the below F1 and AUC scores for 2 different cases Model 1: Precision: 85.11 Recall: 99.04 F1: 91.55 AUC: 69.94 Model 2: Precision: 85.1 Recall: 98.73 F1: 91.41 AUC: 71.69 The main motive of my problem to predict the positive cases correctly,ie, reduce the False Negative cases (FN). Should I use F1 score and choose Model 1 or use AUC and choose Model 2. Thanks  As a rule of thumb, every time you want to compare ROC AUC vs F1 Score, think about it as if you are comparing your model performance based on: Note that Sensitivity is the Recall (they are the same exact metric). Now we need to understand what are: Specificity, Precision and Recall (Sensitivity) intuitively! Specificity: is given by the following formula:  Intuitively speaking, if we have 100% specific model, that means it did NOT miss any True Negative, in other words, there were NO False Positives (i.e. negative result that is falsely labeled as positive). Yet, there is a risk of having a lot of False Negatives! Precision: is given by the following formula:
 Intuitively speaking, if we have a 100% precise model, that means it could catch all True positive but there were NO False Positive. Recall: is given by the following formula:  Intuitively speaking, if we have a 100% recall model, that means it did NOT miss any True Positive, in other words, there were NO False Negatives (i.e. a positive result that is falsely labeled as negative). Yet, there is a risk of having a lot of False Positives! As you can see, the three concepts are very close to each other!  As a rule of thumb, if the cost of having False negative is high, we want to increase the model sensitivity and recall (which are the exact same in regard to their formula)!. For instance, in fraud detection or sick patient detection, we don't want to label/predict a fraudulent transaction (True Positive) as non-fraudulent (False  Negative). Also, we don't want to label/predict a contagious sick patient (True Positive) as not sick (False Negative). This is because the consequences will be worse than a False Positive (incorrectly labeling a a harmless transaction as fraudulent or a non-contagious patient as contagious). On the other hand, if the cost of having False Positive is high, then we want to increase the model specificity and precision!. For instance, in email spam detection, we don't want to label/predict a non-spam email (True Negative) as spam (False Positive). On the other hand, failing to label a spam email as spam (False Negative) is less costly. It's given by the following formula:  F1 Score keeps a balance between Precision and Recall. We use it if there is uneven class distribution, as precision and recall may give misleading results! So we use F1 Score as a comparison indicator between Precision and Recall Numbers! It compares the Sensitivity vs (1-Specificity), in other words, compare the True Positive Rate vs False Positive Rate.  So, the bigger the AUROC, the greater the distinction between True Positives and True Negatives! In general, the ROC is for many different levels of thresholds and thus it has many F score values. F1 score is applicable for any particular point on the ROC curve. You may think of it as a measure of precision and recall at a particular threshold value whereas AUC is the area under the ROC curve. For F score to be high, both precision and recall should be high. Consequently,  when you have a data imbalance between positive and negative samples, you should always use F1-score because ROC averages over all possible thresholds! Further read: Credit Card Fraud: Handling highly imbalance classes and why Receiver Operating Characteristics Curve (ROC Curve) should not be used, and Precision/Recall curve should be preferred in highly imbalanced situations If you look at the definitions, you can that both AUC and F1-score optimize "something" together with the fraction of the sample labeled "positive" that is actually true positive. This "something" is: The difference becomes important when you have highly unbalanced or skewed classes: For example there are many more true negatives than true positives. Suppose you are looking at data from the general population to find people with a rare disease. There are far more people "negative" than "positive", and trying to optimize how well you are doing on the positive and the negative samples simultaneously, using AUC, is not optimal. You want the positive sample to include all positives if possible and you don't want it to be huge, due to a high false positive rate. So in this case you use the F1 score. Conversely if both classes make up 50% of your dataset, or both make up a sizable fraction, and you care about your performance in identifying each class equally, then you should use the AUC, which optimizes for both classes, positive and negative. just adding my 2 cents here: AUC does an implicit weighting of the samples, which F1 does not. In my last use case comparing the effectiveness of drugs on patients, it's easy to learn which drugs are generally strong, and which are weak. The big question is whether you can hit the outliers (the few positives for a weak drug or the few negatives for a strong drug). To answer that, you have to specifically weigh the outliers up using F1, which you don't need to do with AUC. to predict the positive cases correctly one can rewrite a bit your goal and get: when a case is really positive you want classify it as positive too. The probability of such event p(predicted_label = positive | true_label = positive) is a recall by definition. If you want to maximize this property of your model, you'd choose the Model 1.Update(July 2020): Question is 9 years old but still one that I'm deeply interested in. In the time since, machine learning(RNN's, CNN's, GANS,etc), new approaches and cheap GPU's have risen that enable new approaches. I thought it would be fun to revisit this question to see if there are new approaches. I am learning programming (Python and algorithms) and was trying to work on a project that I find interesting. I have created a few basic Python scripts, but I’m not sure how to approach a solution to a game I am trying to build. Here’s how the game will work: Users will be given items with a value. For example, They will then get a chance to choose any combo of them they like (i.e. 100 apples, 20 pears, and one orange). The only output the computer gets is the total value (in this example, it's currently $143). The computer will try to guess what they have. Which obviously it won’t be able to get correctly the first turn. The next turn the user can modify their numbers but no more than 5% of the total quantity (or some other percent we may chose. I’ll use 5% for example.). The prices of fruit can change(at random) so the total value may change based on that also (for simplicity I am not changing fruit prices in this example). Using the above example, on day 2 of the game, the user returns a value of $152 and $164 on day 3. Here's an example: *(I hope the tables show up right, I had to manually space them so hopefully it's not just doing it on my screen, if it doesn't work let me know and I'll try to upload a screenshot.) I am trying to see if I can figure out what the quantities are over time (assuming the user will have the patience to keep entering numbers). I know right now my only restriction is the total value cannot be more than 5% so I cannot be within 5% accuracy right now so the user will be entering it forever. What I have done so far Here’s my solution so far (not much). Basically, I take all the values and figure out all the possible combinations of them (I am done this part). Then I take all the possible combos and put them in a database as a dictionary (so for example for $143, there could be a dictionary entry {apple:143, Pears:0, Oranges :0}..all the way to {apple:0, Pears:1, Oranges :47}. I do this each time I get a new number so I have a list of all possibilities. Here’s where I’m stuck. In using the rules above, how can I figure out the best possible solution? I think I’ll need a fitness function that automatically compares the two days data and removes any possibilities that have more than 5% variance of the previous days data. Questions: So my question with user changing the total and me having a list of all the probabilities, how should I approach this? What do I need to learn? Is there any algorithms out there or theories that I can use that are applicable? Or, to help me understand my mistake, can you suggest what rules I can add to make this goal feasible (if it's not in its current state. I was thinking adding more fruits and saying they must pick at least 3, etc..)?  Also, I only have a vague understanding of genetic algorithms, but I thought I could use them here, if is there something I can use? I'm very very eager to learn so any advice or tips would be greatly appreciated (just please don't tell me this game is impossible). UPDATE: Getting feedback that this is hard to solve. So I thought I'd add another condition to the game that won't interfere with what the player is doing (game stays the same for them) but everyday the value of the fruits change price (randomly). Would that make it easier to solve? Because within a 5% movement and certain fruit value changes, only a few combinations are probable over time. Day 1, anything is possible and getting a close enough range is almost impossible, but as the prices of fruits change and the user can only choose a 5% change, then shouldn't (over time) the range be narrow and narrow. In the above example, if prices are volatile enough I think I could brute force a solution that gave me a range to guess in, but I'm trying to figure out if there's a more elegant solution or other solutions to keep narrowing this range over time. UPDATE2: After reading and asking around, I believe this is a hidden Markov/Viterbi problem that tracks the changes in fruit prices as well as total sum (weighting the last data point the heaviest). I'm not sure how to apply the relationship though. I think this is the case and could be wrong but at the least I'm starting to suspect this is a some type of machine learning problem. Update 3: I am created a test case (with smaller numbers) and a generator to help automate the user generated data and I am trying to create a graph from it to see what's more likely. Here's the code, along with the total values and comments on what the users actually fruit quantities are. We'll combine graph-theory and probability: On the 1st day, build a set of all feasible solutions. Lets denote the solutions set as A1={a1(1), a1(2),...,a1(n)}. On the second day you can again build the solutions set A2. Now, for each element in A2, you'll need to check if it can be reached from each element of A1 (given x% tolerance). If so - connect A2(n) to A1(m). If it can't be reached from any node in A1(m) - you can delete this node. Basically we are building a connected directed acyclic graph. All paths in the graph are equally likely. You can find an exact solution only when there is a single edge from Am to Am+1 (from a node in Am to a node in Am+1). Sure, some nodes appear in more paths than other nodes. The probability for each node can be directly deduced based on the number of paths that contains this node. By assigning a weight to each node, which equals to the number of paths that leads to this node, there is no need to keep all history, but only the previous day. Also, have a look at non-negative-values linear diphantine equations - A question I asked a while ago. The accepted answer is a great way to enumarte all combos in each step. Disclaimer: I changed my answer dramatically after temporarily deleting my answer and re-reading the question carefully as I misread some critical parts of the question. While still referencing similar topics and algorithms, the answer was greatly improved after I attempted to solve some of the problem in C# myself. First, I would like to state what I see two main problems here: The sheer number of possible solutions. Knowing only the number of items and the total value, lets say 3 and 143 for example, will yield a lot of possible solutions. Plus, it is not easy to have an algorithm picking valid solution without inevitably trying invalid solutions (total not equal to 143.) When possible solutions are found for a given day Di, one must find a way to eliminate potential solutions with the added information given by { Di+1 .. Di+n }. Let's lay down some bases for the upcoming examples: In order to solve this more easily I took the liberty to change one constraint, which makes the algorithm converge faster: This rule enables us to rule out solutions more easily. And, with non-tiny ranges, renders Backtracking algorithms still useless, just like your original problem and rules. In my humble opinion, this rule is not the essence of the game but only a facilitator, enabling the computer to solve the problem. For starters, problem 1. can be solved using a Monte Carlo algorithm to find a set of potential solutions. The technique is simple: Generate random numbers for item values and quantities (within their respective accepted range). Repeat the process for the required number of items. Verify whether or not the solution is acceptable. That means verifying if items have distinct values and the total is equal to our target total (say, 143.) While this technique has the advantage of being easy to implement it has some drawbacks: How to get around these drawback? Well... Note that the more you restrict the ranges, the less useful while be the Monte Carlo algorithm is, since there will be few enough valid solutions to iterate on them all in reasonable time. For constraints { 3, [1-10], [0-100] } there is around 741,000,000 valid solutions (not constrained to a target total value.) Monte Carlo is usable there. For { 3, [1-5], [0-10] }, there is only around 80,000. No need to use Monte Carlo; brute force for loops will do just fine. I believe the problem 1 is what you would call a Constraint satisfaction problem (or CSP.) Given the fact that problem 1 is a CSP, I would go ahead and call problem 2, and the problem in general, a Dynamic CSP (or DCSP.) [DCSPs] are useful when the original formulation of a
  problem is altered in some way, typically because the set of
  constraints to consider evolves because of the environment. DCSPs
  are viewed as a sequence of static CSPs, each one a transformation of
  the previous one in which variables and constraints can be added
  (restriction) or removed (relaxation). One technique used with CSPs that might be useful to this problem is called Constraint Recording: For this to work, you need to get a new set of possible solutions every day; Use either brute force or Monte Carlo. Then, compare solutions of Di to Di-1 and keep only solutions that can succeed to previous days' solutions without violating constraints. You will probably have to keep an history of what solutions lead to what other solutions (probably in a directed graph.) Constraint recording enables you to remember possible add-remove quantities and rejects solutions based on that. There is a lot of other steps that could be taken to further improve your solution. Here are some ideas: Given all of this, try and figure out a ranking system based on occurrence of solutions and heuristics to determine a candidate solution. This problem is impossible to solve. Let's say that you know exactly for what ratio number of items was increased, not just what is the maximum ratio for this. A user has N fruits and you have D days of guessing. In each day you get N new variables and then you have in total D*N variables. For each day you can generate only two equations. One equation is the sum of  n_item*price and other is based on a known ratio. In total you have at most 2*D equations if they are all independent. 2*D < N*D for all N > 2 I wrote a program to play the game.  Of course, I had to automate the human side, but I believe I did it all in such a way that I shouldn't invalidate my approach when played against a real human. I approached this from a machine learning perspective and treated the problem as a hidden markov model where the total price was the observation.  My solution is to use a particle filter.  This solution is written in Python 2.7 using NumPy and SciPy. I stated any assumptions I made either explicitly in the comments or implicitly in the code.  I also set some additional constraints for the sake of getting code to run in an automated fashion.  It's not particularly optimized as I tried to err on the side comprehensibility rather than speed. Each iteration outputs the current true quantities and the guess.  I just pipe the output to a file so I can review it easily.  An interesting extension would be to plot the output on a graph either 2D (for 2 fruits) or 3D (for 3 fruits).  Then you would be able to see the particle filter hone in on the solution. Update: Edited the code to include updated parameters after tweaking.  Included plotting calls using matplotlib (via pylab).  Plotting works on Linux-Gnome, your mileage may vary.  Defaulted NUM_FRUITS to 2 for plotting support.  Just comment out all the pylab calls to remove plotting and be able to change NUM_FRUITS to anything. Does a good job estimating the current fxn represented by UnknownQuantities X Prices = TotalPrice.  In 2D (2 Fruits) this is a line, in 3D (3 Fruits) it'd be a plane.  Seems to be too little data for the particle filter to reliably hone in on the correct quantities.  Need a little more smarts on top of the particle filter to really bring together the historical information.  You could try converting the particle filter to 2nd- or 3rd-order. Update 2: I've been playing around with my code, a lot.  I tried a bunch of things and now present the final program that I'll be making (starting to burn out on this idea). Changes: The particles now use floating points rather than integers.  Not sure if this had any meaningful effect, but it is a more general solution.  Rounding to integers is done only when making a guess. Plotting shows true quantities as green square and current guess as red square.  Currently believed particles shown as blue dots (sized by how much we believe them).  This makes it really easy to see how well the algorithm is working.  (Plotting also tested and working on Win 7 64-bit). Added parameters for turning off/on quantity changing and price changing.  Of course, both 'off' is not interesting.   It does a pretty dang good job, but, as has been noted, it's a really tough problem, so getting the exact answer is hard.  Turning off CHANGE_QUANTITIES produces the simplest case.  You can get an appreciation for the difficulty of the problem by running with 2 fruits with CHANGE_QUANTITIES off.  See how quickly it hones in on the correct answer then see how harder it is as you increase the number of fruit. You can also get a perspective on the difficulty by keeping CHANGE_QUANTITIES on, but adjusting the MAX_QUANTITY_CHANGE from very small values (.001) to "large" values (.05). One situation where it struggles is if on dimension (one fruit quantity) gets close to zero.  Because it's using an average of particles to guess it will always skew away from a hard boundary like zero. In general this makes a great particle filter tutorial. For your initial rules: From my school years, I would say that if we make an abstraction of the 5% changes, we have everyday an equation with three unknown values (sorry I don't know the maths vocabulary in English), which are the same values as previous day.
At day 3, you have three equations, three unknown values, and the solution should be direct. I guess the 5% change each day may be forgotten if the values of the three elements are different enough, because, as you said, we will use approximations and round the numbers. For your adapted rules: Too many unknowns - and changing - values in this case, so there is no direct solution I know of. I would trust Lior on this; his approach looks fine! (If you have a limited range for prices and quantities.) I realized that my answer was getting quite lengthy, so I moved the code to the top (which is probably what most people are interested in). Below it there are two things: For those of you interested in either topic, please see below. For the rest of you, here is the code. Code that finds all possible solutions As I explain further down in the answer, your problem is under-determined. In the average case, there are many possible solutions, and this number grows at least exponentially as the number of days increases. This is true for both, the original and the extended problem. Nevertheless, we can (sort of) efficiently find all solutions (it's NP hard, so don't expect too much). Backtracking (from the 1960s, so not exactly modern) is the algorithm of choice here. In python, we can write it as a recursive generator, which is actually quite elegant: This approach essentially structures all possible candidates into a large tree and then performs depth first search with pruning whenever a constraint is violated. Whenever a leaf node is encountered, we yield the result. Tree search (in general) can be parallelized, but that is out of scope here. It will make the solution less readable without much additional insight. The same goes for reducing constant overhead of the code, e.g., working the constraints if ...: continue into the iterator_bounds variable and do less checks. I put the full code example (including a simulator for the human side of the game) at the bottom of this answer. Modern Machine Learning for this problem Question is 9 years old but still one that I'm deeply interested in. In the time since, machine learning(RNN's, CNN's, GANS,etc), new approaches and cheap GPU's have risen that enable new approaches. I thought it would be fun to revisit this question to see if there are new approaches. I really like your enthusiasm for the world of deep neural networks; unfortunately they simply do not apply here for a few reasons: Why the game can not be uniquely solved - Part 1 Let's consider a substitute problem first and lift the integer requirement, i.e., the basket (human choice of N fruits for a given day) can have fractional fruits (0.3 oranges). The total value constraint np.dot(basket, daily_price) == total_value limits the possible solutions for the basket; it reduces the problem by one dimension. Freely pick amounts for N-1 fruits, and you can always find a value for the N-th fruit to satisfy the constraint. So while it seems that there are N choices to make for a day, there are actually only N-1 that we can make freely, and the last one will be fully determined by our previous choices. So for each day the game goes on, we need to estimate an additional N-1 choices/variables. We might want to enforce that all the choices are greater than 0, but that only reduces the interval from which we can choose a number; any open interval of real numbers has infinitely many numbers in it, so we will never run out of options because of this. Still N-1 choices to make. Between two days, the total basket volume np.sum(basket) only changes by at most some_percent of the previous day, i.e. np.abs(np.sum(previous_basket) - np.sum(basket)) <= some_percent * np.sum(previous_basket). Some of the choices we could make at a given day will change the basket by more than some_percent of the previous day. To make sure we never violate this, we can freely make N-2 choices and then have to pick the N-1-th variable so that adding it and adding the N-the variable (which is fixed from our previous choices) stays within some_percent. (Note: This is an inequality constraint, so it will only reduce the number of choices if we have equality, i.e., the basket changes by exactly some_percent. In optimization theory this is known as the constraint being active.) We can again think about the constraint that all choices should be greater 0, but the argument remains that this simply changes the interval from which we can now freely choose N-2 variables. So after D days we are left with N-1 choices to estimate from the first day (no change constraint) and (D-1)*(N-2) choices to estimate for each following day. Unfortunately, we ran out of constraints to further reduce this number and the number of unknowns grows by at least N-2 each day. This is essentially what what Luka Rahne meant with "2*D < N*D for all N > 2". We will likely find many candidates which are all equally probable. The exact food prices each day don't matter for this. As long as they are of some value, they will constrain one of the choices. Hence, if you extend your game in the way you specify, there is always a chance for infinitely many solutions; regardless of the number of days. Why the game can still not be uniquely solved - Part 2 There is one constraint we didn't look at which might help fix this: only allow integer solutions for choices. The problem with integer constraints is that they are very complex to deal with. However, our main concern here is if adding this constraint will allow us to uniquely solve the problem given enough days. For this, there is a rather intuitive counter-example. Suppose you have 3 consecutive days, and for the 1st and 3d day, the total value constraint only allows one basket. In other words, we know the basket for day 1 and day 3, but not for day 2. Here, we only know it's total value, that it is within some_percent of day 1 and that day 3 is within some_percent of day 2. Is this enough information to always work out what is in the basket on day 2? Above is one example, where we know the values for two days thanks to the total value constraint, but that still won't allow us to work out the exact composition of the basket at day 2. Thus, while it may be possible to work it out in some cases, it is not possible in general. Adding more days after day 3 doesn't help figuring out day 2 at all. It might help in narrowing the options for day 3 (which will then narrow the options for day 2), but we already have just 1 choice left for day 3, so it's no use. Full Code When the player selects a combination which will reduce the number of possibilities to 1, computer will win.  Otherwise, the player can pick a combination with the constraint of the total varying within a certain percentage, that computer may never win.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 7 years ago. I am looking for an open source neural network library.  So far, I have looked at FANN, WEKA, and OpenNN.  Are the others that I should look at?  The criteria, of course, is documentation, examples, and ease of use.   Last update: 2020/03/24 (I will update this answer from time to time...) Because neural networks are quite popular in research and industry at the moment ("deep learning") there are many research libraries available. Most of them are kind of easy to set up, integrate, and use. Although not as easy as the libraries mentioned above. They provide leading edge functionality and high performance (with GPUs etc.). Most of these libraries also have automatic differentiation. You can easily specify new architectures, loss functions etc. and don't have to specify the backpropagation manually. A performance comparison for GPU-accelerated libraries can be found here (a bit outdated unfortunately). A comparison of GPUs and library versions can be found here. Inactive: If you want flexibility in defining network configurations, like sharing parameters or creating different types of convolutional architectures, then you should look at the family of Torch libraries: http://www.torch.ch/. I haven't gone through the documentation for Torch 7 yet, but documentation for the other versions was pretty decent and the code is very readable (in Lua and C++). You can use accord.net framework. http://accord-framework.net/ It contains Neural learning algorithms such as Levenberg-Marquardt, Parallel Resilient Backpropagation, the Nguyen-Widrow initialization algorithm, Deep Belief Networks and Restrictured Boltzmann Machines, and many other neural network related items.  Netlab is a commonly used Matlab library. (free and open source) The Netlab toolbox is designed to provide the central tools necessary
  for the simulation of theoretically well founded neural network
  algorithms and related models for use in teaching, research and
  applications development. It is extensively used in the MSc by
  Research in the Mathematics of Complex Systems. The Netlab library includes software implementations of a wide range
  of data analysis techniques, many of which are not yet available in
  standard neural network simulation packages. Netlab works with Matlab
  version 5.0 and higher but only needs core Matlab (i.e. no other
  toolboxes are required). It is not compatible with earlier versions of
  Matlab.I have a number of classes and corresponding feature vectors, and when I run predict_proba() I will get this: I would like to get what probability that corresponds to what class. On this page it says that they are ordered by arithmetical order, i'm not 100% sure of what that means: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba Does it mean that I have go trough my training examples assign the corresponding index to the first encounter of a class, or is there a command like  clf.getClasses() = ['one','two','three']? Just use the .classes_ attribute of the classifier to recover the mapping. In your example that gives: And thanks for putting a minimalistic reproduction script in your question, it makes answering really easy by just copy and pasting in a IPython shell :) As a rule, any attribute in a learner that ends with _ is a learned one. In your case you're looking for clf.classes_. Generally in Python, you can use the dir function to find out which attributes an object has.All the examples I have seen of neural networks are for a fixed set of inputs which works well for images and fixed length data.  How do you deal with variable length data such sentences, queries or source code?  Is there a way to encode variable length data into fixed length inputs and still get the generalization properties of neural networks? I have been there, and I faced this problem.
The ANN was made for fixed feature vector length, and so are many other classifiers such as KNN, SVM, Bayesian, etc. 
 i.e. the input layer should be well defined and not varied, this is a design problem.
However, some researchers opt for adding zeros to fill the missing gap, I personally think that this is not a good solution because those zeros (unreal values) will affect the weights that the net will converge to. in addition there might be a real signal ending with zeros.  ANN is not the only classifier, there are more and even better such as the random forest. this classifier is considered the best among researchers, it uses a small number of random features, creating hundreds of decision trees using bootstrapping an bagging, this might work well, the number of the chosen  features normally the sqrt of the feature vector size. those features are random. each decision tree converges to a solution, using majority rules the most likely class will chosen then. Another solution is to use the dynamic time warping DTW, or even better to use Hidden Markov models HMM. Another solution is the interpolation, interpolate (compensate for missing values along the small signal) all the small signals to be with the same size as the max signal, interpolation methods include and not limited to averaging, B-spline, cubic..... Another solution is to use feature extraction method to use the best features (the most distinctive), this time make them fixed size, those method include PCA, LDA, etc. another solution is to use feature selection (normally after feature extraction) an easy way to select the best features that give the best accuracy. that's all for now, if non of those worked for you, please contact me. You would usually extract features from the data and feed those to the network. It is not advisable to take just some data and feed it to net. In practice, pre-processing and choosing the right features will decide over your success and the performance of the neural net. Unfortunately, IMHO it takes experience to develop a sense for that and it's nothing one can learn from a book. Summing up: "Garbage in, garbage out" Some problems could be solved by a recurrent neural network.
For example, it is good for calculating parity over a sequence of inputs. The recurrent neural network for calculating parity would have just one input feature.
The bits could be fed into it over time. Its output is also fed back to the hidden layer.
That allows to learn the parity with just two hidden units. A normal feed-forward two-layer neural network would require 2**sequence_length hidden units to represent the parity. This limitation holds for any architecture with just 2 layers (e.g., SVM). I guess one way to do it is to add a temporal component to the input (recurrent neural net) and stream the input to the net a chunk at a time (basically creating the neural network equivalent of a lexer and parser) this would allow the input to be quite large but would have the disadvantage that there would not necessarily be a stop symbol to seperate different sequences of input from each other (the equivalent of a period in sentances)   To use a neural net on images of different sizes, the images themselves are often cropped and up or down scaled to better fit the input of the network. I know that doesn't really answer your question but perhaps something similar would be possible with other types of input, using some sort of transformation function on the input? i'm not entirely sure, but I'd say, use the maximum number of inputs (e.g. for words, lets say no word will be longer than 45 characters (longest word found in a dictionary according to wikipedia), and if a shorter word is encountered, set the other inputs to a whitespace character. Or with binary data, set it to 0. the only problem with this approach is if an input filled with whitespace characters/zeros/whatever collides with a valid full length input (not so much a problem with words as it is with numbers).while I'm reading in how to build ANN in pybrain, they say:  Train the network for some epochs. Usually you would set something
  like 5 here, I looked for what is that mean , then I conclude that we use an epoch of data to update weights, If I choose to train the data with 5 epochs as pybrain advice, the dataset will be divided into 5 subsets, and the wights will update 5 times as maximum. I'm familiar with online training where the wights are updated after each sample data or feature vector, My question is how to be sure that 5 epochs will be enough to build a model and setting the weights probably?  what is the advantage of this way on online training? Also the term "epoch" is used on online training, does it mean one feature vector?  One epoch consists of one full training cycle on the training set. Once every sample in the set is seen, you start again - marking the beginning of the 2nd epoch.  This has nothing to do with batch or online training per se. Batch means that you update once at the end of the epoch (after every sample is seen, i.e. #epoch updates) and online that you update after each sample (#samples * #epoch updates). You can't be sure if 5 epochs or 500 is enough for convergence since it will vary from data to data. You can stop training when the error converges or gets lower than a certain threshold. This also goes into the territory of preventing overfitting. You can read up on early stopping and cross-validation regarding that. sorry for reactivating this thread.
im new to neural nets and im investigating the impact of 'mini-batch' training. so far, as i understand it, an epoch (as runDOSrun is saying) is a through use of all in the TrainingSet (not DataSet. because DataSet = TrainingSet + ValidationSet). in mini batch training, you can sub divide the TrainingSet into small Sets and update weights inside an epoch. 'hopefully' this would make the network 'converge' faster. some definitions of neural networks are outdated and, i guess, must be redefined. The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters.I am using sklearn.linear_model.LogisticRegression in scikit learn to run a Logistic Regression. What does C mean here in simple terms please? What is regularization strength? Regularization is applying a penalty to increasing the magnitude of parameter values in order to reduce overfitting. When you train a model such as a logistic regression model, you are choosing parameters that give you the best fit to the data. This means minimizing the error between what the model predicts for your dependent variable given your data compared to what your dependent variable actually is. The problem comes when you have a lot of parameters (a lot of independent variables) but not too much data. In this case, the model will often tailor the parameter values to idiosyncrasies in your data -- which means it fits your data almost perfectly. However because those idiosyncrasies don't appear in future data you see, your model predicts poorly. To solve this, as well as minimizing the error as already discussed, you add to what is minimized and also minimize a function that penalizes large values of the parameters. Most often the function is λΣθj2, which is some constant λ times the sum of the squared parameter values θj2. The larger λ is the less likely it is that the parameters will be increased in magnitude simply to adjust for small perturbations in the data. In your case however, rather than specifying λ, you specify C=1/λ.To save a model in Keras, what are the differences between the output files of: The saved file from model.save() is larger than the model from model.save_weights(), but significantly larger than a JSON or Yaml model architecture file.  Why is this?   Restating this: Why is size(model.save()) + size(something) = size(model.save_weights()) + size(model.to_json()), what is that "something"? Would it be more efficient to just model.save_weights() and model.to_json(), and load from these than to just do model.save() and load_model()?   What are the differences? save() saves the weights and the model structure to a single HDF5 file. I believe it also includes things like the optimizer state. Then you can use that HDF5 file with load() to reconstruct the whole model, including weights. save_weights() only saves the weights to HDF5 and nothing else. You need extra code to reconstruct the model from a JSON file. Just to add what ModelCheckPoint's output is, if it's relevant for anyone else: used as a callback during model training, it can either save the whole model or just the weights depending on what state the save_weights_only argument is set to. TRUE and weights only are saved, akin to calling model.save_weights(). FALSE (default) and the whole model is saved, as in calling model.save(). Adding to the answers above, as of tf.keras version '2.7.0', the model can be saved in 2 formats using model.save() i.e., the TensorFlow SavedModel format, and the older Keras H5 format. The recommended format is SavedModel and it is the default when model.save() is called. To save to .h5(HDF5) format, use model.save('my_model', save_format='h5') Moretf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) outputs random values from a normal distribution. tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) outputs random values from a truncated normal distribution. I tried googling 'truncated normal distribution'. But didn't understand much. The documentation says it all:
For truncated normal distribution: The values are drawn from a normal distribution with specified mean and standard deviation, discarding and re-drawing any samples that are more than two standard deviations from the mean. Most probably it is easy to understand the difference by plotting the graph for yourself (%magic is because I use jupyter notebook): And now  The point for using truncated normal is to overcome saturation of tome functions like sigmoid (where if the value is too big/small, the neuron stops learning). tf.truncated_normal() selects random numbers from a normal distribution whose mean is close to 0 and values are close to 0. For example, from -0.1 to 0.1. It's called truncated because your cutting off the tails from a normal distribution. tf.random_normal() selects random numbers from a normal distribution whose mean is close to 0, but values can be a bit further apart. For example, from -2 to 2. In machine learning, in practice, you usually want your weights to be close to 0. The API documentation for tf.truncated_normal() describes the function as: Outputs random values from a truncated normal distribution. The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is
more than 2 standard deviations from the mean are dropped and
re-picked.I'm trying to get an agent to learn the mouse movements necessary to best perform some task in a reinforcement learning setting (i.e. the reward signal is the only feedback for learning). I'm hoping to use the Q-learning technique, but while I've found a way to extend this method to continuous state spaces, I can't seem to figure out how to accommodate a problem with a continuous action space. I could just force all mouse movement to be of a certain magnitude and in only a certain number of different directions, but any reasonable way of making the actions discrete would yield a huge action space. Since standard Q-learning requires the agent to evaluate all possible actions, such an approximation doesn't solve the problem in any practical sense. The common way of dealing with this problem is with actor-critic methods. These naturally extend to continuous action spaces. Basic Q-learning could diverge when working with approximations, however, if you still want to use it, you can try combining it with a self-organizing map, as done in "Applications of the self-organising map to reinforcement learning". The paper also contains some further references you might find useful. Fast forward to this year, folks from DeepMind proposes a deep reinforcement learning actor-critic method for dealing with both continuous state and action space. It is based on a technique called deterministic policy gradient. See the paper Continuous control with deep reinforcement learning and some implementations. There are numerous ways to extend reinforcement learning to continuous actions. One way is to use actor-critic methods. Another way is to use policy gradient methods. A rather extensive explanation of different methods can be found in the following paper, which is available online:
Reinforcement Learning in Continuous State and Action Spaces (by Hado van Hasselt and Marco A. Wiering). For what you're doing I don't believe you need to work in continuous action spaces.  Although the physical mouse moves in a continuous space, internally the cursor only moves in discrete steps (usually at pixel levels), so getting any precision above this threshold seems like it won't have any effect on your agent's performance.  The state space is still quite large, but it is finite and discrete. I know this post is somewhat old, but in 2016, a variant of Q-learning applied to continuous action spaces was proposed, as an alternative to actor-critic methods. It is called normalized advantage functions (NAF). Here's the paper: Continuous Deep Q-Learning with Model-based Acceleration Another paper to make the list, from the value-based school, is Input Convex Neural Networks. The idea is to require Q(s,a) to be convex in actions (not necessarily in states). Then, solving the argmax Q inference is reduced to finding the global optimum using the convexity, much faster than an exhaustive sweep and easier to implement than other value-based approaches. Yet, likely at the expense of a reduced representation power than usual feedforward or convolutional neural networks.I'm trying to evaluate multiple machine learning algorithms with sklearn for a couple of metrics (accuracy, recall, precision and maybe more). For what I understood from the documentation here and from the source code(I'm using sklearn 0.17), the cross_val_score function only receives one scorer for each execution. So for calculating multiple scores, I have to : Implement my (time consuming and error prone) scorer I've executed multiple times with this code : And I get this output:  Which is ok, but it's slow for my own data. How can I measure all scores ? Since the time of writing this post scikit-learn has updated and made my answer obsolete, see the much cleaner solution below You can write your own scoring function to capture all three pieces of information, however a scoring function for cross validation must only return a single number in scikit-learn (this is likely for compatibility reasons). Below is an example where each of the scores for each cross validation slice prints to the console, and the returned value is just the sum of the three metrics. If you want to return all these values, you're going to have to make some changes to cross_val_score (line 1351 of cross_validation.py) and _score (line 1601 or the same file). Which gives: As of scikit-learn 0.19.0 the solution becomes much easier Which gives: I ran over the same problem and I created a module that can support multiple metrics in cross_val_score.
In order to accomplish what you want with this module, you can write: You can check and download this module from GitHub.
Hope it helps.I want to perform GridSearchCV in a SVC model, but that uses the one-vs-all strategy. For the latter part, I can just do this: My problem is with the parameters. Let's say I want to try the following values: In order to perform GridSearchCV, I should do something like: However, then I execute it I get: Basically, since the SVC is inside a OneVsRestClassifier and that's the estimator I send to the GridSearchCV, the SVC's parameters can't be accessed.  In order to accomplish what I want, I see two solutions: I'm yet to find a way to do any of the mentioned alternatives. Do you know if there's a way to do any of them? Or maybe you could suggest another way to get to the same result? Thanks! When you use nested estimators with grid search you can scope the parameters with __ as a separator. In this case the SVC model is stored as an attribute named estimator inside the OneVsRestClassifier model: That yields: For Python 3, the following code should be usedI know that principal component analysis does a SVD on a matrix and then generates an eigen value matrix. To select the principal components we have to take only the first few eigen values. Now, how do we decide on the number of eigen values that we should take from the eigen value matrix? To decide how many eigenvalues/eigenvectors to keep, you should consider your reason for doing PCA in the first place.  Are you doing it for reducing storage requirements, to reduce dimensionality for a classification algorithm, or for some other reason?  If you don't have any strict constraints, I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order).  If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues). There is no correct answer, it is somewhere between 1 and n. Think of a principal component as a street in a town you have never visited before. How many streets should you take to get to know the town? Well, you should obviously visit the main street (the first component), and maybe some of the other big streets too. Do you need to visit every street to know the town well enough? Probably not. To know the town perfectly, you should visit all of the streets. But what if you could visit, say 10 out of the 50 streets, and have a 95% understanding of the town? Is that good enough?  Basically, you should select enough components to explain enough of the variance that you are comfortable with.  As others said, it doesn't hurt to plot the explained variance. If you use PCA as a preprocessing step for a supervised learning task, you should cross validate the whole data processing pipeline and treat the number of PCA dimension as an hyperparameter to select using a grid search on the final supervised score (e.g. F1 score for classification or RMSE for regression). If cross-validated grid search on the whole dataset is too costly try on a 2 sub samples, e.g. one with 1% of the data and the second with 10% and see if you come up with the same optimal value for the PCA dimensions. There are a number of heuristics use for that. E.g. taking the first k eigenvectors that capture at least 85% of the total variance. However, for high dimensionality, these heuristics usually are not very good. Depending on your situation, it may be interesting to define the maximal allowed relative error by projecting your data on ndim dimensions. Matlab example I will illustrate this with a small matlab example. Just skip the code if you are not interested in it. I will first generate a random matrix of n samples (rows) and p features containing exactly 100 non zero principal components. The image will look similar to:  For this sample image, one can calculate the relative error made by projecting your input data to ndim dimensions as follows: Plotting the relative error in function of the number of dimensions (principal components) results in the following graph:  Based on this graph, you can decide how many principal components you need to take into account. In this theoretical image taking 100 components result in an exact image representation. So, taking more than 100 elements is useless. If you want for example maximum 5% error, you should take about 40 principal components. Disclaimer: The obtained values are only valid for my artificial data. So, do not use the proposed values blindly in your situation, but perform the same analysis and make a trade off between the error you make and the number of components you need. Code reference I highly recommend the following paper by Gavish and Donoho: The Optimal Hard Threshold for Singular Values is 4/sqrt(3).  I posted a longer summary of this on CrossValidated (stats.stackexchange.com). Briefly, they obtain an optimal procedure in the limit of very large matrices. The procedure is very simple, does not require any hand-tuned parameters, and seems to work very well in practice. They have a nice code supplement here: https://purl.stanford.edu/vg705qn9070There doesn't seem to be too many options for deploying predictive models in production which is surprising given the explosion in Big Data.   I understand that the open-source PMML can be used to export models as an XML specification. This can then be used for in-database scoring/prediction.  However it seems that to make this work you need to use the PMML plugin by Zementis which means the solution is not truly open source.  Is there an easier open way to map PMML to SQL for scoring? Another option would be to use JSON instead of XML to output model predictions.  But in this case, where would the R model sit? I'm assuming it would always need to be mapped to SQL...unless the R model could sit on the same server as the data and then run against that incoming data using an R script? Any other options out there?  The following is a list of the alternatives that I have found so far to deploy an R model in production. Please note that the workflow to use these products varies significantly between each other, but they are all somehow oriented to facilitate the process of exposing a trained R model as a service: The answer really depends on what your production environment is.  If your "big data" are on Hadoop, you can try this relatively new open source PMML "scoring engine" called Pattern. Otherwise you have no choice (short of writing custom model-specific code) but to run R on your server. You would use save to save your fitted models in .RData files and then load and run corresponding predict on the server. (That is bound to be slow but you can always try and throw more hardware at it.) How you do that really depends on your platform. Usually there is a way to add "custom" functions written in R. The term is UDF (user-defined function). In Hadoop you can add such functions to Pig (e.g. https://github.com/cd-wood/pigaddons) or you can use RHadoop to write simple map-reduce code that would load the model and call predict in R. If your data are in Hive, you can use Hive TRANSFORM to call external R script. There are also vendor-specific ways to add functions written in R to various SQL databases. Again look for UDF in the documentation. For instance, PostgreSQL has PL/R. You can create RESTful APIs for your R scripts using plumber (https://github.com/trestletech/plumber). I wrote a blog post about it (http://www.knowru.com/blog/how-create-restful-api-for-machine-learning-credit-model-in-r/) using deploying credit models as an example. In general, I do not recommend PMML because the packages you used might not support translation to PMML. A common practice is scoring a new/updated dataset in R and moving only the results (IDs, scores, probabilities, other necessary fields) into the production environment/data warehouse. I know this has its limitations (infrequent refreshes, reliance upon IT, data set size/computing power restrictions) and may not be the cutting edge answer many (of your bosses) are looking for; but for many use-cases this works well (and is cost friendly!). It’s been a few years since the question was originally asked.  For rapid prototyping I would argue the easiest approach currently is to use the Jupyter Kernel Gateway. This allows you to add REST endpoints to any cell in your Jupyter notebook. This works for both R and Python, depending on the kernel you’re using.  This means you can easily call any R or Python code through a web interface. When used in conjunction with Docker it lends itself to a microservices approach to deploying and scaling your application.  Here’s an article that takes you from start to finish to quickly set up your Jupyter Notebook with the Jupyter Kernel Gateway. Learn to Build Machine Learning Services, Prototype Real Applications, and Deploy your Work to Users  For moving solutions to production the leading approach in 2019 is to use Kubeflow. Kubeflow was created and is maintained by Google, and makes "scaling machine learning (ML) models and deploying them to production as simple as possible." From their website:  You adapt the configuration to choose the platforms and services that you want to use for each stage of the ML workflow: data preparation, model training, prediction serving, and service management.
You can choose to deploy your workloads locally or to a cloud environment. Elise from Yhat here.  Like @Ramnath and @leo9r mentioned, our software allows you to put any R (or Python, for that matter) model directly into production via REST API endpoints.  We handle real-time or batch, as well as all of the model testing and versioning + systems management associated with the process. This case study we co-authored with VIA SMS might be useful if you're thinking about how to get R models into production (their data sci team was recoding into PHP prior to using Yhat).  Cheers!Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it. Closed 8 years ago. Assume you know a student who wants to study Machine Learning and Natural Language Processing. What specific computer science subjects should they focus on and which programming languages are specifically designed to solve these types of problems? I am not looking for your favorite subjects and tools, but rather industry standards. Example: I'm guessing that knowing Prolog and Matlab might help them.  They also might want to study Discrete Structures*, Calculus, and Statistics. *Graphs and trees. Functions: properties, recursive definitions, solving recurrences. Relations: properties, equivalence, partial order. Proof techniques, inductive proof. Counting techniques and discrete probability.  Logic: propositional calculus, first-order predicate calculus. Formal reasoning: natural deduction, resolution. Applications to program correctness and automatic reasoning. Introduction to algebraic structures in computing. This related stackoverflow question has some nice answers: What are good starting points for someone interested in natural language processing? This is a very big field.  The prerequisites mostly consist of probability/statistics, linear algebra, and basic computer science, although Natural Language Processing requires a more intensive computer science background to start with (frequently covering some basic AI).  Regarding specific langauges: Lisp was created "as an afterthought" for doing AI research, while Prolog (with it's roots in formal logic) is especially aimed at Natural Language Processing, and many courses will use Prolog, Scheme, Matlab, R, or another functional language (e.g. OCaml is used for this course at Cornell) as they are very suited to this kind of analysis.   Here are some more specific pointers: For Machine Learning, Stanford CS 229: Machine Learning is great: it includes everything, including full videos of the lectures (also up on iTunes), course notes, problem sets, etc., and it was very well taught by Andrew Ng. Note the prerequisites: Students are expected to have the following background: Knowledge of
  basic computer science principles and skills, at a level sufficient to write
  a reasonably non-trivial computer program. Familiarity with the basic probability theory. 
  Familiarity with the basic linear algebra. The course uses Matlab and/or Octave.  It also recommends the following readings (although the course notes themselves are very complete): For Natural Language Processing, the NLP group at Stanford provides many good resources.  The introductory course Stanford CS 224: Natural Language Processing includes all the lectures online and has the following prerequisites: Adequate experience with programming
  and formal structures.  Programming
  projects will be written in Java 1.5,
  so knowledge of Java (or a willingness
  to learn on your own) is required. 
  Knowledge of standard concepts in
  artificial intelligence and/or
  computational linguistics.  Basic
  familiarity with logic, vector spaces,
  and probability. Some recommended texts are: The prerequisite computational linguistics course requires basic computer programming and data structures knowledge, and uses the same text books.  The required articificial intelligence course is also available online along with all the lecture notes and uses: This is the standard Artificial Intelligence text and is also worth reading. I use R for machine learning myself and really recommend it.  For this, I would suggest looking at The Elements of Statistical Learning, for which the full text is available online for free.  You may want to refer to the Machine Learning and Natural Language Processing views on CRAN for specific functionality.   My recommendation would be either or all (depending on his amount and area of interest) of these: The Oxford Handbook of Computational Linguistics: 
(source: oup.com)  Foundations of Statistical Natural Language Processing:  Introduction to Information Retrieval:  String algorithms, including suffix trees. Calculus and linear algebra. Varying varieties of statistics. Artificial intelligence optimization algorithms. Data clustering techniques... and a million other things. This is a very active field right now, depending on what you intend to do. It doesn't really matter what language you choose to operate in. Python, for instance has the NLTK, which is a pretty nice free package for tinkering with computational linguistics. I would say probabily & statistics is the most important prerequisite. Especially Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) are very important both in machine learning and natural language processing (of course these subjects may be part of the course if it is introductory). Then, I would say basic CS knowledge is also helpful, for example Algorithms, Formal Languages and basic Complexity theory. Stanford CS 224: Natural Language Processing course that was mentioned already includes also videos online (in addition to other course materials). The videos aren't linked to on the course website, so many people may not notice them. Jurafsky and Martin's Speech and Language Processing http://www.amazon.com/Speech-Language-Processing-Daniel-Jurafsky/dp/0131873210/ is very good. Unfortunately the draft second edition chapters are no longer free online now that it's been published :( Also, if you're a decent programmer it's never too early to toy around with NLP programs. NLTK comes to mind (Python). It has a book you can read free online that was published (by OReilly I think).  How about Markdown and an Introduction to Parsing Expression Grammars (PEG) posted by cletus on his site cforcoding? ANTLR seems like a good place to start for natural language processing. I'm no expert though. Broad question, but I certainly think that a knowledge of finite state automata and hidden Markov models would be useful. That requires knowledge of statistical learning, Bayesian parameter estimation, and entropy. Latent semantic indexing is a commonly yet recently used tool in many machine learning problems. Some of the methods are rather easy to understand. There are a bunch of potential basic projects.  EDIT: Nonnegative matrix factorization (NMF) is a tool that has grown considerably in popularity due to its simplicity and effectiveness. It's easy to understand. I currently research the use of NMF for music information retrieval; NMF has shown to be useful for latent semantic indexing of text corpora, as well. Here is one paper. PDF Prolog will only help them academically it is also limited for logic constraints and semantic NLP based work. Prolog is not yet an industry friendly language so not yet practical in real-world. And, matlab also is an academic based tool unless they are doing a lot of scientific or quants based work they wouldn't really have much need for it. To start of they might want to pick up the 'Norvig' book and enter the world of AI get a grounding in all the areas. Understand some basic probability, statistics, databases, os, datastructures, and most likely an understanding and experience with a programming language. They need to be able to prove to themselves why AI techniques work and where they don't. Then look to specific areas like machine learning and NLP in further detail. In fact, the norvig book sources references after every chapter so they already have a lot of further reading available. There are a lot of reference material available for them over internet, books, journal papers for guidance. Don't just read the book try to build tools in a programming language then extrapolate 'meaningful' results. Did the learning algorithm actually learn as expected, if it didn't why was this the case, how could it be fixed.I need to classify some data with (I hope) nearest-neighbour algorithm. I've googled this problem and found a lot of libraries (including PyML, mlPy and Orange), but I'm unsure of where to start here.  How should I go about implementing k-NN using Python? Particularly given the technique (k-Nearest Neighbors) that you mentioned in your Q, i would strongly recommend scikits.learn. [Note: after this Answer was posted, the lead developer of this Project informed me of a new homepage for this Project.] A few features that i believe distinguish this library from the others (at least the other Python ML libraries that i have used, which is most of them): an extensive diagnostics & testing library (including plotting
modules, via Matplotlib)--includes feature-selection algorithms,
confusion matrix, ROC, precision-recall, etc.; a nice selection of 'batteries-included' data sets (including
handwriting digits, facial images, etc.) particularly suited for ML techniques; extensive documentation (a nice surprise given that this Project is
only about two years old) including tutorials and step-by-step
example code (which use the supplied data sets); Without exception (at least that i can think of at this moment) the python ML libraries are superb. (See the PyMVPA homepage for a list of the dozen or so most popular python ML libraries.) In the past 12 months for instance, i have used ffnet (for MLP), neurolab (also for MLP), PyBrain (Q-Learning), neurolab (MLP), and PyMVPA (SVM) (all available from the Python Package Index)--these vary significantly from each other w/r/t maturity, scope, and supplied infrastructure, but i found them all to be of very high quality.  Still, the best of these might be scikits.learn; for instance, i am not aware of any python ML library--other than scikits.learn--that includes any of the three features i mentioned above (though a few have solid example code and/or tutorials, none that i know of integrate these with a library of research-grade data sets and diagnostic algorithms). Second, given you the technique you intend to use (k-nearest neighbor) scikits.learn is a particularly good choice. Scikits.learn includes kNN algorithms for both regression (returns a score) and classification (returns a class label), as well as detailed sample code for each. Using the scikits.learn k-nearest neighbor module (literally) couldn't be any easier: What's more, unlike nearly all other ML techniques, the crux of k-nearest neighbors is not coding a working classifier builder, rather the difficult step in building a production-grade k-nearest neighbor classifier/regressor is the persistence layer--i.e., storage and fast retrieval of the data points from which the nearest neighbors are selected. For the kNN data storage layer, scikits.learn includes an algorithm for a ball tree (which i know almost nothing about other than is apparently superior to the kd-tree (the traditional data structure for k-NN) because its performance doesn't degrade in higher dimensional features space. Additionally, k-nearest neighbors requires an appropriate similarity metric (Euclidean distance is the usual choice, though not always the best one). Scikits.learn includes a stand-along module comprised of various distance metrics as well as testing algorithms for selection of the appropriate one. Finally, there are a few libraries that i have not mentioned either because they are out of scope (PyML, Bayesian); they are not primarily 'libraries' for developers but rather applications for end users (e.g., Orange), or they have unusual or difficult-to-install dependencies (e.g., mlpy, which requires the gsl, which in turn must be built from source) at least for my OS, which is Mac OS X. (Note: i am not a developer/committer for scikits.learn.)Is it possible to train a model in Xgboost that have multiple continuous outputs (multi regression)?
What would be the objective to train such a model? Thanks in advance for any suggestions My suggestion is to use sklearn.multioutput.MultiOutputRegressor as a wrapper of xgb.XGBRegressor. MultiOutputRegressor trains one regressor per target and only requires that the regressor implements fit and predict, which xgboost happens to support.  This is probably the easiest way to regress multi-dimension targets using xgboost as you would not need to change any other part of your code (if you were using the sklearn API originally). However this method does not leverage any possible relation between targets. But you can try to design a customized objective function to achieve that.  Multiple output regression is now available in the nightly build of XGBoost, and will be included in XGBoost 1.6.0. See https://github.com/dmlc/xgboost/blob/master/demo/guide-python/multioutput_regression.py for an example. It generates warnings: reg:linear is now deprecated in favor of reg:squarederror, so I update an answer based on @ComeOnGetMe's Out: I would place a comment but I lack the reputation. In addition to @Jesse Anderson, to install the most recent version, select the top link from here:
https://s3-us-west-2.amazonaws.com/xgboost-nightly-builds/list.html?prefix=master/ Make sure to select the one for your operating system. Use pip install to install the wheel. I.e. for macOS: pip install https://s3-us-west-2.amazonaws.com/xgboost-nightly-builds/master/xgboost-1.6.0.dev0%2B4d81c741e91c7660648f02d77b61ede33cef8c8d-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl You can use Linear regression, random forest regressors and some other related algorithms in Scikit-learn to produce multi-output regression. Not sure about XGboost. The boosting regressor in Scikit does not allow multiple outputs. For people who asked, when it may be necessary one example would be to forecast multi-steps of time-series a head. Based on the above discussion, I have extended the univariate XGBoostLSS to a multivariate framework called Multi-Target XGBoostLSS Regression that models multiple targets and their dependencies in a probabilistic regression setting. Code follows soon.It is not yet clear for me what metrics are (as given in the code below). What exactly are they evaluating? Why do we need to define them in the model? Why we can have multiple metrics in one model? And more importantly what is the mechanics behind all this? 
Any scientific reference is also appreciated. So in order to understand what metrics are, it's good to start by understanding what a loss function is. Neural networks are mostly trained using gradient methods by an iterative process of decreasing a loss function. A loss is designed to have two crucial properties - first, the smaller its value is, the better your model fits your data, and second, it should be differentiable. So, knowing this, we could fully define what a metric is: it's a function that, given predicted values and ground truth values from examples, provides you with a scalar measure of a "fitness" of your model, to the data you have. So, as you may see, a loss function is a metric, but the opposite doesn't always hold. To understand these differences, let's look at the most common examples of metrics usage: Measure a performance of your network using non-differentiable functions: e.g. accuracy is not differentiable (not even continuous) so you cannot directly optimize your network w.r.t. to it. However, you could use it in order to choose the model with the best accuracy. Obtain values of different loss functions when your final loss is a combination of a few of them: Let's assume that your loss has a regularization term which measures how your weights differ from 0, and a term which measures the fitness of your model. In this case, you could use metrics in order to have a separate track of how the fitness of your model changes across epochs. Track a measure with respect to which you don't want to directly optimize your model: so - let's assume that you are solving a multidimensional regression problem where you are mostly concerned about mse, but at the same time you are interested in how a cosine-distance of your solution is changing in time. Then, it's the best to use metrics. I hope that the explanation presented above made obvious what metrics are used for, and why you could use multiple metrics in one model. So now, let's say a few words about mechanics of their usage in keras. There are two ways of computing them while training: Using metrics defined while compilation: this is what you directly asked. In this case, keras is defining a separate tensor for each metric you defined, to have it computed while training. This usually makes computation faster, but this comes at a cost of additional compilations, and the fact that metrics should be defined in terms of keras.backend functions. Using keras.callback: It is nice that you can use Callbacks in order to compute your metrics. As each callback has a default attribute of model, you could compute a variety of metrics using model.predict or model parameters while training. Moreover, it makes it possible to compute it, not only epoch-wise, but also batch-wise, or training-wise. This comes at a cost of slower computations, and more complicated logic - as you need to define metrics on your own. Here you can find a list of available metrics, as well as an example on how you could define your own. As in keras metrics page described:  A metric is a function that is used to judge the performance of your
  model Metrics are frequently used with early stopping callback to terminate training and avoid overfitting Reference: Keras Metrics Documentation As given in the documentation page of keras metrics, a metric judges the performance of your model. The metrics argument in the compile method holds the list of metrics that needs to be evaluated by the model during its training and testing phases.
Metrics like: binary_accuracy categorical_accuracy sparse_categorical_accuracy top_k_categorical_accuracy and sparse_top_k_categorical_accuracy are the available metric functions that are supplied in the metrics parameter when the model is compiled. Metric functions are customizable as well. When multiple metrics need to be evaluated it is passed in the form of a dictionary or a list.  One important resource you should refer for diving deep into metrics can be found here From an implementation point of view, losses and metrics are actually identical functions in Keras: Loss helps find the best solution your model can produce. Metric actually tells us how good it is. Imagine, we found the regression line (that has the least minimum squared error). Is that a good enough solution? This is what  the metric will answer(considering the shape and spread of data, ideally!).What is the difference between standardscaler and normalizer in sklearn.preprocessing module? 
Don't both do the same thing? i.e remove mean and scale using deviation? From the Normalizer docs: Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. And StandardScaler Standardize features by removing the mean and scaling to unit variance In other words Normalizer acts row-wise and StandardScaler column-wise. Normalizer does not remove the mean and scale by deviation but scales the whole row to unit norm. This visualization and article by Ben helps a lot in illustrating the idea.  The StandardScaler assumes your data is normally distributed within each feature. By "removing the mean and scaling to unit variance", you can see in the picture now they have the same "scale" regardless of its original one. In addition to the excellent suggestion by @vincentlcy to view this article, there is now an example in the Scikit-Learn documentation here. An important difference is that Normalizer() is applied to each sample (i.e., row) rather than column. This may work only for certain datasets that fit its assumption of similar types of data in each column. StandardScaler() standardizes features (such as the features of the person data i.e height, weight)by removing the mean and scaling to unit variance.  (unit variance:  Unit variance means that the standard deviation of a sample as well as the variance will tend towards 1 as the sample size tends towards infinity.) Normalizer() rescales each sample. For example rescaling each company's stock price independently of the other. Some stocks are more expensive than others. To account for this, we normalize it. The Normalizer will separately transform each company's stock price to a relative scale. The main difference is that Standard Scalar is applied on Columns, while Normalizer is applied on rows, So make sure you reshape your data before normalizing it. StandardScaler standardizes features by removing the mean and scaling to unit variance, Normalizer rescales each sample. Perhaps a helpful example: With Normalizer, it seems that the default operation is to divide each data point in a row, by the  norm of the row. For example, given a row [4,1,2,2], the  norm is: . The normalized row is then: This is the first row of the example from the SKLearn docs. Building off of the answer from @TerrenceJ, here is the code to manually calculate the Normalizer-transformed result from the example in the first SKLearn documentation (and note that this reflects the default "l2" normalization).I've been exploring the xgboost package in R and went through several demos as well as tutorials but this still confuses me: after using xgb.cv to do cross validation, how does the optimal parameters get passed to xgb.train? Or should I calculate the ideal parameters (such as nround, max.depth) based on the output of xgb.cv? Looks like you misunderstood xgb.cv, it is not a parameter searching function. It does k-folds cross validation, nothing more. In your code, it does not change the value of param.  To find best parameters in R's XGBoost, there are some methods. These are 2 methods,  (1) Use mlr package, http://mlr-org.github.io/mlr-tutorial/release/html/ There is a XGBoost + mlr example code in the Kaggle's Prudential challenge,  But that code is for regression, not classification. As far as I know, there is no mlogloss metric yet in mlr package, so you must code the mlogloss measurement from scratch by yourself. CMIIW.  (2) Second method, by manually setting the parameters then repeat, example,  Then, you find the best (minimum) mlogloss,  min_logloss is the minimum value of mlogloss, while min_logloss_index is the index (round).  You must repeat the process above several times, each time change the parameters manually (mlr does the repeat for you). Until finally you get best global minimum min_logloss.  Note: You can do it in a loop of 100 or 200 iterations, in which for each iteration you set the parameters value randomly. This way, you must save the best [parameters_list, min_logloss, min_logloss_index] in  variables or in a file.  Note: better to set random seed by set.seed() for reproducible result. Different random seed yields different result. So, you must save [parameters_list, min_logloss, min_logloss_index, seednumber] in the  variables or file.  Say that finally you get 3 results in 3 iterations/repeats:  Then you must use the third parameters (it has global minimum min_logloss of 1.9745). Your best index (nrounds) is 780.  Once you get best parameters, use it in the training,  I don't think you need watchlist in the training, because you have done the cross validation. But if you still want to use watchlist, it is just okay.  Even better you can use early stopping in xgb.cv.  With this code, when mlogloss value is not decreasing in 8 steps, the xgb.cv will stop. You can save time. You must set maximize to FALSE, because you expect minimum mlogloss.  Here is an example code, with 100 iterations loop, and random chosen parameters.  With this code, you run cross validation 100 times, each time with random parameters. Then you get best parameter set, that is in the iteration with minimum min_logloss.  Increase the value of early.stop.round in case you find out that it's too small (too early stopping). You need also to change the random parameter values' limit based on your data characteristics.  And, for 100 or 200 iterations, I think you want to change verbose to FALSE.  Side note: That is example of random method, you can adjust it e.g. by Bayesian optimization for better method. If you have Python version of XGBoost, there is a good hyperparameter script for XGBoost, https://github.com/mpearmain/BayesBoost to search for best parameters set using Bayesian optimization.  Edit: I want to add 3rd manual method, posted by "Davut Polat" a Kaggle master, in the Kaggle forum. Edit: If you know Python and sklearn, you can also use GridSearchCV along with xgboost.XGBClassifier or xgboost.XGBRegressor This is a good question and great reply from silo with lots of details! I found it very helpful for someone new to xgboost like me. Thank you. The method to randomize and compared to boundary is very inspiring. Good to use and good to know. Now in 2018 some slight revise are needed, for example, early.stop.round should be early_stopping_rounds. The output mdcv is organized slightly differently: And depends on the application (linear, logistic,etc...), the objective, eval_metric and parameters shall be adjusted accordingly.  For the convenience of anyone who is running a regression, here is the slightly adjusted version of code (most are the same as above).   I found silo's answer is very helpful. 
In addition to his approach of random research, you may want to use Bayesian optimization to facilitate the process of hyperparameter search, e.g. rBayesianOptimization library. 
The following is my code with rbayesianoptimization library.Since an LSTM RNN uses previous events to predict current sequences, why do we shuffle the training data? Don't we lose the temporal ordering of the training data? How is it still effective at making predictions after being trained on shuffled training data? In general, when you shuffle the training data (a set of sequences), you shuffle the order in which sequences are fed to the RNN, you don't shuffle the ordering within individual sequences. This is fine to do when your network is stateless: Stateless Case: The network's memory only persists for the duration of a sequence. Training on sequence B before sequence A doesn't matter because the network's memory state does not persist across sequences. On the other hand: Stateful Case: The network's memory persists across sequences. Here, you cannot blindly shuffle your data and expect optimal results. Sequence A should be fed to the network before sequence B because A comes before B, and we want the network to evaluate sequence B with memory of what was in sequence A.I can't get the dtypes to match, either the loss wants long or the model wants float if I change my tensors to long. The shape of the tensors are 42000, 1, 28, 28 and 42000. I'm not sure where I can change what dtypes are required for the model or loss.  I'm not sure if dataloader is required, using Variable didn't work either. Which gives LongTensor is synonymous with integer. PyTorch won't accept a FloatTensor as categorical target, so it's telling you to cast your tensor to LongTensor. This is how you should change your target dtype: This is very well documented on the PyTorch website, you definitely won't regret spending a minute or two reading this page. PyTorch essentially defines nine CPU tensor types and nine GPU tensor types:I have started using sckikit-learn for my work. So I was going through the tutorial which gives standard procedure to load some datasets: However, for my convenience, I tried loading the data in the following way: However, this throws following error: However, if I use the apparently similar method: It works without problem. In fact the following also works: I am completely confused about this. Am I missing something very trivial? What is the difference between the two approaches? sklearn is a package. This answer said it very succinctly: when you import a package, only variables/functions/classes in the __init__.py file of that package are directly visible, not sub-packages or modules. datasets is a sub-package of sklearn. This is why this happens: However, the reason why this works:  is that when you load the sub-package datasets by doing from sklearn import datasets it is automatically added to the namespace of the package sklearn. This is one of the lesser-known "traps" of the Python import system. Also, note that if you look at the __init__.py for sklearn you will see 'datasets' as a member of __all__, but this only allows you to do: One last point to note is that if you inspect either sklearn or datasets you will see that, although they are packages, their type is module. This is because all packages are considered modules - however, not all modules are packages.I've fit a Pipeline object with RandomizedSearchCV I want to access the coef_ attribute of the best_estimator_ but I'm unable to do that. I've tried accessing coef_ with the code below. sgd_randomized_pipe.best_estimator_.coef_ However I get the following AttributeError...  AttributeError: 'Pipeline' object has no attribute 'coef_' The scikit-learn docs say that coef_ is an attribute of SGDClassifier, which is the class of my base_estimator_.  What am I doing wrong? You can always use the names you assigned to them while making the pipeline by using the named_steps dict. and then access all the attributes like coef_, intercept_ etc. which are available to corresponding fitted estimator. This is the formal attribute exposed by the Pipeline as specified in the documentation: named_steps : dict Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters. I think this should work: I've found one way to do this is by chained indexing with the steps attribute... sgd_randomized_pipe.best_estimator_.steps[1][1].coef_ Is this best practice, or is there another way? In short, in scikit-learn there are two ways to access the estimators chained together in a Pipline: either retrieved by index or retrieved by name. (And each way again has two flavours, i.e. directly vs. indirectly.) Firstly, as the User Guide of sklearn points out, The Pipline is built using a list of (key, value) pairs (i.e. steps), where the key is a string containing the name you want to give this step and value is an estimator object. Which indicates that: a pipline is constructed by one or multiple estimator objects, in order. (just like a list) and each estimator object has a name, either appointed by the user (with the key) or automatically set (e.g. by using make_pipeline utility function) So finaly, we can access the estimators in a Pipline either From here on, I hope we could play around the piplines like a skilled plumber.The keras BatchNormalization layer uses axis=-1 as a default value and states that the feature axis is typically normalized. Why is this the case? I suppose this is surprising because I'm more familiar with using something like StandardScaler, which would be equivalent to using axis=0. This would normalize the features individually. Is there a reason why samples are individually normalized by default (i.e. axis=-1) in keras as opposed to features? Edit: example for concreteness It's common to transform data such that each feature has zero mean and unit variance. Let's just consider the "zero mean" part with this mock dataset, where each row is a sample: Wouldn't it make more sense to subtract the axis=0 mean, as opposed to the axis=1 mean? Using axis=1, the units and scales can be completely different. Edit 2: The first equation of section 3 in this paper seems to imply that axis=0 should be used for calculating expectations and variances for each feature individually, assuming you have an (m, n) shaped dataset where m is the number of samples and n is the number of features. Edit 3: another example I wanted to see the dimensions of the means and variances BatchNormalization was calculating on a toy dataset: The input X has shape (150, 4), and the BatchNormalization layer calculated 4 means, which means it operated over axis=0. If BatchNormalization has a default of axis=-1 then shouldn't there be 150 means? The confusion is due to the meaning of axis in np.mean versus in BatchNormalization. When we take the mean along an axis, we collapse that dimension and preserve all other dimensions. In your example data.mean(axis=0) collapses the 0-axis, which is the vertical dimension of data. When we compute a BatchNormalization along an axis, we preserve the dimensions of the array, and we normalize with respect to the mean and standard deviation over every other axis. So in your 2D example BatchNormalization with axis=1 is subtracting the mean for axis=0, just as you expect. This is why bn.moving_mean has shape (4,). I know this post is old, but am still answering it because the confusion still lingers on in Keras documentation. I had to go through the code to figure this out: if your mini-batch is a matrix A mxn, i.e. m samples and n features, the normalization axis should be axis=0. As your said, what we want is to normalize every feature individually, the default axis = -1 in keras because when it is used in the convolution-layer, the dimensions of figures dataset are usually (samples, width, height, channal), and the batch samples are normalized long the channal axis(the last axis).I've been reading up on Decision Trees and Cross Validation, and I understand both concepts. However, I'm having trouble understanding Cross Validation as it pertains to Decision Trees. Essentially Cross Validation allows you to alternate between training and testing when your dataset is relatively small to maximize your error estimation. A very simple algorithm goes something like this:  The problem I can't figure out is at the end you'll have k Decision trees that could all be slightly different because they might not split the same way, etc. Which tree do you pick? One idea I had was pick the one with minimal errors (although that doesn't make it optimal just that it performed best on the fold it was given - maybe using stratification will help but everything I've read say it only helps a little bit).  As I understand cross validation the point is to compute in node statistics that can later be used for pruning. So really each node in the tree will have statistics calculated for it based on the test set given to it. What's important are these in node stats, but if your averaging your error.  How do you merge these stats within each node across k trees when each tree could vary in what they choose to split on, etc. What's the point of calculating the overall error across each iteration?  That's not something that could be used during pruning. Any help with this little wrinkle would be much appreciated.  The problem I can't figure out is at the end you'll have k Decision trees that could all be slightly different because they might not split the same way, etc. Which tree do you pick? The purpose of cross validation is not to help select a particular instance of the classifier (or decision tree, or whatever automatic learning application) but rather to qualify the model, i.e. to provide metrics such as the average error ratio, the deviation relative to this average etc. which can be useful in asserting the level of precision one can expect from the application.  One of the things cross validation can help assert is whether the training data is big enough. With regards to selecting a particular tree, you should instead run yet another training on 100% of the training data available, as this typically will produce a better tree. (The downside of the Cross Validation approach is that we need to divide the [typically little] amount of training data into "folds" and as you hint in the question this can lead to trees which are either overfit or underfit for particular data instances). In the case of decision tree, I'm not sure what your reference to statistics gathered in the node and used to prune the tree pertains to.  Maybe a particular use of cross-validation related techniques?... For the first part, and like the others have pointed out, we usually use the entire dataset for building the final model, but we use cross-validation (CV) to get a better estimate of the generalization error on new unseen data. For the second part, I think you are confusing CV with the validation set, used to avoid overfitting the tree by pruning a node when some function value computed on the validation set does not increase before/after the split. Cross validation isn't used for buliding/pruning the decision tree. It's used to estimate how good the tree (built on all of the data) will perform by simulating arrival of new data (by building the tree without some elements just as you wrote). I doesn't really make sense to pick one of the trees generated by it because the model is constrained by the data you have (and not using it all might actually be worse when you use the tree for new data).
    The tree is built over the data that you choose (usualy all of it). Pruning is usually done by using some heuristic (i.e. 90% of the elements in the node belongs to class A so we don't go any further or the information gain is too small). The main point of using cross-validation is that it gives you better estimate of the performance of your trained model when used on different data. Which tree do you pick? One option would be that you bulid a new tree using all your data for training set. It has been mentioned already that the purpose of the cross-validation is to qualify the model. In other words cross-validation provide us with an error/accuracy estimation of model generated with the selected "parameters" regardless of the used data. 
The corss-validation process can be repeated using deferent parameters untill we are satisfied with the performance. Then we can train the model with the best parameters on the whole data. I am currently facing the same problem, and I think there is no “correct” answer, since the concepts are contradictory and it’s a trade-off between model robustness and model interpretation. 
I basically chose the decision tree algorithm for the sake of easy interpretability, visualization and straight forward hands-on application.
On the other hand, I want to proof the robustness of the model using cross-validation. 
I think I will apply a two step approach:
1. Apply k-fold cross-validation to show robustness of the algorithm with this dataset
2. Use the whole dataset for the final decision tree for interpretable results.  You could also randomly choose a tree set of the cross-validation or the best performing tree, but then you would loose information of the hold-out set.I'm new in machine learning. I'm preparing my data for classification using Scikit Learn SVM. In order to select the best features I have used the following method:  Since my dataset consist of negative values, I get the following error: Can someone tell me how can I transform my data ? The error message Input X must be non-negative says it all: Pearson's chi square test (goodness of fit) does not apply to negative values. It's logical because the chi square test assumes frequencies distribution and a frequency can't be a negative number. Consequently, sklearn.feature_selection.chi2 asserts the input is non-negative. You are saying that your features are "min, max, mean, median and FFT of accelerometer signal". In many cases, it may be quite safe to simply shift each feature to make it all positive, or even normalize to [0, 1] interval as suggested by EdChum. If data transformation is for some reason not possible (e.g. a negative value is an important factor), you should pick another statistic to score your features: Since the whole point of this procedure is to prepare the features for another method, it's not a big deal to pick anyone, the end result usually the same or very close.I'm working on a particular binary classification problem with a highly unbalanced dataset, and I was wondering if anyone has tried to implement specific techniques for dealing with unbalanced datasets (such as SMOTE) in classification problems using Spark's MLlib. I'm using MLLib's Random Forest implementation and already tried the simplest approach of randomly undersampling the larger class but it didn't work as well as I expected. I would appreciate any feedback regarding your experience with similar issues. Thanks, As of this very moment, the class weighting for the Random Forest algorithm is still under development (see here) But If you're willing to try other classifiers - this functionality has been already added to the Logistic Regression.  Consider a case where we have 80% positives (label == 1) in the dataset, so theoretically we want to "under-sample" the positive class. 
The logistic loss objective function should treat the negative class (label == 0) with higher weight.  Here is an example in Scala of generating this weight, we add a new column to the dataframe for each record in the dataset:   Then, we create a classier as follow: For more details, watch here: https://issues.apache.org/jira/browse/SPARK-9610 A different issue you should check - whether your features have a "predictive power" for the label you're trying to predict. In a case where after under-sampling you still have low precision, maybe that has nothing to do with the fact that your dataset is imbalanced by nature. I would do a exploratory data analysis  - If the classifier doesn't do better than a random choice, there is a risk that there simply is no connection between features and class.  Overfitting - a low error on your training set and a high error on your test set might be an indication that you overfit using an overly flexible feature set. Bias variance - Check whether your classifier suffers from a high bias or high variance problem. I used the solution by @Serendipity, but we can optimize the balanceDataset function to avoid using a udf. I also added the ability to change the label column being used. This is the version of the function I ended up with: We create the classifier as he stated wtih: @dbakr Did you get an answer for your biased prediction on your imbalanced dataset ? Though I'm not sure it was your original plan, note that if you first subsample the majority class of your dataset by a ratio r, then, in order to get unbaised predictions for Spark's logistic regression, you can either:
-  use the rawPrediction provided by the transform() function and adjust the intercept with log(r) 
-  or you can train your regression with weights using .setWeightCol("classWeightCol") (see the article cited here to figure out the value that must be set in the weights).I want to evaluate a regression model build with scikitlearn using cross-validation and getting confused, which of the two functions cross_val_score and cross_val_predict I should use.
One option would be : An other one, to use the cv-predictions with the standard r2_score: I would assume that both methods are valid and give similar results. But that is only the case with small k-folds. While the r^2 is roughly the same for 10-fold-cv, it gets increasingly lower for higher k-values in the case of the first version using "cross_vall_score". The second version is mostly unaffected by changing numbers of folds. Is this behavior to be expected and do I lack some understanding regarding CV in SKLearn?  cross_val_score returns score of test fold where cross_val_predict returns predicted y values for the test fold. For the cross_val_score(), you are using the average of the output, which will be affected by the number of folds because then it may have some folds which may have high error (not fit correctly). Whereas, cross_val_predict() returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. [Note that only cross-validation strategies that assign all elements to a test set exactly once can be used]. So the increasing the number of folds, only increases the training data for the test element, and hence its result may not be affected much. Edit (after comment) Please have a look the following answer on how cross_val_predict works: How is scikit-learn cross_val_predict accuracy score calculated? I think that cross_val_predict will be overfit because as the folds increase, more data will be for train and less will for test. So the resultant label is more dependent on training data. Also as already told above, the prediction for one sample is done only once, so it may be susceptible to the splitting of data more. 
Thats why most of the places or tutorials recommend using the cross_val_score for analysis. So this question also bugged me and while the other's made good points, they didn't answer all aspects of OP's question. The true answer is: The divergence in scores for increasing k is due to the chosen metric R2 (coefficient of determination). For e.g. MSE, MSLE or MAE there won't be any difference in using cross_val_score or cross_val_predict. See the definition of R2: R^2 = 1 - (MSE(ground truth, prediction)/ MSE(ground truth, mean(ground truth))) The bold part explains why the score starts to differ for increasing k: the more splits we have, the fewer samples in the test fold and the higher the variance in the mean of the test fold.
Conversely, for small k, the mean of the test fold won't differ much of the full ground truth mean, as sample size is still large enough to have small variance. Proof: Output will be: Of course, there is another effect not shown here, which was mentioned by others.
With increasing k, there are more models trained on more samples and validated on fewer samples, which will effect the final scores, but this is not induced by the choice between cross_val_score and cross_val_predict. I think the difference can be made clear by inspecting their outputs. Consider this snippet: Notice the shapes: why are these so?
scores.shape has length 5 because it is a score computed with cross-validation over 5 folds (see argument cv=5). Therefore, a single real value is computed for each fold. That value is the score of the classifier: given true labels and predicted labels, how many answers the predictor were right in a particular fold? In this case, the y labels given in input are used twice: to learn from data and to evaluate the performances of the classifier. On the other hand, y_pred.shape has length 7040, which is the shape of the dataset. That is the length of the input dataset. This means that each value is not a score computed on multiple values, but a single value: the prediction of the classifier: given the input data and their labels, what is the prediction of the classifier on a specific example that was in a test set of a particular fold? Note that you do not know what fold was used: each output was computed on the test data of a certain fold, but you can't tell which (from this output, at least). In this case, the labels are used just once: to train the classifier. It's your job to compare these outputs to the true outputs to compute the score. If you just average them, as you did, the output is not a score, it's just the average prediction.Is it possible to get the file names that were loaded using flow_from_directory ? 
I have : I have a custom generator for my multi output model like: Node that at the moment I am generating random numbers for a but for real training , I wish to load up a json file that contains the bounding box coordinates for my images. For that I will need to get the file names that were generated using train_generator.next() method. After I have that , I can load the file, parse the json and pass it instead of a. It is also necessary that the ordering of the x variable and the list of the file names that I get is the same.  Yes is it possible, at least with version 2.0.4 (don't know about earlier version). The instance of ImageDataGenerator().flow_from_directory(...) has an attribute with filenames which is a list of all the files in the order the generator yields them and also an attribute batch_index. So you can do it like this: And every iteration on generator you can get the corresponding filenames like this: This will give you the filenames of the images in the current batch. You can make a pretty minimal subclass that returns the image, file_path tuple by inheriting the DirectoryIterator: In the init, I added a attribute that is the numpy version of self.filepaths so that we can easily index into that array to get the paths on each batch generation.   The only other change to the base class is to return a tuple that is the image batch super()._get_batches_of_transformed_samples(index_array) and the file paths self.filenames_np[index_array]. With that, you can make your generator like so: And then check with at least with version 2.2.4,you can do it like this or get the file path Here is an example that works with  shuffle=True as well. And also properly handles last batch. To make one pass: the below code might help. Overriding the flow_from_directory And to check your images and file path returned I needed exactly this and I developed a simple function that works with shuffle=True or shuffle=False. Then, you would use it as follows:I am using dropout in neural network model in keras. Little bit code is like For testing, I am using preds = model_1.predict_proba(image). But while testing Dropout is also participating to predict the score which should not be happen. I search a lot to disable the dropout but didn't get any hint yet. Do anyone have solution to disable the Dropout while testing in keras?? Keras does this by default. In Keras dropout is disabled in test mode. You can look at the code here and see that they use the dropped input in training and the actual input while testing. As far as I know you have to build your own training function from the layers and specify the training flag to predict with dropout (e.g. its not possible to specify a training flag for the predict functions). This is a problem in case you want to do GANs, which use the intermediate output for training and also train the network as a whole, due to a divergence between generated training images and generated test images. As previously stated, dropout in Keras happens only at train time (with proportionate weight adjustment during training such that learned weights are appropriate for prediction when dropout is disabled). This is not ideal for cases in which we wish to use a dropout NNET as a probabilistic predictor (such that it produces a distribution when asked to predict the same inputs repeatedly).  In other words, Keras' Dropout layer is designed to give you regularization at train time, but the "mean function" of the learned distribution when predicting. If you want to retain dropout for prediction, you can easily implement a permanent dropout ("PermaDropout") layer (this was based on suggestions made by F. Chollet on the GitHub discussion area for Keras):  By replacing any dropout layer in a Keras model with "PermaDropout", you'll get the probabilistic behavior in prediction as well. To activate dropout for inference time u simply have to specify training=True in the layer of interest (Dropout in our case): with training=False with training=True by default, training is set to False HERE a full example of the usage of droput at inference time Dropout removes certain neurons form play, and to compensate for that we usually take one of two ways. And keras uses the second form of correction as you can see here As newer Tensorflow versions are usually eager, you can try things like: This will give you predictions considering the behavior of the desired phase.
For custom training loops (where instead of model.fit you make the eager predictions and apply the gradients yourself), it's important to use this: I never tested the following, but in non-eager mode, you can also probably build a new model using the existing layers, but passing training=False to the call (functional API model): The Dropout layer has a call argument named 'training', when you use model.fit, Keras sets automatically this argument to true, and when you call model and use model(input), Keras sets this argument to false. You can use this argument in custom layers and models to control Dropout manually. See Keras's official documentation for more information.I already know "xgboost.XGBRegressor is a Scikit-Learn Wrapper interface for XGBoost." But do they have any other difference? xgboost.train is the low-level API to train the model via gradient boosting method.  xgboost.XGBRegressor and xgboost.XGBClassifier are the wrappers (Scikit-Learn-like wrappers, as they call it) that prepare the DMatrix and pass in the corresponding objective function and parameters. In the end, the fit call simply boils down to: This means that everything that can be done with XGBRegressor and XGBClassifier is doable via underlying xgboost.train function. The other way around it's obviously not true, for instance, some useful parameters of xgboost.train are not supported in XGBModel API. The list of notable differences includes: @Maxim, as of xgboost 0.90 (or much before), these differences don't exist anymore in that xgboost.XGBClassifier.fit: What I find is different is evals_result, in that it has to be retrieved separately after fit (clf.evals_result()) and the resulting dict is different because it can't take advantage of the name of the evals in the watchlist ( watchlist = [(d_train, 'train'), (d_valid, 'valid')] ) . From my opinion the main difference is the training/prediction speed. For further reference I will call the xgboost.train - 'native_implementation' and XGBClassifier.fit - 'sklearn_wrapper' I have made some benchmarks on a dataset shape (240000, 348) Fit/train time:
sklearn_wrapper time = 89 seconds
native_implementation time = 7 seconds Prediction time:
sklearn_wrapper = 6 seconds
native_implementation = 3.5 milliseconds I believe this is reasoned by the fact that sklearn_wrapper is designed to use the pandas/numpy objects as input where the native_implementation needs the input data to be converted into a xgboost.DMatrix object. In addition one can optimise n_estimators using a native_implementation. @Danil is suggesting significant differences in speed and @Mohammad correctly points out the necessity to convert data to DMatrix structure. So I have tried to replicate the benchmark in the Kaggle notebook environment. The results showed no major training/predicting speed difference among xgboost native and sklearn_wrapper. '1.6.1' CPU times: user 6min 8s, sys: 700 ms, total: 6min 9s
Wall time: 1min 34s CPU times: user 818 ms, sys: 1.01 ms, total: 819 ms
Wall time: 209 ms CPU times: user 6min 15s, sys: 1.2 s, total: 6min 16s
Wall time: 1min 37s XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
early_stopping_rounds=None, enable_categorical=False,
eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise', importance_type=None, interaction_constraints='',
learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=0,
num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,
reg_lambda=1, ...) CPU times: user 1.48 s, sys: 1.99 ms, total: 1.48 s
Wall time: 380 msThese are questions on how to calculate & reduce overfitting in machine learning. I think many new to machine learning will have the same questions, so I tried to be clear with my examples and questions in hope that answers here can help others. I have a very small sample of texts and I'm trying to predict values associated with them. I've used sklearn to calculate tf-idf, and insert those into a regression model for prediction. This gives me 26 samples with 6323 features - not a lot.. I know: Inserting those 26 samples of 6323 features (X) and associated scores (y), into a LinearRegression model, gives good predictions. These are obtained using leave-one-out cross validation, from cross_validation.LeaveOneOut(X.shape[0], indices=True) : Pretty good! Using ngrams (n=300) instead of unigrams (n=1), similar results occur, which is obviously not right. No 300-words occur in any of the texts, so the prediction should fail, but it doesn't: Question 1: This might mean that the prediction model is overfitting the data. I only know this because I chose an extreme value for the ngrams (n=300) which I KNOW can't produce good results. But if I didn't have this knowledge, how would you normally tell that the model is over-fitting? In other words, if a reasonable measure (n=1) were used, how would you know that the good prediction was a result of being overfit vs. the model just working well? Question 2: What is the best way of preventing over-fitting (in this situation) to be sure that the prediction results are good or not?  Question 3: If LeaveOneOut cross validation is used, how can the model possibly over-fit with good results? Over-fitting means the prediction accuracy will suffer - so why doesn't it suffer on the prediction for the text being left out? The only reason I can think of: in a tf-idf sparse matrix of mainly 0s, there is strong overlap between texts because so many terms are 0s - the regression then thinks the texts correlate highly. Please answer any of the questions even if you don't know them all. Thanks!  how would you normally tell that the model is over-fitting? One useful rule of thumb is that you may be overfitting when your model's performance on its own training set is much better than on its held-out validation set or in a cross-validation setting. That's not all there is to it, though. The blog entry I linked to describes a procedure for testing for overfit: plot training set and validation set error as a function of training set size. If they show a stable gap at the right end of the plot, you're probably overfitting. What is the best way of preventing over-fitting (in this situation) to be sure that the prediction results are good or not? Use a held-out test set. Only do evaluation on this set when you're completely done with model selection (hyperparameter tuning); don't train on it, don't use it in (cross-)validation. The score you get on the test set is the model's final evaluation. This should show whether you've accidentally overfit the validation set(s). [Machine learning conferences are sometimes set up like a competition, where the test set is not given to the researchers until after they've delivered their final model to the organisers. In the meanwhile, they can use the training set as they please, e.g. by testing models using cross-validation. Kaggle does something similar.] If LeaveOneOut cross validation is used, how can the model possibly over-fit with good results? Because you can tune the model as much as you want in this cross-validation setting, until it performs nearly perfectly in CV. As an extreme example, suppose that you've implemented an estimator that is essentially a random number generator. You can keep trying random seeds until you hit a "model" that produces very low error in cross-validation, but that doesn't you've hit the right model. It means you've overfit to the cross-validation. See also this interesting warstory.I have trained an XGBoostRegressor model. When I have to use this trained model for predicting for a new input, the predict() function throws a feature_names mismatch error, although the input feature vector has the same structure as the training data. Also, in order to build the feature vector in the same structure as the training data, I am doing a lot inefficient processing such as adding new empty columns (if data does not exist) and then rearranging the data columns so that it matches with the training structure. Is there a better and cleaner way of formatting the input so that it matches the training structure? This is the case where the order of column-names while model building is different from order of column-names while model scoring. I have used the following steps to overcome this error First load the pickle file extraxt all the columns with order in which they were used  reorder the pandas dataframe Try converting data into ndarray before passing it to fit/predict.
For eg:
if your train data is train_df and test data is test_df. Use below code: Now fit the model: Finally, predict: Hope this helps! From what I could find, the predict function does not take the DataFrame (or a sparse matrix) as input. It is one of the bugs which can be found here https://github.com/dmlc/xgboost/issues/1238 In order to get around this issue, use as_matrix() function in case of a DataFrame or toarray() in case of a sparse matrix. This is the only workaround till the bug is fixed or the feature is implemented in a different manner. I also had this problem when i used pandas DataFrame (non-sparse representation). I converted training and testing data into numpy ndarray. This how i got rid of that Error! I came across the same problem and it's been solved by adding passing the train dataframe column name to the test dataframe via adding the following code:  Check the exception. What you should see are two arrays. One is the column names of the dataframe you’re passing in and the other is the XGBoost feature names. They should be the same length. If you put them side by side in an Excel spreadsheet you will see that they are not in the same order. My guess is that the XGBoost names were written to a dictionary so it would be a coincidence if the names in then two arrays were in the same order.  The fix is easy. Just reorder your dataframe columns to match the XGBoost names: I'm contributing an answer as I experienced this problem when putting a fitted XGBRegressor model into production. Thus, this is a solution for cases where you cannot select column names from a y training or testing DataFrame, though there may be cross-over which could be helpful. The model had been fit on a Pandas DataFrame, and I was attempting to pass a single row of values as a np.array to the predict function. Processing the values of the array had already been performed (reverse label encoded, etc.), and the array was all numeric values. I got the familiar error: ValueError: feature_names mismatch followed by a list of the features, followed by a list of the same length: ['f0', 'f1' ....] While there are no doubt more direct solutions, I had little time and this fixed the problem: names = model.get_booster().feature_names result = model.predict(vector[names].iloc[[-1]]) The iloc transformation I found here. Selecting the feature names – as models in the Scikit Learn implementation do not have a feature_names attribute – using get_booster( ).feature_names I found in @Athar post above. Check out the the documentation to learn more. Hope this helps. Do this while creating the DMatrix for XGB: Do not pass X_train and X_test directly. XGBoostRegressor needs the columns(features) to be in the same order. Try Apply it on both train and test feature datasets. I was also facing the same issue and tried all techniques all failed. I was using the Pima diabetes dataset model. fit() was good but when it comes to manual testing using predict it was throwing errors in missing features names. Then I've tried something which works for me. The Columns are basically the independent variables columns in my dataset. Now there will be a question then each time do I need to try this complex method to just predict some. so the answer is No. After you pickle the model you can easily pass the model.predict([[test]]) there will be no problem you can see the complete code here Instead of xgb.predict(12,34,344), try: The issue is actually just the namings since xgboost internally converts the pd.DataFrame.columns to ctypes and does not revert that step. You can manually reset the state by just replacing the "wrong" feature names with the correct ones after the training:I'm wondering how to calculate precision and recall measures for multiclass multilabel classification, i.e. classification where there are more than two labels, and where each instance can have multiple labels? For multi-label classification you have two ways to go
First consider the following. The metrics are computed in a per datapoint manner. For each predicted label its only its score is computed, and then these scores are aggregated over all the datapoints. There are other metrics as well. Here the things are done labels-wise. For each label the metrics (eg. precision, recall) are computed and then these label-wise metrics are aggregated. Hence, in this case you end up computing the precision/recall for each label over the entire dataset, as you do for a binary classification (as each label has a binary assignment), then aggregate it. The easy way is to present the general form. This is just an extension of the standard multi-class equivalent. Macro averaged  Micro averaged  Here the  are the true positive, false positive, true negative and false negative counts respectively for only the  label.  Here $B$ stands for any of the confusion-matrix based metric. In your case you would plug in the standard precision and recall formulas. For macro average you pass in the per label count and then sum, for micro average you average the counts first, then apply your metric function. You might be interested to have a look into the code for the mult-label metrics here , which a part of the package mldr in R. Also you might be interested to look into the Java multi-label library MULAN. This is a nice paper to get into the different metrics:  A Review on Multi-Label Learning Algorithms  The answer is that you have to compute precision and recall for each class, then average them together. E.g. if you classes A, B, and C, then your precision is: Same for recall. I'm no expert, but this is what I have determined based on the following sources: https://list.scms.waikato.ac.nz/pipermail/wekalist/2011-March/051575.html
http://stats.stackexchange.com/questions/21551/how-to-compute-precision-recall-for-multiclass-multilabel-classification Now, to compute recall for label A you can read off the values from the confusion matrix and compute: Now, let us compute precision for label A, you can read off the values from the confusion matrix and compute: You just need to do the same for the remaining labels B and C. This applies to any multi-class classification problem. Here is the full article that talks about how to compute precision and recall for any multi-class classification problem, including examples. In python using sklearn and numpy: Simple averaging will do if the classes are balanced. Otherwise, recall for each real class needs to be weighted by prevalence of the class, and precision for each predicted label needs to be weighted by the bias (probability) for each label.  Either way you get Rand Accuracy. A more direct way is to make a normalized contingency table (divide by N so table adds up to 1 for each combination of label and class) and add the diagonal to get Rand Accuracy. But if classes aren't balanced, the bias remains and a chance corrected method such as kappa is more appropriate, or better still ROC analysis or a chance correct measure such as informedness (height above the chance line in ROC).I am using Scikit-learn for text classification. I want to calculate the Information Gain for each attribute with respect to a class in a (sparse) document-term matrix. (It was suggested that the formula above for Information Gain is the same measure as mutual information. This matches also the definition in wikipedia. Is it possible to use a specific setting for mutual information in scikit-learn to accomplish this task?) You can use scikit-learn's mutual_info_classif 
here is an example this will output a dictionary of each attribute, i.e. item in the vocabulary as keys and their information gain as values here is a sample of the output Here is my proposition to calculate the information gain using pandas: Using pure python:This question does not appear to be about programming within the scope defined in the help center. Closed 1 year ago. I know in regular neural nets people use batch norm before activation and it will reduce the reliance on good weight initialization. I wonder if it would do the same to RNN/lstm RNN when i use it. Does anyone have any experience with it? No, you cannot use Batch Normalization on a recurrent neural network, as the statistics are computed per batch, this does not consider the recurrent part of the network. Weights are shared in an RNN, and the activation response for each "recurrent loop" might have completely different statistical properties. Other techniques similar to Batch Normalization that take these limitations into account have been developed, for example Layer Normalization. There are also reparametrizations of the LSTM layer that allow Batch Normalization to be used, for example as described in Recurrent Batch Normalization by Coijmaans et al. 2016. Batch normalization applied to RNNs is similar to batch normalization applied to CNNs: you compute the statistics in such a way that the recurrent/convolutional properties of the layer still hold after BN is applied. For CNNs, this means computing the relevant statistics not just over the mini-batch, but also over the two spatial dimensions; in other words, the normalization is applied over the channels dimension.  For RNNs, this means computing the relevant statistics over the mini-batch and the time/step dimension, so the normalization is applied only over the vector depths. This also means that you only batch normalize the transformed input (so in the vertical directions, e.g. BN(W_x * x)) since the horizontal (across time) connections are time-dependent and shouldn't just be plainly averaged. In any non-recurrent network (convnet or not) when you do BN each layer gets to adjust the incoming scale and mean so the incoming distribution for each layer doesn't keep changing (which is what the authors of the BN paper claim is the advantage of BN). The problem with doing this for the recurrent outputs of an RNN is that the parameters for the incoming distribution are now shared between all timesteps (which are effectively layers in backpropagation-through-time, or BPTT). So the distribution ends up being fixed across the temporal layers of BPTT. This may not make sense as there may be structure in the data that varies (in a non-random way) across the time series. For example, if the time series is a sentence certain words are much more likely to appear in the beginning or end. So having the distribution fixed might reduce the effectiveness of BN. It is not commonly used, though I found this paper from 2017 shows a way to use batch normalization in the input-to-hidden and the hidden-to-hidden transformations trains faster and generalizes better on some problems. The answer is Yes and No. Why Yes, according to the paper layer normalization, in section it clearly indicates the usage of BN in RNNs.  Why No? The distribution of output at each timestep has to be stored and calcualted to conduct BN. Imagine that you pad the sequence input so all examples have the same length, so if the predict case is longer than all training cases, at some time step you have no mean/std of the output distribution summarized from the SGD training procedure. Meanwhile, at least in Keras, I believe the BN layer only consider the normalization in vertical direction, i.e., the sequence output. The horizontal direction, i.e., hidden_status, cell_status, are not normalized. Correct me if I an wrong here. In multiple-layer RNNs, you may consider using layer normalization tricks.While digging through the topic of neural networks and how to efficiently train them, I came across the method of using very simple activation functions, such as the rectified linear unit (ReLU), instead of the classic smooth sigmoids. The ReLU-function is not differentiable at the origin, so according to my understanding the backpropagation algorithm (BPA) is not suitable for training a neural network with ReLUs, since the chain rule of multivariable calculus refers to smooth functions only.
However, none of the papers about using ReLUs that I read address this issue. ReLUs seem to be very effective and seem to be used virtually everywhere while not causing any unexpected behavior. Can somebody explain to me why ReLUs can be trained at all via the backpropagation algorithm? To understand how backpropagation is even possible with functions like ReLU you need to understand what is the most important property of derivative that makes backpropagation algorithm works so well. This property is that : If you treat x0 as actual value of your parameter at the moment - you can tell (knowing value of a cost function and it's derivative)  how the cost function will behave when you change your parameters a little bit. This is most crucial thing in backpropagation. Because of the fact that computing cost function is crucial for a cost computation - you will need your cost function to satisfy the property stated above. It's easy to check that ReLU satisfy this property everywhere except a small neighbourhood of 0. And this is the only problem with ReLU - the fact that we cannot use this property when we are close to 0. To overcome that you may choose the value of ReLU derivative in 0 to either 1 or 0. On the other hand most of researchers don't treat this problem as serious simply because of the fact, that being close to 0 during ReLU computations is relatively rare.  From the above - of course - from the pure mathematical point of view it's not plausible to use ReLU with backpropagation algorithm. On the other hand - in practice it usually doesn't make any difference that it has this weird behaviour around 0.I have a TensorFlow model, and one part of this model evaluates the accuracy. The accuracy is just another node in the tensorflow graph, that takes in logits and labels. When I want to plot the training accuracy, this is simple: I have something like: Then, during my training loop, I have something like: Also inside that for loop, every say, 100 iterations, I want to evaluate the validation accuracy. I have a separate feed_dict for this, and I am able to evaluate the validation accuracy very nicely in python.  However, here is my problem: I want to make another summary for the validation accuracy, by using the accuracy node. I am not clear on how to do this though. Since I have the accuracy node it makes sense that I should be able to re-use it, but I am unsure how to do this exactly, such that I can also get the validation accuracy written out as a separate scalar_summary...  How might this be possible? You can reuse the the accuracy node but you need to use two different SummaryWriters, one for the training runs and one for the test data. Also you have to assign the scalar summary for accuracy to a variable. Then in your training loop you have the normal training and record your summaries with the train_writer. In addition you run the graph on the test set each 100th iteration and record only the accuracy summary with the test_writer. You can then point TensorBoard to the parent directory (summaries_dir) and it will load both data sets. This can be also found in the TensorFlow HowTo's https://www.tensorflow.org/versions/r0.11/how_tos/summaries_and_tensorboard/index.html To run the same operation but get summaries with different feed_dict data, simply attach two summary ops to that op. Say you want to run accuracy op on both validation and test data and want to get summaries for both: Also remember you can always pull raw (scalar) data out of the protobuff summary_str like this and do your own logging.I have a tensor of pictures, and would like to randomly select from it. I'm looking for the equivalent of np.random.choice().  Let's say I want 10 of these pictures. torch has no equivalent implementation of np.random.choice(), see the discussion here. The alternative is indexing with a shuffled index or random integers. Read more about torch.randint and torch.randperm. Second code snippet is inspired by this post in PyTorch Forums. torch.multinomial provides equivalent behaviour to numpy's random.choice (including sampling with/without replacement): For this size of tensor: The following code works fairly fast. It takes around 0.2s: Using torch.randperm, however, would take more than 20s: Try this: As the other answer mentioned, torch does not have choice. You can use randint or permutation instead:I am following google cloud machine learning tutorial and I am unable to Launch TensorBoard  I've followed the steps in the above tutorial (also set up my environment using docker container) until typing the below command in the terminal Where the terminal outputs the below prompt  When I visit http://172.17.0.2:8080 in my browser I see nothing (the server where this page is located is not responding).  Can someone please advice how I can launch Tensor Board ? Had the same problem this morning. Solved it with Navigated the browser to http://localhost:8088 Try This will open socket listening to all network interfaces, so you can connect from local host (same pc) or from the local network(from pc/mobile in network). I faced the same problem when used Tensorboard inside a Docker container. The successful steps in my case were:   docker run --name my_tensorboard_container -p 7777:8080 my_tensorboard_image bash tensorboard --bind_all --port 8080 --logdir ./logs Hope, it works in your case as well. It looks like the port 8080 is not open on your machine.
You can check it with this command line tool: netstat -a.   To open a specific port on google cloud platform, see this answer from SO. If you are using Google Cloud Shell you have to click on icon placed in upper left of shell window. There are 2 solutions(as far as i could check) to solve this problem: Instead of using the http://name:port_number, use 
http://localhost:port_number.
This is if you are using Chrome browser. If you are using firefox(recommended as it's really convenient), then you can open the link(which has your PC name) directly, which is displayed after executing the "tensorboard --logdir=logs/" command in cmd, i.e; http://name:port_number will work here.  (name here refers to the PC or user name) I don't know if that's the case, but tensorboard has some visualization problems in several browsers. Try to connect to http://172.17.0.2:8080 with a different browser (for example, on my macbook, safari doesn't work very well with tensorboard and I use google Chrome). as stated by 'rodrigo-silveira' this works for me as well.
just change the name of graph directory. here the directory is data/ here, the directory is logs, so when I typed below command in cmd, below window appeared. this is the window pop up you have to change port to the tensorboard port in the right toolbar in Gloud shell  In my case, the port was not open.
Opening the port, helped me. This process was done on Linux. Here is what I did: First I checked if the port is open or not. You can check it by above code. Then open the port by following code. Just change the port number in place of 8080 This should solve the issue.Can anyone give me a practicale example of a recurrent neural network in (pybrain) python in order to predict the next value of a sequence ?
(I've read the pybrain documentation and there is no clear example for it I think.)
I also found this question. But I fail to see how it works in a more general case. So therefore I'm asking if anyone here could work out a clear example of how to predict the next value of a sequence in pybrain, with a recurrent neural network. To give an example. Say for example we have a sequence of numbers in the range [1,7]. Now given for example the start of a new sequence: 1 3 5 7 2 4 6 7 1 3  what is/are the next value(s) This question might seem lazy, but I think there lacks a good and decent example of how to do this with pybrain.  Additionally: How can this be done if more than 1 feature is present: Example: Say for example we have several sequences (each sequence having 2 features) in the range [1,7]. Now given for example the start of a new sequences:  what is/are the next value(s) Feel free to use your own example as long it is similar to these examples and has some in depth explanation. Issam Laradji's worked for me to predict sequence of sequences, except my version of pybrain required a tuple for the UnserpervisedDataSet object: gives: =>  [1, 2, 5, 6, 2, 4, 5, 6, 1, 2, 5, 6, 7, 1, 4, 6, 1, 2, 2, 3, 6] To predict smaller sequences, just train it up as such, either as sub sequences or as overlapping sequences (overlapping shown here): gives: =>  [3, 5, 6, 2, 4, 5, 6, 1, 2, 5, 6] Not too good... These steps are meant to perform what you ask for in the first part of the question. 1) Create a supervised dataset that expects a sample and a target in its arguments, A succeeding sample is the target or label y of its predecessor x. We put the number 21 because each sample has 21 numbers or features. Please note that for standard notations in the second half of your question it is better to call feature1 and feature2 as sample1 and sample2 for a sequence, and let features denote the numbers in a sample. 2) Create Network, initialize trainer and run for 100 epochs Make sure to set the recurrent argument as True 3) Create the test data We created an unsupervised dataset because of the assumption that we don't have the labels or targets. 4) Predict the test sample using the trained network This should display the values of the expected fourth run. For the second case when a sequence can have more than sample, instead of creating a supervised dataset, create a sequential one ds = SequentialDataSet(21,21). Then, everytime you get a new sequence, call ds.newSequence() and add the samples -that you call features- in that sequence using ds.addSample(). Hope this is clear-cut :) If you wish to have the full code to save the trouble of importing the libraries, please let me know.Recently I started toying with neural networks. I was trying to implement an AND gate with Tensorflow. I am having trouble understanding when to use different cost and activation functions. This is a basic neural network with only input and output layers, no hidden layers. First I tried to implement it in this way. As you can see this is a poor implementation, but I think it gets the job done, at least in some way. So, I tried only the real outputs, no one hot true outputs. For activation functions, I used a sigmoid function and for cost function I used squared error cost function (I think its called that, correct me if I'm wrong).  I've tried using ReLU and Softmax as activation functions (with the same cost function) and it doesn't work. I figured out why they don't work. I also tried the sigmoid function with Cross Entropy cost function, it also doesn't work. after 5000 iterations: Question 1 - Is there any other activation function and cost function, that can work(learn) for the above network, without changing the parameters(meaning without changing W, x, b).  Question 2 - I read from a StackOverflow post here: [Activation Function] selection depends on the problem. So there are no cost functions that can be used anywhere? I mean there is no standard cost function that can be used on any neural network. Right? Please correct me on this.  I also implemented the AND gate with a different approach, with the output as one-hot true. As you can see the train_Y [1,0] means that the 0th index is 1, so the answer is 0. I hope you get it.  Here I have used a softmax activation function, with cross entropy as cost function. Sigmoid function as activation function fails miserably.  after 5000 iteration Question 3 So in this case what cost function and activation function can I use? How do I understand what type of cost and activation functions I should use? Is there a standard way or rule, or just experience only? Should I have to try every cost and activation function in a brute force manner? I found an answer here. But I am hoping for a more elaborate explanation.  Question 4 I have noticed that it takes many iterations to converge to a near accurate prediction. I think the convergance rate depends on the learning rate (using too large of will miss the solution) and the cost function (correct me if I'm wrong). So, is there any optimal way (meaning the fastest) or cost function for converging to a correct solution? I will answer your questions a little bit out of order, starting with more general answers, and finishing with those specific to your particular experiment. Activation functions Different activation functions, in fact, do have different properties. Let's first consider an activation function between two layers of a neural network. The only purpose of an activation function there is to serve as an nonlinearity. If you do not put an activation function between two layers, then two layers together will serve no better than one, because their effect will still be just a linear transformation. For a long while people were using sigmoid function and tanh, choosing pretty much arbitrarily, with sigmoid being more popular, until recently, when ReLU became the dominant nonleniarity. The reason why people use ReLU between layers is because it is non-saturating (and is also faster to compute). Think about the graph of a sigmoid function. If the absolute value of x is large, then the derivative of the sigmoid function is small, which means that as we propagate the error backwards, the gradient of the error will vanish very quickly as we go back through the layers. With ReLU the derivative is 1 for all positive inputs, so the gradient for those neurons that fired will not be changed by the activation unit at all and will not slow down the gradient descent. For the last layer of the network the activation unit also depends on the task. For regression you will want to use the sigmoid or tanh activation, because you want the result to be between 0 and 1. For classification you will want only one of your outputs to be one and all others zeros, but there's no differentiable way to achieve precisely that, so you will want to use a softmax to approximate it. Your example. Now let's look at your example. Your first example tries to compute the output of AND in a following form: Note that W1 and W2 will always converge to the same value, because the output for (x1, x2) should be equal to the output of (x2, x1). Therefore, the model that you are fitting is: x1 + x2 can only take one of three values (0, 1 or 2) and you want to return 0 for the case when x1 + x2 < 2 and 1 for the case when x1 + x2 = 2. Since the sigmoid function is rather smooth, it will take very large values of W and B to make the output close to the desired, but because of a small learning rate they can't get to those large values fast. Increasing the learning rate in your first example will increase the speed of convergence. Your second example converges better because the softmax function is good at making precisely one output be equal to 1 and all others to 0. Since this is precisely your case, it does converge quickly. Note that sigmoid would also eventually converge to good values, but it will take significantly more iterations (or higher learning rate). What to use. Now to the last question, how does one choose which activation and cost functions to use. These advices will work for majority of cases: If you do classification, use softmax for the last layer's nonlinearity and cross entropy as a cost function. If you do regression, use sigmoid or tanh for the last layer's nonlinearity and squared error as a cost function. Use ReLU as a nonlienearity between layers. Use better optimizers (AdamOptimizer, AdagradOptimizer) instead of GradientDescentOptimizer, or use momentum for faster convergence, Cost function and activation function play an important role in the learning phase of a neural network. The activation function, as explained in the first answer, gives the possibility to the network to learn non-linear functions, besides assuring to have small change in the output in response of small change in the input. A sigmoid activation function works well for these assumptions. Other activation functions do the same but may be less computational expensive, see activation functions for completeness. But, in general Sigmoid activation function should be avoid because the vanishing gradient problem. The cost function C plays a crucial role in the speed of learning of the neural network. Gradient-based neural networks learn in an iterative way by minimising the cost function, so computing the gradient of the cost function, and changing the weights in according to it. If a quadratic cost function is used, this means that its gradient with respect the weights, is proportional to the activation function first derivate. Now, if a sigmoid activation function is used this implies that when the output is close to 1 the derivate is very small, as you can see from the image, and so the neurons learns slow.  The cross-entropy cost function allows to avoid this problem. Even if you are using a sigmoid function, using a cross-entropy function as cost function, implies that its derivates with respect to the weights are not more proportional to the first derivate of the activation function, as happened with the quadratic function , but instead they are proportional to the output error. This implies that when the prediction output is far away to the target your network learns more quickly, and viceversa. Cross-entropy cost function should be used always instead of using a quadratic cost function, for classification problem, for the above explained. Note that, in neural networks the cross-entropy function has not always the same meaning as the cross-entropy function you meet in probability, there it is used to compare two probability distribution. In neural networks this can be true if you have a unique sigmoid output to the final layer and want to think about it as a probability distribution. But this losses meaning if you have multi-sigmoid neurons at the final layer.I need to code a Maximum Likelihood Estimator to estimate the mean and variance of some toy data.  I have a vector with 100 samples, created with numpy.random.randn(100). The data should have zero mean and unit variance Gaussian distribution. I checked Wikipedia and some extra sources, but I am a little bit confused since I don't have a statistics background. Is there any pseudo code for  a maximum likelihood estimator? I get the intuition of MLE but I cannot figure out where to start coding. Wiki says taking argmax of log-likelihood. What I understand is: I need to calculate log-likelihood by using different parameters and then I'll take the parameters which gave the maximum probability. What I don't get is: where will I find the parameters in the first place? If I randomly try different mean & variance to get a high probability, when should I stop trying? I just came across this, and I know its old, but I'm hoping that someone else benefits from this. Although the previous comments gave pretty good descriptions of what ML optimization is, no one gave pseudo-code to implement it. Python has a minimizer in Scipy that will do this. Here's pseudo code for a linear regression. This works great for me. Granted, this is just the basics. It doesn't profile or give CIs on the parameter estimates, but its a start. You can also use ML techniques to find estimates for, say, ODEs and other models, as I describe here. I know this question was old, hopefully you've figured it out since then, but hopefully someone else will benefit. If you do maximum likelihood calculations, the first step you need to take is the following: Assume a distribution that depends on some parameters. Since you generate your data (you even know your parameters), you "tell" your program to assume Gaussian distribution. However, you don't tell your program your parameters (0 and 1), but you leave them unknown a priori and compute them afterwards. Now, you have your sample vector (let's call it x, its elements are x[0] to x[100]) and you have to process it. To do so, you have to compute the following (f denotes the probability density function of the Gaussian distribution): As you can see in my given link, f employs two parameters (the greek letters µ and σ). You now have to calculate the values for µ and σ in a way such that f(x[0]) * ... * f(x[100]) takes the maximum possible value. When you've done that, µ is your maximum likelihood value for the mean, and σ is the maximum likelihood value for standard deviation. Note that I don't explicitly tell you how to compute the values for µ and σ, since this is a quite mathematical procedure I don't have at hand (and probably I would not understand it); I just tell you the technique to get the values, which can be applied to any other distributions as well. Since you want to maximize the original term, you can "simply" maximize the logarithm of the original term - this saves you from dealing with all these products, and transforms the original term into a sum with some summands. If you really want to calculate it, you can do some simplifications that lead to the following term (hope I didn't mess up anything):  Now, you have to find values for µ and σ such that the above beast is maximal. Doing that is a very nontrivial task called nonlinear optimization. One simplification you could try is the following: Fix one parameter and try to calculate the other. This saves you from dealing with two variables at the same time. You need a numerical optimisation procedure. Not sure if anything is implemented in Python, but if it is then it'll be in numpy or scipy and friends. Look for things like 'the Nelder-Mead algorithm', or 'BFGS'. If all else fails, use Rpy and call the R function 'optim()'. These functions work by searching the function space and trying to work out where the maximum is. Imagine trying to find the top of a hill in fog. You might just try always heading up the steepest way. Or you could send some friends off with radios and GPS units and do a bit of surveying. Either method could lead you to a false summit, so you often need to do this a few times, starting from different points. Otherwise you may think the south summit is the highest when there's a massive north summit overshadowing it. As joran said, the maximum likelihood estimates for the normal distribution can be calculated analytically.  The answers are found by finding the partial derivatives of the log-likelihood function with respect to the parameters, setting each to zero, and then solving both equations simultaneously.   In the case of the normal distribution you would derive the log-likelihood with respect to the mean (mu) and then deriving with respect to the variance (sigma^2) to get two equations both equal to zero.  After solving the equations for mu and sigma^2, you'll get the sample mean and sample variance as your answers. See the wikipedia page for more details.I am working on a signal classification problem and would like to scale the dataset matrix first, but my data is in a 3D format (batch, length, channels).
I tried to use Scikit-learn Standard Scaler: But I've got this error message: Found array with dim 3. StandardScaler expected <= 2 I think one solution would be to split the matrix by each channel in multiples 2D matrices, scale them separately and then put back in 3D format, but I wonder if there is a better solution.
Thank you very much. With only 3 line of code... You'll have to fit and store a scaler for each channel If you want to scale each feature differently, like StandardScaler does, you can use this: It simply flattens the features of the input before giving it to sklearn's StandardScaler. Then, it reshapes them back. The usage is the same as for the StandardScaler: prints The arguments with_mean and with_std are directly passed to StandardScaler and thus work as expected. copy=False won't work, since the reshaping does not happen inplace. For 2-D inputs, the NDStandardScaler works like the StandardScaler: prints just like in the sklearn example for StandardScaler. An elegant way of doing this is using class Inheritance as follows: Usage: I used Normalization scheme for my spatio-temporal data having shape of (2500,512,642) --> (samples, timesteps, features/spatial-locations).
The following code can be used for Normalization and its inverse also. Just reshaped the data like so. For zero padded use similar: You can use this class if you're dealing with pipelines There is another simple way by without using for-loops and making functions or methods. I just flattened the given input array by "array.reshape(-1,1)" then fit inside minmaxscaler. Later, after transformation, convert it into its original shape. For example I have implemented on cifar10 dataset from tensorflow:I implemented a gradient descent algorithm to minimize a cost function in order to gain a hypothesis for determining whether an image has a good quality. I did that in Octave. The idea is somehow based on the algorithm from the machine learning class by Andrew Ng Therefore I have 880 values "y" that contains values from 0.5 to ~12. And I have 880 values from 50 to 300 in "X" that should predict the image's quality. Sadly the algorithm seems to fail, after some iterations the value for theta is so small, that theta0 and theta1 become "NaN". And my linear regression curve has strange values... here is the code for the gradient descent algorithm:
(theta = zeros(2, 1);, alpha= 0.01, iterations=1500) And here is the computation for the costfunction: If you are wondering how the seemingly complex looking for loop can be vectorized and cramped into a single one line expression, then please read on. The vectorized form is: theta = theta - (alpha/m) * (X' * (X * theta - y))  Given below is a detailed explanation for how we arrive at this vectorized expression using gradient descent algorithm: This is the gradient descent algorithm to fine tune the value of θ:
 Assume that the following values of X, y and θ are given:  Here Further, whole objective of machine learning is to minimize Errors in predictions. Based on the above corollary, our Errors matrix is m x 1 vector matrix as follows:  To calculate new value of θj, we have to get a summation of all errors (m rows) multiplied by jth feature value of the training set X. That is, take all the values in E, individually multiply them with jth feature of the corresponding training example, and add them all together. This will help us in getting the new (and hopefully better) value of θj. Repeat this process for all j or the number of features. In matrix form, this can be written as:  This can be simplified as:
 More succinctly, it can be written as:
 Since (A * B)' = (B' * A'), and A'' = A, we can also write the above as  This is the original expression we started out with: i vectorized the theta thing...
may could help somebody I think that your computeCost function is wrong. 
I attended NG's class last year and I have the following implementation (vectorized): The rest of the implementation seems fine to me, although you could also vectorize them. Afterwards you are setting the temporary thetas (here called theta_1 and theta_2) correctly back to the "real" theta. Generally it is more useful to vectorize instead of loops, it is less annoying to read and to debug. If you are OK with using a least-squares cost function, then you could try using the normal equation instead of gradient descent. It's much simpler -- only one line -- and computationally faster. Here is the normal equation:
http://mathworld.wolfram.com/NormalEquation.html And in octave form: Here is a tutorial that explains how to use the normal equation: http://www.lauradhamilton.com/tutorial-linear-regression-with-octave While not scalable like a vectorized version, a loop-based computation of a gradient descent should generate the same results. In the example above, the most probably case of the gradient descent failing to compute the correct theta is the value of alpha.  With a verified set of cost and gradient descent functions and a set of data similar with the one described in the question, theta ends up with NaN values just after a few iterations if alpha = 0.01. However, when set as alpha = 0.000001, the gradient descent works as expected, even after 100 iterations. Using only vectors here is the compact implementation of LR with Gradient Descent in Mathematica: Note: Of course one assumes that X is a n * 2 matrix, with X[[,1]] containing only 1s' This should work:- its cleaner this way, and vectorized also If you remember the first Pdf file for Gradient Descent form machine Learning course, you would take care of learning rate. Here is the note from the mentioned pdf.  Implementation Note: If your learning rate is too large, J(theta) can di-
verge and blow up', resulting in values which are too large for computer
calculations. In these situations, Octave/MATLAB will tend to return
NaNs. NaN stands fornot a number' and is often caused by undened
operations that involve - infinity and +infinity.Is it possible  to use something like 1 - cosine similarity with scikit learn's KNeighborsClassifier? This answer says no, but on the documentation for KNeighborsClassifier, it says the metrics mentioned in DistanceMetrics are available. Distance metrics don't include an explicit cosine distance, probably because it's not really a distance, but supposedly it's possible to input a function into the metric. I tried inputting the scikit learn linear kernel into KNeighborsClassifier but it gives me an error that the function needs two arrays as arguments. Anyone else tried this? The cosine similarity is generally defined as xT y / (||x|| * ||y||), and outputs 1 if they are the same and goes to -1 if they are completely different. This definition is not technically a metric, and so you can't use accelerating structures like ball and kd trees with it. If you force scikit learn to use the brute force approach, you should be able to use it as a distance if you pass it your own custom distance metric object. There are methods of transforming the cosine similarity into a valid distance metric if you would like to use ball trees (you can find one in the JSAT library) Notice though, that xT y / (||x|| * ||y||) = (x/||x||)T (y/||y||). The euclidean distance can be equivalently written as sqrt(xTx + yTy − 2 xTy). If we normalize every datapoint before giving it to the KNeighborsClassifier, then x^T x = 1 for all x. So the euclidean distance will degrade to  sqrt(2 − 2x^T y). For completely the same inputs, we would get sqrt(2-2*1) = 0 and for complete opposites sqrt(2-2*-1)= 2. And it is clearly a simple shape, so you can get the same ordering as the cosine distance by normalizing your data and then using the euclidean distance. So long as you use the uniform weights option, the results will be identical to having used a correct Cosine Distance. KNN family class constructors have a parameter called metric, you can switch between different distance metrics you want to use in nearest neighbour model.
A list of available distance metrics can be found here If you want to use cosine metric for ranking and classification problem, you can use norm 2 Euclidean distance on normalized feature vector, that gives you same ranking/classification (predictions that made by argmax or argmin operations) results.I'm using Scikit-learn. Sometimes I need to have the probabilities of labels/classes instead of the labels/classes themselves. Instead of having Spam/Not Spam as labels of emails, I wish to have only for example: 0.78 probability a given email is Spam. For such purpose, I'm using predict_proba() with RandomForestClassifier as following: And I got those results: Where the second column is for class: Spam. However, I have two main issues with the results about which I am not confident. The first issue is that the results represent the probabilities of the labels without being affected by the size of my data? The second issue is that the results show only one digit which is not very specific in some cases where the 0.701 probability is very different from 0.708. Is there any way to get the next 5 digit for example? A RandomForestClassifier is a collection of DecisionTreeClassifier's. No matter how big your training set, a decision tree simply returns: a decision. One class has probability 1, the other classes have probability 0. The RandomForest simply votes among the results. predict_proba() returns the number of votes for each class (each tree in the forest makes its own decision and chooses exactly one class), divided by the number of trees in the forest. Hence, your precision is exactly 1/n_estimators. Want more "precision"? Add more estimators. If you want to see variation at the 5th digit, you will need 10**5 = 100,000 estimators, which is excessive. You normally don't want more than 100 estimators, and often not that many. I get more than one digit in my results, are you sure it is not due to your dataset ? (for example using a very small dataset would yield to simple decision trees and so to 'simple' probabilities). Otherwise it may only be the display that shows one digit, but try to print predictions[0,0]. I am not sure to understand what you mean by "the probabilities aren't affected by the size of my data". If your concern is that you don't want to predict, eg, too many spams, what is usually done is to use a threshold t such that you predict 1 if proba(label==1) > t. This way you can use the threshold to balance your predictions, for example to limit the global probabilty of spams. And if you want to globally analyse your model, we usually compute the Area under the curve (AUC) of the Receiver operating characteristic (ROC) curve (see wikipedia article here). Basically the ROC curve is a description of your predictions depending on the threshold t. Hope it helps! I am afraid the top-voted answer isn't correct (at least for the latest sklearn implementation). According to the docs, the probability of prediction is computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf. In other words, since Random Forest is a collection of decision trees, it predicts the probability of a new sample by averaging over its trees. A single tree calculates the probability by looking at the distribution of different classes within the leaf. Look at this image of a single decision tree to understand what it means to have different classes within the leaf. right leaf in 2nd child split has 75% yellow so prediction probability of class yellow will be 75%.
 The scenario mentioned in the top-voted answer will only occur when every leaf of all trees have data points belonging to only one class in them. References:pandas.factorize encodes input values as an enumerated type or categorical variable.  But how can I easily and efficiently convert many columns of a data frame? What about the reverse mapping step? Example: This data frame contains columns with string values such as "type 2" which I would like to convert to numerical values - and possibly translate them back later.  You can use apply if you need to factorize each column separately: If you need for the same string value the same numeric one: If you need to apply the function only for some columns, use a subset: Solution with factorize: Translate them back is possible via map by dict, where you need to remove duplicates by drop_duplicates: I also found this answer quite helpful:
https://stackoverflow.com/a/20051631/4643212 I was trying to take values from an existing column in a Pandas DataFrame (a list of IP addresses named 'SrcIP') and map them to numerical values in a new column (named 'ID' in this example). Solution: Result: I would like to redirect my answer: https://stackoverflow.com/a/32011969/1694714 Old answer  Another readable solution for this problem, when you want to keep the categories consistent across the the resulting DataFrame is using replace: Performs slightly worse than the example by @jezrael, but easier to read. Also, it might escalate better for bigger datasets. I can do some proper testing if anyone is interested.The R^2 value returned by scikit learn (metrics.r2_score()) can be negative. The docs say:  "Unlike most other scores, R² score may be negative (it need not
  actually be the square of a quantity R)." However the wikipedia article on R^2 mentions no R (not squared) quantity. Perhaps it uses absolute differences instead of square differences. I really have no idea The R^2 in scikit learn is essentially the same as what is described in the wikipedia article on the coefficient of determination (grep for "the most general definition"). It is 1 - residual sum of square / total sum of squares. The big difference between a classical stats setting and what you usually try to do with machine learning, is that in machine learning you evaluate your score on unseen data, which can lead to results outside [0,1]. If you apply R^2 to the same data you used to fit your model, it will lie within [0, 1] See also this very similar question Since R^2 = 1 - RSS/TSS, the only case where RSS/TSS > 1 happens when our model is even worse than the worst model assumed (which is the absolute mean model).  here RSS = sum of squares of difference between actual values(yi) and predicted values(yi^)
and TSS = sum of squares of difference between actual values (yi) and mean value (Before applying Regression). 
So you can imagine TSS representing the best(actual) model, and RSS being in between our best model and the worst absolute mean model in which case we'll get RSS/TSS < 1. 
If our model is even worse than the worst mean model then in that case RSS > TSS(Since difference between actual observation and mean value < difference predicted value and actual observation). Check here for better intuition with visual representation: https://ragrawal.wordpress.com/2017/05/06/intuition-behind-r2-and-other-regression-evaluation-metrics/I've trained a tree model with R caret. I'm now trying to generate a confusion matrix and keep getting the following error: Error in confusionMatrix.default(predictionsTree,  testdata$catgeory)
  : the data and reference factors must have the same number of levels The error occurs when generating the confusion matrix. The levels are the same on both objects. I cant figure out what the problem is. Their structure and levels are given below.
They should be the same. Any help would be greatly appreciated as its making me cracked!! Try use:  Thats worked for me. Maybe your model is not predicting a certain factor.
Use the table() function instead of confusionMatrix() to see if that is the problem. Try specifying na.pass for the na.action option: Change them into a data frame and then use them in confusionMatrix function: I had same issue but went ahead and changed it after reading data file like so.. data = na.omit(data) Thanks all for pointer! Might be there are missing values in the testdata, Add the following line before "predictionsTree <- predict(treeFit, testdata)" to remove NAs. I had the same error and now it works for me. The length problem you're running into is probably due to the presence of NAs in the training set -- either drop the cases that are not complete, or impute so that you do not have missing values.  make sure you installed the package with all its dependencies: If your data contains NAs then sometimes it will be considered as a factor level,So omit these NAs initially Then,if your model fit is predicting some incorrect level,then it is better to use tables   I just ran into the same problem, I solved it by using R ordered factor data type.Let's consider a multivariate regression problem (2 response variables: Latitude and Longitude). Currently, a few machine learning model implementations like Support Vector Regression sklearn.svm.SVR do not currently provide naive support of multivariate regression. For this reason, sklearn.multioutput.MultiOutputRegressor can be used. Example: My goal is to tune the parameters of SVR by sklearn.model_selection.GridSearchCV. Ideally, if the response was a single variable and not multiple, I would perform an operation as follows: However, as my response y_train is 2-dimensional, I need to use the MultiOutputRegressor on top of SVR. How can I modify the above code to enable this GridSearchCV operation? If not possible, is there a better alternative? For use without pipeline, put estimator__ before parameters: I just found a working solution. In the case of nested estimators, the parameters of the inner estimator can be accessed by estimator__.     Thank you, Marco. Adding to your answer here is a short illustrative example of a Randomized Search applied to a Multi-Ouput GradientBoostingRegressor.I'm looking for a way to produce a non-linear (preferably quadratic) curve, based on a 2D data set, for predictive purposes. Right now I'm using my own implementation of ordinary least squares (OLS) to produce a linear trend, but my trends are much more suited to a curve model. The data I'm analysing is system load over time. Here's the equation that I'm using to produce my linear coefficients:  I've had a look at Math.NET Numerics and a few other libs, but they either provide interpolation instead of regression (which is of no use to me), or the code just doesn't work in some way. Anyone know of any free open source libs or code samples that can produce the coefficients for such a curve? I used the MathNet.Iridium release because it is compatible with .NET 3.5 and VS2008. The method is based on the Vandermonde matrix. Then I created a class to hold my polynomial regression which then I use it like this: Calculated coefficients of [1,0.57,-0.15] with the output: Which matches the quadratic results from Wolfram Alpha.

 Edit 1
To get to the fit you want try the following initialization for x_data and y_data: which produces the following coefficients (from lowest power to highest) @ja72 code is pretty good. But I ported it on the present version of Math.NET (MathNet.Iridium is not supported for now as I understand) and optimized code size and performance (For instance, Math.Pow function is not used in my solution because of slow performance). It's also available on github:gist. I don't think you want non linear regression. Even if you are using a quadratic function, it is still called linear regression. What you want is called multivariate regression. If you want a quadratic you just add a x squared term to your dependent variables. I would take a look at http://mathforum.org/library/drmath/view/53796.html to try get an idea about how it can be done. Then this has a nice implementation that I think will help you.I am using scipy.optimize.fmin_l_bfgs_b to solve a gaussian mixture problem. The means of mixture distributions are modeled by regressions whose weights have to be optimized using EM algorithm. But sometimes I got a warning 'ABNORMAL_TERMINATION_IN_LNSRCH' in the information dictionary:  I do not get this warning every time, but sometimes. (Most get 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL' or 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'). I know that it means the minimum can be be reached in this iteration. I googled this problem. Someone said it occurs often because the objective and gradient functions do not match. But here I do not provide gradient function because I am using 'approx_grad'. What are the possible reasons that I should investigate? What does it mean by "rounding error dominate computation"? ====== I also find that the log-likelihood does not monotonically increase: It usually start decrease at the second or the third iteration, even through 'ABNORMAL_TERMINATION_IN_LNSRCH' does not occurs. I do not know whether it this problem is related to the previous one. Scipy calls the original L-BFGS-B implementation. Which is some fortran77 (old but beautiful and superfast code) and our problem is that the descent direction is actually going up.  The problem starts on line 2533 (link to the code at the bottom) In other words, you are telling it to go down the hill by going up the hill. The code tries something called line search a total of 20 times in the descent direction that you provide and realizes that you are NOT telling it to go downhill, but uphill. All 20 times. The guy who wrote it (Jorge Nocedal, who by the way is a very smart guy) put 20 because pretty much that's enough. Machine epsilon is 10E-16, I think 20 is actually a little too much. So, my money for most people having this problem is that your gradient does not match your function. Now, it could also be that "2. rounding errors dominate computation". By this, he means that your function is a very flat surface in which increases are of the order of machine epsilon (in which case you could perhaps rescale the function),
Now, I was thiking that maybe there should be a third option, when your function is too weird. Oscillations? I could see something like $\sin({\frac{1}{x}})$ causing this kind of problem. But I'm not a smart guy, so don't assume that there's a third case. So I think the OP's solution should be that your function is too flat. Or look at the fortran code. https://github.com/scipy/scipy/blob/master/scipy/optimize/lbfgsb/lbfgsb.f Here's line search for those who want to see it. https://en.wikipedia.org/wiki/Line_search Note. This is 7 months too late. I put it here for future's sake. As pointed out in the answer by Wilmer E. Henao, the problem is probably in the gradient. Since you are using approx_grad=True, the gradient is calculated numerically. In this case, reducing the value of epsilon, which is the step size used for numerically calculating the gradient, can help. I also got the error "ABNORMAL_TERMINATION_IN_LNSRCH" using the L-BFGS-B optimizer. While my gradient function pointed in the right direction, I rescaled the actual gradient of the function by its L2-norm. Removing that or adding another appropriate type of rescaling worked. Before, I guess that the gradient was so large that it went out of bounds immediately. The problem from OP was unbounded if I read correctly, so this will certainly not help in this problem setting. However, googling the error "ABNORMAL_TERMINATION_IN_LNSRCH" yields this page as one of the first results, so it might help others... I had a similar problem recently. I sometimes encounter the ABNORMAL_TERMINATION_IN_LNSRCH message after using fmin_l_bfgs_b function of scipy. I try to give additional explanations of the reason why I get this. I am looking for complementary details or corrections if I am wrong. In my case, I provide the gradient function, so approx_grad=False. My cost function and the gradient are consistent. I double-checked it and the optimization actually works most of the time. When I get ABNORMAL_TERMINATION_IN_LNSRCH, the solution is not optimal, not even close (even this is a subjective point of view). I can overcome this issue by modifying the maxls argument. Increasing maxls helps to solve this issue to finally get the optimal solution. However, I noted that sometimes a smaller maxls, than the one that produces ABNORMAL_TERMINATION_IN_LNSRCH, results in a converging solution. A dataframe summarizes the results. I was surprised to observe this. I expected that reducing maxls would not improve the result. For this reason, I tried to read the paper describing the line search algorithm but I had trouble to understand it. The line "search algorithm generates a sequence of
nested intervals {Ik} and a sequence of iterates αk ∈ Ik ∩ [αmin ; αmax] according to the [...] procedure". If I understand well, I would say that the maxls argument specifies the length of this sequence. At the end of the maxls iterations (or less if the algorithm terminates in fewer iterations), the line search stops. A final trial point is generated within the final interval Imaxls. I would say the the formula does not guarantee to get an αmaxls that respects the two update conditions, the minimum decrease and the curvature, especially when the interval is still wide. My guess is that in my case, after 11 iterations the generated interval I11 is such that a trial point α11 respects both conditions. But, even though I12 is smaller and still containing acceptable points, α12 is not. Finally after 24 iterations, the interval is very small and the generated αk respects the update conditions. Is my understanding / explanation accurate?
If so, I would then be surprised that when maxls=12, since the generated α11 is acceptable but not α12, why α11 is not chosen in this case instead of α12? Pragmatically, I would recommend to try a few higher maxls when getting ABNORMAL_TERMINATION_IN_LNSRCH.I'm very new to Keras. I trained a model and would like to predict some images stored in subfolders (like for training). For testing, I want to predict 2 images from 7 classes (subfolders). The test_generator below sees 14 images, but I get 196 predictions. Where is the mistake? Thanks a lot! You can change the value of batch_size in flow_from_directory from default value (which is batch_size=32 ) to batch_size=1. Then set the steps of predict_generator to the total number of your test images. Something like this:  Default batch_size in generator is 32. If you want to make 1 prediction for every sample of total nb_samples you should devide your nb_samples with the batch_size. Thus with a batch_size of 7 you only need 14/7=2 steps for your 14 images The problem is the inclusion of nb_samples in the predict_generator which is creating 14 batches of 14 images Use fit and predict, TensorFlow now supports both the methods with generators. Just incase someone finds himself here in future wondering why the accuracy score gotten from using model.predict and model.predictor differs. just use the model.predict_generator option regardless of the depracation warning. There seems to be an issue with model.predict when used with generators.I built a simple generator that yields a tuple(inputs, targets) with only single items in the inputs and targets lists. Basically, it is crawling the data set, one sample item at a time. I pass this generator into:  I get that: But what is max_q_size for and why would it default to 10?  I thought the purpose of using a generator was to batch data sets into reasonable chunks, so why the additional queue?  This simply defines the maximum size of the internal training queue which is used to "precache" your samples from generator. It is used during generation of the the queues In other words you have a thread filling the queue up to given, maximum capacity directly from your generator, while (for example) training routine consumes its elements (and sometimes waits for the completion) and why default of 10? No particular reason, like most of the defaults - it simply makes sense, but you could use different values too. Construction like this suggests, that authors thought about expensive data generators, which might take time to execture. For example consider downloading data over a network in generator call - then it makes sense to precache some next batches, and download next ones in parallel for the sake of efficiency and to be robust to network errors etc. You might want to pay attention of using max_q_size in combination with fit_generator. In fact, the batch size you declare and use in the generator function will be considered as one single input, which is not the case. So a batch size of 1000 images and a max_q_size of 2000 will result into a real max_q_size of 2000x1000 = 2,000,000 images, which is not healthy for your memory. This is the reason why sometimes the Keras model never stop getting increased in the memory until the training process crashesI want to use the flow_from_directory method of the ImageDataGenerator
to generate training data for a regression model, where the target value can be any float value between 1 and -1. flow_from_directory has a "class_mode" parameter with the description class_mode: one of "categorical", "binary", "sparse" or None. Default:
"categorical". Determines the type of label arrays that are returned:
"categorical" will be 2D one-hot encoded labels, "binary" will be 1D
binary labels, "sparse" will be 1D integer labels. Which of these values should I take? None of them seems to really fit... With Keras 2.2.4 you can use flow_from_dataframe which solves what you want to do, allowing you to flow images from a directory for regression problems. You should store all your images in a folder and load a dataframe containing in one column the image IDs and in the other column the regression score (labels) and set class_mode='other' in flow_from_dataframe. Here you can find an example where the images are in image_dir, the dataframe with the image IDs and the regression scores is loaded with pandas from the "train file" I think that organizing your data differently, using a DataFrame (without necessarily moving your images to new locations) will allow you to run a regression model.  In short, create columns in your DataFrame containing the file path of each image and the target value.  This allows your generator to keep regression values and images properly synced even when you shuffle your data at each epoch. Here is an example showing how to link images with binomial targets, multinomial targets and regression targets just to show that "a target is a target is a target" and only the model might change: I describe how to do this in great detail with examples here: https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43 At this moment (newest version of Keras from January 21st 2017) the flow_from_directory could only work in a following manner: You need to have a directories structured in a following manner: So as you can see it could only be used for a classification case and all options provided in a documentation specify only a way in which the class is provided to your classifier. But, there is a neat hack which could make a flow_from_directory useful for a regression task: You need to structure your directory in a following manner:  You also need to have a list list_of_values = [1st value, 2nd value, ...]. Then your generator is defined in a following manner: And it's crucial for a flow_from_directory_gen to have a class_mode='sparse' to make this work. Of course this is a little bit cumbersome but it works (I used this solution :) ) There's just one glitch in the accepted answer that I would like to point out. The above code fails with an error message like: This is because y is an array. The fix is simple: The method to generate the list_of_values can be found in https://stackoverflow.com/a/47944082/4082092When doing regression or classification, what is the correct (or better) way to preprocess the data? Which of the above is more correct, or is the "standardized" way to preprocess the data? By "normalize" I mean either standardization, linear scaling or some other techniques. You should normalize the data before doing PCA. For example, consider the following situation. I create a data set X with a known correlation matrix C: If I now perform PCA, I correctly find that the principal components (the rows of the weights vector) are oriented at an angle to the coordinate axes: If I now scale the first feature of the data set by 100, intuitively we think that the principal components shouldn't change: However, we now find that the principal components are aligned with the coordinate axes: To resolve this, there are two options. First, I could rescale the data: (The weird bsxfun notation is used to do vector-matrix arithmetic in Matlab - all I'm doing is subtracting the mean and dividing by the standard deviation of each feature). We now get sensible results from PCA: They're slightly different to the PCA on the original data because we've now guaranteed that our features have unit standard deviation, which wasn't the case originally. The other option is to perform PCA using the correlation matrix of the data, instead of the outer product: In fact this is completely equivalent to standardizing the data by subtracting the mean and then dividing by the standard deviation. It's just more convenient. In my opinion you should always do this unless you have a good reason not to (e.g. if you want to pick up differences in the variation of each feature). You need to normalize the data first always. Otherwise, PCA or other techniques that are used to reduce dimensions will give different results. Normalize the data at first. Actually some R packages, useful to perform PCA analysis, normalize data automatically before performing PCA. 
If the variables have different units or describe different characteristics, it is mandatory to normalize. the answer is the 3rd option as after doing pca we have to normalize the pca output as the whole data will have completely different standard. we have to normalize the dataset before and after PCA as it will more accuarate. I got another reason in PCA objective function.
May you see detail in this link
enter link description here
Assuming the X matrix has been normalized before PCA.I saw this tutorial in R w/ autoplot.  They plotted the loadings and loading labels: 
https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html I prefer Python 3 w/ matplotlib, scikit-learn, and pandas for my data analysis.  However, I don't know how to add these on?   How can you plot these vectors w/ matplotlib?  I've been reading Recovering features names of explained_variance_ratio_ in PCA with sklearn but haven't figured it out yet Here's how I plot it in Python  You could do something like the following by creating a biplot function.  Nice article here: https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=friends_link&sk=65bf5440e444c24aff192fedf9f8b64f RESULT  Try the ‘pca’ library. This will plot the explained variance, and create a biplot. I found the answer here by @teddyroland: https://github.com/teddyroland/python-biplot/blob/master/biplot.py I'd like to add a generic solution to this topic. After doing some careful research on existing solutions (including Python and R) and datasets (especially biological "omic" datasets). I figured out the following Python solution, which has the advantages of: Scale the scores (samples) and loadings (features) properly to make them visually pleasing in one plot. It should be pointed out that the relative scales of samples and features do not have any mathematical meaning (but their relative directions have), however, making them similarly sized can facilitate exploration. Can handle high-dimensional data where there are many features and one could only afford to visualize the top several features (arrows) that drive the most variance of data. This involves explicit selection and scaling of the top features. An example of final output (using "Moving Pictures", a classical dataset in my research field):  Preparation: We will use the iris dataset (150 samples by 4 features). Here comes the critical part: Scale the features (arrows) properly to match the samples (points). The following code scales by the maximum absolute value of samples on each axis. Another way, as discussed in seralouk's answer, is to scale by range (max - min). But it will make the arrows larger than points. Then plot out the points and arrows:  Compare the result with that of the R solution. You can see that they are quite consistent. (Note: it is known that PCAs of R and scikit-learn have opposite axes. You can flip one of them to make the directions consistent.)   We will use the digits dataset (1797 samples by 64 features). Now, we will find the top k features that best explain our data. Method 1: Find top k arrows that appear the longest (i.e., furthest from the origin) in the visible plot: Method 2: Find top k features that drive most variance in the visible PCs: Now there is a new problem: When the feature number is large, because the top k features are only a very small portion of all features, their contribution to data variance is tiny, thus they will look tiny in the plot. To solve this, I came up with the following code. The rationale is: For all features, the sum of square loadings is always 1 per PC. With a small portion of features, we should bring them up such that the sum of square loadings of them is also 1. This method is tested and working, and generates nice plots. Then we will scale the arrows to match the samples (as discussed above): Now we can render the biplot:  I hope my answer is useful to the community. To plot the PCA loadings and loading labels in a biplot using matplotlib and scikit-learn, you can follow these steps: After fitting the PCA model using decomposition.PCA, retrieve the loadings matrix using the components_ attribute of the model. The loadings matrix is a matrix of the loadings of each original feature on each principal component. Determine the length of the loadings matrix and create a list of tick labels using the names of the original features. Normalize the loadings matrix so that the length of each loading vector is 1. This will make it easier to visualize the loadings on the biplot. Plot the loadings as arrows on the biplot using pyplot.quiver. Set the length of the arrows to the absolute value of the loading and the angle to the angle of the loading in the complex plane. Add the tick labels to the biplot using pyplot.xticks and pyplot.yticks. Here is an example of how you can modify your code to plot the PCA loadings and loading labels in a biplot
Add the loading labels to the biplot using pyplot.text. You can specify the position of the label using the coordinates of the corresponding loading vector, and set the font size and color using the font size and color parameters. Plot the data points on the biplot using pyplot.scatter. Add a legend to the plot using pyplot.legend to distinguish the different species. Here is the complete code with the above modifications applied:i am trying to use scikit learn 0.17 with anaconda 2.7 for a multilabel classification problem. here is my code and here is what my data looks like training test but i get the error what does this mean? here is the full stacktrace how do i fix this? do i need to change the format of my data? why does gridSearchTS.fit(Xtrain, ytrain) fail? how do i make X and y suitable for the fit function?  Edit I tried  but now i get  on  do i have to binarize X as well? why do i need to convert the X dimension to float?  The documentation gives this example: MultiLabelBinarizer.fit_transform takes in your labeled sets and can output the binary array. The output should then be alright to pass to your fit function.Official documents state that "It is not recommended to use pickle or cPickle to save a Keras model." However, my need for pickling Keras model stems from hyperparameter optimization using sklearn's RandomizedSearchCV (or any other hyperparameter optimizers). It's essential to save the results to a file, since then the script can be executed remotely in a detached session etc. Essentially, I want to: As of now, Keras models are pickle-able. But we still recommend using model.save() to save model to disk. This works like a charm http://zachmoshe.com/2017/04/03/pickling-keras-models.html: PS. I had some problems, due to my model.to_json() raised TypeError('Not JSON Serializable:', obj) due to circular reference, and this error has been swallowed by the code above somehow, hence resulting in pickle function running forever.  Have a look at this link:   Unable to save DataFrame to HDF5 ("object header message is too large") You can Pickle a Keras neural network by using the deploy-ml module which can be installed via pip  Full training and deployment of a kera neural network using the deploy-ml wrapper looks like this: The Pickled file contains the model, the metrics from the testing, a list of variable names and their order in which they have to be inputted, the version of Keras and python used, and if a scaler is used it will also be stored in the file. Documentation is here. Loading and using the file is done by the following: I appreciate that the training can be a bit restrictive. Deploy-ml supports importing your own model for Sk-learn but it's still working on this support for Keras. However, I've found that you can create a deploy-ml NeuralNetworkBase object, define your own Keras neural network outside of Deploy-ml, and assign it to the deploy-ml model attribute and this works just fine:I am dealing with highly imbalanced data set and my idea is to obtain values of feature weights from my libSVM model. As for now I am OK with the linear kernel, where I can obtain feature weights, but when I am using rbf or poly, I fail to reach my objective. Here I am using sklearn for my model and it's easy to obtain feature weights for linear kernel using .coef_. Can anyone help me to do same thing for rbf or poly? What I've tried to do so far is given below: This is not only impossible, as stated in the documentation: Weights asigned to the features (coefficients in the primal problem). This is only available in the case of linear kernel. but also it doesn't make sense. In linear SVM the resulting separating plane is in the same space as your input features. Therefore its coefficients can be viewed as weights of the input's "dimensions".  In other kernels, the separating plane exists in another space - a result of kernel transformation of the original space. Its coefficients are not directly related to the input space. In fact, for the rbf kernel the transformed space is infinite-dimensional (you can get a starting point on this on Wikipedia of course). I was stuck with a similar problem but for a different reason.
My objective was to compute the inference not using the built-in SVC.predict.
Assuming that: I would like to compute predictions for trained models only using algebra.
Now the formula for linear inference is easy:  where  collectively are called weights. What makes matters super easy is that clf.coef_ gets you the weights.
So: Side note: the sum of multiplications is exactly what dot does on two vectors, and reshape for input vector is needed to conform with the expected predict input shape. But of course, for other kernels, it is more complicated than that, from this formula 
and previous answers we cannot pre-compute the weights since  are all tied in together. Now, this is where I've got stuck until I've got some help from a friend.
Who discovered this documentation page. It says that  is clf.dual_coef_ in scikit learn terms.
Once you know that this equation becomes easy as well. We now know the value of . One thing left to do is to calculate the kernel function, which depends on type of the kernel, for polynomial kernel of 3rd degree (this is the default degree for poly SVM in scikit)  roughly translates to np.power(clf.support_vectors_.dot(X), clf.degree). ** Now let's combine everything we've learned into this code snippet: If you run it you'll see that the assertions are passing, WOO HOO! We now can predict the results not using the predict, and I hope it may help with the question asked. Since now you can adjust dual coefficients the same way you wanted to adjust weights. ** But please pay attention that if you do not use gamma, also remove it from the "manual calculations", since it will just break otherwise. Also, it is an example of inference for polynomial kernel, for other kernels inference function should be adjusted accordingly. See documentationWant to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 2 years ago. I want to create a dataset that has the same format as the cifar-10 data set to use with Tensorflow. It should have images and labels. I'd like to be able to take the cifar-10 code but different images and labels, and run that code. First we need to understand the format in which the CIFAR10 data set is in. If we refer to: https://www.cs.toronto.edu/~kriz/cifar.html, and specifically, the Binary Version section, we see: the first byte is the label of the first image, which
is a number in the range 0-9. The next 3072 bytes are the values of
the pixels of the image. The first 1024 bytes are the red channel
values, the next 1024 the green, and the final 1024 the blue. The
values are stored in row-major order, so the first 32 bytes are the
red channel values of the first row of the image. Intuitively, we need to store the data in this format. What you can do next as sort of a baseline experiment first, is to get images that are exactly the same size and same number of classes as CIFAR10 and put them in this format. This means that your images should have a size of 32x32x3 and have 10 classes. If you can successfully run this, then you can go further on to factor cases like single channels, different size inputs, and different classes. Doing so would mean that you have to change many variables in the other parts of the code. You have to slowly work your way through. I'm in the midst of working out a general module. My code for this is in https://github.com/jkschin/svhn. If you refer to the svhn_flags.py code, you will see many flags there that can be changed to accommodate your needs. I admit it's cryptic now, as I haven't cleaned it up such that it is readable, but it works. If you are willing to spend some time taking a rough look, you will figure something out. This is probably the easy way to run your own data set on CIFAR10. You could of course just copy the neural network definition and implement your own reader, input format, batching, etc, but if you want it up and running fast, just tune your inputs to fit CIFAR10. EDIT: Some really really basic code that I hope would help. This would convert an image into a byte file that is ready for use in CIFAR10. For multiple images, just keep concatenating the arrays, as stated in the format above. To check if your format is correct, specifically for the Asker's use case, you should get a file size of 4274273 + 1 = 546988 bytes. Assuming your pictures are RGB and values range from 0-255. Once you verify that, you're all set to run in TensorFlow. Do use TensorBoard to perhaps visualize one image, just to guarantee correctness. EDIT 2: As per Asker's question in comments, If you really wanna it to work as it is, you need to study the function calls of CIFAR10 code. In cifar10_input, the batches are hardcoded. So you have to edit this line of code to fit the name of the bin file. Or, just distribute your images into 6 bin files evenly. I didn't find any of the answers to do what I wanted to I made my own solution. It can be found on my github here: https://github.com/jdeepee/machine_learning/tree/master This script will convert and amount of images to training and test data where the arrays are the same shape as the cifar10 dataset.  The code is commented so should be easy enough to follow. I should note it iterated through a master directory containing multiple folders which contain the images. for SVHN dataset
You can try like this for multiple input images:I have a computer vision algorithm I want to tune up using scipy.optimize.minimize. Right now I only want to tune up two parameters but the number of parameters might eventually grow so I would like to use a technique that can do high-dimensional gradient searches. The Nelder-Mead implementation in SciPy seemed like a good fit.  I got the code all set up but it seems that the minimize function really wants to use floating point values with a step size that is less than one.The current set of parameters are both integers and one has a step size of one and the other has a step size of two (i.e. the value must be odd, if it isn't the thing I am trying to optimize will convert it to an odd number). Roughly one parameter is a window size in pixels and the other parameter is a threshold (a value from 0-255).  For what it is worth I am using a fresh build of scipy from the git repo. Does anyone know how to tell scipy to use a specific step size for each parameter? Is there some way I can roll my own gradient function? Is there a scipy flag that could help me out? I am aware that this could be done with a simple parameter sweep, but I would eventually like to apply this code to much larger sets of parameters.  The code itself is dead simple: This is what my output looks like. As you can see we are repeating a lot of runs and not getting anywhere in the minimization.  Assuming that the function to minimize is arbitrarily complex (nonlinear), this is a very hard problem in general. It cannot be guaranteed to be solved optimal unless you try every possible option. I do not know if there are any integer constrained nonlinear optimizer (somewhat doubt it) and I will assume you know that Nelder-Mead should work fine if it was a contiguous function. Edit: Considering the comment from @Dougal I will just add here: Set up a coarse+fine grid search first, if you then feel like trying if your Nelder-Mead works (and converges faster), the points below may help... But maybe some points that help: Unfortunately, Scipy's built-in optimization tools don't easily allow for this.  But never fear; it sounds like you have a convex problem, and so you should be able to find a unique optimum, even if it won't be mathematically pretty.  Two options that I've implemented for different problems are creating a custom gradient descent algorithm, and using bisection on a series of univariate problems.  If you're doing cross-validation in your tuning, your loss function unfortunately won't be smooth (because of noise from cross-validation on different datasets), but will be generally convex. To implement gradient descent numerically (without having an analytical method for evaluating the gradient), choose a test point and a second point that is delta away from your test point in all dimensions.  Evaluating your loss function at these two points can allow you to numerically compute a local subgradient. It is important that delta be large enough that it steps outside of local minima created by cross-validation noise. A slower but potentially more robust alternative is to implement bisection for each parameter you're testing. If you know that the problem in jointly convex in your two parameters (or n parameters), you can separate this into n univariate optimization problems, and write a bisection algorithm which recursively hones in on the optimal parameters.  This can help handle some types of quasiconvexity (e.g. if your loss function takes a background noise value for part of its domain, and is convex in another region), but requires a good guess as to the bounds for the initial iteration. If you simply snap the requested x values to an integer grid without fixing xtol to map to that gridsize, you risk having the solver request two points within a grid cell, receiving the same output value, and concluding that it is at a minimum. No easy answer, unfortunately. Snap your floats x, y (a.k.a. winsize, threshold) to an integer grid inside your function, like this: Then Nelder-Mead will see function values only on the grid, and should give you near-integer x, y. (If you'd care to post your code someplace, I'm looking for test cases for a Nelder-Mead
with restarts.) The Nelder-Mead minimize method now lets you specify the initial simplex vertex points, so you should be able to set the simplex points far apart, and the simplex will then flop around and find the minimum and converge when the simplex size drops below 1.  https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html#optimize-minimize-neldermead The problem is that the algorithm gets stuck trying to shrink its (N+1) simplex.
I'd highly recommend for anyone new to the concept to learn more about the geographical shape of a simplex and figure out how the input parameters relate to the points on the simplex. Once you get a grasp of that then as I.P. Freeley suggested the problem can be solved by defining strong initial points for your simplex, Note that this is different than defining your x0 and goes into nelder-mead's dedicated options. Here is an example of a higher --4-- dimensional problem. Also note that the initial simplex has to have N+1 points in this case 5 and in your case 3. In this example the x0 gets ignored by the definition of the initial_simplex. Other useful option in high dimensional problems is the 'adaptive' option, which takes the number of parameters into acount while trying to set the models operational coefficients (ie. α, γ,ρ and σ for reflection, expansion, contraction and shrink respectively). And if you haven't already, I'd also recommend familiarizing yourself with the steps of the algorithm. Now as for the reason this problem is happening its because the method gets no good results in an expansion so it keeps shrinking the simplex smaller and smaller  trying to find out a better solution that may or may not exist.Need help in sklearn's Polynomial Features. It works quite well with one feature but whenever I add multiple features, it also outputs some values in the array besides the values raised to the power of the degrees.
For ex: For this array, when I try to  It outputs  Here, what is 8.69778000e+03,1.59229200e+04,2.61576000e+03 ? If you have features [a, b, c] the default polynomial features(in sklearn the degree is 2) should be [1, a, b, c, a^2, b^2, c^2, ab, bc, ca]. 2.61576000e+03 is 37.8x62.2=2615,76 (2615,76 = 2.61576000 x 10^3) In a simple way with the PolynomialFeatures you can create new features. There is a good reference here. Of course there are and disadvantages("Overfitting") of using PolynomialFeatures(see here). Edit:
We have to be careful when using the polynomial features. The formula for calculating the number of the polynomial features is N(n,d)=C(n+d,d) where n is the number of the features, d is the degree of the polynomial, C is binomial coefficient(combination). In our case the number is C(3+2,2)=5!/(5-2)!2!=10 but when the number of features or the degree is height the polynomial features becomes too many. For example: So in this case you may need to apply regularization to penalize some of the weights. It is quite possible that the algorithm will start to suffer from curse of dimensionality (here is also a very nice discussion). PolynomialFeatures generates a new matrix with all polynomial combinations of features with given degree. Like [a] will be converted into [1,a,a^2] for degree 2. You can visualize input being transformed into matrix generated by PolynomialFeatures. Output: You can see matrix generated in form of [1,a,a^2] To observe polynomial features on scatter plot, let's use number 1-100. Output:  Changing degree=3 ,we get:  The general way to check the features is with poly.get_feature_names(). In this case, it would be and 8.69778000e+03,1.59229200e+04,2.61576000e+03 would correspond to the a*b, a*c and b*cterms, correspondingly. You have 3-dimensional data and the following code generates all poly features of degree 2: This can also be generated with the following code: According scikit's 0.23 docs (and as far back as 0.15), PolynomialFeatures will [generate] a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].I am trying to create a CNN to classify data. My Data is X[N_data, N_features]
I want to create a neural net capable of classifying it. My problem is concerning the input shape of a Conv1D for the keras back end.  I want to repeat a filter over.. let say 10 features and then keep the same weights for the next ten features. 
For each data my convolutional layer would create N_features/10 New neurones.
How can i do so? What should I put in input_shape?   Any advice?
thank you! Try:  And reshape your x to shape (nb_of_examples, nb_of_features, 1). EDIT: Conv1D was designed for a sequence analysis - to have convolutional filters which would be the same no matter in which part of sequence we are. The second dimension is so called features dimension where you could have a vector of multiple features at each of timesteps. One may think about sequence dimension the same as spatial dimensions and feature dimension the same as channel dimension or color dimension in Conv2D. As @putonspectacles mentioned in his comment - you may set sequence dimension to None in order to make your network input length invariant. @Marcin's answer might work, but might suggestion given the documentation here: When using this layer as the first layer in a model, provide an
  input_shape argument (tuple of integers or None, e.g. (10, 128) for
  sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for
  variable-length sequences of 128-dimensional vectors. would be: Note that since input data (N_Data, N_features), we set the number of examples as unspecified (None). The strides argument controls the size of of the timesteps in this case. To input a usual feature table data of shape (nrows, ncols) to Conv1d of Keras, following 2 steps are needed:  For example, taking first 4 features of iris dataset:  To see usual format and its shape:  The output shows usual format and its shape:  Following code alters the format:  Output of above code data format and its shape:  This works well for Conv1d of Keras. For input_shape (4,1) is needed.I am trying to train neural networks in R using package nnet. Following is the information about my training data. I have truncated this information. When I run the following: I get the following error: When I try changing weights in the argument like: Then I get the following error: What is the mistake I am making? How do I avoid or correct this error? Maybe the problem is with my understanding of "weights". Either increase MaxNWts to something that will accommodate the size of your model, or reduce size to make your model smaller. You probably also want to think some more on exactly which variables to include in the model. Just looking at the data provided, name is a factor with more than 8000 levels; you're not going to get anything sensible out of it with only 10000 observations. city might be more useful, but again, 61 levels in something as complex as a neural net is likely to be marginal. Increase 'MaxNWts' option to something larger than 84581.  The option to set to increase the number of weights allowed in the network is MaxNWts, not weights (set to specify weights for each sample). Increase the MaxNWts parameter by passing it directlyI a trying to merge 2 sequential models in keras. Here is the code: Here is the error log: File
  "/nics/d/home/dsawant/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py",
  line 392, in is_keras_tensor
      raise ValueError('Unexpectedly found an instance of type ' + str(type(x)) + '. ' ValueError: Unexpectedly found an instance of
  type <class 'keras.models.Sequential'>. Expected a symbolic tensor
  instance. Some more log: ValueError: Layer merge_1 was called with an input that isn't a
  symbolic tensor. Received type: class 'keras.models.Sequential'.
  Full input: [keras.models.Sequential object at 0x2b32d518a780,
  keras.models.Sequential object at 0x2b32d521ee80]. All inputs to the
  layer should be tensors. How can I merge these 2 Sequential models that use different window sizes and apply functions like 'max', 'sum' etc to them? Using the functional API brings you all possibilities.  When using the functional API, you need to keep track of inputs and outputs, instead of just defining layers.  You define a layer, then you call the layer with an input tensor to get the output tensor. Models and layers can be called exactly the same way. For the merge layer, I prefer using other merge layers that are more intuitive, such as Add(), Multiply() and Concatenate() for instance.  This same idea apply to all the following layers. We keep updating the output tensor giving it to each layer and getting a new output (if we were interested in creating branches, we would use a different var for each output of interest to keep track of them): Now that we created the "path", it's time to create the Model. Creating the model is just like telling at which input tensors it starts and where it ends: Notice that since this model has two inputs, you have to train it with two different X_training vars in a list: Now, suppose you wanted only one input, and both model1 and model2 would take the same input.  The functional API allows that quite easily by creating an input tensor and feeding it to the models (we call the models as if they were layers):    In this case, the Model would consider this input:I'm using Keras to predict a time series. As standard I'm using 20 epochs.
I want to check if my model is learning well, by predicting for each one of the 20 epochs. By using model.predict() I'm getting only one prediction among all epochs (not sure how Keras selects it). I want all predictions, or at least the 10 best. Would anyone know how to help me? I think there is a bit of a confusion here. An epoch is only used while training the neural network, so when training stops (in this case, after the 20th epoch), then the weights correspond to the ones computed on the last epoch. Keras prints current loss values on the validation set during training after each epoch. If the weights after each epoch are not saved, then they are lost.  You can save weights for each epoch with the ModelCheckpoint callback, and then load them back with load_weights on your model. You can compute your predictions after each training epoch by implementing an appropriate callback by subclassing Callback and calling predict on the model inside the on_epoch_end function. Then to use it, you instantiate your callback, make a list and use it as keyword argument callbacks to model.fit. The following code will do the desired job: In case you want to make predictions on the test data, after every epoch while the training is going-on you can try this You need mention the callback during model.fit Similar to on_epoch_end there are many other methods provided by kerasMost examples of neural networks for classification tasks I've seen use the a softmax layer as output activation function. Normally, the other hidden units use a sigmoid, tanh, or ReLu function as activation function. Using the softmax function here would - as far as I know - work out mathematically too. I haven't found any publications about why using softmax as an activation in a hidden layer is not the best idea (except Quora question which you probably have already read) but I will try to explain why it is not the best idea to use it in this case : 1. Variables independence : a lot of regularization and effort is put to keep your variables independent, uncorrelated and quite sparse. If you use softmax layer as a hidden layer - then you will keep all your nodes (hidden variables) linearly dependent which may result in many problems and poor generalization. 2. Training issues : try to imagine that to make your network working better you have to make a part of activations from your hidden layer a little bit lower. Then - automaticaly you are making rest of them to have mean activation on a higher level which might in fact increase the error and harm your training phase. 3. Mathematical issues : by creating constrains on activations of your model you decrease the expressive power of your model without any logical explaination. The strive for having all activations the same is not worth it in my opinion. 4. Batch normalization does it better : one may consider the fact that constant mean output from a network may be useful for training. But on the other hand a technique called Batch Normalization has been already proven to work better, whereas it was reported that setting softmax as activation function in hidden layer may decrease the accuracy and the speed of learning. Softmax layers can be used within neural networks such as in Neural Turing Machines (NTM) and an improvement of those which are Differentiable Neural Computer (DNC).  To summarize, those architectures are RNNs/LSTMs which have been modified to contain a differentiable (neural) memory matrix which is possible to write and access through time steps.  Quickly explained, the softmax function here enables a normalization of a fetch of the memory and other similar quirks for content-based addressing of the memory. About that, I really liked this article which illustrates the operations in an NTM and other recent RNN architectures with interactive figures.  Moreover, Softmax is used in attention mechanisms for, say, machine translation, such as in this paper. There, the Softmax enables a normalization of the places to where attention is distributed in order to "softly" retain the maximal place to pay attention to: that is, to also pay a little bit of attention to elsewhere in a soft manner. However, this could be considered like to be a mini-neural network that deals with attention, within the big one, as explained in the paper. Therefore, it could be debated whether or not Softmax is used only at the end of neural networks. Hope it helps! Edit - More recently, it's even possible to see Neural Machine Translation (NMT) models where only attention (with softmax) is used, without any RNN nor CNN: http://nlp.seas.harvard.edu/2018/04/03/attention.html Use a softmax activation wherever you want to model a multinomial distribution. This may be (usually) an output layer y, but can also be an intermediate layer, say a multinomial latent variable z. As mentioned in this thread for outputs {o_i}, sum({o_i}) = 1 is a linear dependency, which is intentional at this layer. Additional layers may provide desired sparsity and/or feature independence downstream. Page 198 of Deep Learning (Goodfellow, Bengio, Courville) Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the softmax function. This can be seen as a generalization of the sigmoid function which was used to represent a probability
  distribution over a binary variable.
  Softmax functions are most often used as the output of a classifier, to represent the probability distribution over n different classes. More rarely, softmax functions can be used inside the model itself, if we wish the model to choose between one of n different options for some internal variable. Softmax function is used for the output layer only (at least in most cases) to ensure that the sum of the components of output vector is equal to 1 (for clarity see the formula of softmax cost  function). This also implies what is the probability of occurrence of each component (class) of the output and hence sum of the probabilities(or output components) is equal to 1. Softmax function is one of the most important output function used in deep learning within the neural networks (see Understanding Softmax in minute by Uniqtech). The Softmax function is apply where there are three or more classes of outcomes. The softmax formula takes the e raised to the exponent score of each value score and devide it by the sum of e raised the exponent scores values. For example, if I know the Logit scores of these four classes to be: [3.00, 2.0, 1.00, 0.10], in order to obtain the probabilities outputs, the softmax function can be apply as follows: import numpy as np  def softmax(x): print(softmax(scores)) Output: probabilities (p) = 0.642  0.236  0.087  0.035 The sum of all probabilities (p) = 0.642 + 0.236 + 0.087 + 0.035 = 1.00. You can try to substitute any value you know in the above scores, and you will get a different values. The sum of all the values or probabilities will be equal to one. That’s makes sense, because the sum of all probability is equal to one, thereby turning Logit scores to probability scores, so that we can predict better. Finally, the softmax output, can help us to understand and interpret Multinomial Logit Model. If you like the thoughts, please leave your comments below.I have imported nltk in python to calculate BLEU Score on Ubuntu. I understand how sentence-level BLEU score works, but I don't understand how corpus-level BLEU score work. Below is my code for corpus-level BLEU score: For some reason, the bleu score is 0 for the above code. I was expecting a corpus-level BLEU score of at least 0.5. Here is my code for sentence-level BLEU score Here the sentence-level BLEU score is 0.71 which I expect, taking into account the brevity-penalty and the missing word "a". However, I don't understand how corpus-level BLEU score work. Any help would be appreciated. TL;DR: (Note: You have to pull the latest version of NLTK on the develop branch in order to get a stable version of the BLEU score implementation) In Long: Actually, if there's only one reference and one hypothesis in your whole corpus, both corpus_bleu() and sentence_bleu() should return the same value as shown in the example above. In the code, we see that sentence_bleu is actually a duck-type of corpus_bleu: And if we look at the parameters for sentence_bleu: The input for sentence_bleu's references is a list(list(str)). So if you have a sentence string, e.g. "This is a cat", you have to tokenized it to get a list of strings, ["This", "is", "a", "cat"] and since it allows for multiple references, it has to be a list of list of string, e.g. if you have a second reference, "This is a feline", your input to sentence_bleu() would be: When it comes to corpus_bleu() list_of_references parameter, it's basically a list of whatever the sentence_bleu() takes as references: Other than look at the doctest within the nltk/translate/bleu_score.py, you can also take a look at the unittest at nltk/test/unit/translate/test_bleu_score.py to see how to use each of the component within the bleu_score.py. By the way, since the sentence_bleu is imported as bleu in the (nltk.translate.__init__.py](https://github.com/nltk/nltk/blob/develop/nltk/translate/init.py#L21), using  would be the same as: and in code: Let's take a look: You're in a better position than me to understand the description of the algorithm, so I won't try to "explain" it to you.  If the docstring does not clear things up enough, take a look at the source itself. Or find it locally:Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 22 days ago. The community reviewed whether to reopen this question 22 days ago and left it closed: Original close reason(s) were not resolved I have a classification problem and I would like to test all the available algorithms to test their performance in tackling the problem. If you know any classification algorithm other than these listed below, please list it here. The answers did not provide the full list of classifiers, so I have listed them below. You may want to look at the following question: How to list all scikit-learn classifiers that support predict_proba() The accepted answer shows the method to get all estimators in scikit which support predict_probas method. Just iterate and print all names without checking the condition and you get all estimators. (Classifiers, regressors, cluster etc) For only classifiers, modify it like below to check all classes that implement ClassifierMixin For versions >= 0.22, use this: instead of sklearn.utils.testing Points to note: You should check their respective reference docs before using them Another alternative is to use the module from sklearn.utils import all_estimators. Here's an example for importing all classifiers: Here's Colaboratory code with it working. Some code from Shaheer Akram answer is deprecated, so you can get actual import code with it (sklearn 1.0.2): Output Output (41 estimators)I am trying to use: train = optimizer.minimize(loss) but the standard optimizers do not work with tf.float64. Therefore I want to truncate my loss from tf.float64 to only tf.float32.   The short answer is that you can convert a tensor from tf.float64 to tf.float32 using the tf.cast() op: The longer answer is that this will not solve all of your problems with the optimizers. (The lack of support for tf.float64 is a known issue.) The optimizers require that all of the tf.Variable objects that you are trying to optimize must also have type tf.float32.How to get access of individual trees of a xgboost model in python/R ? Below I'm getting from Random Forest trees from sklearn. Do you want to inspect the trees?    In Python, you can dump the trees as a list of strings: > Or dump them to a file (with nice formatting): >I want to make a model which predicts the future response of the input signal, the architecture of my network is [3, 5, 1]: My questions are: So, I think it'd clear most of this up if we were to step back and discuss the role the bias unit is meant to play in a NN. A bias unit is meant to allow units in your net to learn an appropriate threshold (i.e. after reaching a certain total input, start sending positive activation), since normally a positive total input means a positive activation. For example if your bias unit has a weight of -2 with some neuron x, then neuron x will provide a positive activation if all other input adds up to be greater then -2. So, with that as background, your answers:I want to parallelize over single examples or batch of example (in my situation is that I only have cpus, I have up to 112). I tried it but I get a bug that the losses cannot have the gradient out of separate processes (which entirely ruins my attempt). I still want to do it and it essential that after the multiproessing happens that I can do an optimizer step. How do I get around it? I made a totally self contained example: Error: I am sure I want to do this. How should I be doing this? it seems it works but my question is in line 74 do I need to do this or for it to work properly in multiple CPUs? My current issue is that the cpu parallel job is slower than the serially running one when only cpus are available. I want to know how to set up python and parallel cpus. e.g. if I have X cpus how many processes should I be running...X? or what? How do I choose this number, even if its heursitics rough. related links from research: Torch will use multiple CPU to parallelize operations, so your serial is maybe using multi-core vectorization. Take this simple example No code is used to parallelize, however 80% of 12 CPUs with the default configuration.  You can use torch.set_num_threads to set intraop parallelism on CPU. In particular if you are running multiple process and you want each process to use a single CPU you may want to set in each process the intraop parallelism to 1. However, parallelizing the operations has a cost, I am unable go into the implementation details but we can run a quick experiment that shows the overhead of using multiple threads. The operations tends to run faster with parallelism
 But if we take the total CPU time, by multiplying by the number of threads, we see that the single thread version is more efficient.  If you are able to parallelize your experiment at a higher level, by running independent processes, you should try that with a single core for each process, otherwise each process will try to use all the CPUs and all of them will run very slowly because your system is overloaded. I modified hyperparameters of your example scripts intentionally in a way that weights in favor of multi process. If I remove the line Using DDP in a single node seems not particularly advantageous. Unless you have a model that does a lot of work that is particularly not well handled by pytorch intraop parallelism, have large batches, and preferrably models with less parameters and more operations, meaning less gradients to synchronize, e.g. a convolutional model on a very large input. Other scenarios where DDP might be helpful is if you are using too much python in your model, instead of vectorized operations.I am trying to use AdaBoostClassifier with a base learner other than DecisionTree. I have tried SVM and KNeighborsClassifier but I get errors. What are the classifiers that can be used with AdaBoostClassifier? Ok, we have a systematic method to find out all the base learners supported by AdaBoostClassifier. Compatible base learner's fit method needs to support sample_weight, which can be obtained by running following code: This results in following output: If the classifier doesn't implement predict_proba, you will have to set AdaBoostClassifier parameter algorithm = 'SAMME'. You shouldnot use SVM with Adaboost. Adaboost should use weak-classifier. Using of classifiers like SVM will result in overfitting. Any classifier that supports passing sample weights should work. SVC is one such classifier. What specific error message (and traceback) do you get? Can you provide a minimalistic reproduction case for this error (e.g. as a http://gist.github.com )?I am doing a text classification task with R, and I obtain a document-term matrix with size 22490 by 120,000 (only 4 million non-zero entries, less than 1% entries). Now I want to reduce the dimensionality by utilizing PCA (Principal Component Analysis). Unfortunately, R cannot handle this huge matrix, so I store this sparse matrix in a file in the "Matrix Market Format", hoping to use some other techniques to do PCA. So could anyone give me some hints for useful libraries (whatever the programming language), which could do PCA with this large-scale matrix with ease, or do a longhand PCA by myself, in other words, calculate the covariance matrix at first, and then calculate the eigenvalues and eigenvectors for the covariance matrix.  What I want is to calculate all PCs (120,000), and choose only the top N PCs, who accounts for 90% variance. Obviously, in this case, I have to give a threshold a priori to set some very tiny variance values to 0 (in the covariance matrix), otherwise, the covariance matrix will not be sparse and its size would be 120,000 by 120,000, which is impossible to handle with one single machine. Also, the loadings (eigenvectors) will be extremely large, and should be stored in sparse format.  Thanks very much for any help ! Note: I am using a machine with 24GB RAM and 8 cpu cores.  The Python toolkit scikit-learn has a few PCA variants, of which RandomizedPCA can handle sparse matrices in any of the formats supported by scipy.sparse. scipy.io.mmread should be able to parse the Matrix Market format (I never tried it, though). Disclaimer: I'm on the scikit-learn development team. EDIT: the sparse matrix support from RandomizedPCA has been deprecated in scikit-learn 0.14. TruncatedSVD should be used in its stead. See the documentation for details. Instead of running PCA, you could try Latent Dirichlet Allocation (LDA), which decomposes the document-word matrix into a document-topic and topic-word matrix. Here is a link to an R implementation: http://cran.r-project.org/web/packages/lda/ - there are quite a few implementations out there, though if you google. With LDA you need to specify a fixed number of topics (similar to principle components) in advance. A potentially better alternative is HDP-LDA (http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/npbayes-r21.tgz), which learns the number of topics that form a good representation of your corpus. If you can fit our dataset in memory (which it seems like you can), then you also shouldn't have a problem running the LDA code. As a number of people on the scicomp forum pointed out, there should be no need to compute all of the 120k principle components. Algorithms like http://en.wikipedia.org/wiki/Power_iteration calculate the largest eigenvalues of a matrix, and LDA algorithms will converge to a minimum-description-length representation of the data given the number of topics specified. In R big.PCA of bigpca package http://cran.r-project.org/web/packages/bigpca/bigpca.pdf does the job. text classification task  I resolved almost same problem using a  technique for PCA of sparse matrix .
This technique can handle very large sparse matrix.
The result shows such simple PCA outperfoms word2vec.
It intends the simple PCA outperforms LDA. I suppose you wouldn't be able to compute all principle components. But still you can obtain reduced dimension version of your dataset matrix. I've implemented a simple routine in MATLAB, which can easily be replicated in python. Compute the covariance matrix of your input dataset, and convert it to a dense matrix. Assuming S is you input 120,000 * 22490  sparse matrix, this would be like: Apply eigs function on the covariance matrix to obtain the first N dominant eigenvectors, And obtain pcs by projecting the zero centered matrix on eigenvectors, Sr is the reduced dimension version of S.I'd like to implement my own Gaussian kernel in Python, just for exercise. I'm using:
sklearn.svm.SVC(kernel=my_kernel) but I really don't understand what is going on. I expect the function my_kernel to be called with the columns of the X matrix as parameters, instead I got it called with X, X as arguments. Looking at the examples things are not clearer. What am I missing? This is my code: After reading the answer above, and some other questions and sites (1, 2, 3, 4, 5), I put this together for a gaussian kernel in svm.SVC(). Call svm.SVC() with kernel=precomputed. Then compute a Gram Matrix a.k.a. Kernel Matrix (often abbreviated as K).  Then use this Gram Matrix as the first argument (i.e. X) to svm.SVC().fit(): I start with the following code: that calls sklearn.svm.SVC() in svmTrain(), and then sklearn.svm.SVC().fit(): the Gram Matrix computation - used as a parameter to sklearn.svm.SVC().fit() - is done in gaussianKernelGramMatrix(): which uses gaussianKernel() to get a radial basis function kernel between x1 and x2 (a measure of similarity based on a gaussian distribution centered on x1 with sigma=0.1): Then, once the model is trained with this custom kernel, we predict with "the [custom] kernel between the test data and the training data": In short, to use a custom SVM gaussian kernel, you can use this snippet: For efficiency reasons, SVC assumes that your kernel is a function accepting two matrices of samples, X and Y (it will use two identical ones only during training) and you should return a matrix G where: and K is your "point-level" kernel function. So either implement a gaussian kernel that works in such a generic way, or add a "proxy" function like: and use it like:I am using the gbm function in R (gbm package) to fit stochastic gradient boosting models for multiclass classification. I am simply trying to obtain the importance of each predictor separately for each class, like in this picture from the Hastie book (the Elements of Statistical Learning) (p. 382).  However, the function summary.gbm only returns the overall importance of the predictors (their importance averaged over all classes). Does anyone know how to get the relative importance values? I think the short answer is that on page 379, Hastie mentions that he uses MART, which appears to only be available for Splus. I agree that the gbm package doesn't seem to allow for seeing the separate relative influence. If that's something you're interested in for a mutliclass problem, you could probably get something pretty similar by building a one-vs-all gbm for each of your classes and then getting the importance measures from each of those models. So say your classes are a, b, c, & d. You model a vs. the rest and get the importance from that model. Then you model b vs. the rest and get the importance from that model. Etc. Hopefully this function helps you. For the example I used data from the ElemStatLearn package. The function figures out what the classes for a column are, splits the data into these classes, runs the gbm() function on each class and plots the bar plots for these models. `  I did some digging into how the gbm package calculates importance and it is based on the ErrorReduction which is contained in the trees element of the result and can be accessed with pretty.gbm.trees(). Relative influence is obtained by taking the sum of this ErrorReduction over all trees for each variable. For a multiclass problem there are actually n.trees*num.classes trees in the model. So if there are 3 classes you can calculate the sum of the ErrorReduction for each variable over every third tree to get the importance for one class. I have written the following functions to implement this and then plot the results: In my real use for this I have over 40 features so I give an option to specify the number of features to plot. I also couldn't use faceting if I wanted the plots to be sorted separately for each class, which is why I used gridExtra.  Seems to give the same results as the built in relative.influence function if you sum the results over all the classes.How should I approach a situtation when I try to apply some ML algorithm (classification, to be more specific, SVM in particular) over some high dimensional input, and the results I get are not quite satisfactory? 1, 2 or 3 dimensional data can be visualized, along with the algorithm's results, so you can get the hang of what's going on, and have some idea how to aproach the problem. Once the data is over 3 dimensions, other than intuitively playing around with the parameters I am not really sure how to attack it? What do you do to the data? My answer: nothing. SVMs are designed to handle high-dimensional data. I'm working on a research problem right now that involves supervised classification using SVMs. Along with finding sources on the Internet, I did my own experiments on the impact of dimensionality reduction prior to classification. Preprocessing the features using PCA/LDA did not significantly increase classification accuracy of the SVM.  To me, this totally makes sense from the way SVMs work. Let x be an m-dimensional feature vector. Let y = Ax where y is in R^n and x is in R^m for n < m, i.e., y is x projected onto a space of lower dimension. If the classes Y1 and Y2 are linearly separable in R^n, then the corresponding classes X1 and X2 are linearly separable in R^m. Therefore, the original subspaces should be "at least" as separable as their projections onto lower dimensions, i.e., PCA should not help, in theory. Here is one discussion that debates the use of PCA before SVM: link What you can do is change your SVM parameters. For example, with libsvm link, the parameters C and gamma are crucially important to classification success. The libsvm faq, particularly this entry link, contains more helpful tips. Among them: EDIT: Let me just add this "data point." I recently did another large-scale experiment using the SVM with PCA preprocessing on four exclusive data sets. PCA did not improve the classification results for any choice of reduced dimensionality. The original data with simple diagonal scaling (for each feature, subtract mean and divide by standard deviation) performed better. I'm not making any broad conclusion -- just sharing this one experiment. Maybe on different data, PCA can help. Some suggestions: Project data (just for visualization) to a lower-dimensional space (using PCA or MDS or whatever makes sense for your data) Try to understand why learning fails. Do you think it overfits? Do you think you have enough data? Is it possible there isn't enough information in your features to solve the task you are trying to solve? There are ways to answer each of these questions without visualizing the data. Also, if you tell us what the task is and what your SVM output is, there may be more specific suggestions people could make. You can try reducing the dimensionality of the problem by PCA or the similar technique. Beware that PCA has two important points. (1) It assumes that the data it is applied to is normally distributed and (2) the resulting data looses its natural meaning (resulting in a blackbox). If you can live with that, try it. Another option is to try several parameter selection algorithms. Since SVM's were already mentioned here, you might try the approach of Chang and Li (Feature Ranking Using Linear SVM) in which they used linear SVM to pre-select "interesting features" and then used RBF - based SVM on the selected features. If you are familiar with Orange, a python data mining library, you will be able to code this method in less than an hour. Note that this is a greedy approach which, due to its "greediness" might fail in cases where the input variables are highly correlated. In that case, and if you cannot solve this problem with PCA (see above), you might want to go to heuristic methods, which try to select best possible combinations of predictors. The main pitfall of this kind of approaches is the high potential of overfitting. Make sure you have a bunch "virgin" data that was not seen during the entire process of model building. Test your model on that data only once, after you are sure that the model is ready. If you fail, don't use this data once more to validate another model, you will have to find a new data set. Otherwise you won't be sure that you didn't overfit once more.  List of selected papers on parameter selection:
Feature selection for high-dimensional genomic microarray data Oh, and one more thing about SVM. SVM is a black box. You better figure out what is the mechanism that generate the data and model the mechanism and not the data. On the other hand, if this would be possible, most probably you wouldn't be here asking this question (and I wouldn't be so bitter about overfitting). List of selected papers on parameter selection I would approach the problem as follows: What do you mean by "the results I get are not quite satisfactory"?  If the classification rate on the training data is unsatisfactory, it implies that either  If the classification rate on the test data is unsatisfactory, it implies that your model overfits the data: Of course it may be a mixture of the above elements. These are all "blind" methods to attack the problem. In order to gain more insight into the problem you may use visualization methods by projecting the data into lower dimensions or look for models which are suited better to the problem domain as you understand it (for example if you know the data is normally distributed you can use GMMs to model the data ...) If I'm not wrong, you are trying to see which parameters to the SVM gives you the best result. Your problem is model/curve fitting.
I worked on a similar problem couple of years ago. There are tons of libraries and algos to do the same. I used Newton-Raphson's algorithm and a variation of genetic algorithm to fit the curve. Generate/guess/get the result you are hoping for, through real world experiment (or if you are doing simple classification, just do it yourself). Compare this with the output of your SVM. The algos I mentioned earlier reiterates this process till the result of your model(SVM in this case) somewhat matches the expected values (note that this process would take some time based your problem/data size.. it took about 2 months for me on a 140 node beowulf cluster). If you choose to go with Newton-Raphson's, this might be a good place to start.I have a dataset of reviews which has a class label of positive/negative. I am applying Naive Bayes to that reviews dataset. Firstly, I am converting into Bag of words. Here sorted_data['Text'] is reviews and final_counts is a sparse matrix I am splitting the data into train and test dataset. I am applying the naive bayes algorithm as follows Here X_test is test dataset in which pred variable gives us whether the vector in X_test is positive or negative class. The X_test shape is (54626 rows, 82343 dimensions) length of pred is 54626  My question is I want to get the words with highest probability in each vector so that I can get to know by the words that why it predicted as positive or negative class. Therefore, how to get the words which have highest probability in each vector? You can get the importantance of each word out of the fit model by using the coefs_ or feature_log_prob_ attributes. For example Prints the top 10 most predictive words for each of your classes. I had the same trouble, maybe this is for datascience exchange forum but I want to post it here since I achieved a very good result.   First:
+ Stands for positive class ,
- Stands for negative class. 
P() stands for proability. We are going to build odds ratio, which can be demostrated that it is equal to
P(word i ,+) / P(word i ,-) (let me know if you need the demostration of it guys). 
If this ratio is greater than 1 means that the word i is more likely to occur 
in a positive texts than in negative text.  These are the priors in the naive bayes model: Create a dataframe for storing the words Most important words. This will hive you a >1 ratio. For example a 
odds_ratio_negative =2  for the word "damn" means that this word
is twice likely to occur when the comment or your class is negative in comparison
with your positive class.  Try this:When I use google colab I get this error more than once and randomly Sometimes it works and sometimes not  is this error occur when I interface with google drive ? 
any solutions for this bug  From the FAQ -- Google Drive operations can time out when the number of files or
  subfolders in a folder grows too large. If thousands of items are
  directly contained in the top-level "My Drive" folder then mounting
  the drive will likely time out. Repeated attempts may eventually
  succeed as failed attempts cache partial state locally before timing
  out. If you encounter this problem, try moving files and folders
  directly contained in "My Drive" into sub-folders. A similar problem
  can occur when reading from other folders after a
  successfuldrive.mount(). Accessing items in any folder containing many
  items can cause errors like OSError: [Errno 5] Input/output error
  (python 3) or IOError: [Errno 5] Input/output error (python 2). Again,
  you can fix this problem by moving directly contained items into
  sub-folders. I ran into this error while using os.listdir on a google drive folder that had over 5.5k files in it, and a little window in the bottom left corner of my colab notebook popped up saying a timeout had occurred.  Because I have Colab Pro, I tried switching my runtime Hardware Accelerator to a GPU and Runtime Shape to High-Ram. This fixed the problem for me. It might have been one or both of those options together, not sure.  The problem with the top answer is that you might need some simple functionality in Colab (like os.listdir) in order to efficiently move files and create sub-folders to achieve reduced folder contents. If you can't even list what's in a folder without a timeout error occurring, you may just need to upgrade to Colab Pro to gain those advanced runtime options for a more powerful computing environment.  Another possible solution would be to save your files in a different (new folder) directory. I think @bob-smith's solution is one the best solutions for this problem, I am just showing a variation of the original solution which worked for me.   I face it almost regularly along with a dialogue- A Google Drive timeout has occurred (most recently at 12:46:20 PM). More info. Sometimes if I run a code cell three times, the error doesn't occur anymore; sometimes I have to run the cell as much as 8-9 times to successfully execute it. The problem always happens during the data loading, as expected. In the data loading cell, I usually have splitting, item transforms, and batch transforms defined. So they add extra time cost when running the cell multiple times. What I do instead of running the data loading cell multiple times, I run an ls command on Bash using the ! method in a different cell. I usually look for a file (with a known file name) in the training directory and pass that pattern to a grep piped to the ls. Like so- If this cell executes successfully after n number of tries, and shows the desired filename in the output, the data loading cell runs successfully 100% of the time, and you can begin the training. It is important to note that I am not running a ls with the entire training directory without a grep because that will always fail as my training directory sometimes have about 100k files. This is an ugly hack but it works every time.There are two classes, let's call them X and O. A number of elements belonging to these classes are spread out in the xy-plane. Here is an example where the two classes are not linearly separable. It is not possible to draw a straight line that perfectly divides the Xs and the Os on each side of the line.  How to determine, in general, whether the two classes are linearly separable?. I am interested in an algorithm where no assumptions are made regarding the number of elements or their distribution. An algorithm of the lowest computational complexity is of course preferred. If you found the convex hull for both the X points and the O points separately (i.e. you have two separate convex hulls at this stage) you would then just need to check whether any segments of the hulls intersected or whether either hull was enclosed by the other. If the two hulls were found to be totally disjoint the two data-sets would be geometrically separable. Since the hulls are convex by definition, any separator would be a straight line. There are efficient algorithms that can be used both to find the convex hull (the qhull algorithm is based on an O(nlog(n)) quickhull approach I think), and to perform line-line intersection tests for a set of segments (sweepline at O(nlog(n))), so overall it seems that an efficient O(nlog(n)) algorithm should be possible. This type of approach should also generalise to general k-way separation tests (where you have k groups of objects) by forming the convex hull and performing the intersection tests for each group. It should also work in higher dimensions, although the intersection tests would start to become more challenging... Hope this helps. Computationally the most effective way to decide whether two sets of points are linearly separable is by applying linear programming. GLTK is perfect for that purpose and pretty much every highlevel language offers an interface for it - R, Python, Octave, Julia, etc. Let's say you have a set of points A and B:  Then you have to minimize the 0 for the following conditions: (The A below is a matrix, not the set of points from above)  "Minimizing 0" effectively means that you don't need to actually optimize an objective function because this is not necessary to find out if the sets are linearly separable.  In the end 
() is defining the separating plane.  In case you are interested in a working example in R or the math details, then check this out. Here is a naïve algorithm that I'm quite sure will work (and, if so, shows that the problem is not NP-complete, as another post claims), but I wouldn't be surprised if it can be done more efficiently: If a separating line exists, it will be possible to move and rotate it until it hits two of the X'es or one X and one O. Therefore, we can simply look at all the possible lines that intersect two X'es or one X and one O, and see if any of them are dividing lines. So, for each of the O(n^2) pairs, iterate over all the n-2 other elements to see if all the X'es are on one side and all the O's on the other. Total time complexity: O(n^3). Linear perceptron is guaranteed to find such separation if one exists.  See: http://en.wikipedia.org/wiki/Perceptron . You can probably apply linear programming to this problem. I'm not sure of its computational complexity in formal terms, but the technique is successfully applied to very large problems covering a wide range of domains. Computing a linear SVM then determining which side of the computed plane with optimal marginals each point lies on will tell you if the points are linearly separable. This is overkill, but if you need a quick one off solution, there are many existing SVM libraries that will do this for you. As mentioned by ElKamina, Linear Perceptron is guaranteed to find a solution if one exists. This approach is not efficient for large dimensions. Computationally the most effective way to decide whether two sets of points are linearly separable is by applying linear programming. A code with an example to solve using Perceptron in Matlab is here In general that problem is NP-hard but there are good approximate solutions like K-means clustering. Well, both Perceptron and SVM (Support Vector Machines) can tell if two data sets are separable linearly, but SVM can find the Optimal Hiperplane of separability. Besides, it can work with n-dimensional vectors, not only points. It is used in applications such as face recognition. I recomend to go deep into this topic.I am trying to use Ordinary Least Squares for multivariable regression. But it says that there is no attribute 'OLS' from statsmodels. formula. api library.
I am following the code from a lecture on Udemy
The code is as follows: The error is as follows: Just for completeness, the code should look like this if statsmodels.version is 0.10.0: Use this import.   I have tried the above mentioned methods and while  the import works for me. When I run the next piece of code it gives me this error. TypeError: ufunc 'isfinite' not supported for the input types, and the
  inputs could not be safely coerced to any supported types according to
  the casting rule ''safe'' If you are getting the above mentioned error, you can solve it by specifying dtype for the np.array.  Replace  with  Try this instead, worked for me:  This is the working solution that I tried today. 
use this in the import and your rest of the fix is mentioned below This should work because it did work for me. As @Josef mentions in the comment, use ols() instead of OLS(), OLS() truly does not exist there.Here's a brief description of my problem: I plot the ROC graphs of several classifiers and all present a great AUC, meaning that the classification is good. However, when I test the classifier and compute the f-measure I get a really low value. I know that this issue is caused by the class skewness of the dataset and, by now, I discover two options to deal with it: I went for the first option and that solved my issue (f-measure is satisfactory). BUT, now, my question is: which of these methods is preferable? And what are the differences? P.S: I am using Python with the scikit-learn library. Both weighting (cost-sensitive) and thresholding are valid forms of cost-sensitive learning.  In the briefest terms, you can think of the two as follows: Essentially one is asserting that the ‘cost’ of misclassifying the rare class is worse than misclassifying the common class.  This is applied at the algorithmic level in such algorithms as SVM, ANN, and Random Forest.  The limitations here consist of whether the algorithm can deal with weights.  Furthermore, many applications of this are trying to address the idea of making a more serious misclassification (e.g. classifying someone who has pancreatic cancer as non having cancer).  In such circumstances, you know why you want to make sure you classify specific classes even in imbalanced settings.  Ideally you want to optimize the cost parameters as you would any other model parameter. If the algorithm returns probabilities (or some other score), thresholding can be applied after a model has been built.  Essentially you change the classification threshold from 50-50 to an appropriate trade-off level.  This typically can be optimized by generated a curve of the evaluation metric (e.g. F-measure).  The limitation here is that you are making absolute trade-offs.  Any modification in the cutoff will in turn decrease the accuracy of predicting the other class.  If you have exceedingly high probabilities for the majority of your common classes (e.g. most above 0.85) you are more likely to have success with this method.  It is also algorithm independent (provided the algorithm returns probabilities). Sampling is another common option applied to imbalanced datasets to bring some balance to the class distributions.  There are essentially two fundamental approaches. Under-sampling Extract a smaller set of the majority instances and keep the minority.  This will result in a smaller dataset where the distribution between classes is closer; however, you have discarded data that may have been valuable.  This could also be beneficial if you have a very large amount of data. Over-sampling Increase the number of minority instances by replicating them.  This will result in a larger dataset which retains all the original data but may introduce bias.  As you increase the size, however, you may begin to impact computational performance as well. Advanced Methods There are additional methods that are more ‘sophisticated’ to help address potential bias.  These include methods such as SMOTE, SMOTEBoost and EasyEnsemble as referenced in this prior question regarding imbalanced datasets and CSL. One further note regarding building models with imbalanced data is that you should keep in mind your model metric.  For example, metrics such as F-measures don’t take into account the true negative rate.  Therefore, it is often recommended that in imbalanced settings to use metrics such as Cohen’s kappa metric. Before trying to solve the problem (and I think @cdeterman's answer covers that thoroughly), it's best to first define measures. Apart from "all-in-one" metrics like Cohen's kappa, I find it extremely useful to just compute common metrics (such as precision, recall and f-measure) per each of the classes in the problem. Scikit-learn's classification_report does that quite conveniently: If you want a more visual output, you can use one of the Deepchecks built-in checks (disclosure - I'm one of the maintainers) : Using such per-class metrics would have alerted you from the very beginning that your model is under-performing on certain classes (and on which ones). Running it again after using some cost-sensitive learning would let you know if you managed to balance out your performance between classes.I have created a simple neural network (Python, Theano) to estimate a persons age based on their spending history from a selection of different stores. Unfortunately, it is not particularly accurate. The accuracy might be hurt by the fact that the network has no knowledge of ordinality. For the network there is no relationship between the age classifications. It is currently selecting the age with the highest probability from the softmax output layer. I have considered changing the output classification to an average of the weighted probability for each age. E.g Given age probabilities: (Age 10 : 20%, Age 20 : 20%, Age 30: 60%) This solution feels sub optimal. Is there a better was to implement ordinal classification in neural networks, or is there a better machine learning method that can be implemented? (E.g logistic regression) This problem came up in a previous Kaggle competition (this thread references the paper I mentioned in the comments). The idea is that, say you had 5 age groups, where 0 < 1 < 2 < 3 < 4, instead of one-hot encoding them and using a softmax objective function, you can encode them into K-1 classes and use a sigmoid objective.  So, as an example, your encodings would be Then the net will learn the orderings.I'm feeding in a dynamic shaped Tensor using:  x = tf.placeholder(tf.int32, shape=[None, vector_size]) I need to turn this into a list of Tensors that have shape=[1, vector_size] using x_list = tf.unpack(x, 0) But it raises a ValueError because the length of the first dimension is not known i.e. it's None.  I've been trying to get around this by using another tf.placeholder to dynamically supply the shape of x but the parameter shape cannot be a Tensor. How can I use tf.unpack() in this situation?  Or is there another function that can also turn the variable that I feed in into a list of Tensors? Thanks in advance.  I don't think you can unpack a tensor with the argument num unspecified and non-inferrable. As their documentation says: Raises ValueError if num is unspecified and cannot be inferred. It has something to do with how TensorFlow's internal design for operations like unpack. In this other tread, Yaroslav Bulatov explained  Operations like unpack compile into "tensor-in/tensor-out" ops during graph construction time. Hence TensorFlow needs to know the specific value of num to pass compiling. Although, I'd try to get around this by using TensorArray. (see the following code for illustration). TensorArray is a class for wrapping dynamically sized arrays of Tensors. When initialize a TensorArray object in this application, TensorArr = tf.TensorArray(tf.int32, 1, dynamic_size=True, infer_shape=False), set dynamic_size=True and infer_shape=False since the shape of placeholder x is only partly defined. To access each unpacked element: Then at evaluation time: Note that when trying to access each unpacked element, if the index value is out of bound, you'd be able to pass the compiling but you'll get an error during runtime suggesting index out of bound. Additionally, the shape of the unpacked tensor would be TensorShape(None), since the shape of x is only partially determined until being evaluated. Probably tf.dynamic_partition may help, but it requires static number of output tensors. If you can establish a maximum number of tensors then you can use it. This outputs the following:I'm trying to solve my issue in my own but I couldn't, I'm trying to run this code in every format you can imagine and in ArcGIS pro software it's the same I can't find this error message in any other issue. From similar issues, it seems some data files could be missing? i get this error also i tried it in arcgis pro i got the same to make sure this is pyproj error rather than geopandas. if the above runtime error is the same, we can be sure this error is due to pyproj. just conda remove pyproj and install it with pip. at least this works for me. Today(July 30), I resintalled from miniconda, conda remove pyproj did not work for me, instead I pip uninstall pyproj and pip install pyproj makes everything fine. The problem is problably within the pyproj instalation of Anaconda on Windows platform. Just like Stephen said, solution is to edit the path in "datadir.py" (located in ...Anaconda3\Lib\site-packages\pyproj). Correct path is ".../Anaconda3/Library/share". Make sure full path is complete (may contain username etc.). I also needed to change \ to /.
This change worked for me. Yes and after this change, it is necesary to restart Spyder (or whatever you use). Is there an initial crs defined?
I ran into the same problem only when I passed only the epsg command:  gdf.to_crs('epsg:4326'). As you show my_geoseries.crs = {'init' :'epsg:3857'} should be the first step and then transforming to gdf = gdf.to_crs({'init': 'epsg:4326'}) If you are working in ArcGIS you could also check in the properties whether the initial epsg is defined ? I'm using Pycharm.
I had to use a combination of both Stone Shi's remark and Dorregaray's. According to Stone Shi, the above proves that it's a pyproj err.
So I used Pycharm->Settings and reinstalled pyproj.
Then So, it's a pyproj err but Pycharm->Settings reinstalling pyproj does not help me. I then edited my C:\Anaconda3\Lib\site-packages\pyproj\datadir.py 
from:  to Dorregaray's: Then test again: No Runtime Error! Then test on my  For me upgrading pyproj and geopandas, fixed this issue: Using Geopandas, try that (it should work) :   You should redefine well your geodataframe, 
then define the initial geo referential
and finally convert it in the good one.
Don't forget to drop the NaN if there are any. I came across the same error. I was working with Python version 3.6.3 and Geopandas version 0.4.0. It was solved by using the following instead of df = df.to_crs({'init': 'epsg:4326'}): you can force reinstall pyproj from pip directly using pip install --upgrade --force-reinstall pyproj instead of uninstalling and reinstall again which will also uninstall all the dependent librariesI'm using scikit-learn's GridSearchCV to iterate over a parameter space to tune a model. Specifically, I'm using it to test different hyperparameters in a neural network. The grid is as follows: The problem is that I end up running redundant models when hidden num_hidden_layers is set to 0. It will run a model with 0 hidden layers and 64 units, another with 128 units, and another with 256 units. All of these models are equivalent since there is no hidden layer. This is highly inefficient and it means I need to write more code to remove redundancy in the results. Is there a way to prevent such parameter combinations, perhaps by passing a tuple of parameters? The sklearn documentation suggests two parameter grids. So you could do something like this: GridSearchCV allows you to pass list of dictionaries to params: param_grid : dict or list of dictionaries Dictionary with parameters names (string) as keys and lists of
  parameter settings to try as values, or a list of such dictionaries,
  in which case the grids spanned by each dictionary in the list are
  explored. This enables searching over any sequence of parameter
  settings. So you can specify these dictionaries to be certain subdictionaries of your original dictionary. Thus, you could avoid irrelevant combinations.I have splitted my training dataset into 80% train and 20% validation data and created DataLoaders as shown below. However I do not want to limit my model's training. So I thought of splitting my data into K(maybe 5) folds and performing cross-validation. However I do not know how to combine the datasets to my dataloader after splitting them. I just wrote a cross validation function work with dataloader and dataset.
Here is my code, hope this is helpful. In order to give an intuition of correctness for what we are doing, see the output below: Take a look at Cross validation for MNIST dataset with pytorch and sklearn . The question asker implemented kFold Crossvalidation. Take especially a look a his own answer ( answered Nov 23 '19 at 10:34 ). He doesn't rely on random_split() but on sklearn.model_selection.KFold and from there constructs a DataSet and from there a Dataloader.  You could achieve this by using KFOLD from sklearn and dataloader. so when you write the DataLoader part, use the subsetRandomSampler, in this way, the sampler in the dataloader will always sample the train/valid indices generated by the kfold function randomly.I am trying to build a model that takes multiple inputs and multiple outputs using a functional API. I followed this to create the code. My model.fit command looks like this: The shapes of input data are as follows:
train_data is (192,13)
new_train_data is (192,6)
train-labels,new_target_labels is (192,)
The code runs for a few steps then raises this error: The jupyter-notebook with complete code is here you have to provide validation_data in the correct format (like your train). you have to pass 2 input data and 2 targets... you are passing only one this is a dummy exampleIs there a way to to find all the sub-sentences of a sentence that still are meaningful and contain at least one subject, verb, and a predicate/object? For example, if we have a sentence like "I am going to do a seminar on NLP at SXSW in Austin next month". We can extract the following meaningful sub-sentences from this sentence: "I am going to do a seminar", "I am going to do a seminar on NLP", "I am going to do a seminar on NLP at SXSW", "I am going to do a seminar at SXSW", "I am going to do a seminar in Austin", "I am going to do a seminar on NLP next month", etc. Please note that there is no deduced sentences here (e.g. "There will be a NLP seminar at SXSW next month". Although this is true, we don't need this as part of this problem.) . All generated sentences are strictly part of the given sentence. How can we approach solving this problem? I was thinking of creating annotated training data that has a set of legal sub-sentences for each sentence in the training data set. And then write some supervised learning algorithm(s) to generate a model. I am quite new to NLP and Machine Learning, so it would be great if you guys could suggest some ways to solve this problem. You can use dependency parser provided by Stanford CoreNLP.
Collapsed output of your sentence will look like below.   The last 5 of your sentence output are optional. You can remove one or more parts that are not essential to your sentence.
Most of this optional parts are belong to prepositional and modifier e.g : prep_in, prep_do, advmod, tmod, etc.  See Stanford Dependency Manual.   For example, if you remove all modifier from the output, you will get I am going to do a seminar on NLP at SXSW in Austin. There's a paper titled "Using Discourse Commitments to Recognize Textual Entailment" by Hickl et al that discusses the extraction of discourse commitments (sub-sentences). The paper includes a description of their algorithm which in some level operates on rules. They used it for RTE, and there may be some minimal levels of deduction in the output. Text simplification maybe a related area to look at. The following paper http://www.mpi-inf.mpg.de/~rgemulla/publications/delcorro13clausie.pdf processes the dependencies from the Stanford parser and contructs simple clauses (text-simplification). See the online demo - https://d5gate.ag5.mpi-sb.mpg.de/ClausIEGate/ClausIEGate One approach would be with a parser such as a PCFG. Trying to just train a model to detect 'subsentences' is likely to suffer from data sparsity. Also, I am doubtful that you could write down a really clean and unambiguous definition of a subsentence, and if you can't define it, you can't get annotators to annotate for it.I want check my loss values during the training time so I can observe the loss at each iteration. So far I haven't found an easy way for scikit learn to give me a history of loss values, nor did I find a functionality already within scikit to plot the loss for me. If there was no way to plot this, it'd be great if I could simply fetch the final loss values at the end of classifier.fit. Note: I am aware of the fact that some solutions are closed form. I'm using several classifiers which do not have analytical solutions, such as logistic regression and svm. Does anyone have any suggestions?  So I couldn't find very good documentation on directly fetching the loss values per iteration, but I hope this will help someone in the future: This code will take a normal SGDClassifier(just about any linear classifier), and intercept the verbose=1 flag, and will then split to get the loss from the verbose printing. Obviously this is slower but will give us the loss and print it. Use model.loss_curve_. You can use the verbose option to print the values on each iteration but if you want the actual values, this is not the best way to proceed because you will need to do some hacky stuff to parse them.  It's true, the documentation doesn't mention anything about this attribute, but if you check in the source code, you may notice that one of MLPClassifier base classes (BaseMultilayerPerceptron) actually defines an attribute loss_curve_ where it stores the values on each iterarion.  As you get all the values in a list, plotting should be trivial using any library. Notice that this attribute is only present while using a stochastic solver (i.e. sgd or adam). I just adapted and updated the answer from @OneRaynyDay. Using context manager is way more elegant. Defining Context Manager: Usage:I wrote following code and test it on small data: Where X, y  (X - 30000x784 matrix, y - 30000x1) are numpy arrays. On small data algorithm works well and give me right results. But I run my program about 10 hours ago... And it is still in process.  I want to know how long it will take, or it stuck in some way? 
(Laptop specs 4 GB Memory, Core i5-480M) SVM training can be arbitrary long, this depends on dozens of parameters: in general, basic SMO algorithm is O(n^3), so in case of 30 000 datapoints it has to run number of operations proportional to the2 700 000 000 000which is realy huge number. What are your options?I know how to visualize a tensorflow graph after training with tensorboard. Now, is it possible to visualize just the forward part of the graph, i.e., with no training operator defined? The reason I'm asking this is that I'm getting this error: I'd like to inspect the graph to find out where the gradient tensor flow (pun intended) is broken. Yes, you can visualize any graph. Try this simple script: Then run... ... and you'll see:  So you can simply create a session just to write the graph to the FileWriter and not do anything else.For a project I am comparing a number of decision trees, using the regression algorithms (Random Forest, Extra Trees, Adaboost and Bagging) of scikit-learn.
To compare and interpret them I use the feature importance , though for the bagging decision tree this does not look to be available. My question: Does anybody know how to get the feature importances list for Bagging? Greetings, Kornee Are you talking about BaggingClassifier? It can be used with many base estimators, so there is no feature importances implemented. There are model-independent methods for computing feature importances (see e.g. https://github.com/scikit-learn/scikit-learn/issues/8898), scikit-learn doesn't use them. In case of decision trees as base estimators you can compute feature importances yourselves: it'd be just an average of tree.feature_importances_ among all trees in bagging.estimators_: RandomForestClassifer does the same computation internally. Extending what CharlesG posted, here's my solution for overloading the BaggingRegressor (same should work for BaggingClassifier). This correctly handles if max_features <> 1.0, though I suppose won't work exactly if bootstrap_features=True. I suppose it's because sklearn has evolved a lot since 2017, but couldn't get it to work with the constructor, and it doesn't seem entirely necessary - the only reason to have that is to pre-specify the feature_importances_ attribute as None. However, it shouldn't even exist until fit() is called anyway. I encountered the same problem, and average feature importance was what I was interested in. Furthermore, I needed to have a feature_importance_ attribute exposed by (i.e. accessible from) the bagging classifier object. This was necessary to be used in another scikit-learn algorithm (i.e. RFE with an ROC_AUC scorer). I chose to overload the BaggingClassifier, to gain a direct access to the mean feature_importance (or "coef_" parameter) of the base estimators. Here is how to do so:I am a little confused about how should I use/insert "BatchNorm" layer in my models.
I see several different approaches, for instance: "BatchNorm" layer is followed immediately with "Scale" layer:  In the cifar10 example provided with caffe, "BatchNorm" is used without any "Scale" following it: batch_norm_param: use_global_scale is changed between TRAIN and TEST phase: How should one use"BatchNorm" layer in caffe? If you follow the original paper, the Batch normalization should be followed by Scale and Bias layers (the bias can be included via the Scale, although this makes the Bias parameters inaccessible). use_global_stats should also be changed from training (False) to testing/deployment (True) - which is the default behavior. Note that the first example you give is a prototxt for deployment, so it is correct for it to be set to True. I'm not sure about the shared parameters. I made a pull request to improve the documents on the batch normalization, but then closed it because I wanted to modify it. And then, I never got back to it. Note that I think lr_mult: 0 for "BatchNorm" is no longer required (perhaps not allowed?), although I'm not finding the corresponding PR now. After each BatchNorm, we have to add a Scale layer in Caffe. The reason is that the Caffe BatchNorm layer only subtracts the mean from the input data and divides by their variance, while does not include the γ and β parameters that respectively scale and shift the normalized distribution 1. Conversely, the Keras BatchNormalization layer includes and applies all of the parameters mentioned above. Using a Scale layer with the parameter “bias_term” set to True in Caffe, provides a safe trick to reproduce the exact behavior of the Keras version.
https://www.deepvisionconsulting.com/from-keras-to-caffe/We have trained an Extra Tree model for some regression task. Our model consists of 3 extra trees, each having 200 trees of depth 30. On top of the 3 extra trees, we use a ridge regression. We trained our model for several hours and pickled the trained model (the entire class object), for later use. However, the size of saved trained model is too big, about 140 GB! Is there a way to reduce the size of the saved model? Are there any configuration in pickle that could be helpful, or any alternative for pickle? You can try using joblib with compression parameter. compress - from 0 to 9. Higher value means more compression, but also slower read and write times. Using a value of 3 is often a good compromise. You can use python standard compression modules zlib, gzip, bz2, lzma and xz. To use that you can just specify the format with specific extension Example: More information, see the link. In the best case (binary trees), you will have 3 * 200 * (2^30 - 1) = 644245094400 nodes or 434Gb assuming each one node would only cost 1 byte to store. I think that 140GB is a pretty decent size in comparision.I'm doing a classification using rpart in R. The tree model is trained by: The accuracy for this tree model is: I read a tutorial to prune the tree by cross validation: The accuracy rate for the pruned tree is still the same: I want to know what's wrong with my pruned tree? And how can I prune the tree model using cross validation in R? Thanks. You have used the minimum cross-validated error tree. An alternative is to use the smallest tree that is within 1 standard error of the best tree (the one you are selecting). The reason for this is that, given the CV estimates of the error, the smallest tree within 1 standard error is doing just as good a job at prediction as the best (lowest CV error) tree, yet it is doing it with fewer "terms". Plot the cost-complexity vs tree size for the un-pruned tree via: Find the tree to the left of the one with minimum error whose cp value lies within the error bar of one with minimum error. There could be many reasons why pruning is not affecting the fitted tree. For example the best tree could be the one where the algorithm stopped according to the stopping rules as specified in ?rpart.control.I have fitted a CountVectorizer to some documents in scikit-learn. I would like to see all the terms and their corresponding frequency in the text corpus, in order to select stop-words. For example Is there any built-in function for this? If cv is your CountVectorizer and X is the vectorized corpus, then returns a list of (term, frequency) pairs for each distinct term in the corpus that the CountVectorizer extracted. (The little asarray + ravel dance is needed to work around some quirks in scipy.sparse.) There is no built-in. I have found a faster way to do it based on Ando Saabas's answer: outputI have a model (fit), based on historic information until last month. Now I would like to predict using my model for the current month. When I try to invoke the following code: I get the following error: Notes: The predict is a generic function that will invoke the specific predict function based on the first input argument. In my case it will be:  >fit$modelInfo$label [1] "Random Forest" Therefore the predict method invoked will be: predict.randomForest. See [caret documentation][3] for more info. Here the summary source code for generating the model and invoking it:  Note: The execution time for generating the model is about 3 hours, that is why I save the object for reusing after that. The data set from the training model as the following structure: now the testData will have the following structure: The variable structure is the same, just that some factor variables has different levels because some variable has new values. For example: Acuity in the model has 3-levels and in the testing data 4-levels. I don't have from upfront a way to know all possible level for all variables. Any advice, please... Thanks in advance, David I think I found why this happened...The predict is a generic function from: stats package. I use the namespace ::-notation for invoking the functions from the caret package (that is the recommendation for creating a user packages) and the equivalent predict function from caret package is: predict.train, that is an internal function, that cannot be invoked by an external application. The only way to invoke this function, is using the generic predict function from stats package, then based on the class of the first input argument: predicted <- predict(fit, testData[-$Readmit]) it identifies the particular predict function will be invoked.  For this particular case the class of this function is train, so it would call actually the function: train.predict from caret package. This function also handles the particular function requested for prediction based on the algorithm (method) used, for example: predict.gbm or predict.glm, etc. It is explained, in detail, in the caret documentation section: "5.7 Extracting Predictions and Class Probabilities". Therefore the ::-notation works well for other functions in the package, such as: caret.train for example, but not for this particular one: predict. In such cases it is necessary to explicitly load the library, so it internally can invoke predict.train function. In short, the solution is just adding the following line before invoking the predict function: Then error disappears.  Based on the answer from @David Leal, I tried loading library(caret) before calling the predict function but it did not help.  After trying a bit, I realized that I had to load the library that contains the model itself. In my case, I had to call library(kenlab) for Support Vectors.I was trying to use the higher library for meta-learning and I was having issues understanding what the copy_initial_weights mean. The docs say: copy_initial_weights – if true, the weights of the patched module are copied to form the initial weights of the patched module, and thus are not part of the gradient tape when unrolling the patched module. If this is set to False, the actual module weights will be the initial weights of the patched module. This is useful when doing MAML, for example. but that doesn't make much sense to me because of the following: For example, "the weights of the patched module are copied to form the initial weights of the patched module" doesn't make sense to me because when the context manager is initiated a patched module does not exist yet. So it is unclear what we are copying from and to where (and why copying is something we want to do). Also, "unrolling the patched module" does not make sense to me. We usually unroll a computaiton graph caused by a for loop. A patched module is just a neural net that has been modified by this library. Unrolling is ambiguous. Also, there isn't a technical definition for "gradient tape". Also, when describing what false is, saying that it's useful for MAML isn't actually useful because it doesn't even hint why it's useful for MAML. Overall, it's impossible to use the context manager. Any explanations and examples of what the that flag does in more precise terms would be really valuable. Related: Short version Call to higher.innerloop_ctx with model as argument create temporary patched model and unrolled optimizer for that model: (fmodel, diffopt). It is expected that in the inner loop fmodel will iteratively receive some input, compute output and loss and then diffopt.step(loss) will be called. Each time diffopt.step is called fmodel will create next version of parameters fmodel.parameters(time=T) which is a new tensor computed using previous ones (with the full graph allowing to compute gradients through the process). If at any point user calls backward on any tensor, regular pytorch gradient computation/accumulation will start in a way allowing gradients to propagate to e.g. optimizer's parameters (such as lr, momentum - if they were passed as tensors requiring gradients to higher.innerloop_ctx using override). Creation-time version of fmodel's parameters fmodel.parameters(time=0) is a copy of original model parameters. If copy_initial_weights=True provided (default) then fmodel.parameters(time=0) will be a clone+detach'ed version of model's parameters (i.e. will preserve values, but will severe all connections to the original model). If copy_initial_weights=False provided, then fmodel.parameters(time=0) will be clone'd version of model's parameters and thus will allow gradients to propagate to original model's parameters (see pytorch doc on clone). Terminology clarifications gradient tape here is referring to the graph pytorch uses to go through computations to propagate gradients to all leaf tensors requiring gradients. If at some point you cut the link to some leaf tensor requiring parameters (e.g. how it is done for fnet.parameters() for copy_initial_weights=True case) then the original model.parameters() won't be "on gradient tape" anymore for your meta_loss.backward() computation. unrolling the patched module here refers to the part of meta_loss.backward() computation when pytorch is going through all fnet.parameters(time=T) starting from the latest and ending with the earliest (higher doesn't control the process - this is just regular pytorch gradient computation, higher is just in charge of how these new time=T parameters are being created from previous ones each time diffopt.step is called and how fnet is always using the latest ones for forward computation). Long version Let's start from the beginning. Main functionality (only functionality, really) of higher library is unrolling of a model's parameter optimization in a differentiable manner. It can come either in the form of directly using differentiable optimizer through e.g. higher.get_diff_optim as in this example or in the form of higher.innerloop_ctx as in this example. The option with higher.innerloop_ctx is wrapping the creation of "stateless" model fmodel from existing  model for you and gives you an "optimizer" diffopt for this fmodel. So as summarized in the README.md of higher it allows you to switch from: to The difference between training model and doing diffopt.step to update fmodel is that fmodel is not updating the parameters in-place as opt.step() in the original part would do. Instead each time diffopt.step is called new versions of parameters are created in such a way, that fmodel would use new ones for the next step, but all previous ones are still preserved. I.e. fmodel starts with only fmodel.parameters(time=0) available, but after you called diffopt.step N times you can ask fmodel to give you fmodel.parameters(time=i) for any i up to N inclusive. Notice that fmodel.parameters(time=0) doesn't change in this process at all, just every time fmodel is applied to some input it will use the latest version of parameters it currently has. Now, what exactly is fmodel.parameters(time=0)? It is created here and depends on copy_initial_weights. If copy_initial_weights==True then fmodel.parameters(time=0) are clone'd and detach'ed parameters of model. Otherwise they are only clone'd, but not detach'ed! That means that when we do meta-optimization step, the original model's parameters will actually accumulate gradients if and only if copy_initial_weights==False. And in MAML we want to optimize model's starting weights so we actually do need to get gradients from meta-optimization step. I think one of the issues here is that higher lack of simpler toy examples to demonstrate what is going on, instead rushing to show more serious things as the examples. So let me try to fill that gap here and demonstrate what is going on using the simplest toy example I could come up with (model with 1 weight which multiplies input by that weight): Which produces this output: I think it's more or less clear what this means now to me. First I'd like to make some notation clear, specially with respect to indices wrt inner time step and outer time step (also known as episodes): At the beginning of training a neural net has params: and are held inside it's module. For the sake of explanation the specific tensor (for the base model) will be denoted: and will be updated with with an in-place operation (this is important since W is the placeholder for all W^<0,outer_i> for all outer step values during "normal" meta-learning) by the outer optimizer. I want to emphasize that W is the tensor for the normal Pytorch neural net base model. By changing this in-place with an outer optimizer (like Adam) we are effectively training the initialization. The outer optimizer will use the gradients wrt this tensor to do the update through the whole unrolled inner loop process. When we say copy_initial_weights=False we mean that we will have a gradient path directly to W with whatever value it currently has. Usually the context manager is done before a inner loop after an outer step has been done so W will have W^<0,outer_i> for the current step. In particular the code that does this is this one for copy_initial_weight=False: this might look confusing if you're not familiar with clone but what it's doing is making a copy of the current weight of W. The unusual thing is that clone also remembers the gradient history from the tensor it came from (.clone() is as identity). It's main use it to add an extra layer of safety from the user doing dangerous in-place ops in it's differentiable optimizer. Assuming the user never did anything crazy with in-place ops one could in theory remove the .clone(). the reason this is confusing imho is because "copying in Pytorch" (clinging) does not automatically block gradient flows, which is what a "real" copy would do (i.e. create a 100% totally separate tensor). This is not what clone does and that is not what copy_initial_weights does. When copy_initial_weights=True what really happens is that the weights are cloned and detached. See the code it eventually runs (here and here): which runs copy tensor (assuming they are doing a safe copy i.e. doing the extra clone): Note that .detach() does not allocate new memory. It shares the memory with the original tensor, which is why the .clone() is needed to have this op be "safe" (usually wrt in-place ops). So when copy_initial_weights they are copying and detaching the current value of W. This is usually W^<0,outer_i> if it's doing usual meta-learning in the inner adaptation loop. So the intended semantics of copy_initial_weight is that and the initial_weight they simply mean W. The important thing to note is that the intermediate tensors for the net in the inner loop are not denoted in my notation but they are fmodel.parameters(t=inner_i). Also if things are usually meta-learning we have fmodel.parameters(t=0) = W and it gets update in-place by the outer optimizer. Note that because of the outer optimizer's in-place op and the freeing of the graphs we never take the derivate Grad_{W^<0,0>} with respect to the initial value of W. Which was something I initially thought we were doing.I'm starting to dive into deploying a predictive model to a web app using Flask, and unfortunately getting stuck at the starting gate.  What I did: I pickled my model in my model.py program: then I created an api.py file in the same directory as my demo_model.pkl, per this tutorial (https://blog.hyperiondev.com/index.php/2018/02/01/deploy-machine-learning-models-flask-api/): I also made a templates/index.html file in the same directory with this info: running: gives me an error with the pickler: Why is the main module of the program getting involved with my NeuralNetwork model? I'm very confused at the moment... any advice would be appreciated. UPDATE: Adding a class definition class NeuralNetwork(object): pass to my api.py program fixed the bug.  If anyone would be willing to offer me an explanation of what was going on that would be hugely appreciated!  The specific exception you're getting refers to attributes in __main__, but that's mostly a red herring. I'm pretty sure the issue actually has to do with how you dumped the instance. Pickle does not dump the actual code classes and functions, only their names. It includes the name of the module each one was defined in, so it can find them again. If you dump a class defined in a module you're running as a script, it will dump the name __main__ as the module name, since that's what Python uses as the name for the main module (as seen in the if __name__ == "__main__" boilerplate code). When you run model.py as a script and pickle an instance of a class defined in it, that class will be saved as __main__.NeuralNetwork rather than model.NeuralNetwork. When you run some other module and try to load the pickle file, Python will look for the class in the __main__ module, since that's where the pickle data tells it to look. This is why you're getting an exception about attributes of __main__. To solve this you probably want to change how you're dumping the data. Instead of running model.py as a script, you should probably run some other module and have it do import model, so you get the module under it's normal name. (I suppose you could have model.py import itself in an if __name__ == "__main__" block, but that's super ugly and awkward). You probably also need to avoid recreating and dumping the instance unconditionally when the model is imported, since that needs to happen when you load the pickle file (and I assume the whole point of the pickle is to avoid recreating the instance from scratch). So remove the dumping logic from the bottom of model.py, and add a new file like this: When you dump the NeuralNetwork using this script, it will correctly identify model as the module the class was defined in, and so the loading code will be able to import that module and make an instance of the class correctly. Your current "fix" for the issue (defining an empty NeuralNetwork class in the __main__ module when you are loading the object) is probably a bad solution. The instance you get from loading the pickle file will be an instance of the new class, not the original one. It will be loaded with the attributes of the old instance, but it won't have any methods or other class variables set on it (which isn't an issue with the class you've shown, but probably will be for any kind of object that's more complicated). If you are using Keras library to build your neural network then pickle will not work. pickle only works fine the model built using scikit libraries. Save your neural network model using json . Keras provides the ability to describe any model using JSON format with a to_json() function. This can be saved to file and later loaded via the model_from_json() function that will create a new model from the JSON specification.I have been doing reading about Self Organizing Maps, and I understand the Algorithm(I think), however something still eludes me.  How do you interpret the trained network?  How would you then actually use it for say, a classification task(once you have done the clustering with your training data)? All of the material I seem to find(printed and digital) focuses on the training of the Algorithm. I believe I may be missing something crucial. Regards SOMs are mainly a dimensionality reduction algorithm, not a classification tool. They are used for the dimensionality reduction just like PCA and similar methods (as once trained, you can check which neuron is activated by your input and use this neuron's position as the value), the only actual difference is their ability to preserve a given topology of output representation.  So what is SOM actually producing is a mapping from your input space X to the reduced space Y (the most common is a 2d lattice, making Y a 2 dimensional space). To perform actual classification you should transform your data through this mapping, and run some other, classificational model (SVM, Neural Network, Decision Tree, etc.). In other words - SOMs are used for finding other representation of the data. Representation, which is easy for further analyzis by humans (as it is mostly 2dimensional and can be plotted), and very easy for any further classification models. This is a great method of visualizing highly dimensional data, analyzing "what is going on", how are some classes grouped geometricaly, etc.. But they should not be confused with other neural models like artificial neural networks or even growing neural gas (which is a very similar concept, yet giving a direct data clustering) as they serve a different purpose. Of course one can use SOMs directly for the classification, but this is a modification of the original idea, which requires other data representation, and in general, it does not work that well as using some other classifier on top of it. EDIT There are at least few ways of visualizing the trained SOM:I am trying to implement the following softplus function: I've tried it with math/numpy and float64 as data type, but whenever x gets too large (e.g. x = 1000) the result is inf. Can you assist me on how to successfully handle this function with large numbers? TLDR: Explanation:
There is a relation which one can use: So a safe implementation, as well as mathematically sound, would be: This works both for math and numpy functions (use e.g.: np.log, np.exp, np.abs, np.maximum). Since for x>30 we have log(1+exp(x)) ~= log(exp(x)) = x, a simple stable implementation is In fact | log(1+exp(30)) - 30 | < 1e-10, so this implementation makes errors smaller than 1e-10 and never overflows. In particular for x=1000 the error of this approximation will be much smaller than float64 resolution, so it is impossible to even measure it on the computer. i use this code to work in arrays What I'm currently using (slightly inefficient but clean and vectorized): If you're using TensorFlow, you can simply use the tf.math.softplus.I have a dataset of time series that I use as input to an LSTM-RNN for action anticipation. The time series comprises a time of 5 seconds at 30 fps (i.e. 150 data points), and the data represents the position/movement of facial features. I sample additional sub-sequences of smaller length from my dataset in order to add redundancy in the dataset and reduce overfitting. In this case I know the starting and ending frame of the sub-sequences. In order to train the model in batches, all time series need to have the same length, and according to many papers in the literature padding should not affect the performance of the network. Example: Original sequence: Subsequences: considering that my network is trying to anticipate an action (meaning that as soon as P(action) > threshold as it goes from t = 0 to T = tmax, it will predict that action) will it matter where the padding goes?  Option 1: Zeros go to substitute original values Option 2: all zeros at the end Moreover, some of the time series are missing a number of frames, but it is not known which ones they are - meaning that if we only have 60 frames, we don't know whether they are taken from 0 to 2 seconds, from 1 to 3s, etc. These need to be padded before the subsequences are even taken. What is the best practice for padding in this case? Thank you in advance. The most powerful attribute of LSTMs and RNNs in general is that their parameters are shared along the time frames(Parameters recur over time frames) but the parameter sharing relies upon the assumption that the same parameters can be used for different time steps i.e. the relationship between the previous time step and the next time step does not depend on t as explained here in page 388, 2nd paragraph. In short, padding zeros at the end, theoretically should not change the accuracy of the model. I used the adverb theoretically because at each time step LSTM's decision depends on its cell state among other factors and this cell state is kind of a short summary of the past frames. As far as I understood, that past frames may be missing in your case. I think what you have here is a little trade-off. I would rather pad zeros at the end because it doesn't completely conflict with the underlying assumption of RNNs and it's more convenient to implement and keep track of. On the implementation side, I know tensorflow calculates the loss function once you give it the sequences and the actual sequence size of each sample(e.g. for 4 5 6 7 0 0 0 0 0 0 you also need to give it the actual size which is 4 here) assuming you're implementing the option 2. I don't know whether there is an implementation for option 1, though. Better go for padding zeroes in the beginning, as this paper suggests Effects of padding on LSTMs and CNNs,  Though post padding model peaked it’s efficiency at 6 epochs and started to overfit after that, it’s accuracy is way less than pre-padding. Check table 1, where the accuracy of pre-padding(padding zeroes in the beginning) is around 80%, but for post-padding(padding zeroes in the end), it is only around 50% In case you have sequences of variable length, pytorch provides a utility function torch.nn.utils.rnn.pack_padded_sequence. The general workflow with this function is One can collect the embedding of each token by Using this function is better than padding by yourself, because torch will limit RNN to only inspecting the actual sequence and stop before the padded token.I'm trying to use TensorFlow as backend yesterday I can use it, but today when I use it to show some error message when I'm trying to import Keras, so here's my code: it shows this error: while therefore I was using TensorFlow version 2.2 and Keras version 2.3.1, yesterday I can run, but today it seems can't. did I was the wrong version import for my Keras and TensorFlow for today? Edit:
when I use from tensorFlow import keras the output I want using tensorflow backend doesn't show up, And then when I load import segmentation_models as sm it shows the same error when I use import Keras like on above. Here is the solution to your problem, I've tested it on colab. You don't need to install any specific version of tensorflow / keras. Any version above 2.x would be ok to run, i.e tf 2.4/ 2.5/ 2.6. However, in colab, you need to restart the kernel to see the effect. but if you run on the kaggle kernel, you don't need to restart the kernel. See below: In colab: It will auto-restart the kernel. After restarting, run the following code in the new cell. In Kaggle Kernel: specifying below, before importing segmentation models,  alone worked for me in colab I tried a lot of answers but none of them worked for me.
The reason of the error: AttributeError: module 'tensorflow.compat.v2.internal.distribute' has no attribute 'strategy_supports_no_merge_call'
in my case was that I had tensorflow 2.7.0 and keras 2.6.0 installed on my device. just match the versions, it worked for me. I was getting this error message after upgrading Tensorflow to 2.7.0 . Downgrading to 2.5.0 is a temporary working fix. pip install tensorflow==2.5.0My network has two time-series inputs. One of the input has a fixed vector repeating for every time step. Is there an elegant way to load this fixed vector into the model just once and use it for computation? You can create a static input using the tensor argument as described by jdehesa, however the tensor should be a Keras (not tensorflow) variable. You can create this as follows: EDIT: Apparently the answer below does not work (nowadays anyway). See Creating constant value in Keras for a related answer. Looking at the source (I haven't been able to find a reference in the docs), it looks like you can just use Input and pass it a constant Theano/TensorFlow tensor. This will "wrap" the tensor (actually more like "extend" it with metadata) so you can use it with any Keras layer. Something to add: 
When you come to compile the model you need to give the constant input as an input otherwise the graph disconnects when you train you can just feed in the data you have, you don't need the constant layer anymore. I have found that no matter what you try it's usually easier to just use a custom layer and take advantage of the power of numpy:EDIT(1/3/16): corresponding github issue I'm using Tensorflow (Python interface) to implement a q-learning agent with function approximation trained using stochastic gradient descent. At each iteration of the experiment, a step function in the agent is called that updates the parameters of the approximator based on the new reward and activation, and then chooses a new action to perform. Here is the problem(with reinforcement learning jargon): So, is there a way that I can (without reinforcement learning jargon): Of course, I've considered the obvious solutions: Just hardcode the gradients:  This would be easy for the really simple approximators I'm using now but would be really inconvenient if I were experimenting with different filters and activation functions in a big convolutional network.  I'd really like to use the Optimizer class if possible. Call the environment simulation from within the agent:  This system does this, but it would make mine more complicated, and remove a lot of the modularity and structure.  So, I don't want to do this. I've read through the API and whitepaper several times, but can't seem to come up with a solution.  I was trying to come up with some way to feed the target into a graph to calculate the gradients, but couldn't come up with a way to build that graph automatically. If it turns out this isn't possible in TensorFlow yet, do you think it would be very complicated to implement this as a new operator? (I haven't used C++ in a couple of years, so the TensorFlow source looks a little intimidating.)  Or would I be better off switching to something like Torch, which has the imperative differentiation Autograd, instead of symbolic differentiation? Thanks for taking the time to help me out with this.  I was trying to make this as concise as I could. EDIT:  After doing some further searching I came across this previously asked question.  It's a little different than mine (they are trying to avoid updating an LSTM network twice every iteration in Torch), and doesn't have any answers yet. Here is some code if that helps: Right now what you want to do is very difficult in Tensorflow (0.6). Your best bet is to bite the bullet and call run multiple times at the cost of recomputing the activations. However, we are very aware of this issue internally. A prototype "partial run" solution is in the works, but there is no timeline for its completion right now. Since a truly satisfactory answer might require modifying tensorflow itself, you could also make a github issue for this and see if anyone else has anything to say on this there. Edit: Experimental support for partial_run is now in. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session.py#L317I am trying to run keras for the first time.  I installed the modules with: and then tried to run https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py. However it gives me: These are the versions I am using. What can I do to get keras to run with tensorflow? There is an issue between Keras and TF, Probably tf.python.control_flow_ops does not exist or not visible anymore.
 using below import statements you can resolve this issue For Details check:
https://github.com/fchollet/keras/issues/3857 I got this problem when it turns out keras was using the Theano backend. To fix it do one of these: See Keras backend documentation for more information. If not Using TensorFlow 1.0.0; use tf.python_io in later versionsWhile working with the DecisionTreeClassifier I visualized it using graphviz, and I have to say, to my astonishment, it seems it takes categorical data and uses it as continuous data. All my features are categorical and for example you can see the following tree (please note that the first feature, X[0], has 6 possible values 0, 1, 2, 3, 4, 5:

From what I found here the class uses a tree class which is a binary tree, so it is a limitation in sklearn. Anyone knows a way that I am missing to use the tree categorically? (I know it is not better for the task but as I need categories currently I am using one hot vectors on the data). A sample of the original data looks like this: where X[0] = f1 and I encoded strings to integers as sklearn does not accept strings. Well, I am surprised, but it turns out that sklearn's decision tree cannot handle categorical data indeed. There is a Github issue on this (#4899) from June 2015, but it is still open (UPDATE: it is now closed, but continued in #12866, so the issue is still not resolved). The problem with coding categorical variables as integers, as you have done here, is that it imposes an order on them, which may or may not be meaningful, depending on the case; for example, you could encode ['low', 'medium', 'high'] as [0, 1, 2], since 'low' < 'medium' < 'high' (we call these categorical variables ordinal), although you are still implicitly making the additional (and possibly undesired) assumption that the distance between 'low' and 'medium' is the same with the distance between 'medium' and 'high' (of no impact in decision trees, but of importance e.g. in k-nn and clustering). But this approach fails completely in cases like, say, ['red','green','blue'] or ['male','female'], since we cannot claim any meaningful relative order between them. So, for non-ordinal categorical variables, the way to properly encode them for use in sklearn's decision tree is to use the OneHotEncoder module. The Encoding categorical features section of the user's guide might also be helpful.I have time-series data, as followed:  I would like to add, in the most simple way, a linear trend (with intercept) onto this graph. Also, I would like to compute this trend only conditional on data before, say, 2006. I've found some answers here, but they all include statsmodels. First of all, these answers might be not up to date: pandas improved, and now itself includes an OLS component. Second, statsmodels appears to estimate an individual fixed-effect for each time period, instead of a linear trend. I suppose I could recalculate a running-quarter variable, but there most be a more comfortable way of doing this? How do I, in the simplest way possible, estimate this trend and add the predicted values as a column to my data frame? Here's a quick example on how to do this using pandas.ols:  In general you should create your matplotlib figure and axes object ahead of time, and explicitly plot the dataframe on that: Then you still have that axes object around to use directly to plot your line:I understand that random_state is used in various sklearn algorithms to break tie between different predictors (trees) with same metric value (say for example in GradientBoosting). But the documentation does not clarify or detail on this. Like  1 ) where else are these seeds used for random number generation ? Say for RandomForestClassifier , random number can be used to find a set of random features to build a predictor. Algorithms which use sub sampling, can use random numbers to get different sub samples. Can/Is the same seed (random_state) playing a role in multiple random number generations  ? What I am mainly concerned about is  2) how far reaching is the effect of this random_state variable. ? Can the value make a big difference in prediction (classification or regression). If yes, what kind of data sets should I care for more ? Or is it more about stability than quality of results? 3) If it can make a big difference, how best to choose that random_state?. Its a difficult one to do GridSearch on, without an intuition. Specially if the data set is such that one CV can take an hour. 4) If the motive is to only have steady result/evaluation of my models and cross validation scores across repeated runs, does it have the same effect if I set random.seed(X) before I use any of the algorithms (and use random_state as None).  5) Say I am using a random_state value on a GradientBoosted Classifier, and I am cross validating to find the goodness of my model (scoring on the validation set every time). Once satisfied, I will train my model on the whole training set before I apply it on the test set. Now, the full training set has more instances than the smaller training sets in the cross validation. So the random_state value can now result in a completely different behavior (choice of features and individual predictors) when compared to what was happening within the cv loop. Similarly things like min samples leaf etc can also result in a inferior model now that the settings are w.r.t the number of instances in CV while the actual number of instances is more. Is this a correct understanding ? What is the approach to safeguard against this ? Yes, the choice of the random seeds will impact your prediction results and as you pointed out in your fourth question, the impact is not really predictable. The common way to guard against predictions that happen to be good or bad just by chance is to train several models (based on different random states) and to average their predictions in a meaningful way. Similarly, you can see cross validation as a way to estimate the "true" performance of a model by averaging the performance over multiple training/test data splits. 1 ) where else are these seeds used for random number generation ? Say for RandomForestClassifier , random number can be used to find a set of random features to build a predictor. Algorithms which use sub sampling, can use random numbers to get different sub samples. Can/Is the same seed (random_state) playing a role in multiple random number generations ? random_state is used wherever randomness is needed: If your code relies on a random number generator, it should never use functions like numpy.random.random or numpy.random.normal. This approach can lead to repeatability issues in unit tests. Instead, a numpy.random.RandomState object should be used, which is built from a random_state argument passed to the class or function.   2) how far reaching is the effect of this random_state variable. ? Can the value make a big difference in prediction (classification or regression). If yes, what kind of data sets should I care for more ? Or is it more about stability than quality of results? Good problems should not depend too much on the random_state. 3) If it can make a big difference, how best to choose that random_state?. Its a difficult one to do GridSearch on, without an intuition. Specially if the data set is such that one CV can take an hour. Do not choose it. Instead try to optimize the other aspects of classification to achieve good results, regardless of random_state. 4) If the motive is to only have steady result/evaluation of my models and cross validation scores across repeated runs, does it have the same effect if I set random.seed(X) before I use any of the algorithms (and use random_state as None). As of Should I use `random.seed` or `numpy.random.seed` to control random number generation in `scikit-learn`?, random.seed(X) is not used by sklearn. If you need to control this, you could set np.random.seed() instead. 5) Say I am using a random_state value on a GradientBoosted Classifier, and I am cross validating to find the goodness of my model (scoring on the validation set every time). Once satisfied, I will train my model on the whole training set before I apply it on the test set. Now, the full training set has more instances than the smaller training sets in the cross validation. So the random_state value can now result in a completely different behavior (choice of features and individual predictors) when compared to what was happening within the cv loop. Similarly things like min samples leaf etc can also result in a inferior model now that the settings are w.r.t the number of instances in CV while the actual number of instances is more. Is this a correct understanding ? What is the approach to safeguard against this ? How can I know training data is enough for machine learning's answers mostly state that the more data the better. If you do a lot of model-selection, maybe Sacred can help, too. Among other things, it sets and can log the random seed for each evaluation, f.ex.: During the experiment, for tune-up and reproducibility, you fix temporarily random state but you repeat the experiment with different random states and take the mean of the results. For production system, you remove random state by setting it to NoneI have an SVM in R and I would now like to plot the classification space for this machine. I have found some examples on the Internet, but I can't seem to make sense of them. My R script is as follows: I cannot get the plot command to work. I would like a graph something like this http://bm2.genes.nig.ac.jp/RGM2/R_current/library/e1071/man/images/plot.svm_001.png  First of all, the plot.svm function assumes that the data varies across two dimensions. The data you have used in your example is only one-dimensional and so the decision boundary would have to be plotted on a line, which isn't supported. Secondly, the function seems to need a data frame as input and you are working with vectors. This should work... Alternatively, you can use the kernlab package:I'm using LogisticRegression as a model to train an estimator in scikit-learn. The features I use are (mostly) categorical; and so are the labels. Therefore, I use a DictVectorizer and a LabelEncoder, respectively, to encode the values properly.  The training part is fairly straightforward, but I'm having problems with the test part. The simple thing to do is to use the "predict" method of the trained model and get the predicted label. However, for the processing I need to do afterwards, I need the probability for each possible label (class) for each particular instance. I decided to use the "predict_proba" method. However, I get different results for the same test instance, whether I use this method when the instance is by itself or accompanied by others.  Next, is a code that reproduces the problem. Following is the output obtained: As can be seen, the values obtained with "predict_proba" for the instance in "X_test1" change when that same instance is with others in X_test2. Also, "X_test3" just reproduces the "X_test2" and adds one more instance (that is equal to the last in "X_test2"), but the probability values for all of them change. Why does this happen?
Also, I find it really strange that ALL the probabilities for "X_test1" are 1, shouldn't the sum of all be 1? Now, if instead of using "predict_proba" I use "decision_function", I get the consistency in the values obtained that I need. The problem is that I get negative coefficients, and even some of the positives ones are greater than 1.  So, what should I use? Why do the values of "predict_proba" change that way? Am I not understanding correctly what those values mean? Thanks in advance for any help you could give me. UPDATE As suggested, I changed the code so as to also print the encoded "X_test1", "X_test2" and "X_test3", as well as their shapes. This doesn't appear to be the problem, as the encoding is consistant for the same instances between the test sets.  As indicated on the question's comments, the error was caused by a bug in the implementation for the version of scikit-learn I was using. The problem was solved updating to the most recent stable version 0.12.1How would I implement this neural network cost function in matlab:  Here are what the symbols represent: I'm having problems with the nested sums, the bias nodes, and the general complexity of this equation. I'm also struggling because there are 2 matrices of weights, one connecting the inputs to the hidden layer, and one connecting the hidden layer to the outputs. Here's my attempt so far. Define variables Hypothesis using these weights equals... Cost Function using these weights equals... (This is where I am struggling) I just keep writing things like this and then realising it's all wrong. I can not for the life of me work out how to do the nested sums, or include the input matrix, or do any of it. It's all very complicated. How would I create this equation in matlab? Thank you very much! 
 Note: The code has strange colours as stackoverflow doesn't know I am programing in MATLAB. I have also wrote the code straight into stackoverflow, so it may have syntax errors. I am more interested in the general idea of how I should go about doing this rather than just having a code to copy and paste. This is the reason I haven't bothered with semi colons and such. I've implemented neural networks using the same error function as the one you've mentioned above. Unfortunately, I haven't worked with Matlab for quite some time, but I'm fairly proficient in Octave,which hopefully you can still find useful, since many of the functions in Octave are similar to those of Matlab. @sashkello provided a good snippet of code for computing the cost function. However, this code is written with a loop structure, and I would like to offer a vectorized implementation. In order to evaluate the current theta values, we need to perform a feed forward/ forward propagation throughout the network. I'm assuming you know how to write the feed forward code, since you're only concerned with the J(theta) errors. Let the vector representing the results of your forward propagation be F Once you've performed feedforward, you'll need to carry out the equation. Note, I'm implementing this in a vectorized manner. This will compute the part of the summation concerning:  Now we must add the regularization term, which is:  Typically, we would have arbitrary number of theta matrices, but in this case we have 2, so we can just perform several sums to get: Notice how in each sum I'm only working from the second column through the rest. This is because the first column will correspond to the theta values we trained for the `bias units. So there's a vectorized implementation of the computation of J.  I hope this helps!  I think Htheta is a K*2 array. Note that you need to add bias (x0 and a0) in the forward cost function calculation. I showed you the array dimensions in each step under the assumption that you have two nodes at input , hidden, and output layers as comments in the code. Well, as I understand your question has nothing to do with neural networks, but basically asking how to make a nested sum in matlab. I don't really want to type in the whole equation above, but, i.e., the first part of the first sum will look like: where Jtheta is your result. This works on any number of hidden layers:The following extremely simplified DataFrame represents a much larger DataFrame containing medical diagnoses: Problem: For machine learning, I need to randomly split this dataframe into three subframes in the following way: ...where the split array specifies the fraction of the complete data that goes into each subframe. If you want to generalise to n splits, np.array_split is your friend (it works with DataFrames well). A windy solution using train_test_split for stratified splitting.  Where X is a DataFrame of your features, and y is a single-columned DataFrame of your labels. Here is a Python function that splits a Pandas dataframe into train, validation, and test dataframes with stratified sampling. It performs this split by calling scikit-learn's function train_test_split() twice. Below is a complete working example. Consider a dataset that has a label upon which you want to perform the stratification. This label has its own distribution in the original dataset, say 75% foo, 15% bar and 10% baz. Now let's split the dataset into train, validation, and test into subsets using a 60/20/20 ratio, where each split retains the same distribution of the labels. See the illustration below:  Here is the example dataset: Now, let's call the split_stratified_into_train_val_test() function from above to get train, validation, and test dataframes following a 60/20/20 ratio. The three dataframes df_train, df_val, and df_test contain all the original rows but their sizes will follow the above ratio. Further, each of the three splits will have the same distribution of the label, namely 75% foo, 15% bar and 10% baz. To split into train / validation / test in the ratio 70 / 20 / 10%:I'm playing with a one-vs-all Logistic Regression classifier using Scikit-Learn (sklearn).  I have a large dataset that is too slow to run all at one go; also I would like to study the learning curve as the training proceeds.   I would like to use batch gradient descent to train my classifier in batches of, say, 500 samples.  Is there some way of using sklearn to do this, or should I abandon sklearn and "roll my own"? This is what I have so far: I.e. it correctly identifies a training sample (yes, I realize it would be better to evaluate it with new data -- this is just a quick smoke-test).   R.e. batch gradient descent: I haven't gotten as far as creating learning curves, but can one simply run fit repeatedly on subsequent subsets of the training data?  Or is there some other function to train in batches?  The documentation and Google are fairly silent on the matter.  Thanks! What you want is not batch gradient descent, but stochastic gradient descent; batch learning means learning on the entire training set in one go, while what you describe is properly called minibatch learning. That's implemented in sklearn.linear_model.SGDClassifier, which fits a logistic regression model if you give it the option loss="log". With SGDClassifier, like with LogisticRegression, there's no need to wrap the estimator in a OneVsRestClassifier -- both do one-vs-all training out of the box. Then, to train on minibatches, use the partial_fit method instead of fit. The first time around, you have to feed it a list of classes because not all classes may be present in each minibatch: (Here, I'm passing classes for each minibatch, which isn't necessary but doesn't hurt either and makes the code shorter.)I am trying to build a sentiment analyzer using scikit-learn/pandas. Building and evaluating the model works, but attempting to classify new sample text does not.  My code:  The error: I'm not sure what the issue could be. In my classify method, I create a brand new vectorizer to process the text I want to classify, separate from the vectorizer used to create training and test data from the model. Thanks  You've fitted a vectorizer, but you throw it away because it doesn't exist past the lifetime of your vectorize function. Instead, save your model in vectorize after it's been transformed: Then in your classify function, don't create a new vectorizer. Instead, use the one you'd fitted to the training data: Save vectorizer as a pickle or joblib file and load it when you want to predict. You can save both the model and the vectorizer and use them later on as well: here is how I did it:I am working on a image segmentation machine learning project and I would like to test it out on Google Colab. For the training dataset, I have 700 images, mostly 256x256, that I need to upload into a python numpy array for my project. I also have thousands of corresponding mask files to upload. They currently exist in a variety of subfolders on Google drive, but I have been unable to upload them to Google Colab for use in my project. So far I have attempted using Google Fuse which seems to have very slow upload speeds and PyDrive which has given me a variety of authentication errors. I have been using the Google Colab I/O example code for the most part. How should I go about this? Would PyDrive be the way to go? Is there code somewhere for uploading a folder structure or many files at a time? You can put all your data into your google drive and then mount drive. This is how I have done it. Let me explain in steps. Step 1:
Transfer your data into your google drive. Step 2:
Run the following code to mount you google drive. Step 3:
Run the following line to check if you can see your desired data into mounted drive. Step 4: Now load your data into numpy array as follows. I had my exel files having my train and cv and test data. Edit For downloading the data into your drive from the colab notebook environment, you can run the following code. Here are few steps to upload large dataset to Google Colab 1.Upload your dataset to free cloud storage like dropbox, openload, etc.(I used dropbox)

2.Create a shareable link of your uploaded file and copy it.

3.Open your notebook in Google Colab and run this command in one of the cell:
 That's it!
You can compress your dataset in zip or rar file and later unizp it after downloading it in Google Colab by using this command: Zip you file first then upload it to Google Drive. See this simple command to unzip: Example: Step1: Mount the Drive, by running the following command: This will output a link. Click on the link, hit allow, copy the authorization code and paste it the box present in colab cell with the text "Enter your authorization code:" written on top of it.
This process is just giving permission for colab to access your Google Drive. Step2: Upload your folder(zipped or unzipped depending on the size of the folder) to Google Drive Step3: Now work your way into the Drive directories and files to locate your uploaded folder/zipped file. This process may look something like this:
The current working directory in colab when you start off will be /content/
Just to make sure, run the following command in the cell: It will show you the current directory you are in. (pwd stands for "print working directory")
Then use the commands like: to list the directories and files in the directory you are in
and the command: to move into the directories to locate your uploaded folder or the uploaded .zip file. And just like that, you are ready to get your hands dirty with your Machine Learning model! :) Hopefully, these simple steps will prevent you from spending too much unnecessary time on figuring out how colab works when you should actually be spending the majority of your time figuring out the Machine learning model, its hyperparameters, pre-processing... Google Colab had made it more convenient for users to upload files [from the local machine, Google drive, or github]. You need to click on Mount Drive Option to the pane on the left side of the notebook and you'll get access to all the files stored in your drive.  Select the file -> right-click -> Copy path Refer this Use python import methods to import files from this path, e.g., for example: For importing multiple files in one go, you may need to write a function. There are many ways to do so :  You might want to push your data into a github repository then in Google Colab code cell you can run :  !git clone https://www.github.com/{repo}.git You can upload your data to Google drive  then in your code cell :  from google.colab import drive drive.mount('/content/drive') Use transfer.sh tool : you can visit here to see how it works :  transfer.shI'm a beginner in Keras and just write a toy example. It reports a TypeError. The code and error are as follows: Code: Error: So how can I deal with it?  The input to a RNN layer would have a shape of (num_timesteps, num_features), i.e. each sample consists of num_timesteps timesteps where each timestep is a vector of length num_features. Further, the number of timesteps (i.e. num_timesteps) could be variable or unknown (i.e. None) but the number of features (i.e. num_features) should be fixed and specified from the beginning. Therefore, you need to change the shape of Input layer to be consistent with the RNN layer. For example: Then, you also need to change the shape of input data (i.e. data) as well to be consistent with the input shape you have specified (i.e. it must have a shape of (num_samples, num_timesteps, num_features)). As a side note, you could define the RNN layer more simply by using the SimpleRNN layer directly: I think @today's answer is very clear. However, not complete. The key thing here is that, if your input doesn't contain num_features, you have to make a Embedding layer next to the input. So if you use: it also works.I have a simple Python code for a machine learning project. I have a relatively big database of spontaneous speech. I started to train my speech model. Since it's a huge database I let it work overnight. In the morning I woke up and saw a mysterious Killed: 9 line in my Terminal. Nothing else. There is no other error message or something to work with. The code run well for about 6 hours which is 75% of the whole process so I really don't understand whats went wrong. What is Killed:9 and how to fix it? It's very frustrating to lose hours of computing time... I'm on macOS Mojave beta if it's matter. Thank you in advance! I faced this issue when I updated my Mac OS version from Catalina to Big Sur. I was trying to run a binary and facing the Killed: 9 issue. I was able to resolve this issue by following the steps below (I referred to this Apple StackExchange post for these steps) :- Try to change the node version.
In my case, that helps.Suppose I have text based training data and testing data. To be more specific, I have two data sets - training and testing - and both of them have one column which contains text and is of interest for the job at hand. I used tm package in R to process the text column in the training data set. After removing the white spaces, punctuation, and stop words, I stemmed the corpus and finally created a document term matrix of 1 grams containing the frequency/count of the words in each document. I then took a pre-determined cut-off of, say, 50 and kept only those terms that have a count of greater than 50.   Following this, I train a, say, GLMNET model using the DTM and the dependent variable (which was present in the training data). Everything runs smooth and easy till now. However, how do I proceed when I want to score/predict the model on the testing data or any new data that might come in the future? Specifically, what I am trying to find out is that how do I create the exact DTM on new data?  If the new data set does not have any of the similar words as the original training data then all the terms should have a count of zero (which is fine). But I want to be able to replicate the exact same DTM (in terms of structure) on any new corpus. Any ideas/thoughts? tm has so many pitfalls... See much more efficient text2vec and vectorization vignette which fully answers to the question. For tm here is probably one more simple way to reconstruct DTM matrix for second corpus: If I understand correctly, you have made a dtm, and you want to make a new dtm from new documents that has the same columns (ie. terms) as the first dtm. If that's the case, then it should be a matter of sub-setting the second dtm by the terms in the first, perhaps something like this: First set up some reproducible data... This is your training data... And this is your testing data...     Here is the bit that does what you want:  Now we keep only the terms in the testing data that are present in the training data... Finally add to the testing data all the empty columns for terms in the training data that are not in the testing data... So zz is a data frame of the testing documents, but has the same structure as the training documents (ie. same columns, though many of them contain NA, as SchaunW notes). Is that along the lines of what you want?I am trying to apply Random Projections method on a very sparse dataset. I found papers and tutorials about Johnson Lindenstrauss method, but every one of them is full of equations which makes no meaningful explanation to me. For example, this document on Johnson-Lindenstrauss Unfortunately, from this document, I can get no idea about the implementation steps of the algorithm. It's a long shot but is there anyone who can tell me the plain English version or very simple pseudo code of the algorithm? Or where can I start to dig this equations? Any suggestions?   For example, what I understand from the algorithm by reading this paper concerning Johnson-Lindenstrauss is that: As far as I understand: first, I need to construct a 100x500 matrix and fill the entries randomly with +1 and -1 (with a 50% probability). Edit:
Okay, I think I started to get it. So we have a matrix A which is mxn. We want to reduce it to E which is mxk.  What we need to do is, to construct a matrix R which has nxk dimension, and fill it with 0, -1 or +1, with respect to 2/3, 1/6 and 1/6 probability. After constructing this R, we'll simply do a matrix multiplication AxR to find our reduced matrix E. But we don't need to do a full matrix multiplication, because if an element of Ri is 0, we don't need to do calculation. Simply skip it. But if we face with 1, we just add the column, or if it's -1, just subtract it from the calculation. So we'll simply use summation rather than multiplication to find E. And that is what makes this method very fast. It turned out a very neat algorithm, although I feel too stupid to get the idea. You have the idea right. However as I understand random project, the rows of your matrix R should have unit length. I believe that's approximately what the normalizing by 1/sqrt(k) is for, to normalize away the fact that they're not unit vectors. It isn't a projection, but, it's nearly a projection; R's rows aren't orthonormal, but within a much higher-dimensional space, they quite nearly are. In fact the dot product of any two of those vectors you choose will be pretty close to 0. This is why it is a generally good approximation of actually finding a proper basis for projection. The mapping from high-dimensional data A to low-dimensional data E is given in the statement of theorem 1.1 in the latter paper - it is simply a scalar multiplication followed by a matrix multiplication.  The data vectors are the rows of the matrices A and E.  As the author points out in section 7.1, you don't need to use a full matrix multiplication algorithm. If your dataset is sparse, then sparse random projections will not work well.
You have a few options here: Option A: Step 1. apply a structured dense random projection (so called fast hadamard transform is typically used). This is a special projection which is very fast to compute but otherwise has the properties of a normal dense random projection Step 2. apply sparse projection on the "densified data" (sparse random projections are useful for dense data only) Option B:
  Apply SVD on the sparse data. If the data is sparse but has some structure SVD is better. Random projection preserves the distances between all points. SVD preserves better the distances between dense regions - in practice this is more meaningful. Also people use random projections to compute the SVD on huge datasets. Random Projections gives you efficiency, but not necessarily the best quality of embedding in a low dimension.
 If your data has no structure, then use random projections.  Option C: For data points for which SVD has little error, use SVD; for the rest of the points use Random Projection Option D:
  Use a random projection based on the data points themselves.
  This is very easy to understand what is going on. It looks something like this: If you are still looking to solve this problem, write a message here, I can give you more pseudocode. The way to think about it is that a random projection is just a random pattern and the dot product (i.e. projecting the data point) between the data point and the pattern gives you the overlap between them. So if two data points overlap with many random patterns, those points are similar. Therefore, random projections preserve similarity while using less space, but they also add random fluctuations in  the pairwise similarities. What JLT tells you is that to make fluctuations 0.1 (eps)
you need about 100*log(n) dimensions. Good Luck! An R  Package to perform Random Projection using Johnson- Lindenstrauss Lemma 
RandProAs you may know, many things changed in OpenCV 3 (in comparision to the openCV2 or the old first version). In the old days, to train SVM one would use: In the third version of API, there is no CvSVMParams nor CvSVM. Surprisingly, there is a documentation page about SVM, but it tells everything, but not how to really use it (at least I cannot make it out).
Moreover, it looks like no one in the Internet uses SVM from OpenCV's 3.0. Currently, I only managed to get the following: Can you please provide me with information, how to rewrite the actual training to openCV 3? with opencv3.0, it's definitely different , but not difficult: I was porting my code from OpenCV 2.4.9 to 3.0.0-rc1 and had the same issue. Unfortunately the API has changes since the answer was posted, so I would like to update it accordingly:  I know this is an old post, but i came across it looking for the same solution. This tutorial is extremely helpful: http://docs.opencv.org/3.0-beta/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.htmlWant to improve this question? Update the question so it can be answered with facts and citations by editing this post. Closed 4 years ago. I have a set of Books objects, classs Book is defined as following : Where title is the title of the book, example : Javascript for dummies. and taglist is a list of tags for our example : Javascript, jquery, "web dev", .. As I said a have a set of books talking about different things : IT, BIOLOGY, HISTORY, ...
Each book has a title and a set of tags describing it.. I have to classify automaticaly those books into separated sets by topic, example : IT BOOKS : HISTORY BOOKS : BIOLOGY BOOKS : Do you guys know a classification algorithm/method to apply for that kind of problems ? A solution is to use an external API to define the category of the text, but the problem here is that books are in different languages : french, spanish, english .. This looks like a reasonably straightforward keyword-based classification task. Since you're using Java, good packages to consider for this would be Classifier4J, Weka, or Lucene Mahout.  Classifier4J  Classifier4J supports classification using naive Bayes and a vector space model.  As seen in this source code snippet on training and scoring using its naive Bayes classifier, the package is reasonably easy to use. It's also distributed under the liberal Apache Software License.  Weka  Weka is a very popular tool for data mining. An advantage of using it is that you'd be able to readily experiment with using numerous different machine learning models to categorize the books into topics including naive Bayes, decision trees, support vector machines, k-nearest neighbor, logistic regression, and even a rule set based learner.  You'll find a tutorial on using Weka for text categorization here. Weka is, however, distributed under the GPL. You won't be able to use it for closed source software that you want to distribute. But, you could still use it to back a web service. Lucene Mahout  Mahout is designed for doing machine learning on very large datasets. It's built on top of Apache Hadoop and supports supervised classification using naive Bayes.  You'll find a tutorial covering how to use Mahout for text classification here. Like Classifier4J, Mahout is distributed under the liberal Apache Software License.  Do you not want something as simple as this? Now m.get("IT") will return all IT books, etc... Sure some books will appear in multiple categories, but that happens in real life, too...  So you are looking to make a Map of Tags that holds a Collection of Books? EDIT: Sounds like you might want to take a look at a Vector Space Model to apply classification of categories. Either Lucene or Classifier4j offer a framework for this. You might want to look up fuzzy matching algorithms such as Soundex and Levenshtein.How do I use sklearn CountVectorizer with both 'word' and 'char' analyzer?
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html I could extract the text features by word or char separately but how do i create a charword_vectorizer? Is there a way to combine the vectorizers? or use more than one analyzer? You can pass a callable as the analyzer argument to get full control over the tokenization, e.g. You can combine arbitrary feature extraction steps with the FeatureUnion estimator: http://scikit-learn.org/dev/modules/pipeline.html#featureunion-combining-feature-extractors In this case this is probably less efficient than larsmans solution, but might be easier to use.I tried to do the following importations for a machine learning project: I got this error message: Please help I tried everything but nothing worked. I tried these solutions as well:
ImportError: DLL load failed: Le module spécifié est introuvable ImportError: DLL load failed: The specified module could not be found This line points to scipy. from scipy.sparse.linalg import lsqr as sparse_lsqr You can try: pip uninstall scipy pip install scipy enjoy! Reinstallation of scipy, numpy, and scikit-learn packages fixed the error in my case. You should open up "C:\Python27\lib\site-packages\sklearn\utils\fixes.py", and edit the contents. There are two specific changes you should make: First, copy-and-paste the contents of https://github.com/scikit-learn/scikit-learn/blob/74a9756fa784d1f22873ad23c8b4948c6e290108/sklearn/utils/fixes.py into the file "C:\Python27\lib\site-packages\sklearn\utils\fixes.py". Second, replace the line if np_version < (1, 12, 0): with if np_version < (1, 12):. More background info and detail available here, in a great answer from user DSM. Install this numpy library instead of the one you use: http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy I assume you have Intel Math Kernal Libary installed. i've found a silly solution, similar to the @saggy ones: iteratively run the script from command line, if compare a "DLL error" look for a package/module/library/wattelapesca name, then pip uninstall thatPackage and re-install it as a pseudocode: For me uninstalling scipy in conda env and then reinstalling using pip works.  Uninstall: conda remove --force scipy Install: pip install scipy DLL missing can happen by a wide range of reasons. In your case it seems there is a mismatch between sklearn and its dependencies(Maybe different 32bit or 64bit installation of packages.). As different answers point out to different packages, a general way to find out dependencies is using: and the output is: Name: scikit-learn Version: 0.23.1 Summary: A set of python modules for machine learning and data mining Home-page: http://scikit-learn.org Author: None Author-email: None License: new BSD Location: c:\users\username\appdata\local\programs\python\python37\lib\site- 
 packages Requires: joblib, numpy, threadpoolctl, scipy So It's probable that the root problem returns to one of 'Requires' packages.
By the way the error lines also can point out which package causes error.
Try reinstalling these packages should solve the problem.I am creating a model in Keras and want to compute my own metric (perplexity). This requires using the unnormalized probabilities/logits. However, the keras model only returns the softmax probabilties: The Keras FAQ have a solution to get the output of intermediate layers here. Another solution is given here. However, these answers store the intermediate outputs in a different model which is not what I need.
I want to use the logits for my custom metric. The custom metric should be included in the model.compile() function such that it's evaluated and displayed during training. So I don't need the output of the Dense layer separated in a different model, but as part of my original model. In short, my questions are: When defining a custom metric as outlined here using def custom_metric(y_true, y_pred), does the y_pred contain logits or normalized probabilities?  If it contains normalized probabilities, how can I get the unnormalized probabilities, i.e. the logits output by the Dense layer? I think I have found a solution First, I change the activation layer to linear such that I receive logits as outlined by @loannis Nasios.  Second, to still get the sparse_categorical_crossentropy as a loss function, I define my own loss function, setting the from_logits parameter to true. try to change last activation from softmax to linear You can make a model for training and another for predictions.  For training, you can use the functional API model and simply take a part of the existing model, leaving the Activation aside: Since you got one model as a part of another, they both will share the same weights.I thought that batch size is only for performance. The bigger the batch, more images are computed at the same time to train my net. But I realized, if I change my batch size, my net accuracy gets better. So I did not understand what batch size is. Can someone explain me what is batch size?   Caffe is trained using Stochastic-Gradient-Descend (SGD): that is, at each iteration it computes the (stochastic) gradient of the parameters w.r.t the training data and makes a move (=change the parameters) in the direction of the gradient.
Now, if you write the equations of the gradient w.r.t. training data you'll notice that in order to compute the gradient exactly you need to evaluate all your training data at each iteration: this is prohibitively time consuming, especially when the training data gets bigger and bigger.
In order to overcome this, SGD approximates the exact gradient, in a stochastic manner, by sampling only a small portion of the training data at each iteration. This small portion is the batch.
Thus, the larger the batch size the more accurate the gradient estimate at each iteration. TL;DR: batch size affect the accuracy of the estimated gradient at each iteration, changing the batch size therefore affect the "path" the optimization takes and may change the results of the training process. Update:
In ICLR 2018 conference an interesting work was presented:
Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le Don't Decay the Learning Rate, Increase the Batch Size.
This work basically relates the effect of changing batch size and learning rate.I have been looking in to Multi-output regression the last view weeks. I am working with the scikit learn package. My machine learning problem has an a input of 3 features an needs to predict two output variables. Some ML models in the sklearn package support multioutput regression nativly. If the models do not support this, the sklearn multioutput regression algorithm can be used to convert it. The multioutput class fits one regressor per target.  1) For your first question, I have divided that into two parts. First part has the answer written in the documentation you linked and also in this user guide topic, which states explicitly that: As MultiOutputRegressor fits one regressor per target it can not take
  advantage of correlations between targets. Second part of first question asks about other algorithms which support this. For that you can look at the "inherently multiclass" part in the user-guide. Inherently multi-class means that they don't use One-vs-Rest or One-vs-One strategy to be able to handle multi-class (OvO and OvR uses multiple models to fit multiple classes and so may not use the relationship between targets). Inherently multi-class means that they can structure the multi-class setting into a single model. This lists the following: Try replacing the 'Classifier' at the end with 'Regressor' and see the documentation of fit() method there. For example let's take DecisionTreeRegressor.fit(): You see that it supports a 2-d array for targets (y). So it may be able to use correlation and underlying relationship of targets. 2) Now for your second question about using neural network or not, it depends on personal preference, the type of problem, the amount and type of data you have, the training iterations you want to do. Maybe you can try multiple algorithms and choose what gives best output for your data and problem.I have a subclass Model of tf.keras.Model,code is following the result is : but I want to see the all layers of mobilenet,then I tried to extract all layers of mobilenet and put in the model: then the resule not changed. then I tried to extract one layer insert to the model : It did not change either.I am so confused. but I find that the parameter of dense layer changed,I dont know what happend. @Ioannis 's answer is perfectly fine, but unfortunately it drops the keras 'Model Subclassing' structure that is present in the question. If, just like me, you want to keep this model subclassing and still show all layers in the summary, you can branch down into all the individual layers of the more complex model using a for loop: After this we can directly build the model and call the summary: In order to be able to view backbone's layers, you' ll have to construct your new model using backbone.input and backbone.output There is an argument expand_nested in the Method summary.I got linearsvc working against training set and test set using load_file method i am trying to get It working on Multiprocessor enviorment. How can i get multiprocessing work on LinearSVC().fit() LinearSVC().predict()? I am not really familiar  with datatypes of scikit-learn yet. I am also thinking about splitting samples into multiple arrays but i am not familiar with numpy arrays and scikit-learn data structures.  Doing this it will be easier to put into multiprocessing.pool() , with that , split samples into chunks , train them and combine trained set  back later , would it work ?  EDIT:
Here is my scenario: lets say , we have 1 million files in training sample set , when we want to distribute processing of Tfidfvectorizer on several processors we have to split those samples (for my case it will only have two categories , so lets say 500000 each samples to train) . My server have 24 cores with 48 GB , so i want to split each topics into number of chunks 1000000 / 24 and process Tfidfvectorizer on them. Like that i would do to Testing sample set , as well as SVC.fit() and decide(). Does it make sense?  Thanks.  PS: Please do not close this . I think using SGDClassifier instead of LinearSVC for this kind of data would be a good idea, as it is much faster. For the vectorization, I suggest you look into the hash transformer PR. For the multiprocessing: You can distribute the data sets across cores, do partial_fit, get the weight vectors, average them, distribute them to the estimators, do partial fit again. Doing parallel gradient descent is an area of active research, so there is no ready-made solution there. How many classes does your data have btw? For each class, a separate will be trained (automatically). If you have nearly as many classes as cores, it might be better and much easier to just do one class per core, by specifying n_jobs in SGDClassifier. For linear models (LinearSVC, SGDClassifier, Perceptron...) you can chunk your data, train independent models on each chunk and build an aggregate linear model (e.g. SGDClasifier) by sticking in it the average values of coef_ and intercept_ as attributes. The predict method of LinearSVC, SGDClassifier, Perceptron compute the same function (linear prediction using a dot product with an intercept_ threshold and One vs All multiclass support) so the specific model class you use for holding the average coefficient is not important. However as previously said the tricky point is parallelizing the feature extraction and current scikit-learn (version 0.12) does not provide any way to do this easily. Edit: scikit-learn 0.13+ now has a hashing vectorizer that is stateless.Two questions: 1) Does anyone know if I can add new image classes to the pre-trained Inception-v3 model?  For example, I wanted to train TensorFlow on a multitude of national flags, but I need to make sure that I can still recognize the images from the ImageNet hierarchy.  I realize that there is a way to wipe the top layer of Inception and completely retrain the model on my classes, but this very limiting and time consuming. 2) Also, is there a way to output the entire hierarchy containing the tag that the image receives?  I wish to be able to not only see specifically what Inception tags the image as, but I want to see all of the more broad 'synsets' from ImageNet.  For example, instead of just seeing the output "toy poodle", I am interested in "Animal/Domesticated Animal/Dog/Poodle/toy poodle". Any responses are greatly appreciated. 1) Output layer is softmax, which means it has predefined number of neurons, each one is defined for one specific class. Technically you could perform Network Surgery so that it has one more neuron in output layer which will represent your new class. But you will have to perform additional training of your network so that it updates all of its weights in order to account for new class. Bad news - it could take a while since update will affect whole network and the network is GIANT. Good news - such change in pretrained existing network will be faster than learning everything from scratch. 2) What makes you think that such hierarchy exists at all? You can't know anything about internal representation of data for sure. Of course, you can inspect activations of neurons in each of the functions and even visualize them... But you will have to try to understand what these activations mean on your own. And probably you won't find any hierarchy which you expect to see. So to sum up - understanding how ANN represents data internally is no easy task. Actually - extremely difficult one. Suggested further reading:
https://github.com/tensorflow/models/tree/master/inception Pay attention to this part of the doc - it is strongly related to your #1 Here is some explanations:
https://github.com/tensorflow/models/issues/2510. So it is possible somehow finetune model if having model checkpoint.
Here is repo link with example of finetuning:
https://github.com/tensorflow/models/tree/master/research/slim/ Yes you can, i recently did something very similar, in my case it was patogenic plant leaves vs healthy plant leaves. The v3 inception is already trained, what you will be doing is transfer learning. Transfer learning is a technique that shortcuts a lot of this work by taking a fully-trained model for a set of categories like ImageNet, and retrains from the existing weights for new classes. Link : Image Retraining at tensorflow.org Video Sources: YouTube video tutorial, Hvass Laboratories has some great video resource to rectify your questions.I need to compute Information Gain scores for >100k features in >10k documents for text classification. Code below works fine but for the full dataset is very slow - takes more than an hour on a laptop. Dataset is 20newsgroup and I am using scikit-learn, chi2 function which is provided in scikit works extremely fast. Any idea how to compute Information Gain faster for such dataset? EDIT: I merged the internal functions and ran cProfiler as below (on a dataset limited to ~15k features and ~1k documents): Result top 20 by tottime: Looks that most of the time is spent in _get_row_slice. I am not entirely sure about the first row, looks it covers the whole block I provided to cProfile.runctx, though I don't know why there is such a big gap between first line totime=60.27 and second one tottime=1.362. Where was the difference spent in? Is it possible to check it in cProfile? Basically, looks the problem is with sparse matrix operations (slicing, getting elements) -- the solution probably would be to calculate Information Gain using matrix algebra (like chi2 is implemented in scikit). But I have no idea how to express this calculation in terms of matrices operations... Anyone has an idea?? Don't know whether it still helps since a year has passed. But now I happen to be faced with the same task for text classification. I've rewritten your code using the nonzero() function provided for sparse matrix. Then I just scan nz, count the corresponding y_value and calculate the entropy.  The following code only needs seconds to run news20 dataset (loaded in using libsvm sparse matrix format). Here is a version that uses matrix operations. The IG for a feature is a mean over its class-specific scores. On a dataset with 1000 instances and 1000 unique features, this implementation is >100 faster than the one without matrix operations. It is this code feature_not_set_indices = [i for i in feature_range if i not in feature_set_indices] takes 90% of the time, try to change to set operationHow do you get a dependency parse (not syntax tree) output from SyntaxNet (https://github.com/tensorflow/models/tree/master/syntaxnet) ? I see a description of dependency parsing...a description of how to train a model, but not how to get dependency parse output. Does SyntaxNet (Specifically the Parsey McParseface model) even do dependency parsing out of the box? Passing --arg_prefix brain_parser to the parser_eval.py should do the trick. But this requires the tagged output to be fed as input. Here's an example where the first pass tags the words and the second pass resolves dependencies: This generates the following output:I'm implementing PCA using eigenvalue decomposition for sparse data. I know matlab has PCA implemented, but it helps me understand all the technicalities when I write code.
I've been following the guidance from here, but I'm getting different results in comparison to built-in function princomp.  Could anybody look at it and point me in the right direction. Here's the code: Here's how I would do it: and an example to compare against the PRINCOMP function from the Statistics Toolbox: You might also be interested in this related post about performing PCA by SVD.I found a piece of code in Chapter 7,Section 1 of deep Deep Learning with Python as follow： as you see this model's input don't have raw data's shape information, then after Embedding layer, the input of LSTM or the output of Embedding are some variable length sequence.  So I want to know:  Additional information: in order to explain what lstm_unit is (I don't know how to call it,so just show it image):  The provided recurrent layers inherit from a base implementation keras.layers.Recurrent, which includes the option return_sequences, which defaults to False. What this means is that by default, recurrent layers will consume variable-length inputs and ultimately produce only the layer's output at the final sequential step. As a result, there is no problem using None to specify a variable-length input sequence dimension. However, if you wanted the layer to return the full sequence of output, i.e. the tensor of outputs for each step of the input sequence, then you'd have to further deal with the variable size of that output. You could do this by having the next layer further accept a variable-sized input, and punt on the problem until later on in your network when eventually you either must calculate a loss function from some variable-length thing, or else calculate some fixed-length representation before continuing on to later layers, depending on your model. Or you could do it by requiring fixed-length sequences, possibly with padding the end of the sequences with special sentinel values that merely indicate an empty sequence item purely for padding out the length. Separately, the Embedding layer is a very special layer that is built to handle variable length inputs as well. The output shape will have a different embedding vector for each token of the input sequence, so the shape with be (batch size, sequence length, embedding dimension). Since the next layer is LSTM, this is no problem ... it will happily consume variable-length sequences as well. But as it is mentioned in the documentation on Embedding: If you want to go directly from Embedding to a non-variable-length representation, then you must supply the fixed sequence length as part of the layer. Finally, note that when you express the dimensionality of the LSTM layer, such as LSTM(32), you are describing the dimensionality of the output space of that layer. In order to avoid the inefficiency of a batch size of 1, one tactic is to sort your input training data by the sequence length of each example, and then group into batches based on common sequence length, such as with a custom Keras DataGenerator. This has the advantage of allowing large batch sizes, especially if your model may need something like batch normalization or involves GPU-intensive training, and even just for the benefit of a less noisy estimate of the gradient for batch updates. But it still lets you work on an input training data set that has different batch lengths for different examples. More importantly though, it also has the big advantage that you do not have to manage any padding to ensure common sequence lengths in the input. How does it deal with units? Units are totally independend of length, so, there is nothing special being done.  Length only increases the "recurrent steps", but recurrent steps use always the same cells over and over.  The number of cells is fixed and defined by the user:  How to deal with variable length?I am using scikit-learn's linearSVC classifier for text mining. I have the y value as a label 0/1 and the X value as the TfidfVectorizer of the text document.  I use a pipeline like below For a prediction, I would like to get the confidence score or probability of a data point being classified as
1 in the range (0,1) I currently use the decision function feature However it returns positive and negative values that seem to indicate confidence. I am not too sure about what they mean either. However, is there a way to get the values in range 0-1? For example here is the output of the decision function for some of the data points You can't.
However you can use sklearn.svm.SVC with kernel='linear' and probability=True It may run longer, but you can get probabilities from this classifier by using predict_proba method. If you insist on using the LinearSVC class, you can wrap it in a sklearn.calibration.CalibratedClassifierCV object and fit the calibrated classifier which will give you a probabilistic classifier. Here is the output: which shows probabilities for each class for each data point.Does scikit-learn provide facility to perform regression using a gaussian or polynomial kernel? I looked at the APIs and I don't see any.
Has anyone built a package on top of scikit-learn that does this? Theory Polynomial regression is a special case of linear regression. With the main idea of how do you select your features. Looking at the multivariate regression with 2 variables: x1 and x2. Linear regression will look like this: y = a1 * x1 + a2 * x2. Now you want to have a polynomial regression (let's make 2 degree polynomial). We will create a few additional features: x1*x2, x1^2 and x2^2. So we will get your 'linear regression': This nicely shows an important concept curse of dimensionality, because the number of new features grows much faster than linearly with the growth of degree of polynomial. You can take a look about this concept here. Practice with scikit-learn You do not need to do all this in scikit. Polynomial regression is already available there (in 0.15 version. Check how to update it here). Either you use Support Vector Regression sklearn.svm.SVR and set the appropritate kernel (see here).  Or you install the latest master version of sklearn and use the recently added sklearn.preprocessing.PolynomialFeatures (see here) and then OLS or Ridge on top of that.I want to setup environment for deep learning using Anaconda (python 3.6). I have system having nvidia get force 1060 with windows installed on it. Now I want to have Ubuntu OS in VB. Can I install Cuda and CuDNN libraries in VB based Ubuntu OS? Any one that can help me? You can not use your GPU on virtual box. Because virtual box cannot pass through the host GPU. However, you can use the windows version of python which can use the GPU on your windows machine. Here is the installation procedure for windows. After installing cuda,  cudnn and anaconda 3.6 I just used, That's all.What I do: What I expect: What I observe:  What I don't understand: My code: [...] [...] [...] Update I found some sites that report on this issue: I tried following some of their suggested solutions without success so far. acc and loss are still different from fit_generator() and evaluate_generator(), even when using the exact same data generated with the same generator for training and validation. Here is what I tried: Please let me know if there are other solutions around that I am missing. I now managed having the same evaluation metrics. I changed the following: With this, I managed to have a similar accuracy and loss from fit_generator() and evaluate_generator(). Also, using the same data for training and testing now results in a similar metrics. Reasons for remaining differences are provided in the keras documentation. Set use_multiprocessing=False at fit_generator level fixes the problem BUT at the cost of slowing down training significantly. A better but still imperfect workround would be to set use_multiprocessing=False for only the validation generator as the code below modified from keras' fit_generator function.  Training for one epoch might not be informative enough in this case. Also your train and test data may not be exactly same, since you are not setting a random seed to the flow_from_directory method. Have a look here. Maybe, you can set a seed, remove augmentations (if any) and save trained model weights to load them later to check.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 3 years ago. I have the sentence below : I want to predict the missing word ,using an NLP model. What NLP model shall I use? Thanks. Try this out: https://github.com/huggingface/pytorch-pretrained-BERT First you have to set it up, properly with Then you can use the "masked language model" from the BERT algorithm, e.g. [out]: To truly understand why you need the [CLS], [MASK] and segment tensors, please do read the paper carefully, https://arxiv.org/abs/1810.04805 And if you're lazy, you can read this nice blogpost from Lilian Weng, https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html Other than BERT, there are a lot of other models that can perform the task of filling in the blank. Do look at the other models in the pytorch-pretrained-BERT repository, but more importantly dive deeper into the task of "Language Modeling", i.e. the task of predicting the next word given a history. There are numerous models you might be able to use. But I think the most recently being used model for such sequence learning problems, are bidirectional RNNs( like Bidirectional LSTM), you can get a hint from here But be advised, Bidirectional RNNs are very expensive to train. Depending on your problem to solve, I highly advice to use some pre-trained model.
Good luck!I want to implement the following algorithm, taken from this book, section 13.6:  I don't understand how to implement the update rule in pytorch (the rule for w is quite similar to that of theta). As far as I know, torch requires a loss for loss.backwward(). This form does not seem to apply for the quoted algorithm. I'm still certain there is a correct way of implementing such update rules in pytorch. Would greatly appreciate a code snippet of how the w weights should be updated, given that V(s,w) is the output of the neural net, parameterized by w. EDIT: Chris Holland suggested a way to implement, and I implemented it. It does not converge on Cartpole, and I wonder if I did something wrong. The critic does converge on the solution to the function gamma*f(n)=f(n)-1 which happens to be the sum of the series gamma+gamma^2+...+gamma^inf
meaning, gamma=1 diverges. gamma=0.99 converges on 100, gamma=0.5 converges on 2 and so on. Regardless of the actor or policy. The code: and EDIT 2:
Chris Holland's solution works. The problem originated from a bug in my code that caused the line to always get called, thus expected_reward_from_t1 was never zero, and thus no stopping condition was specified for the bellman equation recursion. With no reward engineering, gamma=1, lambda=0.6, and a single hidden layer of size 128 for both actor and critic, this converged on a rather stable optimal policy within 500 episodes. Even faster with gamma=0.99, as the graph shows (best discounted episode reward is about 86.6).  BIG thank you to @Chris Holland, who "gave this a try" I am gonna give this a try. .backward() does not need a loss function, it just needs a differentiable scalar output. It approximates a gradient with respect to the model parameters. Let's just look at the first case the update for the value function.  We have one gradient appearing for v, we can approximate this gradient by This gives us a gradient of v which has the dimension of your model parameters. Assuming we already calculated the other parameter updates, we  can calculate the actual optimizer update: We can then use opt.step() to update the model parameters with the adjusted gradient.I have the following evaluation metrics on the test set, after running 6 models for a binary classification problem: I have the following questions: Very briefly, with links (as parts of this have already been discussed elsewhere)... How can model 1 be the best in terms of logloss (the logloss is the closest to 0) since it performs the worst (in terms of accuracy). What does that mean ? Although loss is a proxy for the accuracy (or vice versa), it is not a very reliable one in that matter. A closer look at the specific mechanics between accuracy and loss may be useful here; consider the following SO threads (disclaimer: answers are mine): To elaborate a little: Assuming a sample with true label y=1, a probabilistic prediction from the classifier of p=0.51, and a decision threshold of 0.5 (i.e. for p>0.5 we classify as 1, otherwise as 0), the contribution of this sample to the accuracy is 1/n (i.e. positive), while the loss is Now, assume another sample again with true y=1, but now with a probabilistic prediction of p=0.99; the contribution to the accuracy will be the same, while the loss now will be: So, for two samples that are both correctly classified (i.e. they contribute positively to the accuracy by the exact same quantity), we have a rather huge difference in the corresponding losses... Although what you present here seems rather extreme, it shouldn't be difficult to imagine a situation where many samples of y=1 will be around the area of p=0.49, hence giving a relatively low loss but a zero contribution to accuracy nonetheless... How come does model 6 have lower AUC score than e.g. model 5, when model 6 has better accuracy. What does that mean ? This one is easier. According to my experience at least, most ML practitioners think that the AUC score measures something different from what it actually does: the common (and unfortunate) use is just like any other the-higher-the-better metric, like accuracy, which may naturally lead to puzzles like the one you express yourself. The truth is that, roughly speaking, the AUC measures the performance of a binary classifier averaged across all possible decision thresholds. So, the AUC does not actually measure the performance of a particular deployed model (which includes the chosen decision threshold), but the averaged performance of a family of models across all thresholds (the vast majority of which are of course of not interest to you, as they will be never used). For this reason, AUC has started receiving serious criticism in the literature (don't misread this - the analysis of the ROC curve itself is highly informative and useful); the Wikipedia entry and the references provided therein are highly recommended reading: Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. [...] One recent explanation of the problem with ROC AUC is that reducing the ROC Curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance points plotted and not the performance of an individual system Emphasis mine - see also On the dangers of AUC... Simple advice: don't use it. Is there a way to say which of these 6 models is the best ? Depends of the exact definition of "best"; if "best" means best for my own business problem that I am trying to solve (not an irrational definition for an ML practitioner), then it is the one that performs better according to the business metric appropriate for your problem that you have defined yourself. This can never be the AUC, and normally it is also not the loss...Recently, many deep architectures use "batch normalization" for training. What is "batch normalization"? What does it do mathematically? In what way does it help the training process? How is batch normalization used during training? is it a special layer inserted into the model? Do I need to normalize before each layer, or only once?  Suppose I used batched normalization for training. Does this affect my test-time model? Should I replace the batch normalization with some other/equivalent layer/operation in my "deploy" network? This question about batch normalization only covers part of this question, I was aiming and hoping for a more detailed answer. More specifically, I would like to know how training with batch normalization affect test time prediction, i.e., the "deploy" network and the TEST phase of the net. The batch normalization is for layers that can suffer from deleterious drift.  The math is simple: find the mean and variance of each component, then apply the standard transformation to convert all values to the corresponding Z-scores: subtract the mean and divide by the standard deviation.  This ensures that the component ranges are very similar, so that they'll each have a chance to affect the training deltas (in back-prop). If you're using the network for pure testing (no further training), then simply delete these layers; they've done their job.  If you're training while testing / predicting / classifying, then leave them in place; the operations won't harm your results at all, and barely slow down the forward computations. As for Caffe specifics, there's really nothing particular to Caffe.  The computation is a basic stats process, and is the same algebra for any framework.  Granted, there will be some optimizations for hardware that supports vector and matrix math, but those consist of simply taking advantage of the chip's built-in operations. RESPONSE TO COMMENT If you can afford a little extra training time, yes, you'd want to normalize at every layer.  In practice, inserting them less frequently -- say, every 1-3 inceptions -- will work just fine. You can ignore these in deployment because they've already done their job: when there's no back-propagation, there's no drift of weights.  Also, when the model handles only one instance in each batch, the Z-score is always 0: every input is exactly the mean of the batch (being the entire batch). As a complement to Prune's answer, during testing, batch normalization layer will use the average mean/variance/scale/shift values from different training iterations to normalize its input(subtract mean and divide by the standard deviation).  And the original google's batch normalization paper only said that it should be a moving average method and no more thorough explanation was provided though. Both caffe and tensorflow use an exponential moving average method. In my experience, a simple moving average method usually better than an exponential moving average method, as far as to the validation accuracy(Maybe it need more experiments). For a compare, you can refer to here(I tried the two moving average methods implementations in channel_wise_bn_layer, compared with the batch norm layer in BVLC/caffe).  For what it's worth this link has an example of using "BatchNorm" layers in cifar10 classification net. Specifically, it splits the layer between TRAIN and TEST phases: Batch normalization solves a problem called "internal covariate shift".
To understand why it helps, you’ll need to first understand what covariate shift actually is. “Covariates” is just another name for the input “features”, often written as X. Covariate shift means the distribution of the features is different in different parts of the training/test data, breaking the i.i.d assumption used across most of ML. This problem occurs frequently in medical data (where you have training samples from one age group, but you want to classify something coming from another age group), or finance (due to changing market conditions). "Internal covariate shift" refers to covariate shift occurring within a neural network, i.e. going from (say) layer 2 to layer 3. This happens because, as the network learns and the weights are updated, the distribution of outputs of a specific layer in the network changes. This forces the higher layers to adapt to that drift, which slows down learning. BN helps by making the data flowing between intermediate layers of the network look like whitened data, this means you can use a higher learning rate. Since BN has a regularizing effect it also means you can often remove dropout (which is helpful as dropout usually slows down training).I'm very new to machine learning. I tried to create a model to predict if the number is even.  I used this code https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/
which I changed to my needs. The problem is that there is circa 50% success which is equal to random. Do you know what to do to make it work? Neural networks aren't good at figuring out if a number is even or not. At least not if the input representation is just an integer. Neural networks are good at figuring out and combining linear decision boundaries. In the case of all natural numbers there are an infinite number of decision boundaries to check if a number is even or not. If, however, you were only to get your NN to work on a subset of all numbers then you could make it work. However, you essentially need one neuron per number you want to be able to test in your input layer. So for 0 <= n < 1000 you would need a thousand neurons in your input layer. That's not really a great example of a neural network. If you were to change the representation of your inputs to the binary representation of a number then the NN would have a much easier time of detecting if a number is even or not. eg. As you can see, this is now a rather simple problem to solve: basically the inverse of the last binary digit. This is an example of preprocessing your inputs to create a problem that is easier for the neural net to solve. Here is how I created the model in Keras to classify odd/even numbers in Python 3. It just uses 1 neuron in the first hidden layer with 32 inputs. The output layer has just 2 neurons for one-hot encoding 0 and 1. Here are the predictions: I think it's a good idea for you to read perceptron XOR-problem to understand how a single perceptron works and what is its limitation. Predicting if a number is even is a binary classification problem, with one dimensional input; In classification problem a neural network is trained to separate the classes via a boundary. One way of thinking about this problem is to map its one dimensional input into two dimensional input by adding input number to added dimension (e.g.  map 7 to [7, 7]) and see how even and odd vectors look like in a scatter diagram. If you run the following code in Jupyter notebook You will see something like the following image:  You can see in the first figure it's not really possible to come up with a boundary between even and odd vectors, but If you map the second dimension number to its equivalent negative number then drawing a boundary between two classes (even and odd number vectors) is easy. As a result if you transform your input data to two dimensions and negate the second dimension value based on being even or odd then neural network can learn how to separate even and odd vector classes. You can try something like the following code, and you will see the network will learn and converge to almost 100% accuracy. Note that transforming number into negative space based on being even or odd will work for one dimension as well, but it is easier to demonstrate with a scatter diagram with two dimension vectors. I am not suprised that it doesn't work - neural networks doesn't work like that at all. You have to get better feeling what are you passing as an input to neural network.  When you are passing number it have to have some meaning. That means: if one number is greater than another, it should cause something. Like age -> money, where there should be any dependancy. But when looking for odd number, this meaning is more abstract. Honestly, you should think about your input as an independent string values. Maybe: please try to take as an input: and check whether network will understand that "if second value is greater than zero, number is odd". Keep reading to get better feeeling :) @MIlano
full code will look as This isn't the strangest application of Neural networks I've ever seen. The closest example would be A Compositional Neural-network Solution to Prime-number Testing from back in 2006 that used neural networks to solve a more complex number theory problem. The result of the research was that it could be trained, and I'd suggest you try using a similar construction, but as the paper concludes, there are better solutions to this kind of problem. The goal of Machine Learning is to predict labels (Y for you) on data with features / patterns (X for you here) The issue for you is that your X is only a growing list with no particular pattern, sequence or any explanation. So you're trying to ask a statistical algorithm to explain full random things, which is impossible. Try the very beginning of machine learning with the Titanic Dataset on Kaggle, the reference platform for ML : https://www.kaggle.com/upendr/titanic-machine-learning-from-disaster/data Download it, load it via pandas and try the same algorithm. Your X will be every features like Class, Age, Sex etc.. and your Y is Survived, the value is 1 if he lived, 0 if not. And you will be trying to determine if he lived or not thanks to pattern in the Age, the Sex etc... I can also recommend to look at Andrew Ng : Machine Learning Courses that will explain everything, and really accessible for beginners Have fun ! :)I am relatively new to machine-learning and currently have almost no experiencing in developing it. So my Question is: after training and evaluating the cifar10 dataset from the tensorflow tutorial I was wondering how could one test it with sample images? I could train and evaluate the Imagenet tutorial from the caffe machine-learning framework and it was relatively easy to use the trained model on custom applications using the python API. Any help would be very appreciated! This isn't 100% the answer to the question, but it's a similar way of solving it, based on a MNIST NN training example suggested in the comments to the question. Based on the TensorFlow begginer MNIST tutorial, and thanks to this tutorial, this is a way of training and using your Neural Network with custom data. Please note that similar should be done for tutorials such as the CIFAR10, as @Yaroslav Bulatov mentioned in the comments. For further image conditioning (digits should be completely dark in a white background) and better NN training (accuracy>91%) please check the Advanced MNIST tutorial from TensorFlow or the 2nd tutorial i've mentioned. The below example is not for the mnist tutorial, but a simple XOR example. Note the train() and test() methods. All that we declare & keep globally are the weights, biases, and session. In the test method we redefine the shape of the input and reuse the same weights & biases (and session) that we refined in training.  I recommend taking a look at the basic MNIST tutorial on the TensorFlow website. It looks like you define some function that generates the type of output that you want, and then run your session, passing it this evaluation function (correct_prediction below), and a dictionary containing whatever arguments you require (x and y_ below). If you have defined and trained some network that takes an input x, generates a response y based on your inputs, and you know your expected responses for your testing set y_, you may be able to print out every response to your testing set with something like: This is just a modification of what is done in the tutorial, where instead of trying to print each response, they determine the percent of correct responses. Also note that the tutorial uses one-hot vectors for the prediction y and actual value y_, so in order to return the associated numeral, they have to find which index of these vectors are equal to one with tf.argmax(y, 1). Edit In general, if you define something in your graph, you can output it later when you run your graph. Say you define something that determines the result of the softmax function on your output logits as: then you can output this at run time with:I'm trying to cluster some data I have from the KDD 1999 cup dataset  the output from the file looks like this: with 48 thousand different records in that format. I have cleaned the data up and removed the text keeping only the numbers. The output looks like this now:   I created a comma delimited file in excel and saved as a csv file then created a data source from the csv file in matlab, ive tryed running it through the fcm toolbox in matlab (findcluster outputs 38 data types which is expected with 38 columns).  The clusters however don't look like clusters or its not accepting and working the way I need it to.  Could anyone help finding the clusters? Im new to matlab so don't have any experience and I'm also new to clustering.  The method: This is what I'm trying to achieve:  This is what I'm getting:  Since you are new to machine-learning/data-mining, you shouldn't tackle such advanced problems. After all, the data you are working with was used in a competition (KDD Cup'99), so don't expect it to be easy! Besides the data was intended for a classification task (supervised learning), where the goal is predict the correct class (bad/good connection). You seem to be interested in clustering (unsupervised learning), which is generally more difficult. This sort of dataset requires a lot of preprocessing and clever feature extraction. People usually employ domain knowledge (network intrusion detection) to obtain better features from the raw data.. Directly applying simple algorithms like K-means will generally yield poor results. For starters, you need to normalize the attributes to be of the same scale: when computing the euclidean distance as part of step 3 in your method, the features with values such as 239 and 486 will dominate over the other features with small values as 0.05, thus disrupting the result. Another point to remember is that too many attributes can be a bad thing (curse of dimensionality). Thus you should look into feature selection or dimensionality reduction techniques. Finally, I suggest you familiarize yourself with a simpler dataset...I am tying to plot an ROC curve for Binary classification using RandomForestClassifier  I have two numpy arrays one contains predicted values and one contains true values as follows: How do I port ROC curve and obtain AUC (Area Under Curve) for this binary classification result in ipython? You need probabilities to create ROC curve. Example code from scikit-learn examples:This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed 5 years ago. I want to use nvidia-smi to monitor my GPU for my machine-learning/ AI projects. However, when I run nvidia-smi in my cmd, git bash or powershell, I get the following results: The column GPU Memory Usage shows N/A for every single process. Also, there are a lot more processes listed than I found for most examples on the Internet. What could be the reason for this? I am running a Nvidia GTX 1070 by ASUS, my OS is Windows 10 Pro.     If you perform the following : nvidia-smi -q you will see the following: Not available in WDDM driver model => WDDM stand for Windows Display Driver Model. You can switch to TCC and obtain the information with the command: nvidia-smi -dm 1, however this operation can only performed if you do not have any display attached to the GPU. So... It's mostly impossible...  By the way, don't worry about a high memory usage, Tensorflow reserve as much GPU memory as it can to speed up its processes. If you prefer a finer grained control on the memory taken use the following (it may slow down a little bit your computations): You can create a dual boot on Ubuntu or just forget about this.The question is about vanilla, non-batched reinforcement learning. Basically what is defined here in Sutton's book.
My model trains, (woohoo!) though there is an element that confuses me. Background: In an environment where duration is rewarded (like pole-balancing), we have rewards of (say) 1 per step. After an episode, before sending this array of 1's to the train step, we do the standard discounting and normalization to get returns: The discount_rewards is the usual method, but here is gist if curious.  So an array of rewards [1,1,1,1,1,1,1,1,1] becomes an array of returns [1.539, 1.160, 0.777, 0.392, 0.006, -0.382, -0.773, -1.164, -1.556]. Given that basic background I can ask my question: If positive returns are enforced, and negative returns are discouraged (in the optimize step), then no matter the length of the episode, roughly the first half of the actions will be encouraged, and the latter half will be discouraged. Is that true, or am I misunderstanding something? If its NOT true, would love to understand what I got wrong. If it IS true, then I don't understand why the model trains, since even a good-performing episode will have the latter half of its actions discouraged. To reiterate, this is non-batched learning (so the returns are not relative to returns in another episode in the training step). After each episode, the model trains, and again, it trains well :) Hoping this makes sense, and is short enough to feel like a proper clear question. Background If you increase or decrease all rewards (good and bad) equally, nothing changes really. The optimizer tries to minimize the loss (maximize the reward), that means it's interested only in the delta between values (the gradient), not their absolute value or their sign. Reinforcement Learning Let's say your graph looks something like this: The losses for the individual "classes" get scaled by weights which in this case are q_rewards: The loss is a linear function of the reward, the gradient stays monotonic under linear transformation. Reward Normalization When the agent performs rather badly, it receives much more bad rewards than good rewards. Normalization makes the gradient steeper for (puts more weight on) the good rewards and shallower for (puts less weight on) the bad rewards. When the agent performs rather good, it's the other way 'round. Your questions If positive returns are enforced, and negative returns are discouraged (in the optimize step) ... It's not the sign (absolute value) but the delta (relative values). ... then no matter the length of the episode, roughly the first half of the actions will be encouraged, and the latter half will be discouraged. If there are either much more high or much more low reward values, then you have a smaller half with a steeper gradient (more weight) and a larger half with a shallower gradient (less weight). If it IS true, then I don't understand why the model trains, since even a good-performing episode will have the latter half of its actions discouraged. Your loss value is actually expected to stay about constant at some point. So you have to measure your progress by running the program and looking at the (un-normalized) rewards. For reference, see the example network from Google IO:
github.com/GoogleCloudPlatform/tensorflow-without-a-phd/.../tensorflow-rl-pong/... and search for _rollout_reward This isn't a bad thing, however. It's just that your loss is (more or less) "normalized" as well. But the network keeps improving anyway by looking at the gradient at each training step. Classification problems usually have a "global" loss which keeps falling over time. Some optimizers keep a history of the gradient to adapt the learning rate (effectively scaling the gradient) which means that internally, they also kinda "normalize" the gradient and thus don't care if we do either. If you want to learn more about behind-the-scenes gradient scaling, I suggest taking a look at ruder.io/optimizing-gradient-descent To reiterate, this is non-batched learning (so the returns are not relative to returns in another episode in the training step). After each episode, the model trains, and again, it trains well :) The larger your batch size, the more stable your distribution of rewards, the more reliable the normalization. You could even normalize rewards across multiple episodes. In my opinion, the accepted answer is wrong.
I read it, and I thought it was plausible, and then I stopped worrying about gradient normalization and checked something else. Only much later did I notice that it was precisely the gradient normalization breaking my training process. First off, "Reward Normalization doesn't mess with the sign of the gradient" is just plain wrong. Obviously, if you subtract the mean, that'll flip some signs. So yes, reward normalization does affect the sign of the gradient. Second, tf.losses.softmax_cross_entropy is, in everyday words, a measurement of how many plausible options the AI had when choosing what it did. Select 1 out of 10 actions randomly? Your cross-entropy is very high. Always select the exact same item? Your cross-entropy is low, because the other choices are irrelevant if you statistically never take them. In line with that, what actually does is this: If your reward is positive, it will minimize the cross-entropy, meaning it will increase the chance that the AI will take the exact same action again when it sees a similar situation in the future. If your reward is negative, it will maximize the cross-entropy, meaning it will make the AI choose more randomly when it sees a similar situation in the future. And that's the purpose of reward normalization: Yes, after normalization, half of the items in your trajectory have a positive sign and the other half has a negative sign. What you are basically saying is: Do more of these things that worked, try something random for these things. And that leads to very actionable advice:
If your model is behaving too randomly, make sure you have enough positive rewards (after normalization).
If your model is always doing the same and not exploring, make sure you have enough negative rewards (after normalization).Given X with dimensions (m samples, n sequences, and k features), and y labels with dimensions (m samples, 0/1): Suppose I want to train a stateful LSTM (going by keras definition, where "stateful = True" means that cell states are not reset between sequences per sample -- please correct me if I'm wrong!), are states supposed to be reset on a per epoch basis or per sample basis? Example: In summary, I am not sure if to reset states after each sequence or each epoch (after all m samples are trained in X).  Advice is much appreciated.  If you use stateful=True, you would typically reset the state at the end of each epoch, or every couple of samples. If you want to reset the state after each sample, then this would be equivalent to just using stateful=False.  Regarding the loops you provided: note that the dimension of X are not exactly The dimension is actually Hence, you are not supposed to have the inner loop: Now, regarding the loop since the enumeration over batches is done in keras automatically, you don't have to implement this loop as well (unless you want to reset the states every couple of samples). So if you want to reset only at the end of each epoch, you need only the external loop. Here is an example of such architecture (taken from this blog post): Alternatively it seems possible to a custom callback. This avoids calling fit in a loop which is costly. Something similar to Tensorflow LSTM/GRU reset states once per epoch and not for each new batchI have a csv that is 100,000 rows x 27,000 columns that I am trying to do PCA on to produce a 100,000 rows X 300 columns matrix. The csv is 9GB large. Here is currently what I'm doing: When I run the above code, my program is killed while doing the .from_csv in step. I've been able to get around that by spliting the csv into sets of 10,000; reading them in 1 by 1, and then calling pd.concat. This allows me to get to the normalization step (X - X.mean()).... before getting killed. Is my data just too big for my macbook air? Or is there a better way to do this. I would really love to use all the data I have for my machine learning application.  If i wanted to use incremental PCA as suggested by the answer below, is this how I would do it?: I can't find any good examples online.  Try to divide your data or load it by batches into script, and fit your PCA with Incremetal PCA with it's partial_fit method on every batch. Useful link PCA needs to compute a correlation matrix, which would be 100,000x100,000. If the data is stored in doubles, then that's 80 GB. I would be willing to bet your Macbook does not have 80 GB RAM. The PCA transformation matrix is likely to be nearly the same for a reasonably sized random subset.I would like to study the optimal tradeoff between bias/variance for model tuning. I'm using caret for R which allows me to plot the performance metric (AUC, accuracy...) against the hyperparameters of the model (mtry, lambda, etc.) and automatically chooses the max. This typically returns a good model, but if I want to dig further and choose a different bias/variance tradeoff I need a learning curve, not a performance curve. For the sake of simplicity, let's say my model is a random forest, which has just one hyperparameter 'mtry' I would like to plot the learning curves of both training and test sets. Something like this:  (red curve is the test set) On the y axis I put an error metric (number of misclassified examples or something like that); on the x axis 'mtry' or alternatively the training set size.  Questions: Has caret the functionality to iteratively train models based of training set folds different in size? If I have to code by hand, how can I do that? If I want to put the hyperparameter on the x axis, I need all the models trained by caret::train, not just the final model (the one with maximum performance got after CV). Are these "discarded" model still available after train? Caret will iteratively test lots of cv models for you if you set the
trainControl() function and the parameters (e.g. mtry) using a tuneGrid().
Both of these are then passed as control options to the train()
function. The specifics of the tuneGrid parameters (e.g. mtry, ntree) will be different for each
model type. Yes the final trainFit model will contain the error rate (however you specified it) for all folds of your CV. So you could specify e.g. a 10-fold CV times a grid with 10 values of mtry -which would be 100 iterations. You might want to go get a cup of tea or possibly lunch.  If this sounds complicated ... there is a very good example here - caret being one of the best documented packages about. Here's my code on how I approached this issue of plotting a learning curve in R while using the Caret package to train your model. I use the Motor Trend Car Road Tests in R for illustrative purposes. To begin, I randomize and split the mtcars dataset into training and test sets. 21 records for training and 13 records for the test set. The response feature is mpg in this example. The output plot is as shown below:
 At some point, probably after this question was asked, the caret package added the learning_curve_dat function which helps assess model performance across a range of training set sizes. Here is the example from the function documentation: The performance metric(s) are found for each Training_Size and saved in lda_data along with the Data variable ("Resampling", "Training", and optionally "Testing"). Here is a link to the function documentation: https://rdrr.io/cran/caret/man/learning_curve_dat.html To be clear, this answers the first part of the question but not the second part. NOTE Before at least August 2020 there was a typo in the caret package code and documentation.  The function call was learing_curve_dat before it was corrected to learning_curve_dat.  I've updated my answer to reflect this change.  Make sure you are using a recent version of the caret package.I'm using python3.6 theano, 
with mingw-w64-x86-64 installed, my os is Win10_64, cuda installed, 
and seems everything is ok  the theano.test() is ok, saying my gpu is working, but it just keeps tell me that "error: '::hypot' has not been declared" Any help would be appreciated! I had this error with building an python file using mingw32 .
I opened the file that it says (C:/mingw64/lib/gcc/x86_64-w64-mingw32/6.3.0/include/c++/cmath:1157:11)
and changed that line to   or adding this line just before that :   and after that the problem was solved !!
I know it's not a basic solution but it is the one that I could find !! (This answer was posted in comments originally) I had to keep the original mingw cmath header (otherwise libpng would not build) and I commented out the #define hypot _hypot in pyconfig.h (line 241). What worked for me was to use the combination of the answers above: My guess from your incomplete information is that you aren't compiling in C++11 mode so you aren't picking up ::hypot from C99.How can I add a new Instance to an existing Instances object that I created ? Here is an example: I want to add a new instance to dataRaw. As far as I know I have to use dataRaw.add(Instance i) .... How can I create an instance object if the Instance class is an interface ? Thanks in Advance Let start with some highlights.  instanceValue1[0] = dataRaw.attribute(0).addStringValue("This is a string!"); dataRaw.add(new DenseInstance(1.0, instanceValue1)); Here is the complete running example:  Its output is following:Currently, I am using Trigram to do this.
It assigns the probability of occurrence for a given sentence.
But Its limited to the only context of 2 words.
But LSTM's can do more.
So how to build an LSTM Model that assigns the probability of occurrence for a given sentence? I have just coded a very simple example showing how one might compute the probability of occurrence of a sentence with a LSTM model. The full code can be found here. Suppose we want to predict the probability of occurrence of a sentence for the following dataset (this rhyme was published in Mother Goose's Melody in London around 1765): First of all, let's use keras.preprocessing.text.Tokenizer to create a vocabulary and tokenize the sentences: Our model will take a sequence of words as input (context), and will output the conditional probability distribution of each word in the vocabulary given the context. To this end, we prepare the training data by padding the sequences and sliding windows over them: I decided to slide windows separately for each verse, but this could be done differently. Next, we define and train a simple LSTM model with Keras. The model consists of an embedding layer, a LSTM layer, and a dense layer with a softmax activation (which uses the output at the last timestep of the LSTM to produce the probability of each word in the vocabulary given the context): The joint probability P(w_1, ..., w_n) of occurrence of a sentence w_1 ... w_n can be computed using the rule of conditional probability: P(w_1, ..., w_n)=P(w_1)*P(w_2|w_1)*...*P(w_n|w_{n-1}, ..., w_1) where each of these conditional probabilities is given by the LSTM model. Note that they might be very small, so it is sensible to work in log space in order to avoid numerical instability issues. Putting it all together: NOTE: This is a very small toy dataset and we might be overfitting. UPDATE 29/10/2022: For bigger datasets, it is likely that you'll run out of memory if you process the entire dataset at once. In this case, I recommend using a generator to train your model. Please see this gist for a modified version that uses a data generator.I'm training a textual sentiment classification model with multiple output layers in Keras's Functional API(using a TensorFlow backend). The model takes as input a Numpy array of hashed values produced by the Keras Preprocessing API's hashing_trick() function, and uses a list of Numpy arrays of binary one-hot labels as its targets, as per Keras specifications for training a model with  multiple outputs(see fit()'s documentation here: https://keras.io/models/model/).  Here's the model, sans most of the preprocessing steps: Here's the gist of the error trace training this model produces:  ValueError: Error when checking target: expected dense_3 to have shape (2,) but got array with shape (1,) Your numpy arrays (both for inputs and outputs) should contain a batch dimension. If your labels are currently of shape (2,), you can reshape them to include a batch dimension as follows: I used model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) insted of model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) I changed the loss and it worked for me.I am using Python with Keras and running ImageDataGenerator and using flow_from_directory. I have some problematic image files, so can I use the data generator in order to handle the read errors?  I am getting some "not valid jpg file" on a small portion of the images and would like to treat this without my code crashing. Well, one solution is to modify the ImageDataGenerator code and put error handling mechanism (i.e. try/except) in it. However, one alternative is to wrap your generator inside another generator and use try/except there. The disadvantage of this solution is that it throws away the whole generated batch even if one single image is corrupted in that batch (this may mean that it is possible that some of the samples may not be used for training at all): Another disadvantage of this solution is that since you need to specify the number of steps of generator (i.e. steps_per_epoch) and considering that a batch may be thrown away in a step and a new batch is fetched instead in the same step, you may end up training on some of the samples more than once in an epoch. This may or may not have significant effects depending on how many batches include corrupted images (i.e. if there are a few, then there is nothing to be worried about that much). Finally, note that you may want to use the newer Keras data-generator i.e. Sequence class to read images one by one in the __getitem__ method in each batch and discard corrupted ones. However, the problem of the previous approach, i.e. training on some of the images more than once, is still present in this approach as well since you also need to implement the __len__ method and it is essentially equivalent to steps_per_epoch argument. Although, in my opinion, this approach (i.e. subclassing Sequence class) is superior to the above approach (of course, if you put aside the fact that you may need to write more code) and have fewer side effects (since you can discard a single image and not the whole batch).By names I'm referring to: I have several placeholders which I want to access from outside functions without passing the reference, with the assumption that placeholders holding the given names exist how can you get a reference to them? (this is all during graph construction, not runtime) And my second question is how can I get all variables that hold a given name no matter the scope? Example: All my weights have the name "W" under many scopes, I want to get them all into a list. I do not want to add each one manually. The same can be done with the biases, lets say I want to do a histogram. First of all, you can get the placeholder using tf.Graph.get_tensor_by_name(). For example, assuming that you are working with the default graph: Secondly, I would use the following function to get all variables with a given name (no matter their scope):I am using the roc_auc_score function from scikit-learn to evaluate my model performances.
Howver, I get differents values whether I use predict() or predict_proba() Could advise on that please ? Thanks in advance First look at the difference between predict and predict_proba.  The former predicts the class for the feature set where as the latter predicts the probabilities of various classes. You are seeing the effect of rounding error that is implicit in the binary format of y_test_predicted. y_test_predicted is comprised of 1's and 0's where as p_pred is comprised of floating point values between 0 and 1.  The roc_auc_score routine varies the threshold value and generates the true positive rate and false positive rate, so the score looks quite different. Consider the case where: Note that the ROC curve is generated by considering all cutoff thresholds.  Now consider a threshold of 0.65...  The p_pred case gives: and the y_test_predicted case gives: You can probably see that if these two points are different, then the area under the two curves will be quite different too.   But to really understand it, I suggest looking at the ROC curves themselves to help understand this difference. Hope this helps!I need to have a MAPE function, however I was not able to find it in standard packages ... Below, my implementation of this function. I don't like it, it's super not optimal in terms of speed. How to rewrite the code to be more Pythonic way and boost the speed? Here's one vectorized approach with masking - Probably a faster one with masking after division computation - Runtime test - Another similar way of doing it using masked_Arrays to mask division by zero is:I am studying FLANN, a library for approximate nearest neighbors search. For the LSH method they represent an object (point in search space), as
an array of unsigned int. I am not sure why they do this, and not 
represent a point simply as a double array (which would represent a point
in multi-dimensional vector space). Maybe because LSH is used for binary
features? Can someone share more about the possible use of unsigned int in
this case? Why unsigned int if you only need a 0 and 1 for each feature? Thanks Please note that I will refer to the latest FLANN release, i.e. flann-1.8.3 at the time of writing. For the LSH method they represent an object (point in search space),
  as an array of unsigned int No: this is wrong. The LshIndex class includes a buildIndexImpl method that implements the LSH indexing. Since the LSH is basically a collection of hash tables, the effective indexing occurs on the LshTable class. The elementary indexing method, i.e. the method that indexes one feature vector (aka descriptor, or point) at a time is: Note: the buildIndexImpl method uses the alternative version that simply iterates over the features, and call the above method on each. As you can see this method has 2 arguments which is a pair (ID, descriptor): If you look at the implementation you can see that the first step consists in hashing the descriptor value to obtain the related bucket key (= the identifier of the slot pointing to the bucket in which this descriptor ID will be stored): In practice the getKey hashing function is only implemented for binary descriptors, i.e. descriptors that can be represented as an array of unsigned char: Maybe because LSH is used for binary features? Yes: as stated above, the FLANN LSH implementation works in the Hamming space for binary descriptors. If you were to use descriptors with real values (in R**d) you should refer to the original paper that includes details about how to convert the feature vectors into binary strings so as to use the Hamming space and hash functions. Can someone share more about the possible use of unsigned int in this
  case? Why unsigned int if you only need a 0 and 1 for each feature? See above: the unsigned int value is only used to store the related ID of each feature vector.I have created a classifier in WEKA, i saved it on my hard-disk, now I want to use that classifier in eclipse using weka api. How can i do this? please guide me to this... thank you Here is an example of loading a model to predict the value of instances.  The example model is a J48 decision tree created and saved in the Weka Explorer.  It was built from the nominal weather data provided with Weka.  It is called "tree.model". The output from this is:   A great beginers resource for the Weka api and Serialization is here!I'm using keras' pre-trained model VGG16, following this link: Keras VGG16
I'm trying to decode the prediction output into word of what's in the image: The full error is: ValueError: decode_predictions expects a batch of predictions (i.e.
  a 2D array of shape (samples, 1000)). Found array with shape: (1, 7,
  7, 512) Any comments or suggestion is highly appreciated. Thank you. You should change a first line to: Without this line your model is producing a 512 feature maps with size of 7 x 7 pixels. This the reason behind your error. Just to add on the correct answer by @Marcin Możejko The same applies to the other available models, so you must always include the top three layers:I am studying FLANN, a library for approximate nearest neighbors search. For the LSH method they represent an object (point in search space), as
an array of unsigned int. I am not sure why they do this, and not 
represent a point simply as a double array (which would represent a point
in multi-dimensional vector space). Maybe because LSH is used for binary
features? Can someone share more about the possible use of unsigned int in
this case? Why unsigned int if you only need a 0 and 1 for each feature? Thanks Please note that I will refer to the latest FLANN release, i.e. flann-1.8.3 at the time of writing. For the LSH method they represent an object (point in search space),
  as an array of unsigned int No: this is wrong. The LshIndex class includes a buildIndexImpl method that implements the LSH indexing. Since the LSH is basically a collection of hash tables, the effective indexing occurs on the LshTable class. The elementary indexing method, i.e. the method that indexes one feature vector (aka descriptor, or point) at a time is: Note: the buildIndexImpl method uses the alternative version that simply iterates over the features, and call the above method on each. As you can see this method has 2 arguments which is a pair (ID, descriptor): If you look at the implementation you can see that the first step consists in hashing the descriptor value to obtain the related bucket key (= the identifier of the slot pointing to the bucket in which this descriptor ID will be stored): In practice the getKey hashing function is only implemented for binary descriptors, i.e. descriptors that can be represented as an array of unsigned char: Maybe because LSH is used for binary features? Yes: as stated above, the FLANN LSH implementation works in the Hamming space for binary descriptors. If you were to use descriptors with real values (in R**d) you should refer to the original paper that includes details about how to convert the feature vectors into binary strings so as to use the Hamming space and hash functions. Can someone share more about the possible use of unsigned int in this
  case? Why unsigned int if you only need a 0 and 1 for each feature? See above: the unsigned int value is only used to store the related ID of each feature vector.I am using a lstm on time series data. I have features about the time series that are not time dependent. Imagine company stocks for the series and stuff like company location in the non-time series features. This is not the usecase, but it is the same idea. For this example, let's just predict the next value in the time series. So a simple example would be: however, I am just not sure on how to specify the initial state on the list correctly. I get  which I can see is caused by the 3d batch dimension. I tried using Flatten, Permutation, and Resize layers but I don't believe that is correct. What am I missing and how can I connect these layers? The first problem is that an LSTM(8) layer expects two initial states h_0 and c_0, each of dimension (None, 8). That's what it means by "cell.state_size is (8, 8)" in the error message. If you only have one initial state dense_2, maybe you can switch to GRU (which requires only h_0). Or, you can transform your feature_input into two initial states. The  second problem is that h_0 and c_0 are of shape (batch_size, 8), but your dense_2 is of shape (batch_size, timesteps, 8). You need to deal with the time dimension before using dense_2 as initial states. So maybe you can change your input shape into (data.training_features.shape[1],) or take average over timesteps with GlobalAveragePooling1D. A working example would be:In macOS Sierra, installation for xgboost with openmp enabled always fails. From https://xgboost.readthedocs.io/en/latest/build.html, I've tried: cp make/config.mk ./config.mk; make -j4 With, It fails with, llvm support openmp, but it fails with ld: library not found for -lomp related question
Install xgboost on Mac - ld: library not found But, clang-omp goes to boneyard, and discontinue with llvm's OpenMP support. I've drilled down to, (with -v option) SOLVED llvm installation miss its symbolic link. In my case I solved adding the following linker flag in addition to -lomp: -Lpath_to_libomp.dylib_folder or something like: -Llib/ In my case I used a version of clang that came with Homebrew's llvm instead of the one that came with the Laptop from the factory, and it worked.I'm trying to build a very simple LSTM autoencoder with PyTorch. I always train it with the same data: I have built my model following this link: My code is running with no errors but y_pred converge to: Here is my code:
 In your source code you are using init_hidden_encoder and init_hidden_decoder functions to zero hidden states of both recurrent units in every forward pass. In PyTorch you don't have to do that, if no initial hidden state is passed to RNN-cell (be it LSTM, GRU or RNN from the ones currently available by default in PyTorch), it is implicitly fed with zeroes. So, to obtain the same code as your initial solution (which simplifies next parts), I will scrap unneeded parts, which leaves us with the model seen below: We don't need any superfluous dimensions (like the 1 in [5,1,1]).
Actually, it's the clue to your results equal to 0.2 Furthermore, I left input reshape out of the network (in my opinion, network should be fed with input ready to be processed), to separate strictly both tasks (input preparation and model itself).  This approach gives us the following setup code and training loop: Whole network is identical to yours (for now), except it is more succinct and readable. As your provided Keras code indicates, what we want to do (and actually you are doing it correctly) is to obtain last hiddden state from the encoder (it encodes our entire sequence) and decode the sequence from this state to obtain the original one.  BTW. this approach is called sequence to sequence or seq2seq for short (often used in tasks like language translation). Well, maybe a variation of that approach, but I would classify it as that anyway. PyTorch provides us the last hidden state as a separate return variable from RNNs family.
I would advise against yours encoded[-1]. The reason for it would be bidirectional and multilayered approach. Say, you wanted to sum bidirectional output, it would mean a code along those lines  And that's why the line _, (last_hidden, _) = self.encoder(input) was used. Actually, it was a mistake on your side and only in the last part. Output shapes of your predictions and targets: If those shapes are provided, MSELoss, by default, uses argument size_average=True. And yes, it averages your targets and your output, which essentially calculates loss for the average of your tensor (around 2.5 at the beginning) and average of your target which is 0.2. So the network converges correctly, but your targets are wrong. Provide MSELoss with argument reduction="sum", though it's really temporary and works accidentally. 
Network, at first, will try to get all of the outputs to be equal to sum (0 + 0.1 + 0.2 + 0.3 + 0.4 = 1.0), at first with semi-random outputs, after a while it will converge to what you want, but not for the reasons you want!.  Identity function is the easiest choice here, even for summation (as your input data is really simple). Just pass appropriate shapes to loss function, e.g. batch x outputs, in your case, the final part would look like this: Your target is one dimensional (as batch is of size 1) and so is your output (after squeezing unnecessary dimensions). I changed Adam's parameters to defaults as it converges faster that way. For brevity, here is the code and results: And here are the results after ~60k steps (it is stuck after ~20k steps actually, you may want to improve your optimization and play around with hidden size for better results): Additionally, L1Loss (a.k.a Mean Absolute Error) may get better results in this case: Tuning and correct batching of this network is left for you, hope you'll have some fun now and you get the idea. :) PS. I repeat entire shape of input sequence, as it's more general approach and should work with batches and more dimensions out of the box.In ImageDataGenerator of Keras the flow method has argument x which takes data with rank 4. Why? ValueError: ('Input data in NumpyArrayIterator should have rank 4. You passed an array with shape', (3, 150, 150)) For the reference, my code is as per follow: The forth dimension is the number of samples in a batch.
Look at https://keras.io/preprocessing/image/ at the data_format explanationI am using plot_confusion_matrix from sklearn.metrics. I want to represent those confusion matrices next to each other like subplots, how could I do this? Let's use the good'ol iris dataset to reproduce this, and fit several classifiers to plot their respective confusion matrices with plot_confusion_matrix: Set up - So the way you could compare all matrices at simple sight, is by creating a set of subplots with plt.subplots. Then iterate both over the axes objects and the trained classifiers (plot_confusion_matrix expects the as input) and plot the individual confusion matrices:  if your desired output is that This is my way to see multiple confusion matrices (confusion_matrix) side by side  with ConfusionMatrixDisplay. note: paste your own test and train data names in "metrics.confusion_matrix()" function.I have thought that adding a new module which will do the center pooling.  I was looking in the tensorflow code and there is a file named gen_nn_ops.py which internally calls function from another file by passing "Maxpool", "AvgPool", etc. parameters to do the required computation. I want to do centre pooling which selects the center element in the window. I have code ready for the matlab and c++ versions but need to know how to add a new module in TensorFlow for such computation. Also where to set the backpropogation code.  A custom pooling layer would probably be implemented in C++. To see what you'd need to do, let's see where the implementation of tf.nn.max_pool_with_argmax() lives: The Python wrapper function (tf.nn.max_pool_with_argmax() itself) is automatically generated, in gen_nn_ops.py. This is ultimately imported into nn.py, so that it appears under tf.nn when you import tensorflow as tf. In C++, there is an op registration in ops/nn_ops.cc, and a kernel registration in kernels/maxpooling_op.cc. The kernel itself is defined in kernels/maxpooling_op.cc. The gradient is defined as a separate op—"MaxPoolWithArgmaxGrad"—in the same places. There's a fair amount of work to do to add a new op (see this tutorial for a more complete guide), but hopefully these pointers can help!I've tried to build a sequence to sequence model to predict a sensor signal over time based on its first few inputs (see figure below)
 The model works OK, but I want to 'spice things up' and try to add an attention layer between the two LSTM layers. Model code: I've looked at the documentation but I'm a bit lost. Any help adding the attention layer or comments on the current model would be appreciated  Update:
After Googeling around, I'm starting to think I got it all wrong and I rewrote my code. I'm trying to migrate a seq2seq model that I've found in this GitHub repository. In the repository code the problem demonstrated is predicting a randomly generated sine wave baed on some early samples.   I have a similar problem, and I'm trying to change the code to fit my needs.   Differences: Hyper Params: Encoder code: Decoder code: Model Summary: When trying to fit the model: I get the following error: What am I doing wrong? the attention layer in Keras is not a trainable layer (unless we use the scale parameter). it only computes matrix operation. In my opinion, this layer can result in some mistakes if applied directly on time series, but let proceed with order... the most natural choice to replicate the attention mechanism on our time-series problem is to adopt the solution presented here and explained again here. It's the classical application of attention in enc-dec structure in NLP following TF implementation, for our attention layer, we need query, value, key tensor in 3d format. we obtain these values directly from our recurrent layer. more specifically we utilize the sequence output and the hidden state. these are all we need to build an attention mechanism. query is the output sequence [batch_dim, time_step, features] value is the hidden state [batch_dim, features] where we add a temporal dimension for matrix operation [batch_dim, 1, features] as the key, we utilize as before the hidden state so key = value In the above definition and implementation I found 2 problems: the example: so for this reason, for time series attention I propose this solution query is the output sequence [batch_dim, time_step, features] value is the hidden state [batch_dim, features] where we add a temporal dimension for matrix operation [batch_dim, 1, features] the weights are calculated with softmax(scale*dot(sequence, hidden)). the scale parameter is a scalar value that can be used to scale the weights before applying the softmax operation. the softmax is calculated correctly on the time dimension. the attention output is the weighted product of input sequence and scores. I use the scalar parameter as a fixed value, but it can be tuned or insert as a learnable weight in a custom layer (as scale parameter in Keras attention). In term of network implementation these are the two possibilities available: CONCLUSION I don't know how much added-value an introduction of an attention layer in simple problems can have. If you have short sequences, I suggest you leave all as is. What I reported here is an answer where I express my considerations, I'll accept comment or consideration about possible mistakes or misunderstandings In your model, these solutions can be embedded in this way THIS IS THE ANSWER TO THE EDITED QUESTION first of all, when you call fit, decoder_inputs is a tensor and you can't use it to fit your model. the author of the code you cited, use an array of zeros and so you have to do the same (I do it in the dummy example below) secondly, look at your output layer in the model summary... it is 3D so you have to manage your target as 3D array thirdly, the decoder input must be 1 feature dimension and not 20 as you reported set initial parameters define encoder define decoder (1 feature dimension input!) define model this is my dummy data. the same as yours in shapes. pay attention to decoder_zero_inputs it has the same dimension of your y but is an array of zeros fitting prediction on validationI try to retrieve for each row containing NaN values all the indices of the corresponding columns. I've already done the following : What I want (ideally the name of the column) is get a list like this : Hope I can find a way without doing for each row the test for each column I'm looking for a pandas way to be able to deal with my huge dataset. Thanks in advance. It should be efficient to use a scipy coordinate-format sparse matrix to retrieve the coordinates of the null values: Note that I'm calling the nonzero method in order to just output the coordinates of the nonzero entries in the underlying sparse matrix since I don't care about the actual values which are all True. Another way, extract the rows which are NaN: This gets you most of the way and may be enough.
Although it may be easier to work with the Series: e.g. if you wanted the lists (though I don't think you would need them) another simpler way is: to subset: to get integer index: You can iterate through each row in the dataframe, create a mask of null values, and output their index (i.e. the columns in the dataframe). Try to use : it returns a series of boolean values indicates the columns have NaN values. The index is the column names. Then you retrieve the NaN column(s) by usingI am trying to predict sales demand using recurrent neural networks. Here https://stackoverflow.com/a/2525149/423805 it was mentioned sequences are supported in PyBrain with example code. Even though are data is not exactly categories, I modeled them as such for this example. Data is here Each row is a seperate product, and columns are demand for those products in time. I used this code The error hovers around 245.xx, there is improvement with numbers after the decimal poiint, but the integer part of the errordoes not go any lower. Does it look like the method is working? I just wanted to check with a PyBrain / NN expert to see I am not doing anything wrong.  Correction: Apparently while copying from a PDF file, the data got corrupted. The correct data is shared above. I repeat, the data was bad. With correct data, NN code (also shared) will start from error rate 5.9807501187, and gradually go down. I am very sorry for the confusion I might have caused. Try plotting train error on each iteration. If method works then it should go down on each step. Also have you tried adding bias? Where do you get the error from? Is it the one reported by the trainer? Then it is an error on training set and you suffer from high bias. Things that could help:I am trying out this code The output for this one is I gave the first set of data using fit_transform to vectorizer so it gave me feature names like  [u'education', u'football', u'gravity', u'imporatant', u'movie', u'sport'] after that i applied another train set to the same vectorizer but it gave me the same feature names as I didnt use fit or fit_transform. But I want to know how to update the features of a vectorizer without overwriting the previous oncs. If I use fit_transform again the previous features will get overwritten. So I want to update the feature list of the vectorizer. So i want something like [u'education', u'football', u'gravity', u'imporatant', u'movie', u'sport',u'aims', u'college', u'cricket', u'film', u'transformers'] How can I get  that.  In sklearn terminology, this is called a partial fit and you can't do it with a TfidfVectorizer. There are two ways around this: Example of the first approach: Output: I found this question while googling for the same issue that OP raised. Like mbatchkarov said Scikit-Learn's TfidfVectorizer doesn't natively support partial fitting.  HashingVectorizer is usually a great alternative, but it really depends on your use-case. Specifically, if you care very much about representing infrequent terms precisely, then collisions will hurt performance. So I went ahead and wrote my own implementation of "partial_fit" for both TfidfVectorizer and CountVectorizer (see here). Hope it's useful for other people reaching this post. Note that this kind of partial fitting does change the dimension of the output vector given by the vectorizer since the whole point is to update the vocabulary (so take this into account when using in a pipeline).We have a list of x,y pairs. Every pair represents a point on a 2D space. I want to find the closest point from this list, to a specific point xq,yq. What is the best performance-critical algorithm for this problem? Lisp of points is not going to change; which means I do not need to perform insertion and deletion. I want just to find the nearest neighbor of a target xq,yq point in this set. Edit 1: Thanks to all! As Stephan202 has guessed correctly, I want to do this repeatedly; like a function. An the list is not necessarily sorted (In fact I do not understand how can it be sorted? Like a table with a primary key of 2 columns a and y? If that helps then I will sort it). I will construct the data structure based on the list one time, and then I will use this generated data structure in the function (if this process itself is relevant). Thank you Jacob; It seems that KD-Tree data structure is a good candidate for being the answer (And I feel it is. I will update when I get some relevant results). Edit 2: I have found that, this problem is named "nearest neighbor"! Edit 3: First title was "In Search of an Algorithm (for Spatial-Querying and Spatial-Indexing) (Nearest Neighbor)"; I have chose a new title: "Best Performance-Critical Algorithm for Solving Nearest Neighbor". Since I do not want to perform insertion and deletion operation on my initial data and I want just the nearest one from them to a new point (which is not going to be inserted), I chose to (currently) working on KD-Trees. Thanks to all! As Stephan202 noted, if you plan to find the closest-match for more than one point, you should use a tree. I would recommend a KD-tree, whose implementation can be easily found in several packages like OpenCV 2.0. Or you could implement one yourself! EDIT: I had asked a question about kd-tree implementations here - might be useful. EDIT: KD-trees have been widely used successfully for NN searches :) - Also, if you're willing to accept approximate matches, you can use Fast Library for Approximate Nearest Neigbor (FLANN). The FLANN implementation is present in OpenCV 2.0. If you don't want approximate answers you can tweak the FLANN parameters to search the entire tree. If the query point (xq, yq) varies and the list doesn't, you need to calculate the Voronoi diagram of the list of points.  This will give you a set of polygons or "cells" (some of which are infinite); each polygon corresponds to a point from the original list, called the "site" of that cell.  Any point which lies entirely inside of one polygon is closer to the site of that polygon than it is to the other sites on the original list.  Any point on a boundary between two polygons lies equally distant from each site. Once you've gotten that far, you need an easy way to figure out which polygon you're in.  This is known as the point location problem. A really, really good book for this kind of thing is Computational Geometry: Algorithms and Applications.  They discuss both the Voronoi diagram calculation and the trapezoidal slab method of point location in detail. If you don't want to do the code yourself, and you shouldn't, then try to get a library like CGAL that will do most of the work for you.  This probably applies to the KD-tree answer as well, but I don't specifically know. You need a spatial index. If you roll your own, you can do a lot worse than picking the R-Tree or Quad-tree algorithms. I would go with a quadtree. It is the most simple spatial structure. In 2 dimensions i would generally recommend quadtree instead of kd-tree, because it is simpler, faster. Its downside is more memory consumption if the number of dimensions is high, but in case of 2 dimensions the difference is not significant.  There is a nice optimization trick if your coordinates are floating point typed:
In a query you first will have to find the leaf-node that contains the point to which the most near point is asked. To do this you will have to go in the tree from the root to the leaf - in each iteration deciding which child-node to step on.
Store the identifiers/addresses of the child-nodes in a 4-sized array in the Node structure. Digitize the point coordinates in the query algorithm. Then you will be able to find the proper sub-node just by indexing the array by 2 proper bits of the digitized point coordinates. Digitizing is fast: implement it with a simple static_cast.  But first implement the quadtree without optimization because it is easy to make a bug with the bit-operations. Even without this optimization, it still will be the fastest solution. Iterate through every other point using the distance formula to find the minimum distance from Q (xq,yq). However, you haven't given enough information for a performance-critical answer. For example, if Q is a VERY common point, you might want to calculate the distance to Q and store it with each point. Second example, if you have a huge number of points, you might organize the points into sections and start with points only in the same section and adjacent sections to the section containing Q.I am trying to cluster some products based on the users' behaviors. What I reach at the end are clusters that have a very different number of observations. I have checked k-means clustering parameters and was not able to find a parameter that controls the minimum (or maximum) number of observations per cluster. For example here is how the number of observations is distributed across different clusters. How to deal with this issue? For those who still looking for an answer. I found a good module or this module that deal with this kind of problem Use pip install size-constrained-clustering or pip install git+https://github.com/jingw2/size_constrained_clustering.git and use MinMaxKMeansMinCostFlow where you can select the size_min and size_max This will solve by k-means-constrained pip library.. check here Example:I am starting with scikit-learn and I am trying to transform a set of documents into a format on which I could apply clustering and classification. I have seen the details about the vectorization methods, and the tfidf transformations to load the files and index their vocabularies. However, I have extra metadata for each documents, such as the authors, the division that was responsible, list of topics, etc. How can I add features to each document vector generated by the vectorizing function? You could use the DictVectorizer for the extra categorical data and then use scipy.sparse.hstack to combine them.I am working on a DecisionTreeClassifier model and I want to understand the path chosen by the model. So I need to know what values give the  Well, you are correct in that the documentation is actually obscure about this (but to be honest, I am not sure about its usefulness, too). Let's replicate the example from the documentation with the iris data: Asking for clf.tree_.value, we get: and To realize what exactly this array represents it is useful to look at the tree visualization (also available in the docs, reproduced here for convenience):  As we can see, the tree has 17 nodes; looking closer, we see that the value of each node is actually an element of our clf.tree_.value array. So, to make a long story short: To clarify on the last point with an example, consider the second element of the array, [[ 50.,   0.,   0.]] (which corresponds to the orange-colored node): it says that, in this node, end up 50 samples from the class #0, and zero samples from the other two classes (#1 and #2).I'm trying to replicate the results of Fully Convolutional Network (FCN) for Semantic Segmentation using TensorFlow.  I'm stuck on feeding training images into the computation graph. The fully convolutional network used VOC PASCAL dataset for training. However, the training images in the dataset are of varied sizes.  I just want to ask if they preprocessed the training images to make them have the same size and how they preprocessed the images. If not, did they just feed batches of images of different sizes into the FCN? Is it possible to feed images of different sizes in one batch into a computation graph in TensorFlow? Is it possible to do that using queue input rather than placeholder? It's not possible to feed images of different size into a single input batch. Every batch can have an undefined number of samples (that's the batch size usually, below noted with None) but every sample must have the same dimensions. When you train a fully convolutional network you have to train it like a network with fully connected layers at the end.
So, every input image in the input batch must have the same widht, height and depth. Resize them. The only difference is that while fully connected layers output a single output vector for every sample in the input batch (shape [None, num_classes]) the fully convolutional outputs a probability map of classes. During train, when the input images dimensions are equals to the network input dimensions, the output will be a probability map with shape [None, 1, 1, num_classes]. You can remove the dimensions of size 1 from the output tensor using tf.squeeze and then calculate the loss and accuracy just like you do with a fully connected network. At test time, when you feed the network images with dimensions greater than the input, the output will be a probability map with size [None, n, n, num_classes].I have a very simple machine learning code here: My X values are 59 features with the 60th column being my Y value, a simple 1 or 0 classification label. Considering that I am using financial data, I would like to lookback the past 20 X values in order to predict the Y value. So how could I make my algorithm use the past 20 rows as an input for X for each Y value? I'm relatively new to machine learning and spent much time looking online for a solution to my problem yet I could not find anything simple as my case. Any ideas? This is typically done with Recurrent Neural Networks (RNN), that retain some memory of the previous input, when the next input is received. Thats a very breif explanation of what goes on, but there are plenty of sources on the internet to better wrap your understanding of how they work. Lets break this down in a simple example. Lets say you have 5 samples and 5 features of data, and you want two stagger the data by 2 rows instead of 20. Here is your data (assuming 1 stock and the oldest price value is first). And we can think of each row as a day of the week To use an LSTM in keras, your data needs to be 3-D, vs the current 2-D structure it is now, and the notation for each diminsion is (samples,timesteps,features). Currently you only have (samples,features) so you would need to augment the data. Notice how the data is staggered and 3 dimensional. Now just make an LSTM network. Y remains 2-D since this is a many-to-one structure, however you need to clip the first value.  This is just a brief example to get you moving. There are many different setups that will work (including not using RNN), you need to find the correct one for your data. This seems to be a time series type of task.
I would start by looking at Recurrent Neural Networks keras  If you want to keep using the modeling you have. (I would not recommend)
For time series you may want to transform your data set to some kind of weighted average of last 20 observations (rows).
This way, each of your new data set's observations is the function of the previous 20. This way, that information is present for classification. You can use something like this for each column if you want the runing sum: Alternately, you could pivot your current data set so that each row has the actual numbers: Add 19 x dimension count columns. Populate with previous observation's data in those. Whether this is possible or practical depends on the shape of your data set. This is a simple, not too thorough, way to make sure each observation has the data that you think will make a good prediction.
You need to be aware of these things:   I'm sure there are other considerations that make this not optimal, and am suggesting to use dedicated RNN. I am aware that djk already pointed out this is RNN, I'm posting this after that answer was accepted per OP's request.I've been using sklearn's random forest, and I've tried to compare several models. Then I noticed that random-forest is giving different results even with the same seed. I tried it both ways: random.seed(1234) as well as use random forest built-in random_state = 1234
In both cases, I get non-repeatable results. What have I missed...? Any ideas? Thanks!! EDIT:
Adding a more complete version of my code EDIT: It's been quite a while, but I think using RandomState might solve the problem. I didn't test it yet myself, but if you're reading it, it's worth a shot. Also, it is generally preferable to use RandomState instead of random.seed(). First make sure that you have the latest versions of the needed modules(e.g. scipy, numpy etc). When you type random.seed(1234), you use the numpy generator. When you use random_state parameter inside the RandomForestClassifier, there are several options: int, RandomState instance or None. From the docs here : If int, random_state is the seed used by the random number generator;  If RandomState instance, random_state is the random number generator;  If None, the random number generator is the RandomState instance used by np.random. A way to use the same generator in both cases is the following. I use the same (numpy) generator in both cases and I get reproducible results (same results in both cases). Check if the results are the same: Results:I am writing a code for image classification for two classes using keras with tensorflow backend. My images are stored in folder in computer and i want to give these images as input to my keras model. load_img takes only one input image so i have to use either flow(x,y) or flow_from_directory(directory), but in flow(x,y) we need to also provide labels which is length task so i am using flow_from_directory(directory). My images are of variable sizes like 20*40, 55*43..... but here it is mentioned that fixed target_size is required. In this solution it is given that we can give variable size images as input to convolution layer using input_shape=(1, None, None) or input_shape=(None,None,3) (channel last and color images) but  fchollet mention that it is not useful for flatten layer and my model consist both convolution and flatten layers. In that post only moi90 suggest that try different batches but every batch should have images with same size, but it is not possible me to group images with same sizes because my data is very scatter. So i decided to go with batch size=1 and write following code: Now i am getting following error: I am sure it is not because of img_dim_ordering and backend but because of this i have checked both are th Please help to correct he code or help how i can give variable size images as input to my model. You can train variable sizes, as long as you don't try to put variable sizes in a numpy array.  But some layers do not support variable sizes, and Flatten is one of them. It's impossible to train models containing Flatten layers with variable sizes. You can try, though, to replace the Flatten layer with either a GlobalMaxPooling2D or a GlobalAveragePooling2D layer. But these layers may condense too much information into a small data, so it might be necessary to add more convolutions with more channels before them.  You must make sure that your generator will produce batches containing images of the same size, though. The generator will fail when trying to put two or more images with different sizes in the same numpy array.  See the answer in https://github.com/keras-team/keras/issues/1920
Yo you should change the input to be: The in the end add GlobalAveragePooling2D(): Try something like that ... Unfortunately you can't train a neural network with various size images as it is. You have to resize all images to a given size. Fortunately you don't have to do this in your hard drive, permanently by keras does this for you on hte fly.  Inside your flow_from_directory you should define a target_size like this: Also, if you do so, you can have whatever batch size you want.I am really confused about how to calculate Precision and Recall in Supervised machine learning algorithm using NB classifier Say for example
1) I have two classes A,B
2) I have 10000 Documents out of which 2000 goes to training Sample set (class A=1000,class B=1000)
3) Now on basis of above training sample set classify rest 8000 documents using NB classifier
4) Now after classifying 5000 documents goes to class A and 3000 documents goes to class B
5) Now how to calculate Precision and Recall?  Please help me.. Thanks Hi you have to divide results into four groups - 
True class A (TA) - correctly classified into class A
False class A (FA) - incorrectly classified into class A
True class B (TB) - correctly classified into class B
False class B (FB) - incorrectly classified into class B precision = TA / (TA + FA)
recall = TA / (TA + FB) You might also need accuracy and F-measure: accuracy = (TA + TB) / (TA + TB + FA + FB)
f-measure = 2 * ((precision * recall)/(precision + recall)) More here:
http://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29 Let me explain a bit for clarity. Suppose there are 9 dogs and some cats in a video and the image processing algorithm tells you there are 7 dogs in the scene, out of which only 4 are actually dogs (True positives) while the 3 were cats (False positives) Precision tells us out of the items classified as dogs, how many where actually dogs so Precision = True Positives/(True positives + False positives) = 4/(4+3) = 4/7 While recall tells out of the total number of dogs, how many dogs where actually found. so Recall = True Positives/Total Number = True Positive/(True positive + False Negative) = 4/9 You have to find precision and recall for class A and class B For Class A True positive = (Number of class A documents in the 5000 classified class A documents) False positive = (Number of class B documents in the 5000 classified class A documents) From the above you can find Precision. Recall = True positive/(Total Number of class A documents used while testing) Repeat the above for Class B to find its precision and recall.I have a problem with the connection between convolution layer and lstm layer. The data is of shape(75,5) where there is 75 timesteps x 5 data points for each time step. What I want to do is do a convolution on (75x5), get new convolved (75x5) data and feed that data into lstm layer. However, it does not work because the shape of output of convolution layer has number of filters which I do not need. And therefore the shape of convolution layer output is (1,75,5) and input needed for lstm layer is (75,5). How do I just take the first filter. And this is the error that comes up: You can add Reshape() layer in between to make dimensions compatible. http://keras.io/layers/core/#reshape keras.layers.core.Reshape(dims) Reshape an output to a certain shape. Input shape Arbitrary, although all dimensions in the input shaped must be fixed. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape (batch_size,) + dims Arguments dims: target shape. Tuple of integers, does not include the samples dimension (batch size).Getting started with pyspark.ml and the pipelines API, I find myself writing custom transformers for typical preprocessing tasks in order to use them in a pipeline. Examples: They work, but I was wondering if this is a pattern or antipattern - are such transformers a good way to work with the pipeline API? Was it necessary to implement them, or is equivalent functionality provided somewhere else? I'd say it is primarily opinion based, although it looks unnecessarily verbose and Python Transformers don't integrate well with the rest of the Pipeline API. It is also worth pointing out that everything you have here can be easily achieved with SQLTransformer. For example: or With a little bit of effort you can use SQLAlchemy with Hive dialect to avoid handwritten SQL.How to print the decision path of a randomforest rather than the path of individual trees in a randomforest for a specific sample. decision_path for random forest was introduced in v0.18. (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) However, it outputs a sparse matrix which I'm not certain how to make sense of. Can anyone advise on how best to print the decision path of that specific sample and then visualize it? Output: (<1x1432 sparse matrix of type ''
    with 96 stored elements in Compressed Sparse Row format>, array([   0,  133,  >282,  415,  588,  761,  910, 1041, 1182, 1309, 1432], dtype=int32)) I found this code in the scikit-learn documentation and modified it to fit your problem.  As the RandomForestClassifier is a collection of DecisionTreeClassifier we can iterate over the different trees and retrieve the decision path for the sample in each one.
Hope it helps: And for printing the different trees in the random forest you can just iterate over the estimators this way: This is the output for the first trees in the RandomForestClassifier for sample_id = 0:I am just curious to know, how does PyTorch track operations on tensors (after the .requires_grad is set as True and how does it later calculate the gradients automatically. Please help me understand the idea behind autograd. Thanks. That's a great question!
Generally, the idea of automatic differentiation (AutoDiff) is based on the multivariable chain rule, i.e.

. 
What this means is that you can express the derivative of x with respect to z via a "proxy" variable y; in fact, that allows you to break up almost any operation in a bunch of simpler (or atomic) operations that can then be "chained" together.
Now, what AutoDiff packages like Autograd do, is simply to store the derivative of such an atomic operation block, e.g., a division, multiplication, etc.
Then, at runtime, your provided forward pass formula (consisting of multiple of these blocks) can be easily turned into an exact derivative. Likewise, you can also provide derivatives for your own operations, should you think AutoDiff does not exactly do what you want it to. The advantage of AutoDiff over derivative approximations like finite differences is simply that this is an exact solution. If you are further interested in how it works internally, I highly recommend the AutoDidact project, which aims to simplify the internals of an automatic differentiator, since there is usually also a lot of code optimization involved.
Also, this set of slides from a lecture I took was really helpful in understanding.I am trying with a sample dataFrame : Now from here, I used get_dummies to convert string column to an integer: After conversion the columns are:
Age,Name_Alex,Name_Bob,Name_Clarke,Country_India,Country_SriLanka,Country_USA Now, model is trained. For prediction let say i want to predict the "target" by giving "Name" and "Country".
Like : ["Alex","USA"]. If I used this: obviously it will not work. I suggest you to use sklearn label encoders and one hot encoder packages instead of pd.get_dummies. Once you initialise label encoder and one hot encoder per feature then save it somewhere so that when you want to do prediction on the data you can easily import saved label encoders and one hot encoders and encode your features again. This way you are encoding your features again in the same way as you did while making training set. Below is the code which I use for saving encoders: Now I save this onehotencoder_dict and label encoder_dict and use it later for encoding.I am using SVC classifier with Linear kernel to train my model.
Train data: 42000 records It takes more than 2 hours to train my model.
Am I doing something wrong? 
Also, what can be done to improve the time Thanks in advance There are several possibilities to speed up your SVM training. Let n be the number of records, and d the embedding dimensionality. I assume you use scikit-learn. Reducing training set size. Quoting the docs: The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. O(n^2) complexity will most likely dominate other factors. Sampling fewer records for training will thus have the largest impact on time. Besides random sampling, you could also try instance selection methods. For example, principal sample analysis has been proposed recently. Reducing dimensionality. As others have hinted at in their comments, embedding dimension also impacts runtime. Computing inner products for the linear kernel is in O(d). Dimensionality reduction can, therefore, also reduce runtime. In another question, latent semantic indexing was suggested specifically for TF-IDF representations. Different classifier. You may try sklearn.svm.LinearSVC, which is... [s]imilar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. Moreover, a scikit-learn dev suggested the kernel_approximation module in a similar question. I had the same issue, but scaling the data solved the problem You can try using accelerated implementations of algorithms - such as scikit-learn-intelex - https://github.com/intel/scikit-learn-intelex For SVM you for sure would be able to get higher compute efficiency. First install package And then add in your python script Try using the following code. I had similar issue with similar size of the training data.
  I changed it to following and the response was way fasterBefore my Tensorflow neural network starts training, the following warning prints out: WARNING:tensorflow:Layer my_model is casting an input tensor from
dtype float64 to the layer's dtype of float32, which is new behavior
in TensorFlow 2.  The layer has dtype float32 because it's dtype
defaults to floatx. If you intended to run this layer in float32, you
can safely ignore this warning. If in doubt, this warning is likely
only an issue if you are porting a TensorFlow 1.X model to TensorFlow
2. To change all layers to have dtype float64 by default, call tf.keras.backend.set_floatx('float64'). To change just this layer,
pass dtype='float64' to the layer constructor. If you are the author
of this layer, you can disable autocasting by passing autocast=False
to the base Layer constructor. Now, based on the error message, I am able to silence this error message by setting the backend to 'float64'. But, I would like to get to the bottom of this and set the right dtypes manually. Full code: tl;dr to avoid this, cast your input to float32 or with numpy: Explanation By default, Tensorflow uses floatx, which defaults to float32, which is standard for deep learning. You can verify this: The input you provided (the Iris dataset), is of dtype float64, so there is a mismatch between Tensorflow's default dtype for weights, and the input. Tensorflow doesn't like that, because casting (changing the dtype) is costly. Tensorflow will generally throw an error when manipulating tensors of different dtypes (e.g., comparing float32 logits and float64 labels). The "new behavior" it's talking about: Layer my_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2 Is that it will automatically cast the input dtype to float32. Tensorflow 1.X probably threw an exception in this situation, although I can't say I've ever used it.I am trying to evaluate a multiple linear regression model. I have a data set like this :   This data set has 157 rows * 54 columns. I need to predict ground_truth value from articles. I will add my multiple linear model 7 articles between en_Amantadine with en_Common. I have code for multiple linear regression :  My problem is, I cannot extract data from my DataFrame for X and y variables. In my code X should be: I cannot convert DataFrame to this type of structure. 
How can i do this ?  Any help is appreciated. You can turn the dataframe into a matrix using the method as_matrix directly on the dataframe object. You might need to specify the columns which you are interested in X=df[['x1','x2','X3']].as_matrix() where the different x's are the column names. For the y variables you can use y = df['ground_truth'].values to get an array. Here is an example with some randomly generated data: calling as_matrix() on df returns a numpy.ndarray object Calling values returns a numpy.ndarray from a pandas series Notice: You might get a warning saying:FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. To fix it use values instead of as_matrix as shown belowI'd like to normalize my training set before passing it to my NN so instead of doing it manually (subtract mean and divide by std), I tried keras.utils.normalize() and I am amazed about the results I got. Running this: ​
​Results in that: Unfortunately, the docs don't explain what's happening under the hood. Can you please explain what it does and if I should use keras.utils.normalize instead of what I would have done manually? It is not the kind of normalization you expect. Actually, it uses np.linalg.norm() under the hood to normalize the given data using Lp-norms: For example, in the default case, it would normalize the data using L2-normalization (i.e. the sum of squared of elements would be equal to one). You can either use this function, or if you don't want to do mean and std normalization manually, you can use StandardScaler()  from sklearn or even MinMaxScaler().Does anyone know how to update a subset (i.e. only some indices) of the weights that are used in the forward propagation? My guess is that I might be able to do that after applying compute_gradients as follows: ...and then do something with the list of tuples in grads_vars. You could use a combination of gather and scatter_update. Here's an example that doubles the values at position 0 and 2 You should see Easiest way is to pull the tf.Variable into python (as a numpy array) using npvar = sess.run(tfvar), then perform some operation on it such as npvar[1, 2] = -10.  Then you can upload the modified data back into tensorflow using sess.run(tfvar.assign(npvar)). Obviously this is very slow and not really useful for training but it does work.I have constructed a decision tree using rpart for a dataset.  I have then divided the data into 2 parts - a training dataset and a test dataset. A tree has been constructed for the dataset using the training data. I want to calculate the accuracy of the predictions based on the model that was created. My code is shown below: I now want to calculate the accuracy of the predictions generated by the model by comparing the results with the actual values train and test data however I am facing an error while doing so. My code is shown below: I get an error message that states -  Error in t_pred == t : comparison of these types is not implemented In
  addition: Warning message: Incompatible methods ("Ops.factor",
  "Ops.data.frame") for "==" On checking the type of t_pred, I found out that it is of type integer however the documentation (https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/predict.rpart.html) states that the predict() method must return a vector.  I am unable to understand why is the type of the variable is an integer and not a list. Where have I made the mistake and how can I fix it? Try calculating the confusion matrix first: Now you can calculate the accuracy by dividing the sum diagonal of the matrix - which are the correct predictions - by the total sum of the matrix: My response is very similar to @mtoto's one but a bit more simply... I hope it also helps.I had a problem and I found a solution but I feel it's the wrong way to do it.
Maybe, there is a more 'canonical' way to do it. Problem I have two dataframe that I would like to merge without having extra column and without erasing existing infos. Example : Existing dataframe (df) Dataframe to merge (df2) I would like to update df with df2 if columns 'A' and 'A2' corresponds.
The result would be (: Here is my solution, but I think it's not a really good one. Does anyone has a better way to do ?
Thanks ! Yes, it can be done without merge: You can try this:If I use GridSearchCV in scikit-learn library to find the best model, what will be the final model it returns? That said, for each set of hyper-parameters, we train the number of CV (say 3) models. In this way, will the function return the best model in those 3 models for the best setting of parameters?  The GridSearchCV will return an object with quite a lot information. It does return the model that performs the best on the left-out data: best_estimator_ : estimator or dict Estimator that was chosen by the search, i.e. estimator which gave
  highest score (or smallest loss if specified) on the left out data.
  Not available if refit=False. Note that this is not the model that's trained on the entire data. That means, once you are confident that this is the model you want, you will need to retrain the model on the entire data by yourself. Ref: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html This is given in sklearn: “The refitted estimator is made available at the best_estimator_ attribute and permits using predict directly on this GridSearchCV instance.” So, you don’t need to fit the model again. You can directly get the best model from best_estimator_ attributeWe don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed last year. Which is the book one should start with in the domain of spiking neural networks? I know about Gerstner's "Spiking Neuron Models", published in 2002. Is there a more recent book, or maybe a more suitable one? I have a background in maths and artificial neural networks.  If there are some good articles or overviews in this domain, also add them to the list.  Thanks.  LATER EDIT Karel's answer: " It depends what do you mean by spiking neural networks - there are
  at least several basic points of view. Gerstner represents the first
  one - he is focused on modelling of biological neurons. And his book
  from 2002 is really good starting point for understanding bio-physical
  models of neuron. It the past it was possible to find this book also
  in html .. On the other hand by ¨Spiking neuron" in the computer science context
  is usually meant the SRMo model (Spike Response Model), which can be
  used also as an alternative to classical percepron-based networks. This model is described very well in the works of Wolfgang Maass
  (http://www.igi.tugraz.at/maass/). He has focused on the computational
  power of the model and he compares the SRM model with percepron and
  RBF-unit. If you want to use the model in a network I recommend to you works of
  Sander Bohte (http://homepages.cwi.nl/~sbohte/) who derived SpikeProp
  algorithm. (I personally derived a variant of SpikeProp which was fast enough to
  be used for real-word applications.) " Spiking Neural Networks (SNNs) or Pulsed Neural Networks (PNNs) are artificial neural networks (ANNs) that more closely emulate the behaviours of natural neural mechanisms. I would like to advise you following fundamental books: I personally derived a variant of Remote Supervised Method (ReSuMe) which has better learning rate and morphological advantages compared to ReSuMe introduced by Filip Ponulak. In the meantime, I would like to list some of simulator tools dealing with SNNs. Most of them which I played with are based on Python so please take into account that as well. There might be more others based on other languages. Don't forget BindsNet: Best choice. And trending.I'm using sklearns OrthogonalMatchingPursuit to get a sparse coding of a signal using a dictionary learned by a KSVD algorithm. However, during the fit I get the following RuntimeWarning: In those cases the results are indeed not satisfactory. I don't get the point of this warning as it is common in sparse coding to have an overcomplete dictionary an thus also linear dependency within it. That should not be an issue for OMP. In fact, the warning is also raised if the dictionary is a square matrix. Might this Warning also point to other issues in the application? The problem was in the data vector y in  It contained numbers with very small magnitude. When I normalize y as well as D the fit works with the expected accuracy.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I  performed  Logistic regression on a binary classification problem with data of 50000 X 370  dimensions.I got accuracy of about 90%.But when i did PCA + logistic on data, my accuracy reduced to 10%, I was very shocked to see this result. Can anybody explain what could have gone wrong? There is no guarantee that PCA will ever help, or not harm the learning process. In particular - if you use PCA to reduce amount of dimensions - you are removing information from your data, thus everything can happen - if the removed data was redundant, you will probably get better scores, if it was an important part of the problem - you will get worse. Even without dropping dimensions, but just "rotating" input space through PCA can both beneift and harm the process - one must remember that PCA is just a heuristic, when it comes to supervised learning. The only guarantee of PCA is that each consequtive dimension will explain less and less variance, and that it is the best affine transformation in terms of explaining variance in the first K dimensions. That's all. This can be completely unrelated to actual problem, as PCA does not consider labels at all. Given any dataset PCA will transform it in a way which depends only on the positions of points - so for some labelings (consistent with general shape of the data) - it might help, but for many others (more complex patterns of labels) - it will destroy the previously detectable relations. Futhermore, as PCA leads to change of some scalings, you might need different hyperparameters of your classifier - such as regularization strength for LR. Now getting back to your problem - I would say that in your case the problem is ... a bug in your code. you cannot drop in accuracy significantly below 50%. 10% of accuracy means, that using the opposite of your classifier would give 90% (just answering "false" when it says "true" and the other way around). So even though PCA might not help (or might even harm, as described) - in your case it is an error in your code for sure. PCA while reducing the number of features does not care about the class labels. The only thing that it cares about is preserving the maximum variance which may not always be optimal for classification task. L1-Reg on the other hand pushes those features towards zero that do not have much correlation with the class labels. Hence, L1-Reg strives to reduce the number of features while also getting good classification performance. So L1-Regularization should be preferred over PCA for reduced number of features. To avoid under-fitting, we can always do hyper-parameter tuning to find best lambda.I want to do a Kmeans clustering on a dataset (namely, Sample_Data) with three variables (columns) such as below: in a typical way, after scaling the columns, and determining the number of clusters, I will use this function in R: But, what if there is a preference for the variables? I mean that, suppose variable (column) A, is more important than the two other variables?
how can I insert their weights in the model?
Thank you all You have to use a kmeans weighted clustering, like the one presented in flexclust package: https://cran.r-project.org/web/packages/flexclust/flexclust.pdf The function Perform k-means clustering, hard competitive learning or neural gas on a data matrix.
weights An optional vector of weights to be used in the fitting process. Works only in combination with hard competitive learning. A toy example using iris data: As you can see from the output of cclust, also using competitive learning the family is always kmenas. 
The difference is related to cluster assignment during training phase:  If method is "kmeans", the classic kmeans algorithm as given by
  MacQueen (1967) is used, which works by repeatedly moving all cluster
  centers to the mean of their respective Voronoi sets. If "hardcl",
  on-line updates are used (AKA hard competitive learning), which work
  by randomly drawing an observation from x and moving the closest
  center towards that point (e.g., Ripley 1996). The weights parameter is just a sequence of numbers, in general I use number between 0.01 (minimum weight) and 1 (maximum weight). I had the same problem and the answer here is not satisfying for me.  What we both wanted was an observation-weighted k-means clustering in R. A good readable example for our question is this link: https://towardsdatascience.com/clustering-the-us-population-observation-weighted-k-means-f4d58b370002 However the solution to use the flexclust package is not satisfying simply b/c the used algorithm is not the "standard" k-means algorithm but the "hard competitive learning" algorithm. The difference are well described above and in the package description. I looked through many sites and did not find any solution/package in R in order to use to perform a "standard" k-means algorithm with weighted observations. I was also wondering why the flexclust package explicitly do not support weights with the standard k-means algorithm. If anyone has an explanation for this, please feel free to share! So basically you have two options: First, rewrite the flexclust-algorithm to enable weights within the standard approach. Or second, you can estimate weighted cluster centroids as starting centroids and perform a standard k-means algorithm with only one iteration, then compute new weighted cluster centroids and perform a k-means with one iteration and so on until you reach convergence. I used the second alternative b/c it was the easier way for me. I used the data.table package, hope you are familiar with it. If you want to increase the weight of a variable (column), just multiply it with a constant c > 1. It's trivial to show that this increases the weight in the SSQ optimization objective.I have a question regarding random forests. Imagine that I have data on users interacting with items. The number of items is large, around 10 000. My output of the random forest should be the items that the user is likely to interact with (like a recommender system). For any user, I want to use a feature that describes the items that the user has interacted with in the past. However, mapping the categorical product feature as a one-hot encoding seems very memory inefficient as a user interacts with no more than a couple of hundred of the items at most, and sometimes as little as 5.  How would you go about constructing a random forest when one of the input features is a categorical variable with ~10 000 possible values and the output is a categorical variable with ~10 000 possible values? Should I use CatBoost with the features as categorical? Or should I use one-hot encoding, and if so, do you think XGBoost or CatBoost does better? You could also try entity embeddings to reduce hundreds of boolean features into vectors of small dimension. It is similar to word embedings for categorical features. In practical terms you define an embedding of your discrete space of features into a vector space of low dimension. It can enhance your results and save on memory. The downside is that you do need to train a neural network model to define the embedding before hand. Check this article for more information. XGBoost doesn't support categorical features directly, you need to do the preprocessing to use it with catfeatures. For example, you could do one-hot encoding. One-hot encoding usually works well if there are some frequent values of your cat feature. CatBoost does have categorical features support - both, one-hot encoding and calculation of different statistics on categorical features. To use one-hot encoding you need to enable it with one_hot_max_size parameter, by default statistics are calculated. Statistics usually work better for categorical features with many values. Assuming you have enough domain expertise, you could create a new categorical column from existing column.
ex:-
if you column has below values if you are aware that A,B,C are similar D,E,F are similar and G,H are similar
your new column would be  In your random forest model you should removing previous column and only include this new column. By transforming your features like this you would loose explainability of your mode.This question is in continue to a previous question I've asked. I've trained an LSTM model to predict a binary class (1 or 0) for batches of 100 samples with 3 features each, i.e: the shape of the data is (m, 100, 3), where m is the number of batches. Data: Target: Model code: For the training stage, the model is NOT stateful. When predicting I'm using a stateful model, iterating over the data and outputting a probability for each sample: When looking at the probability at the end of a batch, it is exactly the value I get when predicting with the entire batch (not one by one). However, I've expected that the probability will always continue in the right direction when new samples arrive. What actually happens is that the probability output can spike to the wrong class on an arbitrary sample (see below). Two samples of 100 sample batches over the time of prediction (label = 1):  and Label = 0:
 Is there a way to achieve what I want (avoid extreme spikes while predicting probability), or is that a given fact? Any explanation, advice would be appreciated. Update
Thanks to @today advice, I've tried training the network with the hidden state output for each input time step using return_sequence=True on the last LSTM layer. So now the labels look like so (shape (100,100)): the model summary: However, I get an exception: What do I need to fix? Note: This is just an idea and it might be wrong. Try it if you would like and I would appreciate any feedback. Is there a way to achieve what I want (avoid extreme spikes while
  predicting probability), or is that a given fact? You can do this experiment: set the return_sequences argument of last LSTM layer to True and replicate the labels of each sample as much as the length of each sample. For example if a sample has a length of 100 and its label is 0, then create a new label for this sample which consists of 100 zeros (you can probably easily do this using numpy function like np.repeat). Then retrain your new model and test it on new samples afterwards. I am not sure of this, but I would expect more monotonically increasing/decreasing probability graphs this time. Update: The error you mentioned is caused by the fact that the labels should be a 3D array (look at the output shape of last layer in the model summary). Use np.expand_dims to add another axis of size one to the end. The correct way of repeating the labels would look like this, assuming y_train has a shape of (num_samples,): Actually, I tried the experiment suggested above on the IMDB dataset using a simple model with one LSTM layer. One time, I used only one label per each sample (as in original approach of @Shlomi) and the other time I replicated the labels to have one label per each timestep of a sample (as I suggested above). Here is the code if you would like to try it yourself: Then we can create the stateful replicas of the training models and run them on some test data to compare their results: Actually, the first sample of X_test has a 0 label (i.e. belongs to negative class) and the second sample of X_test has a 1 label (i.e. belongs to positive class). So let's first see what the stateful prediction of test_model (i.e. the one that were trained using one label per sample) for these two samples would look like: The result:  Correct label (i.e. probability) at the end (i.e. timestep 200) but very spiky and fluctuating in between. Now let's compare it with the stateful predictions of the rep_test_model (i.e. the one that were trained using one label per each timestep): The result:  Again, correct label prediction at the end but this time with a much more smoother and monotonic trend, as expected.  Note that this was just an example for demonstration and therefore I have used a very simple model here with just one LSTM layer and I did not attempt to tune it at all. I guess with a better tuning of the model (e.g. adjusting the number of layers, number of units in each layer, activation functions used, optimizer type and parameters, etc.), you might get far better results.I am doing an experiment with three time-series datasets with different characteristics for my experiment whose format is as the following.  The first column is a timestamp. For reproducibility reasons, I am sharing the data here. From column 2, I wanted to read the current row and compare it with the value of the previous row. If it is greater, I keep comparing. If the current value is smaller than the previous row's value, I want to divide the current value (smaller) by the previous value (larger). Accordingly, here is the code: And this produces the following three points - one for each dataset I shared. 

 As we can see from the points in the plots based on the code given above, data1 is pretty consistent whose value is around 1, data2 will have two quotients (whose values will concentrate either around 0.5 or 0.8) and the values of data3 are concentrated around two values (either around 0.5 or 0.7). This way, given a new data point (with quotient and quotient_times), I want to know which cluster it belongs to by building each dataset stacking these two transformed features quotient and quotient_times. I am trying it with KMeans clustering as the following But this is giving me an error: ValueError: n_samples=1 should be >= n_clusters=3. How can we fix this error? Update: samlpe quotient data = array([ 0.7       ,  0.7       ,  0.4973262 ,  0.7008547 ,  0.71287129,
        0.704     ,  0.49723757,  0.49723757,  0.70676692,  0.5       ,
        0.5       ,  0.70754717,  0.5       ,  0.49723757,  0.70322581,
        0.5       ,  0.49723757,  0.49723757,  0.5       ,  0.49723757]) As is, your quotient variable is now one single sample; here I get a different error message, probably due to different Python/scikit-learn version, but the essence is the same: This gives the following error: which, despite the different wording, is not different from yours - essentially it says that your data look like a single sample. Following the first advice(i.e. considering that quotient contains a single feature (column) resolves the issue: Please try the code below. A brief explanation on what I've done:  First I built the dataset sample = np.vstack((quotient_times, quotient)).T and  standardized it, so it would become easier to cluster. Following, I've applied DBScan with multiple hyperparameters (eps and min_samples) until I've found the one that separated the points better. Finally, I've plotted the data with its respective labels, since you are working with 2 dimensional data, it's easy to visualize how good the clustering is.  You are trying to make 3 clusters, while you have only 1 np.array i.e n_samples.I have a minimal example of a neural network with a back-propagation trainer, testing it on the IRIS data set. I started of with 7 hidden nodes and it worked well. I lowered the number of nodes in the hidden layer to 1 (expecting it to fail), but was surprised to see that the accuracy went up. I set up the experiment in azure ml, just to validate that it wasn't my code. Same thing there, 98.3333% accuracy with a single hidden node. Can anyone explain to me what is happening here? First, it has been well established that a variety of classification models yield incredibly good results on Iris (Iris is very predictable); see here, for example. Secondly, we can observe that there are relatively few features in the Iris dataset. Moreover, if you look at the dataset description you can see that two of the features are very highly correlated with the class outcomes. These correlation values are linear, single-feature correlations, which indicates that one can most likely apply a linear model and observe good results. Neural nets are highly nonlinear; they become more and more complex and capture greater and greater nonlinear feature combinations as the number of hidden nodes and hidden layers is increased. Taking these facts into account, that (a) there are few features to begin with and (b) that there are high linear correlations with class, would all point to a less complex, linear function as being the appropriate predictive model-- by using a single hidden node, you are very nearly using a linear model. It can also be noted that, in the absence of any hidden layer (i.e., just input and output nodes), and when the logistic transfer function is used, this is equivalent to logistic regression. Just adding to DMlash's very good answer: The Iris data set can even be predicted with a very high accuracy (96%) by using just three simple rules on only one attribute: In general neural networks are black boxes where you never really know what they are learning but in this case back-engineering should be easy. It is conceivable that it learnt something like the above. The above rules were found by using the OneR package.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 2 years ago. I am researching some Natural Language Processing algorithms to read a piece of text, and if the text seems to be trying to suggest a meeting request, it sets up that meeting for you automatically. For example, if an email text reads:  Let's meet tomorrow someplace in Downtown at 7pm".  The algorithm should be able to detect the Time, date and place of the event. Does someone know of some already existing NLP algorithms that I could use for this purpose? I have been researching some NLP resources (like NLTK and some tools in R), but did not have much success. Thanks This is an application of information extraction, and can be solved more specifically with sequence segmentation algorithms like hidden Markov models (HMMs) or conditional random fields (CRFs). For a software implementation, you might want to start with the MALLET toolkit from UMass-Amherst, it's a popular library that implements CRFs for information extraction. You would treat each token in a sentence as something to be labeled with the fields you are interested in (or 'x' for none of the above), as a function of word features (like part of speech, capitalization, dictionary membership, etc.)... something like this: You will need to provide some hand-labeled training data first, though. You should have a look to http://opennlp.apache.org java toolkit I think you should be able to do this with spacy.
I tried this in jupyter-notebook Output This problem is still in the headlines today. If you are (still) looking for an algorithm, there are solutions using ANTLR a parser generator in a large choice of programming languages (C/C++/C#/JS/Java/...).
Some open-source references:I'm trying to perform object detection with RCNN on my own dataset following the tutorial on Matlab webpage. Based on the picture below:  I'm supposed to put image paths in the first column and the bounding box of each object in the following columns. But in each of my images, there is more than one object of each kind. For example there are 20 vehicles in one image. How should I deal with that? Should I create a separate row for each instance of vehicle in an image? The example found on the website finds the pixel neighbourhood with the largest score and draws a bounding box around that region in the image.  When you have multiple objects now, that complicates things.  There are two approaches that you can use to facilitate finding multiple objects. An alternative approach would be to choose some value k and you would display the top k bounding boxes associated with the k highest scores.  This of course requires that you know what the value of k is before hand and it will always assume that you have found an object in the image like the second approach. In addition to the above logic, the approach that you state where you need to create a separate row for each instance of vehicle in the image is correct.  This means that if you have multiple candidates of an object in a single image, you would need to introduce one row per instance while keeping the image filename the same.  Therefore, if you had for example 20 vehicles in one image, you would need to create 20 rows in your table where the filename is all the same and you would have a single bounding box specification for each distinct object in that image. Once you have done this, assuming that you have already trained the R-CNN detector and you want to use it, the original code to detect objects is the following referencing the website: This only works for one object which has the highest score.  If you wanted to do this for multiple objects, you would use the score that is output from the detect method and find those locations that either accommodate situation 1 or situation 2. If you had situation 1, you would modify it to look like the following. Note that I've stored the original bounding boxes, labels and scores in their original variables while the subset of the ones that surpassed the threshold in separate variables in case you want to cross-reference between the two.  If you wanted to accommodate for situation 2, the code remains the same as situation 1 with the exception of defining the threshold. The code from:  ... would now change to: The end result will be multiple bounding boxes of the detected objects in the image - one annotation per detected object. I think you actually have to put all of the coordinates for that image as a single entry in your training data table. See this MATLAB tutorial for details. If you load the training data to your MATLAB locally and check the vehicleDataset variable, you will actually see this (sorry my score is not high enough to include images directly in my answers).  To summarize, in your training data table, make sure you have one unique entry for each image, and put however many bounding boxes into the corresponding category as a matrix, where each row is in the format of [x, y, width, height].I have learned in some essays (Tomas Mikolov...) that a better way of forming the vector for a sentence is to concatenate the word-vector. but due to my clumsy in mathematics, I am still not sure about the details. for example, supposing that the dimension of word vector is m; and that a sentence has n words. what will be the correct result of concatenating operation? is it a row vector of 1 x m*n ?     or a matrix of m x n ? There are at least three common ways to combine embedding vectors; (a) summing, (b) summing & averaging or (c) concatenating. So in your case, with concatenating, that would give you a 1 x m*a vector, where a is the number of sentences. In the other cases, the vector length stays the same. See gensim.models.doc2vec.Doc2Vec, dm_concat and dm_mean - it allows you to use any of those three options [1,2]. [1] http://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.LabeledLineSentence [2] https://github.com/piskvorky/gensim/blob/develop/gensim/models/doc2vec.pyI am trying to run xgboost using spyder and python, but I keep getting this error:  AttributeError: module ‘xgboost’ has no attribute ‘XGBRegressor’ Here is the code:  Error is I have
Python 3.5.2 :: Anaconda 4.2.0 (x86_64) How do I solve this?  We probably have the same problem. I solved it by telling Python explicitly where to find xgboost library. The reason is that I have more than one scripts with the name xgboost.py. Python might have imported one of them mistakenly, so that it cannot find the definition of 'XGBRegressor'. Here is the command I used: For me, PATH_TO_YOUR_setup.py_file is ~/xgboost/python-package Since your dir call is missing basically everything, my suspicion is that wherever you're starting your script from has an xgboost subfolder with an empty __init__.py in it that is being found first by your import. For my case, I solve this problem pretty easily using from xgboost import XGBRegressor Probably there are many files with the same name of xgboost.That is why python tries to load one of these instead of the original package file. Tip
Check your working directory and see if there are any py files with the same name of xgboost.py.
If so, Change the name to something else. I had the exact same problem with Python 3.6.2 and Anaconda 1.6.8 on windows10 64bits (fall creator update) To get it to work, here is what I did :  1/ Uninstall xgboost from within anaconda, in the chosen environement. 2/ Manually deleted the xgboost directory in C:\ProgramData\Anaconda3 3/ Downloaded xgboost from This page 4/ From Anaconda, launch a command prompt from (from the environment you want xgboost into of course) 5/ CD to the directory you downloaded the whl file to and type : pip install xgboost‑0.6+20171121‑cp36‑cp36m‑win_amd64.whl  (or the exact name of the file you downloaded) I did all these steps and xgboost worked properly I had to make sure to follow all the download instructions from the xgboost website. After installing and compiling, i forgot to run these. 
https://xgboost.readthedocs.io/en/latest/build.html#python-package-installationI'm reading images into my TF network, but I also need the associated labels along with them. So I tried to follow this answer, but the labels that are output don't actually match the images that I'm getting in every batch. The names of my images are in the format dir/3.jpg, so I just extract the label from the image file name. Am I doing something wrong, or does the slice_input_producer not actually ensure that its input tensors are synced? Aside:  I also noticed that when I get a batch from tf.train.batch, the elements in the batch are adjacent to each other in the original list I gave it, but the batch order isn't in the original order.
Example: If my data is ["dir/1.jpg", "dir/2.jpg", "dir/3.jpg", "dir/4.jpg", "dir/5.jpg, "dir/6.jpg"], then I may get the batch (with batch_size=2) ["dir/3.jpg", "dir/4.jpg"], then batch ["dir/1.jpg", "dir/2.jpg"], and then the last one. 
So this makes it hard to even just use a FIFO queue for the labels since the order won't match the batch order. Here is a complete runnable example that reproduces the problem: What this prints is: So basically each eval call runs the operation another time ! Adding the batching does not make a difference to that - just prints batches (the first 11 filenames followed by the next 11 labels and so on) The workaround I see is: which correctly prints: but does nothing for the violation of the principle of the least surprise. EDIT: yet another way of doing it: That's a much better way as tf.convert_to_tensor and co only accept tensors of same type/shape etc. Note that I removed the coordinator for simplicity, which however results in a warning: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\kernels\queue_base.cc:294] _0_input_producer/input_producer/fraction_of_32_full/fraction_of_32_full: Skipping cancelled enqueue attempt with queue not closed See thisI tried to pass to GridSearchCV other scoring metrics like balanced_accuracy for Binary Classification (instead of the default accuracy)  and got this error ValueError: 'balanced_accuracy' is not a valid scoring value. Valid
  options are
  ['accuracy','adjusted_mutual_info_score','adjusted_rand_score','average_precision','completeness_score','explained_variance','f1','f1_macro','f1_micro','f1_samples','f1_weighted','fowlkes_mallows_score','homogeneity_score','mutual_info_score','neg_log_loss','neg_mean_absolute_error','neg_mean_squared_error','neg_mean_squared_log_error','neg_median_absolute_error','normalized_mutual_info_score','precision','precision_macro','precision_micro','precision_samples','precision_weighted','r2','recall','recall_macro','recall_micro','recall_samples','recall_weighted','roc_auc','v_measure_score'] This is strange because 'balanced_accuracy' should be valid 
Without defining balanced_accuracy then the code works fine Also the scoring metrics in the error above seems to be different from the ones in the document Any ideas why? Thank you so much scikit-learn version is 0.19.2 Update your sklearn to the latest version if you want to use balanced_accuracy. As you can see from the 0.19 documentation balanced_accuracy is not a valid scoring metric. It was added in 0.20.Greetings dear members of the community. I am creating a neural network to predict a multi-label y. Specifically, the neural network takes 5 inputs (list of actors, plot summary, movie features, movie reviews, title) and tries to predict the sequence of movie genres. In the neural network I use Embeddings Layer and Global Max Pooling layers. However, I recently discovered the Recurrent Layers with Attention, which are a very interesting topic these days in machine learning translation. So, I wondered if I could use one of those layers but only the Plot Summary input. Note that I don't do ml translation but rather text classification. My neural network in its current state You will see in the above structure that I have 5 input layers, 5 Embedding layers, then I apply a Bidirectional layer on LSTM only in the Plot Summary input. However, with the current bidirectional approach on Plot summary, I got the following error. My problem is how I can utilize the attention in text classification and not solve the error below. So, don't comment solution on this error.  My question is about suggesting ways on how to create a recurrent layer with attention for the plot summary (input 2). Also, do not hesitate to write in comments any article that might help me on achieving this in Keras. I remain at your disposal if any additional information is required regarding the structure of the neural network. If you find the above neural network complicated I can make a simple version of it. However, the above is my original neural network, so I want any proposals do be based on that nn. EDIT: 14.12.2020 Find here the colab notebook with the code I want to execute. The code has included two answers, one proposed in the comments (from an already answered question, and the other written as an official answer to my question. The first approach proposed by @MarcoCerliani works. Although, I would like also the second approach to work. The approach of @Allohvk (both approaches are implemented in the Runtime cell [21] of the attached colab). The latter does not work at the moment. The latest error I get is: ValueError: Input 0 of layer globalmaxpooling_plot_summary_Layer is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 100] I solved the latest error of my edit by removing the globalmaxpooling_plot_summary_Layer from my neural's network structure. Let me summarize the intent. You want to add attention to your code. Yours is a sequence classification task and not a seq-seq translator. You dont really care much about the way it is done, so you are ok with not debugging the error above, but just need a working piece of code. Our main input here is the movie reviews consisting of 'n' words for which you want to add attention. Assume you embed the reviews and pass it to an LSTM layer. Now you want to 'attend' to all the hidden states of the LSTM layer and then generate a classification (instead of just using the last hidden state of the encoder). So an attention layer needs to be inserted. A barebones implementation would look like this: Now call the above Attention layer after your LSTM and before your Dense output layer. You can build on top of this as you seem to want to use other features apart for the movie reviews to come up with the final sentiment. Attention largely applies to reviews..and benefits are to be seen if the sentences are very long. For more specific details, please refer https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9eI am trying to create a custom macro for recall = (recall of class1 + recall of class2)/2. I came up with the following code but I am not sure how to calculate the true positive of class 0. It seems I will have to use Keras.backend.equal(x, y), but how do i create a tensor with shape K.int_shape(y_true)[0] and all values, say x? Edit 1 Based on Marcin's comments, I wanted to create a custom metric based on callback in keras. While browsing issues in Keras, I came across the following code for f1 metric: But how is the callback returning the accuracy? I wanted to implement unweighted recall = (recall class1 + recall class2)/2. I can think of the following code but would appreciate any help to complete it Edit 2: model: keras version (with the mean problem).  Are your two classes actually only one dimension output (0 or 1)? If so: Now, if your two classes are actually a 2-element output: Callback version: For the callback, you can use a LambdaCallback, and you manually print or store the results: Where recall is a function using np instead of K. And epsilon = np.finfo(float).eps or epsilon = np.finfo(np.float32).eps)Looking at https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/ClassifierFloatMobileNet.java, Can you help me understand why they - IMAGE_MEAN and / IMAGE_STD? You'll notice it's not necessary for the Quantized example (see https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/ClassifierQuantizedMobileNet.java). Rough thoughts so far.... 127.5 = 255 / 2. Pixels are frequently represented as colors using a range from 0-255. This is exactly the middle of that range. So every pixel color is being adjusted to be between -1 and 1... This is exactly correct.  but why? Input normalization is a common technique in machine learning. This specific model was trained with input value range -1 to 1, so we should normalize the inference input to the same range to achieve best result.  To give some intuition, what will go wrong if the input isn't normalized to -1 to 1: The range can be arbitrary. -1~1 and 0~1 are often used. The point is the same normalization should be applied to both training & inference.I am trying to use a deep neural network architecture to classify against a binary label value - -1 and +1. Here is my code to do it in tensorflow. This is the output I get when I run this: I don't understand if the values I am getting are correct as there is a real dearth of non-MNIST binary classification examples. The accuracy is nothing like what I expected. I was expecting a percentage instead of that large value. I am also somewhat unsure of the theory behind machine learning which is why I can't tell the correctness of my approach using tensorflow.  Can someone please tell me if my approach towards binary classification is correct?
Also is the accuracy part of my code correct? From this: a binary label value - -1 and +1 . . . I am assuming your values in train_y and test_y are actually -1.0 and +1.0 This is not going to work very well with your chosen loss function sigmoid_cross_entropy_with_logits - which assumes 0.0 and +1.0. The negative y values are causing mayhem! However, the loss function choice is good for binary classification. I suggest change your y values to 0 and 1. In addition, technically the output of your network is not the final prediction. The loss function sigmoid_cross_entropy_with_logits is designed to work with a network with sigmoid transfer function in the output layer, although you have got it right that the loss function is applied before this is done. So your training code appears correct  I'm not 100% sure about the tf.transpose though - I would see what happens if you remove that, personally I.e. Either way, this is the "logit" output, but not your prediction. The value of output can get high for very confident predictions, which probably explains your very high values later due to missing the sigmoid function. So add a prediction tensor (this represents the probability/confidence that the example is in the positive class): You can use that to calculate accuracy. Your accuracy calculation should not be based on L2 error, but sum of correct values - closer to the code you had commented out (which appears to be from a multiclass classification). For a comparison with true/false for binary classification, you need to threshold the predictions, and compare with the true labels. Something like this: The accuracy value should be between 0.0 and 1.0. If you want as a percentage, just multiply by 100 of course.I have about a 30% and 70% for class 0 (minority class) and class 1 (majority class). Since I do not have a lot of data, I am planning to oversample the minority class to balance out the classes to become a 50-50 split. I was wondering if oversampling should be done before or after splitting my data into train and test sets. I have generally seen it done before splitting in online examples, like this: However, wouldn't that mean that the test data will likely have duplicated samples from the training set (because we have oversampled the training set)? This means that testing performance wouldn't necessarily be on new, unseen data. I am fine doing this, but I would like to know what is considered good practice. Thank you! I was wondering if oversampling should be done before or after splitting my data into train and test sets. It should certainly be done after splitting, i.e. it should be applied only to your training set, and not to your validation and test ones; see also my related answer here. I have generally seen it done before splitting in online examples, like this From the code snippet you show, it is not at all obvious that it is done before splitting, as you claim. It depends on what exactly the train variable is here: if it is the product of a train-test split, then the oversampling takes place after splitting indeed, as it should be. However, wouldn't that mean that the test data will likely have duplicated samples from the training set (because we have oversampled the training set)? This means that testing performance wouldn't necessarily be on new, unseen data. Exactly, this is the reason why the oversampling should be done after splitting to train-test, and not before. (I once witnessed a case where the modeller was struggling to understand why he was getting a ~ 100% test accuracy, much higher than his training one; turned out his initial dataset was full of duplicates -no class imbalance here, but the idea is similar- and several of these duplicates naturally ended up in his test set after the split, without of course being new or unseen data...). I am fine doing this You shouldn't :) From my experience this is a bad practice. As you mentioned test data should contain unseen samples so it would not overfit and give you better evaluation of training process. If you need to increase sample sizes - think about data transformation possibilities.
E.g. human/cat image classification, as they are symmetric you can double sample size by mirroring images.I'm trying to fit a (machine-learning) model that takes in an audiofile (.wav) and predicts the emotion from it (multi-label classification).
I'm trying to read the sample rate and signal from the file, but when calling read(filename) from scipy.io.wavfile, I'm getting ValueError: Incomplete wav chunk.  I've tried switching from scipy.read() to librosa.read().
They both output the signal and sample rate, but for some reason librosa takes exponentially longer time than scipy, and is impractical for my task. I've tried sr, y = scipi.io.wavfile.read(open(filename, 'r')) as suggested here, to no avail. I've tried looking into my files and checking what might cause it:
Out of all 2084 wav files, 1057 were good (=scipy managed to read them), and 
1027 were bad (=raised the error).
I couldn't seem to find any thing pointing as to what makes a file pass or fail, but nonetheless it's a weird result, as all files are taken from the same dataset from the same origin.  I've heard people saying I could just re-export the files as wav using some software, and it should work.
I didn't try this because a) I don't have any audio-processing software and it seems like an overkill, and b) I want to understand the actual problem rather than put a bandaid on it. Assume filenames is a subset of all my audio files, containing fn_good and fn_bad, where fn_good is an actual file that gets processed, and fn_bad is an actual file that raises an error.  Using VLC, it seems that the codecs are supported by scipy.io.wavfile, but in either case, both files have the same codec, so it's weird they don't have the same effect... 
Codec of the GOOD file:
  Codec of the BAD file:
 I don't know why scipy.io.wavfile can't read the file--there might be an invalid chunk in there that other readers simply ignore.  Note that even when I read a "good" file with scipy.io.wavfile, a warning (WavFileWarning: Chunk (non-data) not understood, skipping it.) is generated: I can read 'fearful_song_strong_dogs_act06_f_0.wav' using wavio (source code on github: wavio), a package I created that wraps Python's standard wave library with functions that understand NumPy arrays:  I solve the problem by changing this number "4" to "1"  in the file wavefile.py file,
in this condition of the code:
-   len(chunk_id) < 1 but it was by just intuition and good luck, now i wonder why this works and what are the possible reasons?In Keras documentation - steps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples of your dataset divided by the batch size. I have 3000 samples. 
If i set steps_per_epoch=3000 it's work slowly. If i set steps_per_epoch=300 it's work faster and i thought that Batch works!  But then I compared how much video memory is allocated in the first and second cases. And did not notice a big difference. If I use a simple fit() function then the difference is large. So it's real  speed up or i just process 300 examples, instead of 3000? What for this parameter is necessary? And how can I speed up the training? 
My generator code: The steps_per_epoch parameter is the number of batches of samples it will take to complete one full epoch. This is dependent on your batch size. The batch size is set where you initialize your training data. For example, if you're doing this with ImageDataGenerator.flow() or ImageDataGenerator.flow_from_directory(), the batch size is specified with the batch_size parameter in each of these. You said you have 3000 samples.  This is because steps_per_epoch should be equivalent to the total number of samples divided by the batch size. The process of implementing this in Keras is available in the two videos below. The reason why you have to set steps_per_epoch is that the generator is designed to run indefinitely (See the docs:  "The generator is expected to loop over its data indefinitely." ). You implemented this by setting while 1.
Since fit_generator() is supposed to run epochs=x times, the method must know when the next epoch begins within this indefinitely loop (and, hence, the data has to be drawn from the beginning again).I am using the following R code to produce a confusion matrix comparing the true labels of some data to the output of a neural network. However, sometimes the neural network doesn't predict any of a certain class, so the table isn't square (as, for example, there are 5 levels in the test.labels factor, but only 3 levels in the nnetpredict factor). I want to make the table square by adding in any factor levels necessary, and setting their counts to zero. How should I go about doing this? Example: You can see in the table above that there are 7 rows, but 10 columns, because the a factor only has 7 levels, whereas the b factor has 10 levels. What I want to do is to pad the table with zeros so that the row labels and the column labels are the same, and the matrix is square. From the example above, this would produce: The reason I need to do this is two-fold: EDIT - round II to address the additional details in the question. I deleted my first answer since it wasn't relevant anymore. This has produced the desired output for the test cases I've given it, but I definitely advise testing thoroughly with your real data. The approach here is to find the full list of levels for both inputs into the table and set that full list as the levels before generating the table. Two test cases:Im stuck in a dataset that contains some categrotical features with a high cardinality.
like 'item_description' ...
I read about some trick called hashing, but its main idea is still blurry and incomprehensible, i also read about a library called 'Feature engine' but i didn't really find something that might solve my issue. 
Any suggestions please?  Options: i) Use Target encoding. More on target encoding : https://maxhalford.github.io/blog/target-encoding/ Good tutorial on categorical variables here: https://www.coursera.org/learn/competitive-data-science#syllabus [Section: Feature Preprocessing and Generation with Respect to Models , 3rd Video] ii) Use entity embeddings:
In short, this technique represent each category by a vector, then training to obtain the characteristics of the category. Tutorial : https://towardsdatascience.com/deep-learning-structured-data-8d6a278f3088 Notebook implementations: iii) Use Catboost :  Extra: There is a hashing trick technique which might also be helpful: https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087?gi=3045c6e13ee5 You could look into the category_encoders. There you have many different encoders, which you can use to encode columns with high cardinality into a single column. Among them there are what are known as Bayesian encoders, which use information from the target variable to transform a given feature. For instance you have the TargetEncoder, which uses Bayesian principles to replace a categorical feature with the expected value of the target given then values the category takes, which is very similar to LeaveOneOut. You may also check the catboost based CatBoostEncoder which is a common choice for feature encoding.  For variables like "item_description" which are in essence text variables, check this paper and corresponding Python package. Or simply search online for "dirty categorical variables" and if in doubt, it is the article and package are from Gal Varoquaux, one of the main developers from Sklearn.Learning about ragged tensors and how can I use them with tensorflow.
My example The error I am not sure if I can use ragged tensors like this. All examples I found have embedding layer before LSTM, but what I don't want to create additional embedding layer.  I recommend to use Input layer rather than InputLayer, you often not need to use InputLayer, Anyway the probelm that the shape of your input and LSTM layer input shape was wrong , here the modification i have made with some comments.I am using shap library for ML interpretability to better understand k-means segmentation algorithm clusters. In a nutshell I make some blogs, use k-means to cluster them and then take the clusters as label and xgboost to try to predict them.  I have 5 clusters so it is a signle-label multi-class classification problem.  The pictures above make sense as the class is '3'. But why this base_value, shouldn't it be 1/5? I asked myself a while ago a similar question but this time I set already link='logit'.  link="logit" does not seem right for multiclass, as it's only suitable for binary output. This is why you do not see probabilities summing up to 1. Let's streamline your code: Then, what you see as expected values in: are base scores in raw space. The multi-class raw scores can be converted to probabilities with softmax: shap.force_plot(..., link="logit") doesn't make sense for multiclass, and it seems impossible to switch from raw to probability and still maintain additivity (because softmax(x+y) ≠ softmax(x) + softmax(y)). Should you wish to analyze your data in probability space try KernelExplainer:  or summary plot:  which are now additive for shap values in probability space and align well with both base probabilities (see above) and predicted probabilities for 0th datapoint:My computer has the following software installed: Anaconda (3), TensorFlow (GPU), and Keras.
There are two Anaconda virtual environments - one with TensorFlow for Python 2.7 and one for 3.5, both GPU version, installed according to the TF instructions. (I had a CPU version of TensorFlow installed previously in a separate environment, but I've deleted it.) When I run the following: and check nvidia-smi it shows only 3MiB GPU Memory Usage by Python, so it looks like GPU is not used for calculations.
(code.py is a simple deep Q-learning algorithm implemented with Keras) Any ideas what can be going wrong?  A good way to debug these problems is to check which operations have been allocated to which devices. You can check this by passing a configuration parameter to the session: When you run your app, you will see some output indicating which devices are being used. You can find more information here:
https://www.tensorflow.org/tutorials/using_gpu TensorFlow on Windows It took me hours to fix TensorFlow installation issues on windows, so here is summary: To Check if TensorFlow-gpu is working or not (use this code): To Check list of available CPUs or GPUs (use this code): Install Tensorflow GPU on Windows using CUDA and cuDNN Guide Overview Guide Complete details  Make Sure  Hope thats helpful :))  The reason my GPU wasn't running was because of a broken installation of the CuDNN, more precisely libraries and source came from different versions of CuDNN. It was fixed with  the following piece of advice. This is a slippery problem and it's almost as if the computer will use any possible excuse to revert back to CPU.  To ensure tensoflow, keras and pytorch are currently being used, see https://stackoverflow.com/a/53244520/420400I have a frozen inference graph stored in a .pb file, which was obtained from a trained Tensorflow model by the freeze_graph function.  Suppose, for simplicity, that I would like to change some of the sigmoid activations in the model to tanh activations (and let's not discuss whether this is a good idea).  How can this be done with access only to the frozen graph in the .pb file, and without the possibility to retrain the model? I am aware of the Graph Editor library in tf.contrib, which should be able to do this kind of job, but I wasn't able to figure out a simple way to do this in the documentation. The solution is to use import_graph_def: Here I wrote a post about it Can you try this: graph = load_graph(filename)
graph_def = graph.as_graph_def()
# if ReLu op is at node 161
graph_def.node[161].op="tanh"
tf.train.write_graph(graph_def, path2savfrozn, "altered_frozen.pb", False)  Please let know the if it works. The *.pb file contains a SavedModel protocol buffer. You should be able to load it using a SavedModel loader. You can also inpsect it with the SavedModel CLI. The full documentation on SavedModels is here. Something along these lines should work:TFIDFVectorizer takes so much memory ,vectorizing 470 MB of 100k documents takes over 6 GB , if we go 21 million documents it will not fit 60 GB of RAM we have. So we go for HashingVectorizer but still need  to know how to distribute the hashing vectorizer.Fit and partial fit does nothing so how to work with Huge Corpus? I would strongly recommend you to use the HashingVectorizer when fitting models on large dataset. The HashingVectorizer is data independent, only the parameters from vectorizer.get_params() are important. Hence (un)pickling `HashingVectorizer instance should be very fast. The vocabulary based vectorizers are better suited for exploratory analysis on small datasets. One way to overcome the inability of HashingVectorizer to account for IDF is to index your data into elasticsearch or lucene and retrieve termvectors from there using which you can calculate Tf-IDF.I'm getting this error: ValueError: Error when checking input: expected Sequence to have 3 dimensions, but got array with shape (500, 400) These are the below codes that I'm using. Output (here I've 500 rows in each): Code: Any insights? Two things - Conv1D layer expects input to be in the shape (batch_size, x, filters), in your case (500,400,1). 
You need to reshape your input layer, add another axis, of size 1. (this does not change anything in your data).  You are trying to use multiple inputs, Sequential API is not the best choice for that. I would recommend using the Functional API Edit:
Regarding your comment, not sure what you did wrong, but this is a working version of your code (with fake data), with a reshape: With output:I've been reading about convolutional nets and I've programmed a few models myself. When I see visual diagrams of other models it shows each layer being smaller and deeper than the last ones. Layers have three dimensions like 256x256x32. What is this third number? I assume the first two numbers are the number of nodes but I don't know what the depth is. There are many articles and posts out there explaining how convolution layers work. I'll try to answer your question without going into too many details, just focusing on shapes. Assuming you are working with 2D convolution layers, your input and output will both be three-dimensional. That is, without considering the batch which would correspond to a 4th axis... Therefore, the shape of a convolution layer input will be (c, h, w) (or (h, w, c) depending on the framework) where c is the number of channels, h is the width of the input and w the width. You can see it as a c-channel hxw image.
The most intuitive example of such input is the input of the first convolution layer of your convolutional neural network: most likely an image of size hxw with c channels for example c=1 for greyscale or c=3 for RGB... What's important is that for all pixels of that input, the values on each channel gives additional information on that pixel. Having three channels will give each pixel ('pixel' as in position in the 2D input space) a richer content than having a single. Since each pixel will be encoded with three values (three channels) vs. a single one (one channel). This kind of intuition about what channels represent can be extrapolated to a higher number of channels. As we said an input can have c channels. Now going back to convolution layers, here is a good visualization. Imagine having a 5x5 1-channel input. And a convolution layer consisting  of a single 3x3 filter (i.e. kernel_size=3) Now keep in mind the dimension of the output will depend on the stride and padding of the convolution layer. Here the shape of the output is the same as the shape of the filter, it does not necessarily have to be! Take an input shape of (1, 5, 5), with the same convolution settings, you would end up with a shape of (4, 4) (which is different from the filter shape (3, 3). Also, something to note is that if the input had more than one channel: shape (c, h, w), the filter would have to have the same number of channels. Each channel of the input would convolve with each channel of the filter and the results would be averaged into a single 2D feature map. So you would have an intermediate output of (c, 3, 3), which after averaging over the channels, would leave us with (1, 3, 3)=(3, 3). As a result, considering a convolution with a single filter, however many input channels there are, the output will always have a single channel. From there what you can do is assemble multiple filters on the same layer. This means you define your layer as having k 3x3 filters. So a layer consists k filters. For the computation of the output, the idea is simple: one filter gives a (3, 3) feature map, so k filters will give k (3, 3) feature maps. These maps are then stacked into what will be the channel dimension. Ultimately, you're left with an output shape of... (k, 3, 3). Let k_h and k_w, be the kernel height and kernel width respectively. And h', w' the height and width of one outputted feature map: Back to your question: Layers have 3 dimensions like 256x256x32. What is this third number? I assume the first two numbers are the number of nodes but I don't know what the depth is. Convolution layers have four dimensions, but one of them is imposed by your input channel count. You can choose the size of your convolution kernel, and the number of filters. This number will determine is the number of channels of the output. 256x256 seems extremely high and you most likely correspond to the output shape of the feature map. On the other hand, 32 would be the number of channels of the output, which... as I tried to explain is the number of filters in that layer. Usually speaking the dimensions represented in visual diagrams for convolution networks correspond to the intermediate output shapes, not the layer shapes. As an example, take the VGG neural network: 
Very Deep Convolutional Networks for Large-Scale Image Recognition Input shape for VGG is (3, 224, 224), knowing that the result of the first convolution has shape (64, 224, 224) you can determine there is a total of 64 filters in that layer. As it turns out the kernel size in VGG is 3x3. So, here is a question for you: knowing there is a single bias parameter per filter, how many total parameters are in VGG's first convolution layer? Sorry for the short answer, but when you have a digital image, you have 2 dimensions and then you often have 3 for the colors. The convolutional filter looks into parts of the picture with lower height/width dimensions and much more depth channels (in your case 32) to get more information. This is then fed into the neural network to learn. I created the example in PyTorch to demonstrate the output you had: Out: It's a real tensor inside. It may help. You can play with a lot of convolution parameters.I need to keep track of the F1-scores while tuning C & Sigma in SVM,
For example the following code keeps track of the Accuracy, I need to change it to F1-Score but I was not able to do that……. I have seen the following two links  Retraining after Cross Validation with libsvm  10 fold cross-validation in one-against-all SVM (using LibSVM) I do understand that I have to first find the best C and gamma/sigma parameters over the training data, then use these two values to do a LEAVE-ONE-OUT crossvalidation classification experiment,
So what I want now is to first do a grid-search for tuning C & sigma.
Please I would prefer to use MATLAB-SVM and not LIBSVM. 
Below is my code for  LEAVE-ONE-OUT crossvalidation classification. i have changed the code
but i making some mistakes and i am getting errors,  ....... and the function..... So basically you want to take this line of yours: put it in a loop that varies your 'BoxConstraint' and 'RBF_Sigma' parameters and then uses crossval to output the f1-score for that iterations combination of parameters. You can use a single for-loop exactly like in your libsvm code example (i.e. using meshgrid and 1:numel(), this is probably faster) or a nested for-loop. I'll use a nested loop so that you have both approaches: Now we just have to define the function fun. The docs have this to say about fun: fun is a function handle to a function with two inputs, the training
  subset of X, XTRAIN, and the test subset of X, XTEST, as follows: testval = fun(XTRAIN,XTEST) Each time it is called, fun should use
  XTRAIN to fit a model, then return some criterion testval computed on
  XTEST using that fitted model. So fun needs to: You'll notice that fun can't take any extra parameters which is why I've wrapped it in an anonymous function so that we can pass the current C and S values in. (i.e. all that @(...)(fun(...)) stuff above. That's just a trick to "convert" our six parameter fun into the 4 parameter one required by crossval. I found the only problem with target(trainIdx). It's a row vector so I just replaced target(trainIdx) with target(trainIdx) which is a column vector.I have a training dataset (text) for a particular category (say Cancer). I want to train a SVM classifier for this class in weka. But when i try to do this by creating a folder 'cancer' and putting all those training files to that folder and when i run to code i get the following error:
weka.classifiers.functions.SMO: Cannot handle unary class! what I want to do is if the classifier finds a document related to 'cancer' it says the class name correctly and once i fed a non cancer document it should say something like 'unknown'.  What should I do to get this behavior?  The SMO algorithm in Weka only does binary classification between two classes.  Sequential Minimal Optimization is a specific algorithm for solving an SVM and in Weka this a basic implementation of this algorithm. If you have some examples that are cancer and some that are not, then that would be binary, perhaps you haven't labeled them correctly. However, if you are using training data which is all examples of cancer and you want it to tell you whether a future example fits the pattern or not, then you are attempting to do one-class SVM, aka outlier detection. LibSVM in Weka can handle one-class svm.  Unlike the Weka SMO implementation, LibSVM is a standalone program which has been interfaced into Weka and incorporates many different variants of SVM. This post on the Wekalist explains how to use LibSVM for this in Weka.I developed a time series model with LSTM. I can't use it for predicting stock price in future days. I want to use it for predicting stock price for next year and plot it. How to use it for forecasting stock price in future (next year)? One way of doing it is to feed the forecasts back to the model as inputs: at each step you update the input sequence by dropping the oldest value and adding the latest forecast as the most recent value. This is schematically illustrated below, where n is the length of the input sequence and T is the length of the time series.  The code below shows how you could implement this approach for your LSTM model and plot the results.My current project is to build a face authentication system. The constraint I have is: during enrollment, the user gives single image for training. However, I can add and use images given by the user while authentication. The reason I want to add more images into training is, the user environment is not restricted - different lighting conditions, different distance from camera, from different MP cameras. The only relief is the pose is almost frontal. I think, the above problem is similar to the face tagging app widely available. Can anyone suggest a method to use the available images adaptively and smartly?? --Thanks To make your classifier robust you need to use condition independent features. For example, you cannot use face color since it depends on lighting conditions and state of a person itself. However, you can use distance between eyes since it is independent of any changes.  I would suggest building some model of such independent features and retrain classifier each time person starts authentication session. Best model I can think of is Active Appearance Model (one of implementations).  I would recommend that you give SOM(self-organizing maps) a close look. I think it contains the solutions to all the problems and constraints you have mentioned. You can employ it for the single image per person problem. Also, using the multiple SOM-face strategy, you can adapt it for cases when additional images are available for training. Whats pretty neat about the whole concept is that when a new face is encountered, only the new one rather than the whole original database is needed to be re-learned. A few links which you might find helpful along the way: http://en.wikipedia.org/wiki/Self-organizing_map (wiki) http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tnn05.pdf (An interesting research paper which demonstrates the above mentioned technique) Good LuckRecently I came across "Parameter" layer in caffe.
It seems like this layer exposes its internal parameter blob to "top".   What is this layer using for?
Can you give a usage example? This layer was introduced in the pull request #2079, with the following description: This layer simply holds a parameter blob of user-defined shape, and shares it as its single top.  which is exactly what you expected. This was introduced in context of the issue #1474, which basically proposes to treat parameters like normal bottom blobs. To show why this can be useful, consider the following example (taken from issue #1474, by @longjon): The inner product layer calculates C = A * B, where C is the top blob (output), B is a bottom blob, and A must be a parameter blob. This is very restrictive, as this makes it impossible to use the inner product layer for multiplying the inner product between two bottom blobs, e.g. multiplying two input matrices. The issue #1474 suggests to make a fundamental change: make parameters independent of the layer. Instead, treat them like a normal bottom blob. As a first step in that direction, the Parameter layer was introduced. This allows you to define a new parameter, which you can then feed into the bottom of another layer. 
The counterpart - a method for feeding parameters into a layer as a bottom blob - is proposed in pull request #2166, which isn't merged yet, as of Jan. 2017.
Though this is not merged yet, you can still use Parameter layer to define new learnable parameters to feed into other layers as bottom blob.I am trying to predict using the learned .h5 file.
The learning model is as follows. And I wrote the form of the input as follows. I thought the shape was correct, but the following error occurred. ValueError: Error when checking : expected dense_1_input to have shape (3,) but got array with shape (1,) The shape of x is obviously (3,1), but the above error doesn't disappear (the data is from a csv file in the form of (value 1, value 2, value 3, class)). How can I solve this problem? The shape of x is obviously (3,1), but the above error continues. You are right, but that's not what keras expects. It expects (1, 3) shape: by convention, axis 0 denotes the batch size and axis 1 denotes the features. The first Dense layer accepts 3 features, that's why it complains when it sees just one. The solution is simply to transpose x.I have prepared two different .arff files from two different datasets one for testing and other for training.  Each of them have equal instances but different features changing the dimensionality of feature vector for each file. When i did cross-validation on each of these files, they are working perfectly. This shows .arff files are properly prepared and don't have any error. Now if i use the train file having less dimensionality compared to test file for evaluation. I get a following error. Does test file in weka requires same or less number of features as train ?
Code for evaluation Does test file in weka requires same or less number of features as train ? Code for evaluation Same number of features are necessary. You may need to insert ? for class attribute too. According to Weka Architect Mark Hall To be compatible, the header information of the two sets of instances needs to be the same - same 
  number of attributes, with the same names in the same order. Furthermore, any nominal attributes must
  have the same values declared in the same order in both sets of instances. 
  For unknown class values in your test set just set the value of each to missing - i.e "?". According to Weka's wiki, the number of features needs to be same for both the training and test sets. Also the type of these features (e.g., nominal, numeric, etc) needs to be the same. Also, I assume that you didn't apply any Weka filters to either of your datasets. The datasets often become incompatible if you apply filters separately on each dataset (even if it is the same filter). How do I divide a dataset into training and test set? You can use the RemovePercentage filter (package weka.filters.unsupervised.instance). In the Explorer just do the following: training set: -Load the full dataset -select the RemovePercentage filter in the preprocess panel -set the correct percentage for the split -apply the filter -save the generated data as a new file test set: -Load the full dataset (or just use undo to revert the changes to the dataset) -select the RemovePercentage filter if not yet selected -set the invertSelection property to true -apply the filter -save the generated data as new fileI am trying to do cross validation and I am running into an error that says: 'Found input variables with inconsistent numbers of samples: [18, 1]' I am using different columns in a pandas data frame (df) as the features, with the last column as the label. This is derived from the machine learning repository for UC Irvine. When importing the cross-validation package that I have used in the past, I am getting an error that it may have depreciated. I am going to be running a decision tree, SVM, and K-NN.  My code is as such:  Any help would be great! cross_validation module is deprecated. The new module model_selection has taken its place. So everything you did with cross_validation. is now available in model_selection. Then your above code becomes: Now as far as declaring the X and y is concerned, why are you wrapping them in a list. Just use them like this: And then you can simply use your code, without changing anything. And for your last question about folds in cross-validation, there are multiple classes in sklearn which does this (depending upon task). Please have a look at: Which contains fold iterators. And remember, all this is present in model_selection package. The items in your feature list are pandas Series. You don't need to list out each feature in a list like you have done; you just need to pass them all as a single "table". For example, this looks like the bank dataset so: Should work. The only thing to notice here is that xis a DataFrame with 16 columns but its underlying data is a numpy ndarray - not a list of Series but a single "matrix".If I correctly understood the significance of the loss function to the model, it directs the model to be trained based on minimizing the loss value. So for example, if I want my model to be trained in order to have the least mean absolute error, i should use the MAE as the loss function. Why is it, for example, sometimes you see someone wanting to achieve the best accuracy possible, but building the model to minimize another completely different function? For example: How come the model above is trained to give us the best acc, since during it's training it will try to minimize another function (MSE). I know that, when already trained, the metric of the model will give us the best acc found during the training.  My doubt is: shouldn't the focus of the model during it's training to maximize acc (or minimize 1/acc) instead of minimizing MSE? If done in that way, wouldn't the model give us even higher accuracy, since it knows it has to maximize it during it's training? To start with, the code snippet you have used as example: is actually invalid (although Keras will not produce any error or warning) for a very simple and elementary reason: MSE is a valid loss for regression problems, for which problems accuracy is meaningless (it is meaningful only for classification problems, where MSE is not a valid loss function). For details (including a code example), see own answer in What function defines accuracy in Keras when the loss is mean squared error (MSE)?; for a similar situation in scikit-learn, see own answer in this thread. Continuing to your general question: in regression settings, usually we don't need a separate performance metric, and we normally use just the loss function itself for this purpose, i.e. the correct code for the example you have used would simply be without any metrics specified. We could of course use metrics='mse', but this is redundant and not really needed. Sometimes people use something like i.e. optimise the model according to the MSE loss, but show also its performance in the mean absolute error (MAE) in addition to MSE. Now, your question: shouldn't the focus of the model during its training to maximize acc (or minimize 1/acc) instead of minimizing MSE? is indeed valid, at least in principle (save for the reference to MSE), but only for classification problems, where, roughly speaking, the situation is as follows: we cannot use the vast arsenal of convex optimization methods in order to directly maximize the accuracy, because accuracy is not a differentiable function; so, we need a proxy differentiable function to use as loss. The most common example of such a loss function suitable for classification problems is the cross entropy. Rather unsurprisingly, this question of yours pops up from time to time, albeit in slight variations in context; see for example own answers in For the interplay between loss and accuracy in the special case of binary classification, you may find my answers in the following threads useful:I wrote a code for multivariate polynomial regression, I used polynomial features and transformation function from sklearn. Is it possible to make multivariate logarithmic regression?
Does sklearn have some kind of logarithmic transformation, like it has for polynomial features?
How can I write multivariate logarithmic regression in python? This is my code for multivariate polynomial features: If you want to fit with the logarithms of your features, one option is the Box-Cox Transform then OLS, which you can apply in sklearn using the PowerTransformer. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformerI am trying to implement word vectorization using Spark's MLLib. I am following the example given here. I have bunch of sentences which I want to give as input to train the model. But am not sure if this model takes sentences or just takes all the words as a sequence of string.  My input is as below: But when I try to train my word2vec model on this input it does not work. Does Word2Vec not take sentences as input? Your input is correct. However, Word2Vec will automatically remove words that do not occur a minimum number of times in the vocabulary (all sentences combined). By default this value is 5. In your case, it is highly likely that no word occurs 5 or more times in the data you use.  To change the minimum required word occurrences use setMinCount(), for example a min count of 2:Let us take a look at the simple class: This is a fairly straight forward pytorch module. It creates a simple sequential dense network. If we check its hidden parameters, we see the following: This is the expected result ... Now, let us try to generalize this a bit: It turns out that the number of parameters within this module is no longer the same ... I believe understand why this is happening. The bigger question is, how do we overcome this problem? The second, generalized method for example will not be sent to the GPU properly, and will not be trained by an optimizer. The problem is that most of the nn.Linear layers in the "generalized" version are stored in a regular pythonic list (self.fcLayers). pytorch does not know to look for nn.Paramters inside regular pythonic members of nn.Module.  Solution:
If you wish to store nn.Modules in a way that pytorch can manage them, you need to use specialized pytorch containers.
For instance, if you use nn.ModuleList instead of a regular pythonic list: your example should work fine. BTW,
you need pytorch to know that members of your nn.Module are modules themselves not only to get their parameters, but also for other functions, such as moving them to gpu/cpu, setting their mode to eval/training etc.I have the following (shortened) code I am trying to run: What is supposed to happen above is that: While iterating over File 1 during training, I occasionally want to calculate cost an accuracy on dev set data (from File 2) as well.
But when the inner loop finishes reading File 2, it obviously triggers the exception  "tf.errors.OutOfRangeError" which causes my code to leave the outer loop as well. The exception of inner loop simply handled as the exception of outer loop too. But after finishing reading the File 2, I want my code continue training over File 1 in the outer loop. (I have removed some details like num_epochs to train etc to simplify the readibility of the code) Does any one have any suggestion regarding how to solve this problem? I am a bit new in this. Thank you in advance! Solved. Apparently, using queue_runners is not the right way of doing this. Tensorflow documentation indicates that dataset api should be used instead, which took its time to understand. The below code does what I was trying to do previously. Sharing here in case other people may need it as well. I have put some additional training code under www.github.com/loheden/tf_examples/dataset api. I struggled a bit to find complete examples.I have been trying to solve this for days, and although I have found a similar problem here How can i vectorize list using sklearn DictVectorizer, the solution is overly simplified. I would like to fit some features into a logistic regression model to predict 'chinese' or 'non-chinese'. I have a raw_name which I will extract to get two features 1) is just the last name, and 2) is a list of substring of the last name, for example, 'Chan' will give ['ch', 'ha', 'an']. But it seems Dictvectorizer doesn't take list type as part of the dictionary. From the link above, I try to create a function list_to_dict, and successfully, return some dict elements, but I have no idea how to incorporate that in the my_dict = ... before applying the dictvectorizer. Output: Sample data: If I have understood correctly you want a way to encode list values in order to have a feature dictionary that DictVectorizer could use. (One year too late but) something like this can be used depending on the case: Output: Another thing you could do (but I don't recommend) if you don't want to create as many features as the values in your lists is something like this: but the first one means that you can't have any duplicate values and probably both don't make good features, especially if you need fine-tuned and detailed ones. Also, they reduce the possibility of two rows having the same combination of two letter combinations, thus the classification probably won't do well. Output:I am training an LDA model in pyspark (spark 2.1.1) on a customers review dataset. Now based on that model I want to predict the topics in the new unseen text. I am using the following code to make the model Now I have a dataframe with a column having new customer reviews and I want to predict that to which topic cluster they belong. 
I have searched for answers, mostly the following way is recommended, as here Spark MLlib LDA, how to infer the topics distribution of a new unseen document?.  However, I get the following error: 'LDAModel' object has no attribute 'toLocal'.
Neither do it have topicDistribution attribute. So are these attributes not supported in spark 2.1.1? So any other way to infer topics from the unseen data? You're going to need to pre-process the new data:  Then you can just pass it through the trained LDA as a function.  All you need is that bow_corpus:  If you want it out in a csv try this:  I hope this helps :)Possible Duplicates:
How to optimal K in K - Means Algorithm
How do I determine k when using k-means clustering?  Depending on the statistical measures can we decide on the K. Like Standard Deviation, Mean, Variance etc.,
Or Is there any simple method to choose the K in K-means Algorithm? Thanks in advance
Navin If you explicitly want to use k-means you could study the article describing x-means. When using an implementation of x-means the only difference compared to k-means, is that rather than specifying a single k, you specify a range for k. The "best" choice, wrt. some measure, in the range will be part of the output from x-means. You can also look into the
Mean Shift clustering algorithm. If it is computationally feasible with your given data (possibly using sampling as yura suggests), you could do clustering with various k's and evalute the quality of the resulting clusters using some of the standard cluster validity measures. Some of the classic measures are described here: measures. @doug
It is not correct that k-means++ determines an optimal k for the number of clusters before cluster assignments start. k-means++ differs from k-means only by instead of randomly choosing the initial k centroids, it chooses one initial centroid randomly and successively chooses centers until k has been chosen. After the initial completely random choice, data points are chosen as a new centroid with a probability that is determined by a potential function which depends on the datapoint's distance to the already chosen centers. The standard reference for k-means++ is k-means++: The Advantages of Careful Seeding by Arthur and Vassilvitskii. Also, I don't think that in general choosing k to be the number of principal components will improve your clustering. Imagine data points in three-dimensional space all lying in a plane passing through the origo. You will then get 2 principal components, but the "natural" clustering of the points could have any number of clusters. Unfortunately not. There isn't a principled statistical method, simple or complex that can set the "right K". There are heuristics, rules of thumb that sometimes work, sometimes don't.  The situation is more general as many clustering methods have these type of parameters. Well there are two practical solutions to the the problem of intelligent selection
of the number of centroids (k) in common use. The first is to PCA your data, and the output from PCA--which is the 
principal components (eigenvectors) and their cumulate contribution to the variation
observed in the data--obviously suggests an optimal number of centroids.
(E.g., if 95% of the variability in your data is explained by the first three principal
components, then k=3 is a wise choice for k-means.) The second commonly used practical solution to intelligently estimate k is
is a revised implementation of the k-means algorithm, called k-means++. In essence,
k-means++ just differs from the original k-means by the additional of a pre-processing
step. During this step, the number and initial position of the centroids and estimated.  The algorithm that k-means++ relies on to do this is straightforward to understand and to implement in code. A good source for both is a 2007 Post in the LingPipe Blog, which offers an excellent 
explanation of k-means++ as well as includes a citation to the original paper that
first introduced this technique. Aside from providing an optimal choice for k, k-means++ is apparently superior to 
the original k-means in both performance (roughly 1/2 processing time compared 
with k-means in one published comparison) and accuracy (three orders of magnitude 
improvement in error in the same comparison study). Bayesian k-means may be a solution when you don't know the number of clusters. There's a related paper given in the website and the corresponding MATLAB code is also given. The best solution for unkown(by statistical paramters model etc) ML problem is to sample data and find parameters thet best for sub problem, then use them on full problem. In that case select best K for 5% of data.I am using keras to create a LSTM model. While training, I am getting this error. ValueError: Error when checking target: expected dense_4 to have shape (1,) but got array with shape (34,) Here is my model Model Summary: I am calling fit using  y_train is a one-hot-encoded label with shape (299, 34).
X_train is of shape (299, 15). I am not sure why model is looking for shape(1,) as I can see that dense_4 (Dense) has an output shape of `(None, 34). Ok, I found the issue. I am posting this as answer so that it can help others also who is  facing the same issue. It was not the layer configuration but the wrong loss function. I was using sparse_categorical_crossentropy as loss where labels must have the shape [batch_size] and the dtype int32 or int64. I have changed is to categorical_crossentropy which expect label of [batch_size, num_classes]. Error message thrown by keras was misleading.This is a follow up question from here. I am trying to implement k-means based on this implementation. It works great, but I would like to replace groupByKey() with reduceByKey(), but I am not sure how (I am not worried about performance now). Here is the relevant minified code: Notice that println(newCentroids) will give: Map(23 -> (-6.269305E-4, -0.0011746404, -4.08004E-5), 8 -> (-5.108732E-4, 7.336348E-4, -3.707591E-4), 17 -> (-0.0016383086, -0.0016974678, 1.45.. and println(closest): MapPartitionsRDD[6] at map at kmeans.scala:75 Relevant question: Using reduceByKey in Apache Spark (Scala). Some documentation: def reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)] Merge the values for each key using an associative reduce function. def reduceByKey(func: (V, V) ⇒ V, numPartitions: Int): RDD[(K, V)] Merge the values for each key using an associative reduce function. def reduceByKey(partitioner: Partitioner, func: (V, V) ⇒ V): RDD[(K, V)] Merge the values for each key using an associative reduce function. def groupByKey(): RDD[(K, Iterable[V])] Group the values for each key in the RDD into a single sequence. You could use an aggregateByKey() (a bit more natural than reduceByKey()) like this to compute newCentroids: For this to work you will need to compute the dimensionality of your data, i.e. dim, but you only need to do this once. You could probably use something like val dim = data.first._2.length.Iam using a densenet model for one of my projects and have some difficulties using regularization. Without any regularization, both validation and training loss (MSE) decrease. The training loss drops faster though, resulting in some overfitting of the final model.  So I decided to use dropout to avoid overfitting. When using Dropout, both validation and training loss decrease to about 0.13 during the first epoch and remain constant for about 10 epochs.  After that both loss functions decrease in the same way as without dropout, resulting in overfitting again. The final loss value is in about the same range as without dropout. So for me it seems like dropout is not really working.  If I switch to L2 regularization though, Iam able to avoid overfitting, but I would rather use Dropout as a regularizer. Now Iam wondering if anyone has experienced that kind of behaviour? I use dropout in both the dense block (bottleneck layer) and in the transition block (dropout rate = 0.5): Without any regularization, both validation and training loss (MSE) decrease. The training loss drops faster though, resulting in some overfitting of the final model.  This is not overfitting. Overfitting starts when your validation loss starts increasing, while your training loss continues decreasing; here is its telltale signature:  The image is adapted from the Wikipedia entry on overfitting - diferent things may lie in the horizontal axis, e.g. depth or number of boosted trees, number of neural net fitting iterations etc. The (generally expected) difference between training and validation loss is something completely different, called the generalization gap: An important concept for understanding generalization is the generalization gap, i.e., the difference between a model’s performance on training data and its performance on unseen data drawn from the same distribution. where, practically speaking, validation data is unseen data indeed. So for me it seems like dropout is not really working.  It can very well be the case - dropout is not expected to work always and for every problem.  Interesting problem, 
I would recommend plotting the validation loss and the training loss to see if it is really overfitting. if you see that the validation loss didn't change while the training loss dropped ( you will also probably see a large gap between them ) then it is overfitting.  If it is overfitting then try to reduce the number of layers or the number of nodes ( also play a little with the Dropout rate after you do that). Reducing the number of epochs could also be helpful.
If you would like to use a different method instead of dropout I would recommend using the Gaussian Noise layer.
Keras - https://keras.io/layers/noise/
TensorFlow - https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianNoiseI am running the example Tensorflow convolutional neural network (CNN) code from "Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow" (https://github.com/ageron/handson-ml3). I run it on VS code on Windows 11. When I run the code of Chapter 14 and step it to The kernel crashed, prompting: The CUDA and GPU drivers should have been successfully installed on my Windows system. For instance, when running I can see from Windows Task Manager that the GPU is running and the calculation of x3 takes ~2 seconds while the calculation of y3 takes up to minutes. I am happy to announce that I have solved this issue after searching for many solutions. Finally, after 2 weeks! This problem has been solved by simply installing the Zlib and CuDNN. Please see details here: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html. Other people who encountered such a problem may have been caused by running out of memory. Some people reported that they had to downgrade the CuDNN to a historical version to solve this issue. I am using v8.3 for CuDNN (I have not tested the most updated version v8.6). I got this same error when using the Torch library for AI on my M1 Mac when using net.to("mps") and I used the wrong input_size It does not seem like Torch throws an error when using mps, but instead the IPY kernel crashes. When I use net.yo("cpu") as device instead, it throws an error message like normal.So I have been trying for some days now to run ML algorithms inside a map function in Spark. I posted a more specific question but referencing Spark's ML algorithms gives me the following error: Obviously I cannot reference SparkContext inside the apply_classifier function.
My code is similar to what was suggested in the previous question I asked but still haven't found a solution to what I am looking for: I have tried using flatMap instead of map but I get NoneType object is not iterable. I would also like to pass a broadcasted dataset (which is a DataFrame) as parameter inside the apply_classifier function. 
Finally, is it possible to do what I am trying to do? What are the alternatives? is it possible to do what I am trying to do? It is not. Apache Spark doesn't support any form of nesting and distributed operations can be initialized only by the driver. This includes access to distributed data structures, like Spark DataFrame. What are the alternatives? This depends on many factors like the size of the data, amount of available resources, and choice of algorithms. In general you have three options: Use Spark only as task management tool to train local, non-distributed models. It looks like you explored this path to some extent already. For more advanced implementation of this approach you can check spark-sklearn. In general this approach is particularly useful when data is relatively small. Its advantage is that there is no competition between multiple jobs. Use standard multithreading tools to submit multiple independent jobs from a single context. You can use for example threading or joblib. While this approach is possible I wouldn't recommend it in practice. Not all Spark components are thread-safe and you have to pretty careful to avoid unexpected behaviors. It also gives you very little control over resource allocation. Parametrize your Spark application and use external pipeline manager (Apache Airflow, Luigi, Toil) to submit your jobs. While this approach has some drawbacks (it will require saving data to a persistent storage) it is also the most universal and robust and gives a lot of control over resource allocation.I have six different Sequnstial Keras models and I desire to concatenate them like the following: The output dimensions of all models are 1, but the input dimensions of models are different. The above code will run completely without any error. Now, the problem is for training the model. I will try to run the following code to train the model with one data instance (the dimension of each subarray is equal to the input dimension of the corresponding model): However, I have the following error: ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list). I found a related post, but it does not solve my problem. As far as I can tell we can't just pass all variable-size inputs together in fit for the multi-input model. The way you pass your training pairs to the model, it surely unable to unpack for concern input layers. The related post that you mentioned is also an important fact to consider. However, in tensorflow,  we can use tf.ragged.RaggedTensor for variable-length input sequence, discussed here. But not sure if there is any workaround possible by converting to the ragged tensor. It probably would possible if a single input layer takes a different length of the input sequence. If you read the fit method's training pairs input you would see keras expects the x paramer as follows: For your case, option 3 is pretty convenient to choose from, which is passing the dictionary mapped input names with training pairs. Here is one way we can do this. First set some names to the input layer of each model. We set model1, model2, ... etc. Now, build whole the final model where we also set the last layer name, which is set here as target_concatenate. DataSet The sample data you provided is not legal for model training as we mentioned above. Firstly it should not be a list but numpy and secondly, for multi-input variable size, it convenient to pass them separately. When we call .fit, we will pass these datasets as dict mapping model's input and output names to the corresponding array. So, let's check to get the names of the composed model. Great, now we can pass training Paris as follow conveniently to the fit method.Trying to understand how SVM-OVR (One-Vs-Rest) works, I was testing the following code: The outputs are: It means that the sample [1,2] is correctly predicted in the class 100 (it is quite obvious since [1,2] was used also for training). But, let's give a look to the decision functions. SVM-OVA should generate three classifiers, i.e., three lines. The first that separates class1 from class2 U class3, the second that separates class2 from class1 U class3, and the third that separates class3 from class1 U class2. My original goal was exactly to understand what the decision function values mean. I knew that positive values mean that the sample is in the right side of the plane, and viceversa; and that larger is the value then larger is the distance of the sample between the hyperplane (a line in this case), and then larger is the confidence that sample belongs to that class. However, something was clearly wrong since two decision function values are positive, while it was supposed that just the correct class should report a positive decision function (since the predicted value is also a training sample). For this reason, I tried to plot the separating lines. This is what I obtained:  Surprise: indeed, those separating lines represent the hyperplanes when the OVO (One-Vs-One) strategy is computed: indeed, you can notice that those lines separate class1 from class2, class2 from class3 and class1 from class3. I've also tried to add a class: and what happens is that the vector representing the decision functions has length equals to 4 (accordingly with the OVA strategy), but again 6 lines are generated (as if I had implemented the OVO strategy). My final questions: what do the decision function values represent? Why even when applying OVA strategy, n(n-1)/2 hyperplanes are generated, instead of n ones? The point is that, by default, SVM do implement an OvO strategy (see here for reference). SVC and NuSVC implement the “one-versus-one” approach for multi-class classification. At the same time, by default (even though in your case you have made it explicit) decision_function_shape is set to be 'ovr'. "To provide a consistent interface with other classifiers, the decision_function_shape option allows to monotonically transform the results of the “one-versus-one” classifiers to a “one-vs-rest” decision function of shape (n_samples, n_classes). The reason why an OvO strategy is implemented is that SVM algos scale poorly with the size of the training set (and with the OvO strategy each classifier is only trained on the part of the training set which corresponds to the classes it has to distinguish).
In principle, you can force a SVM classifier to implement an OvA strategy via an instance of OneVsRestClassifier, eg:I want to calculate group fairness metrics using AIF360. This is a sample dataset and model, in which gender is the protected attribute and income is the target. It throws out: Similar error for disparate_impact_ratio. It seems the data needs to be entered differently, but I have not been able to figure out how. This can be done by transforming the data to a StandardDataset followed by calling the fair_metrics function below: which returns the correct results (image ref):  Remove the y_true= and y_pred= characters in the function call and retry. As one can see in the documentation, *y within the function prototype stands for arbitrary number of arguments (see this post). So this is the most logical guess. In other words, y_true and y_pred are NOT keyword arguments. So they cannot be passed with their names. Keyword arguments are expressed as **kwargs within a function prototype. I had the same problem. The y_pred_default was array type and the whole dataset was Dataframe. But if you convert the y_pred_default to dataframe you will lose the order of the values and as a result it will show nan values to the new dataset. So i converted the dataset to numpy array, then concat with the y_pred_default array and convert to dataframe. Also you have to change the column names as they were first because now there are numbers. By doing this you have exactly what you want. A dataframe with your x values and the corresponding y predicted values in order to count the spd metric.I've got a 3-class classification problem. Let's define them as classes 0,1 and 2. In my case, class 0 is not important - that is, whatever gets classified as class 0 is irrelevant. What's relevant, however, is accuracy, precision, recall, and error rate only for classes 1 and 2. I would like to define an accuracy metric that only looks at a subsection of the data that relates to 1 and 2 and gives me a measure of that as the model is training. I am not asking for code for accuracy or f1 or precision/recall - those I've found and can implement myself. What I'm asking is for code that can help select a subsection of the categories to perform these metrics on. 
Visually, with a confusion matrix:
Given:  I would like to only perform an accuracy measure in-training for the following subset only: Possible idea:
Concatenate a categorized, argmaxed y_pred and argmaxed y_true, drop all instances where 0 appears, re-unravel them back into a one_hot array, and do a simple binary accuracy on what remains? Edit: 
I've tried to exclude the 0-class through this code, but it doesn't make sense. the 0-category gets effectively wrapped into the 1-category (that is, the true positives of both 0 and 1 end up being labeled as 1). Still looking for help - can anybody help out please? @ Ashwin Geet D'SaHi I am working on Key Point Analysis Task, which is shared by IBM, here is the link. In the given dataset there are more than one rows of text and anyone can please tell me how can I convert the text columns into tensors and again assign them in the same dataFrame because there are other columns of data there.  Here I am facing a problem that I have never seen this kind of data before like have multiple text columns, How can I convert all those columns into tensors and then apply a model. Most of the time data is like : One Text Column
and other columns are label, Example: Movie Reviews , Toxic Comment classification. If I got your question right you will do sth like the following:
 This will convert sentences into token arrays.I am trying to build a machine learning model which predicts a single number from a series of numbers. I am using an LSTM model with Tensorflow. You can imagine my dataset to look something like this: Easily said I just want my model to predict a number (y data) from a sequence of numbers (x data). For example like this: x and y data From my x data I created a numpy array x_train which I want to use to train the network.
Because I am using an LSTM network, x_train should be of shape (samples, time_steps, features).
I reshaped my x_train array to be shaped like this: (57, 10000, 1), because I have 57 samples, which each are of length 10000 and contain a single number. The y data was created similarly and is of shape (57,1) because, once again, I have 57 samples which each contain a single number as the desired y output. Current model attempt My model summary looks like this:
 The model was compiled with model.compile(loss="mse", optimizer="adam") so my loss function is simply the mean squared error and as an optimizer I'm using Adam. Current results Training of the model works fine and I can see that the loss and validation loss decreases after some epochs.
The actual problem occurs when I want to predict some data y_verify from some data x_verify.
I do this after the training is finished to determine how well the model is trained.
In the following example I simply used the data I used for training to determine how well the model is trained (I know about overfitting and that verifying with the training set is not the right way of doing it, but that is not the problem I want to demonstrate right not). In the following graph you can see the y data I provided to the model in blue.
The orange line is the result of calling model.predict(x_verify) where x_verify is of the same shape as x_train.  I also calculated the mean absolute percentage error (MAPE) of my prediction and the actual data and it came out to be around 4% which is not bad, because I only trained for 40 epochs. But this result still is not helpful at all because as you can see in the graph above the curves do not match at all. Question: What is going on here? Am I using an incorrect loss function? Why does it seem like the model tries to predict a single value for all samples rather than predicting a different value for all samples like it's supposed to be? Ideally the prediction should be the y data which I provided so the curves should look the same (more or less). Do you have any ideas? Thanks! :) After some back and forth in the comments, I'll give my best estimation to your questions: What is going on here? Very complex (too many layers deep) model with very little data, trained for too few epochs on non-normalized data (credit to Muhammad in his answer). The biggest issue, as far as I can tell, is the number of training epochs. Am I using an incorrect loss function? MSE is an appropriate loss function for a regression task. Why does it seem like the model tries to predict a single value for all samples rather than predicting a different value for all samples like it's supposed to be? Ideally the prediction should be the y data which I provided so the curves should look the same (more or less). Do you have any ideas? Too few training epochs is the biggest contributor, as far as I can tell. Based on the collab notebook that Luca shared: 30 Epochs, no normalization Way off target, flat predictions (though I can't reproduce how flat the predictions are that Luca posted)  30 Epochs, with normalization Worse off.  2000(!) epochs, no normalization Okay, now the predictions are at least in the ballpark  2000 epochs, with normalization And now the model seems to be starting to figure things out, like we'd hope it should. Given, this is training on the 11 samples that were cobbled together in the notebook, so it's naturally going to overfit. We're just happy to see it learn something.  2000 epochs, normalization, different loss Never be afraid to try out different losses, as some may be better suited than others. Not knowing the domain of this task, I'm just trying out mean_absolute_error instead of mean_squared_error.  Caution! Don't compare loss values between different losses. They're not on the same scale. 2000 epochs, normalization, larger learning rate Okay, so it's taking a long time to learn. Can I nudge it along a little faster? Sure, up the learning rate of the optimizer, and it'll get you to where you're going faster. Here, we up it by a factor of 5. You could even employ a learning rate scheduler that starts big and slowly diminishes it over the course of epochs. Hope this all helps! From the notebook it seems you are not scaling your data. You should normalize or standardize your data before training your model. https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/ can add normalization layer in keras https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization I just wanted to post a quick update.
First of all, this is my current result:
 I am absolutely happy, that I was finally able to achieve what I wanted to. At least to some extent. There were some steps I had to take to achieve this result: In the end my thought of "the more data, the better" was a huge misconception. I was not able to achieve such results with 10000 time steps per sample AT ALL. So I'm glad that I just gave 1000 a shot. Thank you all very much for your answers!
I will try to further imroved my model with your suggestions :) i think it would be helpful if you change loss into huber loss and even change optimizer into sgd and then first try out to define the best learning rate based on a callback (learning rate schedule) cause of small dataset and even normalize or standardize data before training model.I want to write a Naive Base text classificator. 
Because sklearn does not accept 'text form' features I am transforming them using TfidfVectorizer.  I was successfully able to create such classificatory using only the transformed data as features. The code looks like this: Everything works as intended but I am having problems when I want to add another feature eg. flag indicating weather the given text contains a certain keyword.
I tried multiple things to properly transform the 'url' feature and then combine the transformed feature with another boolean feature but I was unsuccessfully.
Any tips how it should be done assuming that I have a pandas frame containing two features: 'url' (which I want to transform) and 'contains_keyword' flag?  The solution which failed looks like this: This produces rows containing a boolean flag and a list which is a wrong input for sklearn model. I have no idea how should i properly transform this. Grateful for any help. Here are test data: url - splitted domain taken from dns query target - target class for classification ads_keyword - flag indicating weather the 'url' contains the 'ads' word. I want to transform the 'url' using the TfidfVectorizer and use the transformed data together with 'ads_keyword' (and possibly more features in the future) as features used to train the Naive Bayes model. Here is a demo, showing how to union features and how to tune up hyperparameters using GridSearchCV. Unfortunately your sample data set is too tiny to train a real model...I followed this great tutorial and successfully trained a model (on CloudML). My code also makes predictions offline, but now I am trying to use Cloud ML to make predictions and have some problems. To deploy my model I followed this tutorial. Now I have a code that generates TFRecords via apache_beam.io.WriteToTFRecord and I want to make predictions for those TFRecords. To do so I am following this article, my command looks like this: But I get only errors:
'Exception during running the graph: Expected serialized to be a scalar, got shape: [64] It seems like it expect data in a different format. I found the format specs for JSON here, but couldn't find how to do it with TFrecords.  UPDATE: here is the output of saved_model_cli show --all --dir When you export your model, you need to make sure that it is "batchable", i.e., the outer dimension of the input Placeholder has shape=[None], e.g., That may require reworking the graph a bit. For instance, I see that the shape of your output is hard-coded to [1,1]. The outermost dimension should be None, this may happen automatically when you fix the placeholder, or it may require other changes. Given that the name of the output is probabilities, I would also expect the innermost dimension to be >1, i.e. the number of classes being predicted, so something like [None, NUM_CLASSES].I am doing a song genre classification (2 classes). For each song, I have chopped them into small frames (5s) to generate MFCC as input features for a neural network and each frame has an associated song genre label. The data looks like the following: I know that I can randomly pick say 80% of songs (their small frames) as training data and the rest as testing. But now the way I write X_train is a frame at the frame level and biney cross-entropy loss function is defined at the frame level. I am wondering how I can customize the loss function such that it is minimized over the aggregation (e.g. majority vote of each frame prediction of the song) of frame level prediction.  currently, what I have is: Also, when I feed into the training and testing data into keras the corresponding ID (name) of the data is lost, is keeping the data (name, lebel, and feature) in a separate pandas dataframe and matching back the prediction from keras a good practice? or are there other good alternatives? Thanks in advance! A customized loss function is usually not needed for genre classification.
A combined model a song split into multiple prediction windows can be setup with Multiple Instance Learning (MIL). MIL is a supervised learning approach where the label not on each independent sample (instances), but instead of a "bag" (unordered set) of instances.
In your case the instance is each 5 second window of MFCC features, and the bag is the entire song. In Keras we use TimeDistributed layer to execute our model for all windows.
Then we combine the result using GlobalAveragePooling1D, effectively
implementing mean-voting across the windows. This is more easily differentiable than majority voting. Below is a runnable example: The example outputs the inner and combined model summaries: And the shape of the feature vector fed to the model: 8 songs, 23 windows each, with 13 MFCC bands, 216 frames in each window.
And a fifth dimension sized 1 to make Keras happy...I'm trying to make categorical cross entropy loss function to better understand intuition behind it.
So far my implementation looks like this: However I do not understand how function should behave to return correct value when: Regarding y_pred being 0 or 1, digging into the Keras backend source code for both binary_crossentropy and categorical_crossentropy, we get: from where you can clearly see that, in both functions, there is a clipping operation of the output (i.e. predictions), in order to avoid infinities from the logarithms: So, here y_pred will never be exactly 0 or 1 in the underlying calculations. The handling is similar in other frameworks. Regarding y_true being 0, there is not any issue involved - the respective terms are set to 0, as they should be according to the mathematical definition.I was going through the omniglot maml example and saw that they have net.train() at the top of their testing code. This seems like a mistake since that means the stats from each task at meta-testing is shared: however whenever I do eval instead I get that my MAML model diverges (though my test is on mini-imagenet): note: related: TLDR: Use mdl.train() since that uses batch statistics (but inference will not be deterministic anymore). You probably won't want to use mdl.eval() in meta-learning. BN intended behaviour: This is likely why I don't see divergence in my testing with the mdl.train(). So just make sure you use mdl.train() (since that uses batch statistics https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d) but that either the new running stats that cheat aren't saved or used later.How can i get know which variables are actually used in a constructed tree? I can see the variables if i write: BUT how can i get in a vector, the indices of which variables are actually used in? You can look at the structure of an object using the str() function. While looking in there you should see a few different places to extract the variables used to make your tree model, here is one example: EDIT: And since you specifically asked for the indices, you can just match() those back the variable names in your dataset (although they may always be in order - I haven't used the tree package before so I can't say). EDIT2: You're right! Try this: I think this is what you're looking for fit <- rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris)
used.var <- setdiff(levels(fit$frame$var), "<leaf>") Quite a while since, and using package rpart instead of tree. I think Brian Ripley's solution used in rpart coded in rpart::printcp() could still be of interest. It goes like this: If you are willing to switch to similar package rpart you can get used variables ordered by importance directly from fitI am working on a fraud analytics project and I need some help with boosting. Previously, I used SAS Enterprise Miner to learn more about boosting/ensemble techniques and I learned that boosting can help to improve the performance of a model. Currently, my group have completed the following models on Python: Naive Bayes, Random Forest, and Neural Network We want to use XGBoost to make the F1-score better. I am not sure if this is possible since I only come across tutorials on how to do XGBoost or Naive Bayes on its own. I am looking for a tutorial where they will show you how to create a Naive Bayes model and then use boosting. After that, we can compare the metrics with and without boosting to see if it improved. I am relatively new to machine learning so I could be wrong about this concept. I thought of replacing the values in the XGBoost but not sure which one to change or if it can even work this way. Naive Bayes XGBoost In theory, boosting any (base) classifier is easy and straightforward with scikit-learn's AdaBoostClassifier. E.g. for a Naive Bayes classifier, it should be: and so on. In practice, we never use Naive Bayes or Neural Nets as base classifiers for boosting (let alone Random Forests, which are themselves an ensemble method). Adaboost (and similar boosting methods that have been derived afterwards, like GBM and XGBoost) was conceived using decision trees (DTs) as base classifiers (more specifically, decision stumps, i.e. DTs with a depth of only 1); there is good reason why still today, if you don't specify explicitly the base_classifier argument in scikit-learn's AdaBoostClassifier above, it assumes a value of DecisionTreeClassifier(max_depth=1), i.e. a decision stump. DTs are suitable for such ensembling because they are essentially unstable classifiers, which is not the case with the other algorithms mentioned, hence the latter are not expected to offer anything when used as base classifiers for boosting algorithms.how to calculate the total number of params in a CNN network here is the code: here is the result
 How to get 320, 18496, 73856, 590336, 2052, could anyone explain it? You can use this general formula: channels_in * kernel_width * kernel_height * channels_out + num_channels So the first example: 1 * 3 * 3 * 32 + 32 = 320 And the second: 32 * 3 * 3 * 64 + 64 = 18,496 The addition of the number of channels is the bias terms.Here is the config of my model :  Once I finished to train a model, I decided to compare what the confusion matrix looks like if I shuffle or not the dataset and the labels.  I shuffled with the line Be aware X and label are two testing sets. So it is not related to the training sets. Confusion matrix with a shuffling phase  Confusion matrix without a shuffling phase As you can see here, the precision for both reports are significantly different. What can explain the gap between those two reports? Data shuffling never hurts performance, and it very often helps, the reason being that it breaks possible biases during data preparation - e.g. putting all the cat images first and then the dog ones in a cat/dog classification dataset.  Take for example the famous iris dataset: As you can clearly see, the dataset has been prepared in such a way that the first 50 samples are all of label 0, the next 50 of label 1, and the last 50 of label 2. Try to perform a 5-fold cross validation in such a dataset without shuffling and you'll find most of your folds containing only a single label; try a 3-fold CV, and all your folds will include only one label. Bad... BTW, it's not just a theoretical possibility, it has actually happened. Since it's very difficult to know beforehand that such bias may exist in our dataset, we always shuffle (as said, it never hurts), just to be on the safe side, and that's why shuffling is a standard procedure in all machine learning pipelines.  So, even if the situation here obviously depends on the details of your data (which we don't know), this behavior is not at all surprising - on the contrary, it is totally expected. Your number of class 0 and class 1 for both confusion matrix is off by one.  You need to make sure that there is no mistake on matching the data to the class label.I am trying to create a simple CNN to classify images in MNIST dataset. The model achieved an acceptable accuracy but I noticed that the model is trained only on 1875 images in each epoch. What could be the cause of it? How can it be fixed? model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) model.fit(train_images,train_labels,epochs=5) screenshot of model in colab screenshot of trained model There's no problem with the training. Model is being trained on 1875 batches of 32 images each, not 1875 images. 1875*32 = 60000 imagesWanted to take something like this
https://github.com/fitzscott/AirQuality/blob/master/HiveDataTypeGuesser.java
and create a Hive UDAF to create an aggregate function that returns a data type guess. Does Spark have something like this already built-in?
Would be very useful for new wide datasets to explore data. Would be helpful for ML too, e.g. to decide categorical vs numerical variables. How do you normally determine data types in Spark? P.S. Frameworks like h2o automatically determine data type scanning a sample of data, or whole dataset. So then one can decide e.g. if a variable should be a categorical variable or numerical. P.P.S. Another use case is if you get an arbitrary data set (we get them quite often), and want to save as a Parquet table.
Providing correct data types make parquet more space effiecient (and probably more query-time performant, e.g. 
better parquet bloom filters than just storing everything as string/varchar). Does Spark have something like this already built-in? Partially. There are some tools in Spark ecosystem which perform schema inference like spark-csv or pyspark-csv and category inference (categorical vs. numerical) like VectorIndexer. So far so good. Problem is that schema inference has limited applicability, is not an easy task in general, can introduce hard to diagnose problems and can be quite expensive: Depending on a data representation it can be impossible to determine correct data type or inferred type can lead to information loss: Automatic schema inference can mask different problems with input data and if it is not supported by additional tools which can highlight possible issues it can be dangerous. Moreover any mistakes during data loading and cleaning can be propagated through complete data processing pipeline.  Arguably we should develop good understanding of input data before we even start to think about possible representation and encoding. Schema inference and / or category inference may require full data scan and / or large lookup tables. Both can be expensive or even not feasible on large datasets. Edit: It looks like schema inference capabilities on CSV files have been added directly to Spark SQL. See CSVInferSchema.I try to solve a common problem in medicine: the combination of a prediction model with other sources, eg, an expert opinion [sometimes heavily emphysised in medicine], called superdoc predictor in this post. This could be solved by stack a model with a logistic regression (that enters the expert opinion) as described on page 26 in this paper: Afshar P, Mohammadi A, Plataniotis KN, Oikonomou A, Benali H. From
Handcrafted to Deep-Learning-Based Cancer Radiomics: Challenges and
Opportunities. IEEE Signal Process Mag 2019; 36: 132–60. Available here I've tried this here without considering overfitting (I did not apply out of fold predictions of the lower learner): Example data Stacked models without considering out of fold predictions: Now I would like to consider out of fold predictions using the mlr3 package family according to this very helpful post: Tuning a stacked learner  Created on 2021-03-15 by the reprex package (v1.0.0) I think mlr3 / mlr3pipelines is well suited for your task. It appears that what you are missing is mainly the PipeOpSelect / po("select"), which lets you extract features based on their name or other properties and makes use of Selector objects. Your code should probably look something like This is what it looks like:  To train and evaluate the model, we need to create Task objects: The model can now be trained, can then be used for prediction, and the quality of the prediction can be evaluated. This works best if we turn the ensemble into a Learner: Get the prediction on the test set: The prediction was made on a Task, so it can be used directly measure performance against ground truth, e.g. using the "classif.auc" Measure: Two notes here:Trained TFLite model of faces and tried to implement it on android. on this line i got an error detector inputs are which parameter should i change it to work? is it problem in java or I should retrain model? Update:
the problem seems to be in tensorflow library, because of mismatch between input and output map. Hope anyone knows how to eliminate this mismatch. Debug variables are given below.
error line input output map Update 2:
First I installed demo app from https://www.tensorflow.org/lite/models/object_detection/overview#get_started.
It can detect many objects (person, cat, laptop and so on). Then I trained model
https://colab.research.google.com/drive/1Ezjt7iytvbtSDO2kgnzfLSoVT_lcKzBJ?usp=sharing  following this link https://www.tensorflow.org/lite/tutorials/model_maker_image_classification . Maybe I trained it not correct, because on original model tensors shapecopy is [1,10,4] shape and matches copy object shape. when I try to apply my model, shapecopy is [1,4] and it raises exception shape mismatch Used custom model has outputs that differs from used in example
To update app for changed model: investigate new (custom) model and determine input and output shapes (I prefer to use Netron app for this - open your custom TFLite model with it) change input and output data buffers in application to match corresponding inputs and outputs to your model: example you use matches with your input but output should be changed to (see near TFLiteObjectDetectionAPIModel.java@195) resultLabel = new float[1][4]; Map<Integer, Object> outputMap = new HashMap<>(); outputMap.put(0, resultLabel);I am new to Machine Learning & Deep Learning. I would like to clarify my doubt related to train_test_split before training I have a data set of size (302, 100, 5), where,  (207,100,5) belongs to class 0 (95,100,5)  belongs to class 1. I would like to perform Classification using LSTM (since, sequence Data) How can i split my data set for training, since the classes do
not have equal distribution sets ?  Option  1 : Consider whole data [(302,100, 5) - both classes (0 & 1)], shuffle it, train_test_split,
proceed training.  Option 2 : Split both class data set equally
[(95,100,5) - class 0 & (95,100,5) - class 1], shuffle it,
train_test_split, proceed training. What will be the better way of splitting before training, so that i can get better results in terms of loss reduction, accuracy, prediction, ?  If there are other options rather than above 2 options, kindly recommend, Based on the comment section i include a part of my data : X_train : shape (241 * 100 * 5)  Each row in every 100*5 corresponds to 1 Time step
Finally 100 rows corresponds to 100 Time steps in milli seconds (ms)  Y_train : shape (241,)  For reference, 
As you can see above, X-train data is large, I cannot include complete set of my entire X_train data.  So I provide only one segment of my data here for better understanding of how my data looks like for 1 segment, (i.e X_train[0] : shape- (100*5)). The remaining 240 will more or less looks like below  TLDR: Try both!
 However, once I stumbled upon the problem of handling imbalanced datasets and came across the techniques of overbalancing and underbalancing. To do this, I would recommend using the library: imblearn You will find various techniques there to handle the cases where one of your classes outnumbers the other one. I personally have used SMOTE a lot and have had relatively better success in such cases.
 https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/ https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28 You can use the stratify option in train test split, which splits each class on the mentioned test size.  I am working on a project where I am experimenting with credit dataset(imbalanced dataset containing 1% of a minority class and 99% of the majority class) for fraud detection using different sampling method and found that SMOTE gives better results with imbalanced datasets. SMOTE (Synthetic Minority Oversampling Technique) is a powerful sampling method that goes beyond simple under or oversampling. This algorithm creates new instances of the minority class by creating convex combinations of neighbouring instances I have used SMOTE sampling methods along with the K-Fold cross-validation. Cross-validation technique assures that model gets the correct patterns from the data, and it is not getting up too much noise. In the case of an imbalanced dataset, the accuracy score of sampling algorithm yields an accuracy of 99% which seems impressive, but minority class could be totally ignored in case of imbalanced datasets. So, I have used Matthew Coefficient Correlation Score, F1 Score measuring algorithm in addition to Accuracy for performance measurement on an Imbalanced Dataset. Code : References : https://www.kaggle.com/qianchao/smote-with-imbalance-dataI'm using the Kobe Bryant Dataset. 
I wish to predict the shot_made_flag with KnnRegressor.  I've used game_date to extract year and month features: and I wish to use season, year, month features to give them more weight in the distance function so events with closer date to the current event will be closer neighbors but still maintain reasonable distances to potential other datapoints, so for example I don't wish an event withing the same day would be the closest neighbor just because of the date features but it'll take into account the other features such as shot_range etc..

To give it more weight I've tried to use metric argument with custom distance function but the arguments of the function are just numpy array without column information of pandas so I'm not sure what I can do and how to implement what I'm trying to do. EDIT: Using larger weights for date features to find the optimal k with cv of 10 running on k from [1, 100]: Runs really slow, any idea on how to make it faster?
The idea of the weighted features is to find neighbors more close to the data point date to avoid data leakage and cv for finding optimal k. First, you have to prepare a numpy 1D weight array, specifying weight for each feature. You could do something like: You can use kobe_data_encoded.columns to find indexes of season, year, month  features in your dataframe to replace 2nd line above. Now define a distance function, which by guideline have to take two 1D numpy array.  And initialize KNeighborsRegressor as: EDIT:
To make things efficient, you can precompute distance matrix, and reuse it in KNN. This should bring in significant speedup by reducing calls to my_dist, since this non-vectorized custom python distance function is quite slow. So now -  I couldn't test it, so let me know if something isn't alright. Just add on Shihab's answer regarding distance computation. Can use scipy pdist as suggested in this post, which is faster and more efficient.The following code prints out leaf: This may or may not be accurate depending on the surrounding context, e.g. Mary leaves the room vs. Dew drops fall from the leaves. How can I tell NLTK to lemmatize words taking surrounding context into account? First tag the sentence, then use the POS tag as the additional parameter input for the lemmatization. For a detailed walkthrough of how and why the POS tag is necessary see https://www.kaggle.com/alvations/basic-nlp-with-nltk Alternatively, you can use pywsd tokenizer + lemmatizer, a wrapper of NLTK's WordNetLemmatizer: Install: Code:Clarifications: I'm running tensorflow 1.6.0-rc0 compiled from source with GPU support on a Macbook with an NVIDIA GeForce GT 750M 2048 MB GPU.  I've attempted to train like so: The initial clarifications I'm looking for are: Update2 Thanks to Farooq's helpful notes, here is a tweaked version of the code that prints a prediction to console: I've ran the above like so: and finally got a prediction: Not a great one (tutorial warns on dataset size and accuracy), but it's a prediction, yaaay! Full execution took 31 seconds with this example. python train_model.py \
--training_data=rnn_tutorial_data/training.tfrecord-?????-of-????? \
--eval_data=rnn_tutorial_data/eval.tfrecord-?????-of-????? \
--classes_file=rnn_tutorial_data/training.tfrecord.classes AFAIK using the above command works as well, it will simply read all the files in the folder where you have download the data files one by one. create_tfrecord_for_prediction is certainly not my own creation, this code was mostly picked from another file from tensorflow guys create_dataset.py Below I have pasted the almost all of the new code i added including the my modifications to the main() function FLAGS.predict_for_data this is the command line argument that holds the strokes data
FLAGS.predict_temp_file is just a name of file i use to create the temporary input data tfrecord file Note1 : Along with this I also modified some code in get_input_fn() you can find this code change in this PR: https://github.com/tensorflow/models/pull/3440 (has not been merged yet) Note2: I also had to modify model_fn() and add the below few lines my additions are after the comment #Compute current predictions The only thing left is then to figure out generating strokes data. For this you can take one of the existing tfrecord file read it and extract a stroke from that read operation or you could write some javascript webpage to generate strokesi'm having this error problem, i have ran this script in jupyter notebook in base (root) environment, the log said that gensim library has been installed and i have run the command !pip install gensim before i import it, but it still can not be imported, and the error said ModuleNotFoundError: No module named 'gensim' Is there anyone who can help this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention  It may be that your jupyter lab maybe running the base kernel and not the kernel of the virtual environment. Check by doing the following: into my notebook and got the result If you get the above instead of the below then that means you're using the wrong kernel. You can solve it by creating a new iPython kernel for your new environment. Read more here.I am running a Python 3 classification script on a server using the following code: I then save the GridSearchCV object using pickle: So I can test the classifiers on smaller datasets on my local machine by running: Which is great if I only want to evaluate the best estimator on a validation set.  But what I'd actually like to do is: GridSearchCV does not include the original data (and it would be arguably absurd if it did). The only data it includes is its own bookkeeping, i.e. the detailed scores & parameters tried per each CV fold. The best_estimator_ returned is the only thing needed to apply the model to any new data encountered, but if, as you say, you would like to dig deeper in the details, the full results are returned in its cv_results_ attribute. Adapting the example from the documentation to the knn classifier with your own knn_parameters grid (but removing n_jobs, which only affects the fitting speed, and it's not a real hyperparameter of the algorithm), and keeping cv=3 for simplicity, we have: So, as said, this last result tells you all you need to know to apply the algorithm to any new data (validation, test, from deployment etc). Also, you may find that actually removing the n_jobs entry from the knn_parameters grid and asking instead for n_jobs=-1 in the GridSearchCV object results in a much faster CV procedure. Nevertheless, if you want to use n_jobs=-1 to your final model, you can easily manipulate the best_estimator_ to do so: This actually answers your second question, since you can similarly manipulate the best_estimator_ to change other hyperparameters, too. So, having found the best model is where most people would stop. But if, for any reason, you want to dig further into the details of the whole grid search process, the detailed results are returned in the cv_results_ attribute, which you can even import to a pandas dataframe for easier inspection: For example, the cv_results dataframe includes a column rank_test_score which, as its name clearly implies, contains the rank of each parameter combination: Here 1 means best, and you can readily see that there are more than one combinations ranked as 1 - so in fact here we have more than one "best" models (i.e. parameter combinations)! Although here this is most probably due to the relative simplicity of the used iris dataset, there is no reason in principle why it cannot happen in a real case, too. In such cases, the returned best_estimator_ is just the first of these occurrences - here the combination number 4: which you can easily see that has the same parameters with our best_estimator_ above. But now you can inspect all the "best" models, simply by: which, in my case, results in no less than 144 models (out of the total 6*12*4*2 = 576 models tried)! So, you can in fact select among more choices, or even use other additional criteria, say the standard deviation of the returned score (the less the better, although here it is at the minimum value of 0), instead of relying simply to the maximum mean score, which is what the automatic procedure will return. Hopefully these will be enough to get you started...Just wrote up my first Neural Network Class in python. Everything as far as I can tell should work, but there is some bug in it that I can't seem to find(Probably staring me right in the face).
I first tried it on 10,000 examples of the MNIST data, then again when trying to replicate the sign function, and again when trying to replicate a XOR Gate. Every time, regardless of the # of epochs, it always produces output from all the output neurons(regardless of how many there may be) that are all roughly the same value, but the cost function seems to be going down.
I am using batch gradient descent, all done using vectors(no loop for each training example). I even implememented Gradient Checking and the values are different, and I thought I would try replacing the Back Propagation updates with the Approx. Gradient Checking values, but that gave the same results, causing me to doubt even my Gradient Checking code. Here are some of the values being produced when training for the XOR Gate: Back Prop. Grad: 0.0756102610697 0.261814503398 0.0292734023876
Grad Approx: 0.05302210631166 0.0416095559674 0.0246847342122
Cost: Before Training: 0.508019225507 After Training 0.50007095103 (After 10000 Epochs)
Output for 4 different examples(after training):
[ 0.49317733] [ 0.49294556] [ 0.50489004] [ 0.50465824] So my question is, is there any obvious problem with my Back Propagation, or my gradient checking? Are there any usual problems when a ANN shows these symptoms(Outputs are all roughly the same/Cost is going down)? I'm not very proficient at reading python code, but your gradient list for XOR contains 3 elements, corresponding for 3 weights. I assume, that these are two inputs and one bias for a single neuron. If true, such network can not learn XOR (minimun NN that can learn XOR need two hidden neurons and one output unit). Now, looking at Feedforward function, if np.dot computes what it name says (i.e dot product of two vectors), and sigmoid is scalar, then this will always correspond to output of one neuron and I don't see the way how you can add more neurons to the layers with this code.  Following advice could be useful to debug any newly implemented NN: 1) Don't start with MNIST or even XOR. Perfectly good implementation may fail to learn XOR because it can easily fell into local minima and you could spent a lot of time hunting for non-existent error. A good starting point will be AND function, that can be learned with single neuron 2) Check forward computation pass by manually computing results on few examples. thats easy to do with small number of weights. Then try to train it with numerical gradient. If it fails, then either your numerical gradient is wrong (check that by hand) or training procedure is wrong. (it can fail to work if you set too large learning rate, but otherwise training must converge since error surface is convex).  3) once you can train it with numerical grad, debug your analytical gradients (check gradient per neuron, and then gradient for individual weights). That again can be computed manually and compared to what you see. 4) Upon completion of step 3, if everything works OK, add one hidden layer and repeat steps 2 and 3 with AND function.  5) after everything works with AND, you can move to XOR function and other more complicated tasks. This procedure may seems time consuming, but it almost aways results in working NN in the endThis is the example keras code that I want to convert to pytorch. My input dataset is 10000*1*102 (two dimensions for labels). The dataset includes 10000 samples. Each sample contains one row with 102 features. I am thinking to use 1dcnn for regression. PS: hyper-parameter (e.g. filters, kernel_size, stride, padding) could be adjusted based on my 10000*1*102 dataset. Welcome to pytorch. :)
I am really glad you decide to switch from Keras to PyTorch. It was an important step for me to understand how NNs work in more detail. If you have any specific questions about code or if it isn't working please let me know. Name: torch Version: 1.11.0.dev20211231+cu113 This article has been very helpful. But the code seems to have changed a little as of the current standards, so I'm leaving a reply.When I try to install TensorFlow Machine Learning library on Ubunto (vmware image) using command : after downloading the package I got this error : Traceback (most recent call last):
        File "", line 14, in 
      IOError: [Errno 2] No such file or directory: '/tmp/pip-GgS7fR-build/setup.py'
      Complete output from command python setup.py egg_info:
      Traceback (most recent call last): File "", line 14, in  IOError: [Errno 2] No such file or directory:
  '/tmp/pip-GgS7fR-build/setup.py' I am using pip, python 2.7 and Ubuntu 12.04 LTS vmware image Can anyone please help me to solve this error? full pip.log file error : Try installing Ubuntu 14.04 in VMware and use the same command.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. What is the difference between a generative and a
discriminative algorithm? Let's say you have input data x and you want to classify the data into labels y. A generative model learns the joint probability distribution p(x,y) and a discriminative model learns the conditional probability distribution p(y|x) - which you should read as "the probability of y given x". Here's a really simple example. Suppose you have the following data in the form (x,y): (1,0), (1,0), (2,0), (2, 1) p(x,y) is p(y|x) is If you take a few minutes to stare at those two matrices, you will understand the difference between the two probability distributions. The distribution p(y|x) is the natural distribution for classifying a given example x into a class y, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model p(x,y), which can be transformed into p(y|x) by applying Bayes rule and then used for classification. However, the distribution p(x,y) can also be used for other purposes. For example, you could use p(x,y) to generate likely (x,y) pairs. From the description above, you might be thinking that generative models are more generally useful and therefore better, but it's not as simple as that. This paper is a very popular reference on the subject of discriminative vs. generative classifiers, but it's pretty heavy going. The overall gist is that discriminative models generally outperform generative models in classification tasks. A generative algorithm models how the data was generated in order to categorize a signal.  It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. Imagine your task is to classify a speech to a language. You can do it by either: or The first one is the generative approach and the second one is the discriminative approach. Check this reference for more details: http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf. In practice, the models are used as follows. In discriminative models, to predict the label y from the training example x, you must evaluate:  which merely chooses what is the most likely class y considering x. It's like we were trying to model the decision boundary between the classes. This behavior is very clear in neural networks, where the computed weights can be seen as a complexly shaped curve isolating the elements of a class in the space.  Now, using Bayes' rule, let's replace the  in the equation by . Since you are just interested in the arg max, you can wipe out the denominator, that will be the same for every y. So, you are left with  which is the equation you use in generative models.  While in the first case you had the conditional probability distribution p(y|x), which modeled the boundary between classes, in the second you had the joint probability distribution p(x, y), since p(x | y) p(y) = p(x, y), which explicitly models the actual distribution of each class. With the joint probability distribution function, given a y, you can calculate ("generate") its respective x. For this reason, they are called "generative" models. Here's the most important part from the lecture notes of CS299 (by Andrew Ng) related to the topic, which really helps me understand the difference between discriminative and generative learning algorithms. Suppose we have two classes of animals, elephant (y = 1) and dog (y = 0). And x is the feature vector of the animals.  Given a training set, an algorithm like logistic regression or the perceptron algorithm (basically) tries to find a straight line — that is, a decision boundary — that separates the elephants and dogs. Then, to classify
a new animal as either an elephant or a dog, it checks on which side of the
decision boundary it falls, and makes its prediction accordingly. We call these discriminative learning algorithm. Here's a different approach. First, looking at elephants, we can build a
model of what elephants look like. Then, looking at dogs, we can build a
separate model of what dogs look like. Finally, to classify a new animal, 
we can match the new animal against the elephant model, and match it against
the dog model, to see whether the new animal looks more like the elephants
or more like the dogs we had seen in the training set. We call these generative learning algorithm. The different models are summed up in the table below:
 Image source: Supervised Learning cheatsheet - Stanford CS 229 (Machine Learning) Generally, there is a practice in machine learning community not to learn something that you don’t want to. For example, consider a classification problem where one's goal is to assign y labels to a given x input. If we use generative model we have to model p(x) which is irrelevant  for the task in hand.  Practical limitations like data sparseness will force us to model p(x) with some weak independence assumptions. Therefore, we intuitively use discriminative models for classification. Many of the answers here rely on the widely-used mathematical definition [1]: Although very useful, this narrow definition assumes the supervised setting, and is less handy when examining unsupervised or semi-supervised methods. It also doesn't apply to many contemporary approaches for deep generative modeling. For example, now we have implicit generative models, e.g. Generative Adversarial Networks (GANs), which are sampling-based and don't even explicitly model the probability density p(x) (instead learning a divergence measure via the discriminator network). But we call them "generative models” since they are used to generate (high-dimensional [10]) samples. A broader and more fundamental definition [2] seems equally fitting for this general question: 
Image source Even so, this question implies somewhat of a false dichotomy [3]. The generative-discriminative "dichotomy" is in fact a spectrum which you can even smoothly interpolate between [4].  As a consequence, this distinction gets arbitrary and confusing, especially when many popular models do not neatly fall into one or the other [5,6], or are in fact hybrid models (combinations of classically "discriminative" and "generative" models). Nevertheless it's still a highly useful and common distinction to make. We can list some clear-cut examples of generative and discriminative models, both canonical and recent: There is also a lot of interesting work deeply examining the generative-discriminative divide [7] and spectrum [4,8], and even transforming discriminative models into generative models [9]. In the end, definitions are constantly evolving, especially in this rapidly growing field :) It's best to take them with a pinch of salt, and maybe even redefine them for yourself and others. An addition informative point that goes well with the answer by StompChicken above. The fundamental difference between discriminative models and generative models is: Discriminative models learn the (hard or soft) boundary between classes Generative models model the distribution of individual classes Edit: A Generative model is the one that can generate data. It models both the features and the class (i.e. the complete data). If we model P(x,y): I can use this probability distribution to generate data points - and hence all algorithms modeling P(x,y) are generative. Eg. of generative models Naive Bayes models P(c) and P(d|c) - where c is the class and d is the feature vector. Also, P(c,d) = P(c) * P(d|c) Hence, Naive Bayes in some form models, P(c,d) Bayes Net Markov Nets A discriminative model is the one that can only be used to discriminate/classify the data points.
You only require to model P(y|x) in such cases, (i.e. probability of class given the feature vector). Eg. of discriminative models: logistic regression Neural Networks Conditional random fields In general, generative models need to model much more than the discriminative models and hence are sometimes not as effective. As a matter of fact, most (not sure if all) unsupervised learning algorithms like clustering etc can be called generative, since they model P(d) (and there are no classes:P) PS: Part of the answer is taken from source A generative algorithm model will learn completely from the training data and will predict the response. A discriminative algorithm job is just to classify or differentiate between the 2 outcomes. All previous answers are great, and I'd like to plug in one more point.  From generative algorithm models, we can derive any distribution; while we can only obtain the conditional distribution P(Y|X) from the discriminative algorithm models(or we can say they are only useful for discriminating Y’s label), and that's why it is called discriminative model. The discriminative model doesn't assume that the X's are independent given the Y($X_i \perp X_{-i} | Y$) and hence is usually more powerful for calculating that conditional distribution.  My two cents:
Discriminative approaches highlight differences
Generative approaches do not focus on differences; they try to build a model that is representative of the class.
There is an overlap between the two.
Ideally both approaches should be used: one will be useful to find similarities and the other will be useful to find dis-similarities.  This article helped me a lot in understanding the concept.    In summary,   Some good reading material:  conditional probability , Joint PDFIs there a rule of thumb (or set of examples) to determine when to use genetic algorithms as opposed to neural networks (and vice-versa) to solve a problem? I know there are cases in which you can have both methods mixed, but I am looking for a high-level comparison between the two methods. From wikipedia: A genetic algorithm (GA) is a search technique used in computing to find exact or approximate solutions to optimization and search problems. and: Neural networks are non-linear statistical data modeling tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data. If you have a problem where you can quantify the worth of a solution, a genetic algorithm can perform a directed search of the solution space. (E.g. find the shortest route between two points) When you have a number of items in different classes, a neural network can "learn" to classify items it has not "seen" before. (E.g. face recognition, voice recognition) Execution times must also be considered. A genetic algorithm takes a long time to find an acceptable solution. A neural network takes a long time to "learn", but then it can almost instantly classify new inputs. A genetic algorithm (despite its sexy name) is, for most purposes, an optimization technique. It primarily boils down to you having a number of variables and wanting to find the best combination of values for these variables.  It just borrows techniques from natural evolution to get there. Neural networks are useful for recognizing patterns. They follow a simplistic model of the brain, and by changing a number of weights between them, attempt to predict outputs based on inputs. They are two fundamentally different entities, but sometimes the problems they are capable of solving overlap.   GAs generate new patterns in a structure that you define. NNs classify (or recognize) existing patterns based on training data that you provide. GAs perform well at efficiently searching a large state-space of solutions, and converging on one or more good solutions, but not necessarily the 'best' solution. NNs can learn to recognize patterns (via training), but it is notoriously difficult to figure out what they have learned, i.e. to extract the knowledge from them once trained, and reuse the knowledge in some other (non-NN). You are comparing two totally different things here.  Neural Networks are used for regression/classification - given a set of (x, y) examples, you want regress the unknown y for some given x. Genetic algorithms are an optimization technique. Given a function f(x), you want to determine the x which minimizes/maximizes f(x). There are many similarities between them, so I will only try to outline their differences. Are able to analyze online patterns (those that change over time). Generally, this is a time-varying sample that needs to be matched and predicted. Examples:  Used when you can code attributes that you think may contribute to a specific, non-changing problem. The emphasis is on being able to code these attributes (sometimes you know what they are) and that the problem is to a large degree unchanging (otherwise evolutions don't converge). Examples:  You can use genetic algorithms as an alternative to the backpropagation algorithm to update weights in neural networks. For an example of this refer to:
http://www.ai-junkie.com/ann/evolved/nnt1.html. Genetic algorithms (usually) work on discrete data (enums, integer ranges, etc.). A typical application for GAs is searching a discrete space for a "good enough" solution when the only available alternative is a brute-force search (evaluating all combinations). Neural networks, on the other hand, (usually) work on continuous data (floats, etc.). A typical application for NNs is function approximation, where you've got a set X of inputs and a set Y of related outputs, but the analytical function f: X → Y. Of course, there are thousands of variants of both, so the line between them is somewhat blurred. There is no rule of thumb. In many cases you can formulate your problem to make use of either of them. Machine learning is still an active area of research and which learning model to use can be debatable. GA's take sexy languages from evolution but you're waiting for your computer to stumble upon a solution through a random process. Study your data, make good assumptions, try to know what you want and pick an approach that can make good use of these. If your first choice gives poor results, know why it was so, and improve the algorithm itself or pick a better one.Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 4 years ago. Expectation Maximization (EM) is a kind of probabilistic method to classify data. Please correct me if I am wrong if it is not a classifier.  What is an intuitive explanation of this EM technique? What is expectation here and what is being maximized? Note: the code behind this answer can be found here. Suppose we have some data sampled from two different groups, red and blue:  Here, we can see which data point belongs to the red or blue group. This makes it easy to find the parameters that characterise each group. For example, the mean of the red group is around 3, the mean of the blue group is around 7 (and we could find the exact means if we wanted). This is, generally speaking, known as maximum likelihood estimation. Given some data, we compute the value of a parameter (or parameters) that best explains that data. Now imagine that we cannot see which value was sampled from which group. Everything looks purple to us:  Here we have the knowledge that there are two groups of values, but we don't know which group any particular value belongs to.  Can we still estimate the means for the red group and blue group that best fit this data? Yes, often we can! Expectation Maximisation gives us a way to do it. The very general idea behind the algorithm is this: These steps need some further explanation, so I'll walk through the  problem described above. I'll use Python in this example, but the code should be fairly easy to understand if you're not familiar with this language.  Suppose we have two groups, red and blue, with the values distributed as in the image above. Specifically, each group contains a value drawn from a normal distribution with the following parameters: Here is an image of these red and blue groups again (to save you from having to scroll up):  When we can see the colour of each point (i.e. which group it belongs to), it's very easy to estimate the mean and standard deviation for each each group. We just pass the red and blue values to the builtin functions in NumPy. For example: But what if we can't see the colours of the points? That is, instead of red or blue, every point has been coloured purple. To try and recover the mean and standard deviation parameters for the red and blue groups, we can use Expectation Maximisation. Our first step (step 1 above) is to guess at the parameter values for each group's mean and standard deviation. We don't have to guess intelligently; we can pick any numbers we like: These parameter estimates produce bell curves that look like this:  These are bad estimates. Both means (the vertical dotted lines) look far off any kind of "middle" for sensible groups of points, for instance. We want to improve these estimates. The next step (step 2) is to compute the likelihood of each data point appearing under the current parameter guesses: Here, we have simply put each data point into the probability density function for a normal distribution using our current guesses at the mean and standard deviation for red and blue. This tells us, for example, that with our current guesses the data point at 1.761 is much more likely to be red (0.189) than blue (0.00003).  For each data point, we can turn these two likelihood values into weights (step 3) so that they sum to 1 as follows: With our current estimates and our newly-computed weights, we can now compute new estimates for the mean and standard deviation of the red and blue groups (step 4). We twice compute the mean and standard deviation using all data points, but with the different weightings: once for the red weights and once for the blue weights. The key bit of intuition is that the greater the weight of a colour on a data point, the more the data point influences the next estimates for that colour's parameters. This has the effect of "pulling" the parameters in the right direction. We have new estimates for the parameters. To improve them again, we can jump back to step 2 and repeat the process. We do this until the estimates converge, or after some number of iterations have been performed (step 5). For our data, the first five iterations of this process look like this (recent iterations have stronger appearance):  We see that the means are already converging on some values, and the shapes of the curves (governed by the standard deviation) are also becoming more stable.  If we continue for 20 iterations, we end up with the following:  The EM process has converged to the following values, which turn out to very close to the actual values (where we can see the colours - no hidden variables): In the code above you may have noticed that the new estimation for standard deviation was computed using the previous iteration's estimate for the mean. Ultimately it does not matter if we compute a new value for the mean first as we are just finding the (weighted) variance of values around some central point. We will still see the estimates for the parameters converge. EM is an algorithm for maximizing a likelihood function when some of the variables in your model are unobserved (i.e. when you have latent variables).   You might fairly ask, if we're just trying to maximize a function, why don't we just use the existing machinery for maximizing a function.  Well, if you try to maximize this by taking derivatives and setting them to zero, you find that in many cases the first-order conditions don't have a solution.  There's a chicken-and-egg problem in that to solve for your model parameters you need to know the distribution of your unobserved data; but the distribution of your unobserved data is a function of your model parameters.   E-M tries to get around this by iteratively guessing a distribution for the unobserved data, then estimating the model parameters by maximizing something that is a lower bound on the actual likelihood function, and repeating until convergence: The EM algorithm  Start with guess for values of your model parameters E-step:  For each datapoint that has missing values, use your model equation to solve for the distribution of the missing data given your current guess of the model parameters and given the observed data (note that you are solving for a distribution for each missing value, not for the expected value).  Now that we have a distribution for each missing value, we can calculate the expectation of the likelihood function with respect to the unobserved variables.  If our guess for the model parameter was correct, this expected likelihood will be the actual likelihood of our observed data; if the parameters were not correct, it will just be a lower bound. M-step:  Now that we've got an expected likelihood function with no unobserved variables in it, maximize the function as you would in the fully observed case, to get a new estimate of your model parameters. Repeat until convergence. Here is a straight-forward recipe to understand the Expectation Maximisation algorithm: 1- Read this EM tutorial paper by Do and Batzoglou. 2- You may have question marks in your head, have a look at the explanations on this maths stack exchange page. 3- Look at this code that I wrote in Python that explains the example in the EM tutorial paper of item 1: Warning : The code may be messy/suboptimal, since I am not a Python developer. But it does the job. Technically the term "EM" is a bit underspecified, but I assume you refer to the Gaussian Mixture Modelling cluster analysis technique, that is an instance of the general EM principle. Actually, EM cluster analysis is not a classifier. I know that some people consider clustering to be "unsupervised classification", but actually cluster analysis is something quite different. The key difference, and the big misunderstanding classification people always have with cluster analysis is that: in cluster analaysis, there is no "correct solution". It is a knowledge discovery method, it is actually meant to find something new! This makes evaluation very tricky. It is often evaluated using a known classification as reference, but that is not always appropriate: the classification you have may or may not reflect what is in the data. Let me give you an example: you have a large data set of customers, including gender data. A method that splits this data set into "male" and "female" is optimal when you compare it with the existing classes. In a "prediction" way of thinking this is good, as for new users you could now predict their gender. In a "knowledge discovery" way of thinking this is actually bad, because you wanted to discover some new structure in the data. A method that would e.g. split the data into elderly people and kids however would score as worse as it can get with respect to the male/female class. However, that would be an excellent clustering result (if the age wasn't given). Now back to EM. Essentially it assumes that your data is composed of multiple multivariate normal distributions (note that this is a very strong assumption, in particular when you fix the number of clusters!). It then tries to find a local optimal model for this by alternatingly improving the model and the object assignment to the model. For best results in a classification context, choose the number of clusters larger than the number of classes, or even apply the clustering to single classes only (to find out whether there is some structure within the class!). Say you want to train a classifier to tell apart "cars", "bikes" and "trucks". There is little use in assuming the data to consist of exactly 3 normal distributions. However, you may assume that there is more than one type of cars (and trucks and bikes). So instead of training a classifier for these three classes, you cluster cars, trucks and bikes into 10 clusters each (or maybe 10 cars, 3 trucks and 3 bikes, whatever), then train a classifier to tell apart these 30 classes, and then merge the class result back to the original classes. You may also discover that there is one cluster that is particularly hard to classify, for example Trikes. They're somewhat cars, and somewhat bikes. Or delivery trucks, that are more like oversized cars than trucks. Other answers being good, i will try to provide another perspective and tackle the intuitive part of the question. EM (Expectation-Maximization) algorithm is a variant of a class of iterative algorithms using duality Excerpt (emphasis mine): In mathematics, a duality, generally speaking, translates concepts,
  theorems or mathematical structures into other concepts, theorems or
  structures, in a one-to-one fashion, often (but not always) by means
  of an involution operation: if the dual of A is B, then the dual of B
  is A. Such involutions sometimes have fixed points, so that the dual
  of A is A itself Usually a dual B of an object A is related to A in some way that preserves some symmetry or compatibility. For example AB = const Examples of iterative algorithms, employing duality (in the previous sense) are: In a similar fashion, the EM algorithm can also be seen as two dual maximization steps: ..[EM] is seen as maximizing a joint function of the parameters and of
  the distribution over the unobserved variables.. The E-step maximizes
  this function with respect to the distribution over the unobserved
  variables; the M-step with respect to the parameters.. In an iterative algorithm using duality there is the explicit (or implicit) assumption of an equilibrium (or fixed) point of convergence (for EM this is proved using Jensen's inequality) So the outline of such algorithms is: Note that when such an algorithm converges to a (global) optimum, it has found a configuration which is best in both senses (i.e in both the x domain/parameters and the y domain/parameters). However the algorithm can just find a local optimum and not the global optimum. i would say this is the intuitive description of the outline of the algorithm For the statistical arguments and applications, other answers have given good explanations (check also references in this answer) The accepted answer references the Chuong EM Paper, which does a decent job explaining EM. There is also a youtube video that explains the paper in more detail.  To recap, here is the scenario: In the case of the first trial's question, intuitively we'd think B generated it since the proportion of heads matches B's bias very well... but that value was just a guess, so we can't be sure.   With that in mind, I like to think of the EM solution like this: This may be an oversimplification (or even fundamentally wrong on some levels), but I hope this helps on an intuitive level! EM is used to maximize the likelihood of a model Q with latent variables Z. It's an iterative optimization. e-step:
given current estimation of Z calculate the expected loglikelihood function  m-step:
find theta which maximizes this Q GMM Example: e-step: estimate label assignments for each datapoint given the current gmm-parameter estimation m-step: maximize a new theta given the new label assigments K-means is also an EM algorithm and there is a lot of explaining animations on K-means. Using the same article by Do and Batzoglou cited in Zhubarb's answer, I implemented EM for that problem in Java. The comments to his answer show that the algorithm gets stuck at a local optimum, which also occurs with my implementation if the parameters thetaA and thetaB are the same. Below is the standard output of my code, showing the convergence of the parameters. Below is my Java implementation of EM to solve the problem in (Do and Batzoglou, 2008). The core part of the implementation is the loop to run EM until the parameters converge. Below is the entire code.I'm reading the paper below and I have some trouble , understanding the concept of negative sampling. http://arxiv.org/pdf/1402.3722v1.pdf Can anyone help , please? The idea of word2vec is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have The numerator is basically the similarity between words c (the context) and w (the target) word. The denominator computes the similarity of all other contexts ci and the target word w. Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts ci. Negative sampling is one of the ways of addressing this problem- just select a couple of contexts ci at random. The end result is that if cat appears in the context of food, then the vector of food is more similar to the vector of cat (as measures by their dot product) than the vectors of several other randomly chosen words (e.g. democracy, greed, Freddy), instead of all other words in language. This makes word2vec much much faster to train. Computing Softmax (Function to determine which words are similar to the current target word) is expensive since requires summing over all words in V (denominator), which is generally very large.  What can be done? Different strategies have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency (e.g hierarchical softmax). Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax (They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute like negative sampling). The loss function in Word2vec is something like:  Which logarithm can decompose into:  With some mathematic and gradient formula (See more details at 6) it converted to:  As you see it converted to binary classification task (y=1 positive class, y=0 negative class). As we need labels to perform our binary classification task, we designate all context words c as true labels (y=1, positive sample), and k randomly selected from corpora as false labels (y=0, negative sample). Look at the following paragraph. Assume our target word is "Word2vec". With window of 3, our context words are: The, widely, popular, algorithm, was, developed. These context words consider as positive labels. We also need some negative labels. We randomly pick some words from corpus (produce, software, Collobert, margin-based, probabilistic) and consider them as negative samples. This technique that we picked some randomly example from corpus is called negative sampling.    Reference : I wrote an tutorial article about negative sampling here. Why do we use negative sampling? -> to reduce computational cost The cost function for vanilla Skip-Gram (SG) and Skip-Gram negative sampling (SGNS) looks like this:  Note that T is the number of all vocabs. It is equivalent to V. In the other words, T = V. The probability distribution p(w_t+j|w_t) in SG is computed for all V vocabs in the corpus with:  V can easily exceed tens of thousand when training Skip-Gram model. The probability needs to be computed V times, making it computationally expensive. Furthermore, the normalization factor in the denominator requires extra V computations.  On the other hand, the probability distribution in SGNS is computed with:  c_pos is a word vector for positive word, and W_neg is word vectors for all K negative samples in the output weight matrix. With SGNS, the probability needs to be computed only K + 1 times, where K is typically between 5 ~ 20. Furthermore, no extra iterations are necessary to compute the normalization factor in the denominator.  With SGNS, only a fraction of weights are updated for each training sample, whereas SG updates all millions of weights for each training sample.  How does SGNS achieve this? -> by transforming multi-classification task into binary classification task. With SGNS, word vectors are no longer learned by predicting context words of a center word. It learns to differentiate the actual context words (positive) from randomly drawn words (negative) from the noise distribution.  In real life, you don't usually observe regression with random words like Gangnam-Style, or pimples. The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.  In the above figure, current positive word-context pair is (drilling, engineer). K=5 negative samples are randomly drawn from the noise distribution: minimized, primary, concerns, led, page. As the model iterates through the training samples, weights are optimized so that the probability for positive pair will output p(D=1|w,c_pos)≈1, and probability for negative pairs will output p(D=1|w,c_neg)≈0.Can someone please explain this? I know bidirectional LSTMs have a forward and backward pass but what is the advantage of this over a unidirectional LSTM? What is each of them better suited for? LSTM in its core, preserves information from inputs that has already passed through it using the hidden state. Unidirectional LSTM only preserves information of the past because the only inputs it has seen are from the past. Using bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards you preserve information from the future and using the two hidden states combined you are able in any point in time to preserve information from both past and future. What they are suited for is a very complicated question but BiLSTMs show very good results as they can understand context better, I will try to explain through an example. Lets say we try to predict the next word in a sentence, on a high level what a unidirectional LSTM will see is The boys went to .... And will try to predict the next word only by this context, with bidirectional LSTM you will be able to see information further down the road for example Forward LSTM: The boys went to ... Backward LSTM: ... and then they got out of the pool You can see that using the information from the future it could be easier for the network to understand what the next word is. Adding to Bluesummer's answer, here is how you would implement Bidirectional LSTM from scratch without calling BiLSTM module. This might better contrast the difference between a uni-directional and bi-directional LSTMs. As you see, we merge two LSTMs to create a bidirectional LSTM. You can merge outputs of the forward and backward LSTMs by using either {'sum', 'mul', 'concat', 'ave'}. In comparison to LSTM, BLSTM or BiLSTM has two networks, one access pastinformation in forward direction and another access future in the reverse direction. wiki A new class Bidirectional is added as per official doc here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional and  activation function can be added like this: Complete example using IMDB data will be like this.The result after 4 epoch. BiLSTM or BLSTM Another use case of bidirectional LSTM might be for word classification in the text. They can see the past and future context of the word and are much better suited to classify the word.  It can also be helpful in Time Series Forecasting problems, like predicting the electric consumption of a household. However, we can also use LSTM in this but Bidirectional LSTM will also do a better job in it.I'm trying to use deep learning to predict income from 15 self reported attributes from a dating site. We're getting rather odd results, where our validation data is getting better accuracy and lower loss, than our training data. And this is consistent across different sizes of hidden layers.
This is our model: And this is an example of the accuracy and losses:
 and . We've tried to remove regularization and dropout, which, as expected, ended in overfitting (training acc: ~85%). We've even tried to decrease the learning rate drastically, with similiar results. Has anyone seen similar results? This happens when you use Dropout, since the behaviour when training and testing are different.  When training, a percentage of the features are set to zero (50% in your case since you are using Dropout(0.5)). When testing, all features are used (and are scaled appropriately). So the model at test time is more robust - and can lead to higher testing accuracies. You can check the Keras FAQ and especially the section "Why is the training loss much higher than the testing loss?".  I would also suggest you to take some time and read this very good article  regarding some "sanity checks" you should always take into consideration when building a NN.  In addition, whenever possible, check if your results make sense. For example, in case of a n-class classification with categorical cross entropy the loss on the first epoch should be -ln(1/n). Apart your specific case, I believe that apart from the Dropout the dataset split may sometimes result in this situation. Especially if the dataset split is not random (in case where temporal or spatial patterns exist) the validation set may be fundamentally different, i.e less noise or less variance, from the train and thus easier to to predict leading to higher accuracy on the validation set than on training.  Moreover, if the validation set is very small compared to the training then by random the model fits better the validation set than the training.] This indicates the presence of high bias in your dataset. It is underfitting. The solutions to issue are:-  Probably the network is struggling to fit the training data. Hence, try a 
little bit bigger network. Try a different Deep Neural Network. I mean to say change the architecture
a bit. Train for longer time. Try using advanced optimization algorithms. This actually a pretty often situation. When there is not so much variance in your dataset you could have the behaviour like this. Here you could find an explaination why this might happen. There are a number of reasons this can happen.You do not shown any information on the size of the data for training, validation and test. If the validation set is to small it does not adequately represent the probability distribution of the data. If your training set is small there is not enough data to adequately train the model. Also your model is very basic and may not be adequate to cover the complexity of the data. A drop out of 50% is high for such a limited model. Try using an established model like MobileNet version 1. It will be more than adequate for even very complex data relationships. Once that works then you can be confident in the data and build your own model if you wish.
Fact is validation loss and accuracy do not have real meaning until your training accuracy
gets reasonably high say 85%. I solved this by simply increasing the number of epochs Adding dropout to your model gives it more generalization, but it doesn't have to be the cause. It could be because your data is unbalanced (has bias) and that's what I think.. I don't think that it is a drop out layer problem. I think that it is more related to the number of images in your dataset. The point here is that you are working on a large training Set and a too small validation/test set so that this latter is way too easy to computed. Try data augmentation and other technique to get your dataset bigger! I agree with @Anas answer, the situation might be solved after you increase the epoch times.
Everything is ok, but sometimes, it is just a coincidence that the initialized model exhibits a better performance in the validation/test dataset compared to the training dataset.Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 6 years ago. I'm Working on document classification tasks in java. Both algorithms came highly recommended, what are the benefits and disadvantages of each and which is more commonly used in the literature for Natural Language Processing tasks?  At the very basics of it, the major difference between the porter and lancaster stemming algorithms is that the lancaster stemmer is significantly more aggressive than the porter stemmer. The three major stemming algorithms in use today are Porter, Snowball(Porter2), and Lancaster (Paice-Husk), with the aggressiveness continuum basically following along those same lines. Porter is the least aggressive algorithm, with the specifics of each algorithm actually being fairly lengthy and technical. Here is a break down for you though: Porter: Most commonly used stemmer without a doubt, also one of the most gentle stemmers. One of the few stemmers that actually has Java support which is a plus, though it is also the most computationally intensive of the algorithms(Granted not by a very significant margin). It is also the oldest stemming algorithm by a large margin. Porter2: Nearly universally regarded as an improvement over porter, and for good reason. Porter himself in fact admits that it is better than his original algorithm. Slightly faster computation time than porter, with a fairly large community around it. Lancaster: Very aggressive stemming algorithm, sometimes to a fault. With porter and snowball, the stemmed representations are usually fairly intuitive to a reader, not so with Lancaster, as many shorter words will become totally obfuscated. The fastest algorithm here, and will reduce your working set of words hugely, but if you want more distinction, not the tool you would want. Honestly, I feel that Snowball is usually the way to go. There are certain circumstances in which Lancaster will hugely trim down your working set, which can be very useful, however the marginal speed increase over snowball in my opinion is not worth the lack of precision. Porter has the most implementations though and so is usually the default go-to algorithm, but if you can, use snowball. Snowball is a small string processing language designed for creating
stemming algorithms for use in Information Retrieval. The Snowball compiler translates a Snowball script into another
language - currently ISO C, C#, Go, Java, Javascript, Object Pascal,
Python and Rust are supported. Since it effectively provides a ‘suffix STRIPPER GRAMmar’, I had toyed
with the idea of calling it ‘strippergram’, but good sense has
prevailed, and so it is ‘Snowball’ named as a tribute to SNOBOL, the
excellent string handling language of Messrs Farber, Griswold, Poage
and Polonsky from the 1960s.
---Martin Porter Stemmers implemented in the Snowball language are sometimes simply referred to as Snowball stemmers. For example, see the Natural Language Toolkit: nltk.stem.snowball.I am using a Naive Bayes Classifier to categorize several thousand documents into 30 different categories. I have implemented a Naive Bayes Classifier, and with some feature selection (mostly filtering useless words), I've gotten about a 30% test accuracy, with 45% training accuracy. This is significantly better than random, but I want it to be better. I've tried implementing AdaBoost with NB, but it does not appear to give appreciably better results (the literature seems split on this, some papers say AdaBoost with NB doesn't give better results, others do). Do you know of any other extensions to NB that may possibly give better accuracy? In my experience, properly trained Naive Bayes classifiers are usually astonishingly accurate (and very fast to train--noticeably faster than any classifier-builder i have everused). so when you want to improve classifier prediction, you can look in several places: tune your classifier (adjusting the classifier's tunable paramaters); apply some sort of classifier combination technique (eg,
ensembling, boosting, bagging); or you can look at the data fed to the classifier--either add more data,
improve your basic parsing, or refine the features you select from
the data. w/r/t naive Bayesian classifiers, parameter tuning is limited; i recommend to focus on your data--ie, the quality of your pre-processing and the feature selection.
 I. Data Parsing (pre-processing) i assume your raw data is something like a string of raw text for each data point, which by a series of processing steps you transform each string into a structured vector (1D array) for each data point such that each offset corresponds to one feature (usually a word) and the value in that offset corresponds to frequency.  stemming: either manually or by using a stemming library? the popular open-source ones are Porter, Lancaster, and Snowball. So for
instance, if you have the terms programmer, program, progamming,
programmed in a given data point, a stemmer will reduce them to a
single stem (probably program) so your term vector for that data
point will have a value of 4 for the feature program, which is
probably what you want. synonym finding: same idea as stemming--fold related words into a single word; so a synonym finder can identify developer, programmer,
coder, and software engineer and roll them into a single term neutral words: words with similar frequencies across classes make poor features  II. Feature Selection consider a prototypical use case for NBCs: filtering spam; you can quickly see how it fails and just as quickly you can see how to improve it. For instance, above-average spam filters have nuanced features like: frequency of words in all caps, frequency of words in title, and the occurrence of exclamation point in the title. In addition, the best features are often not single words but e.g., pairs of words, or larger word groups. III. Specific Classifier Optimizations Instead of 30 classes use a 'one-against-many' scheme--in other words, you begin with a two-class classifier (Class A and 'all else') then the results in the 'all else' class are returned to the algorithm for classification into Class B and 'all else', etc. The Fisher Method (probably the most common way to optimize a Naive Bayes classifier.) To me,
i think of Fisher as normalizing (more correctly, standardizing) the input probabilities An NBC uses the feature probabilities to construct a 'whole-document' probability. The Fisher Method calculates the probability of a category for each feature of the document then combines these feature probabilities and compares that combined probability with the probability of a random set of features. I would suggest using a SGDClassifier as in this and tune it in terms of regularization strength. Also try to tune the formula in TFIDF you're using by tuning the parameters of TFIFVectorizer. I usually see that for text classification problems SVM or Logistic Regressioin when trained one-versus-all outperforms NB. As you can see in this nice article by Stanford people for longer documents SVM outperforms NB. The code for the paper which uses a combination of SVM and NB (NBSVM) is here. Second, tune your TFIDF formula (e.g. sublinear tf, smooth_idf). Normalize your samples with l2 or l1 normalization (default in Tfidfvectorization) because it compensates for different document lengths. Multilayer Perceptron, usually gets better results than NB or SVM because of the non-linearity introduced which is inherent to many text classification problems. I have implemented a highly parallel one using Theano/Lasagne which is easy to use and downloadable here. Try to tune your l1/l2/elasticnet regularization. It makes a huge difference in SGDClassifier/SVM/Logistic Regression. Try to use n-grams which is configurable in tfidfvectorizer. If your documents have structure (e.g. have titles) consider using different features for different parts. For example add title_word1 to your document if word1 happens in the title of the document. Consider using the length of the document as a feature (e.g. number of words or characters). Consider using meta information about the document (e.g. time of creation, author name, url of the document, etc.). Recently Facebook published their FastText classification code which performs very well across many tasks, be sure to try it. Using Laplacian Correction along with AdaBoost.   In AdaBoost, first a weight is assigned to each data tuple in the training dataset. The intial weights are set using the init_weights method, which initializes each weight to be 1/d, where d is the size of the training data set.  Then, a generate_classifiers method is called, which runs k times, creating k instances of the Naïve Bayes classifier. These classifiers are then weighted, and the test data is run on each classifier. The sum of the weighted "votes" of the classifiers constitutes the final classification. We change the probability space to log probability space since we calculate the probability by multiplying probabilities and the result will be very small. when we change to log probability features, we can tackle the under-runs problem.
 Naive Byes works based on the assumption of independence when we have a correlation between features which means one feature depends on others then our assumption will fail.
More about correlation can be found here naive Bayes require less data than logistic regression since it only needs data to understand the probabilistic relationship of each attribute in isolation with the output variable, not the interactions. If the test data set has zero frequency issue, apply smoothing techniques “Laplace Correction” to predict the class of test data set. More than this is well described in the following posts
Please refer below posts. keeping the n size small also make NB to give high accuracy result. and at the core, as the n size increase its accuracy degrade, Select features which have less correlation between them. And try using different combination of features at a time.After I instantiate a scikit model (e.g. LinearRegression), if I call its fit() method multiple times (with different X and y data), what happens? Does it fit the model on the data like if I just re-instantiated the model (i.e. from scratch), or does it keep into accounts data already fitted from the previous call to fit()? Trying with LinearRegression (also looking at its source code) it seems to me that every time I call fit(), it fits from scratch, ignoring the result of any previous call to the same method. I wonder if this true in general, and I can rely on this behavior for all models/pipelines of scikit learn. If you will execute model.fit(X_train, y_train) for a second time - it'll overwrite all previously fitted coefficients, weights, intercept (bias), etc. If you want to fit just a portion of your data set and then to improve your model by fitting a new data, then you can use estimators, supporting "Incremental learning" (those, that implement partial_fit() method) You can use term fit() and train() word interchangeably in machine learning. Based on classification model you have instantiated, may be a clf = GBNaiveBayes() or clf = SVC(),  your model uses specified machine learning technique.
And as soon as you call clf.fit(features_train, label_train) your model starts training using the features and labels that you have passed. you can use clf.predict(features_test) to predict.
If you will again call clf.fit(features_train2, label_train2) it will start training again using passed data and will remove the previous results. Your model will reset the following inside model: You can use partial_fit() method as well if you want your previous calculated stuff to stay and additionally train using next data Beware that the model is passed kind of "by reference". Here, model1 will be overwritten: Returns To avoid this useI am playing with a ANN which is part of Udacity DeepLearning course. I have an assignment which involves introducing generalization to the network with one hidden ReLU layer using L2 loss. I wonder how to properly introduce it so that ALL weights are penalized, not only weights of the output layer. Code for network without generalization is at the bottom of the post (code to actually run the training is out of the scope of the question). Obvious way of introducing the L2 is to replace the loss calculation with something like this (if beta is 0.01): But in such case it will take into account values of output layer's weights. I am not sure, how do we properly penalize the weights which come INTO the hidden ReLU layer. Is it needed at all or introducing penalization of output layer will somehow keep the hidden weights in check also? A shorter and scalable way of doing this would be ; This basically sums the l2_loss of all your trainable variables. You could also make a dictionary where you specify only the variables you want to add to your cost and use the second line above. Then you can add lossL2 with your softmax cross entropy value in order to calculate your total loss.  Edit : As mentioned by Piotr Dabkowski, the code above will also regularise biases. This can be avoided by adding an if statement in the second line ;  This can be used to exclude other variables.  hidden_weights, hidden_biases, out_weights, and out_biases are all the model parameters that you are creating. You can add L2 regularization to ALL these parameters as follows : With the note of @Keight Johnson, to not regularize the bias: In fact, we usually do not regularize bias terms (intercepts). 
So, I go for: By penalizing the intercept term, as the intercept is added to  y values, it will result in changing the y values, adding a constant c to the intercepts.  Having it or not will not change the results but takes some computationsWhen I load the whole dataset in memory and train the network in Keras using following code: This generates a progress bar per epoch with metrics like ETA, accuracy, loss, etc When I train the network in batches, I'm using the following code This will generate a progress bar for each batch instead of each epoch. Is it possible to generate a progress bar for each epoch during batchwise training?  In the above change to verbose=2, as it is mentioned in the documentation: verbose: 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch It'll show your output as: If you want to show a progress bar for completion of epochs, keep verbose=0 (which shuts out logging to stdout) and implement in the following manner: The output will be as follows: If you want to show loss after every n batches, you can use: Though, I haven't ever tried it before. The above example was taken from this keras github issue: Show Loss Every N Batches #2850 You can also follow a demo of NBatchLogger here: You can also use progbar for progress, but it'll print progress batchwise tqdm (version >= 4.41.0) has also just added built-in support for keras so you could do: This turns off keras' progress (verbose=0), and uses tqdm instead. For the callback, verbose=2 means separate progressbars for epochs and batches. 1 means clear batch bars when done. 0 means only show epochs (never show batch bars). you can set verbose=0 and set callbacks that will update progress at the end of each fitting,  https://keras.io/callbacks/#example-model-checkpoints or set callback https://keras.io/callbacks/#remotemonitorI am currently trying to understand the architecture behind the word2vec neural net learning algorithm, for representing words as vectors based on their context. After reading Tomas Mikolov paper I came across what he defines as a projection layer. Even though this term is widely used when referred to word2vec, I couldn't find a precise definition of what it actually is in the neural net context.  My question is, in the neural net context, what is a projection layer? Is it the name given to a hidden layer whose links to previous nodes share the same weights? Do its units actually have an activation function of some kind? ￼Another resource that also refers more broadly to the problem can be found in this tutorial, which also refers to a projection layer around page 67. I find the previous answers here a bit overcomplicated - a projection layer is just a simple matrix multiplication, or in the context of NN, a regular/dense/linear layer, without the non-linear activation in the end (sigmoid/tanh/relu/etc.) The idea is to project the (e.g.) 100K-dimensions discrete vector into a 600-dimensions continuous vector (I chose the numbers here randomly, "your mileage may vary"). The exact matrix parameters are learned through the training process. What happens before/after already depends on the model and context, and is not what OP asks. (In practice you wouldn't even bother with the matrix multiplication (as you are multiplying a 1-hot vector which has 1 for the word index and 0's everywhere else), and would treat the trained matrix as a lookout table (i.e. the 6257th word in the corpus = the 6257th row/column (depends how you define it) in the projection matrix).) The projection layer maps the discrete word indices of an n-gram context to a continuous vector space. As explained in this thesis The projection layer is shared such that for contexts containing the same word multiple times, the same set of weights is applied to form each part of the projection vector.
This organization effectively increases the amount of data available for training the projection layer weights since each word of each context training pattern individually contributes changes to the weight values.  this figure shows the trivial topology how the output of the projection layer can be efficiently assembled by copying columns from the projection layer weights matrix. Now, the Hidden layer: The hidden layer processes the output of the projection layer and is also created with a
number of neurons specified in the topology configuration file. Edit: An explanation of what is happening in the diagram Each neuron in the projection layer is represented by a number of weights equal to the size of the vocabulary. The projection layer differs from the hidden and output layers by not using a non-linear activation function. Its purpose is simply to provide an efficient means of projecting the given n- gram context onto a reduced continuous vector space for subsequent processing by hidden and output layers trained to classify such vectors. Given the one-or-zero nature of the input vector elements, the output for a particular word with index i is simply the ith column of the trained matrix of projection layer weights (where each row of the matrix represents the weights of a single neuron). The continuous bag of words is used to predict a single word given its prior and future entries: thus it is a contextual result.  The inputs are the computed weights from the prior and future entries: and all are given new weights identically: thus the complexity / features count of this model is much smaller than many other NN architectures. RE: what is the projection layer: from the paper you cited the non-linear hidden layer is removed and the projection layer is
  shared for all words (not just the projection matrix); thus, all words
  get projected into the same position (their vectors are averaged). So the projection layer is a single set of shared weights and no activation function is indicated. Note that the weight matrix between the input and the projection layer
  is shared for all word positions in the same way as in the NNLM So the hidden layer is in fact represented by this single set of shared weights - as you correctly implied that is identical across all of the input nodes.I want to know what a learning curve in machine learning is. What is the standard way of plotting it? I mean what should be the x and y axis of my plot? It usually refers to a plot of the prediction accuracy/error vs. the training set size (i.e: how better does the model get at predicting the target as you the increase number of instances used to train it)  Usually both the training and test/validation performance are plotted together so we can diagnose the bias-variance tradeoff (i.e determine if we benefit from adding more training data, and assess the model complexity by controlling regularization or number of features).  I just want to leave a brief note on this old question to point out that learning curve and ROC curve are not synonymous.  As indicated in the other answers to this question, a learning curve conventionally depicts improvement in performance on the vertical axis when there are changes in another parameter (on the horizontal axis), such as training set size (in machine learning) or iteration/time (in both machine and biological learning).  One salient point is that many parameters of the model are changing at different points on the plot.  Other answers here have done a great job of illustrating learning curves. (There is also another meaning of learning curve in industrial manufacturing, originating in an observation in the 1930s that the number of  labor hours needed to produce an individual unit decreases at a uniform rate as the quantity of  units manufactured doubles. It isn't really relevant but is worth noting for completeness and to avoid confusion in web searches.) In contrast, Receiver Operating Characteristic curve, or ROC curve, does not show learning; it shows performance.  An ROC curve is a graphical depiction of classifier performance that shows the trade-off between increasing true positive rates (on the vertical axis) and increasing false positive rates (on the horizontal axis) as the discrimination threshold of the classifier is varied.  Thus, only a single parameter (the decision / discrimination threshold) associated with the model is changing at different points on the plot. This ROC curve (from Wikipedia) shows performance of three different classifiers.  There is no learning being depicted here, but rather performance with respect to two different classes of success/error as the classifier's decision threshold is made more lenient/strict.  By looking at the area under the curve, we can see an overall indication of the ability of the classifier to distinguish the classes.  This area-under-the-curve metric is insensitive to the number of members in the two classes, so it may not reflect actual performance if class membership is unbalanced. The ROC curve has many subtitles and interested readers might check out: Fawcett, Tom. "ROC graphs: Notes and practical considerations for researchers." Machine Learning 31 (2004): 1-38. Swets, John A., Robyn M. Dawes, and John Monahan. "Better decisions through Science." Scientific American (2000): 83. Some people use "learning curve" to refer to the error of an iterative procedure as a function of the iteration number, i.e., it illustrates convergence of some utility function. In the example below, I plot mean-square error (MSE) of the least-mean-square (LMS) algorithm as a function of the iteration number. That illustrates how quickly LMS "learns", in this case, the channel impulse response.  Basically, a machine learning curve allows you to find the point from which the algorithm starts to learn.  If you take a curve and then slice a slope tangent for derivative at the point that it starts to reach constant is when it starts to build its learning ability.  Depending on how your x and y axis are mapped, one of your axis will start to approach a constant value while the other axis's values will keep increasing.  This is when you start seeing some learning. The whole curve pretty much allows you to measure the rate at which your algorithm is able to learn. The maximum point is usually when the slope starts to recede. You can take a number of derivative measures to the maximum/minimum point. So from the above examples you can see that the curve is gradually tending towards a constant value. It initially starts to harness its learning through the training examples and the slope widens at maximum/mimimum point where it tends to approach closer and closer towards the constant state. At this point it is able to pick up new examples from test data and find new and unique results from data.
You would have such x/y axis measures for epochs vs error. In Andrew's machine learning class, a learning curve is the plot of the training/cross-validation error versus the sample size. The learning curve can be used to detect whether the model has the high bias or high variance. If the model suffers from high bias problem, as the sample size increases,  training error will increase and the cross validation error will decrease and at last they will be very close to each other but still at a high error rate for both training and classification error. And increasing the sample size will not help much for high bias problem.  If the model suffers from high variance, as the keep increasing the sample size, the training error will keep increasing and cross-validation error will keep decreasing and they will end up at a low training and cross-validation error rate. So more samples will help to improve the model prediction performance if the model suffer from high variance. How can you determine for a given model whether more training points will be helpful? A useful diagnostic for this are learning curves. •   Plot of the prediction accuracy/error vs. the training set size (i.e.: how better does the model get at predicting the target as you the increase number of instances used to train it) •   Learning curve conventionally depicts improvement in performance on the vertical axis when there are changes in another parameter (on the horizontal axis), such as training set size (in machine learning) or iteration/time •    A learning curve is often useful to plot for algorithmic sanity checking or improving performance •   Learning curve plotting can help diagnose the problems your algorithm will be suffering from Personally, the below two links helped me to understand better about this concept Learning Curve Sklearn Learning Curve use this code to plot : note that history = model.fit(...) It is a Graph that compares the performance of a model on preparing and testing data over a changing number of training instances and these are a generally utilized as analytic instrument in machine learning for calculations that learn from a training dataset incrementally. It allows us to verify when a model has learning as much as it can about the data. There are three kinds of expectations to Learning curves absorb information In simple terms, the learning curve is a plot between the number of instances and a metric such as loss or accuracy. This plot shows the journey learning with the gain of experience and hence is named learning curve. 
Learning curves are widely used in machine learning for algorithms that learn (optimize their internal parameters) incrementally over time, such as deep learning neural networks. Example 
X= Level
y=salary X    Y
0   2000
2   4000 
4   6000
6   8000 Regression gives accuracy 75% it is a state line
polynomial gives accuracy 85% because of the curveI was running TensorFlow and I happen to have something yielding a NaN. I'd like to know what it is but I do not know how to do this. The main issue is that in a "normal" procedural program I would just write a print statement just before the operation is executed. The issue with TensorFlow is that I cannot do that because I first declare (or define) the graph, so adding print statements to the graph definition does not help. Are there any rules, advice, heuristics, anything to track down what might be causing the NaN? In this case I know more precisely what line to look at because I have the following: when this line is present I have it that it returns NaN as declared by my summary writers. Why is this? Is there a way to at least explore what value Z has after its being square rooted? For the specific example I posted, I tried tf.Print(0,Z) but with no success it printed nothing. As in: I actually don't understand what tf.Print is suppose to do. Why does it need two arguments? If I want to print 1 tensor why would I need to pass 2? Seems bizarre to me. I was looking at the function tf.add_check_numerics_ops() but it doesn't say how to use it (plus the docs seem to not be super helpful). Does anyone know how to use this? Since I've had comments addressing the data might be bad, I am using standard MNIST. However, I am computing a quantity that is positive (pair-wise eucledian distance) and then square rooting it. Thus, I wouldn't see how the data specifically would be an issue. There are a couple of reasons WHY you can get a NaN-result, often it is because of too high a learning rate but plenty other reasons are possible like for example corrupt data in your input-queue or a log of 0 calculation. Anyhow, debugging with a print as you describe cannot be done by a simple print (as this would result only in the printing of the tensor-information inside the graph and not print any actual values).  However, if you use tf.print as an op in bulding the graph (tf.print) then when the graph gets executed you will get the actual values printed (and it IS a good exercise to watch these values to debug and understand the behavior of your net). However, you are using the print-statement not entirely in the correct manner. This is an op, so you need to pass it a tensor and request a result-tensor that you need to work with later on in the executing graph. Otherwise the op is not going to be executed and no printing occurs. Try this: I used to find it's much tougher to pinpoint where the nans and infs may occur than to fix the bug. As a complementary to @scai's answer, I'd like to add some points here:  The debug module, you can imported by:  is much better than any print or assert.  You can just add the debug function by changing your wrapper you session by:  And you'll prompt an command line interface, then you enter: 
run -f has_inf_or_nan and lt -f has_inf_or_nan to find where the nans or infs are. The first one is the first place where the catastrophe occurs. By the variable name you can trace the origin in your code.    Reference: https://developers.googleblog.com/2017/02/debug-tensorflow-models-with-tfdbg.html As of version 0.12, TensorFlow is shipped with a builtin debugger called tfdbg. It optimizes the workflow of debugging this type of bad-numerical-value issues (like inf and nan). The documentation is at:
https://www.tensorflow.org/programmers_guide/debugger It look like you can call it after you complete making the graph. check = tf.add_check_numerics_ops() I think this will add the check for all floating point operations.  Then in the sessions run function you can add the check operation. sess.run([check, ...]) First of all, you need to check you input data properly. In most cases this is the reason. But not always, of course. I usually use Tensorboard to see whats happening while training. So you can see the values on each step with  Also you can simply eval and print the current value: For TensorFlow 2, inject some x=tf.debugging.check_numerics(x,'x is nan') into your code. They will throw an InvalidArgument error if xhas any values that are not a number (NaN) or infinity (Inf). Oh and for the next person finding this when hunting a TF2 NaN issue, my case turned out to be an exploding gradient. The gradient itself got to 1e+20, which was not quite NaN yet, but adding that to the variable then turned out too big. The diagnosis that I did was  which revealed the overly large numbers. Running the exact same network on CPU worked fine, but it failed on the GTX 1080 TI in my workstation, thus making a CUDA numerical stability issue likely as the root cause. But since it only occurred sometimes, I duct-taped the whole thing by going with: which will just clip exploding gradients to a sane value. For a network where gradients are always high, that wouldn't help, but since the magnitudes where high only sporadically, this fixed the problem and now the network trains nicely also on GPU. NANs occurring in the forward process are one thing and those occurring in the backward process are another. Make sure that there are no extreme inputs such as NAN inputs or negative labels in the prepared dataset using NumPy tools, for instance: assert not np.any(np.isnan(x)). Switch to a CPU environment to get a more detailed traceback, and test the forward pass only by loss = tf.stop_gradient(loss) before calculating the gradients to see if you can run several batches with no errors. If an error occurs, there are several types of potential bugs and methods: If everything goes well, remove the loss = tf.stop_gradient(loss). As an aside, it's always helpful to make sure that the shape of every tensor is desired. You can try to input fixed-sized batches(drop the remainders) and reshape the feature tensors(where the graph receives data from Dataset) as you expect them to be(otherwise the first dimension would be None sometimes) and then print the shape of the very tensor in the graph with fixed numbers. A Recipe for Training Neural Networks by Andrej Karpathy is a great article on training/debugging neural networks. Current implementation of tfdbg.has_inf_or_nan seems do not break immediately on hitting any tensor containing NaN. When it does stop, the huge list of tensors displayed are not sorted in order of its execution.
A possible hack to find the first appearance of Nans is to dump all tensors to a temporary directory and inspect afterwards.
Here is a quick-and-dirty example to do that. (Assuming the NaNs appear in the first few runs) I was able to fix my NaN issues by getting rid of all of my dropout layers in the network model.  I suspected that maybe for some reason a unit (neuron?) in the network lost too many input connections (so it had zero after the dropout), so then when information was fed through, it had a value of NaN.  I don't see how that could happen over and over again with dropout=0.8 on layers with more than a hundred units each, so the problem was probably fixed for a different reason.  Either way, commenting out the dropout layers fixed my issue. EDIT: Oops! I realized that I added a dropout layer after my final output layer which consists of three units.  Now that makes more sense.  So, don't do that!We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 6 years ago. Where can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media. I find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business? http://www.cs.cornell.edu/home/llee/data/ http://mpqa.cs.pitt.edu/corpora/mpqa_corpus You can use twitter, with its smileys, like this: http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf Hope that gets you started.  There's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc. To get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  Or you could get your data annotated by Mechanical Turkers. This is a list I wrote a few weeks ago, from my blog. Some of these datasets have been recently included in the NLTK Python platform. Opinion Lexicon by Bing Liu MPQA Subjectivity Lexicon SentiWordNet Harvard General Inquirer Linguistic Inquiry and Word Counts (LIWC) Vader Lexicon MPQA Datasets NOTES: GNU Public License. Sentiment140 (Tweets) STS-Gold (Tweets) Customer Review Dataset (Product reviews) Included in the NLTK Python platform Pros and Cons Dataset (Pros and cons sentences) Included in the NLTK Python platform Comparative Sentences (Reviews) Included in the NLTK Python platform Sanders Analytics Twitter Sentiment Corpus (Tweets) 5513 hand-classified tweets wrt 4 different topics. Because of Twitter’s ToS, a small Python script is included to download all of the tweets. The sentiment classifications themselves are provided free of charge and without restrictions. They may be used for commercial products. They may be redistributed. They may be modified. Spanish tweets (Tweets) SemEval 2014 (Tweets) You MUST NOT re-distribute the tweets, the annotations or the corpus obtained (from the readme file) Various Datasets (Reviews) Various Datasets #2 (Reviews) References: Here are a few more; http://inclass.kaggle.com/c/si650winter11 http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html If you have some resources (media channels, blogs, etc) about the domain you want to explore, you can create your own corpus. 
I do this in python:  Creating corpus is a hard work of pre-processing, checking, tagging, etc, but has the benefits of preparing a model for a specific domain many times increasing the accuracy. If you can get already prepared corpus, just go ahead with the sentiment analysis ;)  I'm not aware of any such corpus being freely available, but you could try an unsupervised method on an unlabeled dataset. You can get a large select of online reviews from Datafiniti.  Most of the reviews come with rating data, which would provide more granularity on sentiment than positive / negative.  Here's a list of businesses with reviews, and here's a list of products with reviews.I can not for the life of me figure out how to switch the image ordering. images are read in (x,x,3) format, theano requires it to be in (3,x,x) format. I tried changing the order with
numpy.array([img[:,:,i] for i in range(3)]) which i guess gets the job done, but it is both ugly and i can't figure out how to reverse it to get the original image back. I agree with @Qualia 's comment, np.moveaxis(a, source, destination) is easier to understand. This does the job:  You can use numpy.rollaxis to roll the axis 3 to position 1 (considering you have the batch size as dimension 0). But, if you're using keras, you might want to change its configuration or define it per layer. Theano doesn't require anything from you if you're using Keras. Keras can be configured with channels first or channels last, besides allowing you to define it in every individual layer, so you don't have to change your data. Find the keras.json file and change it. The file is usually installed in C:\Users\yourusername\.keras or ~/.keras depending on your OS. Change "image_data_format": "channels_last" to "channels_first" or vice-versa, as you wish. Usually, working with "channels_last" is less troublesome because of a great amount of other (non convolutional) functions that work only on the last axis. The Keras documentation has all information about parameters for layers, including the data_format parameter. If you're looking at the fastest option, go for .transpose(...). It's even faster than np.einsum.  Using np.moveaxis is effective, but I have found that np.einsum is much faster.  You can also use torch.permute in PyTorch:Trying to plot the decision Boundary of the k-NN Classifier but is unable to do so getting TypeError: '(slice(None, None, None), 0)' is an invalid key Got this when running not very sure what it means dont think the clf.fit have a problem but I am not sure Since you are trying to access directly as array, you are getting that issue. Try this: Using iloc/loc will resolve the issue. You need to use iloc/loc to acces df. Try adding iloc to X so X.iloc[:, 0] I had the same issue with the following Then I added .values property, after that it worked without problem I fixed it by converting the pandas dataframe to a numpy array. Got help from here When you are trying to fetch the dataset using pandas use the below code: I had same issue when using then I solved this by calling .values() function/method to output of iloc, and then as a numpy.ndarray it worked! When importing the datasets, use .values. Change: To: Try run this code before your code writed above. Which library did you use to load the dataset?
If you used Pandas library to load the dataset, you need to add an index-based selection (iloc) function  to the data frame in order to access the values, e.g. For your problem: If you used the NumPy library to load the dataset, you can access the values directly  as an array, e.g. For your problem : you have to create the array  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 This is present in the dataframe you have to first convert the dataframe to array by this
dataframe.values then apply this I changed my input to a numpy array instead and it worked. I have still not been able to sort this issue with a Pandas dataframe input. If it is urgent in your case, I suggest changing your input to numpy and moving ahead. Use .values to access dataframe values. For Example I hope this will help. You can use it for any case according to your needsI've working on a CNN over several hundred GBs of images. I've created a training function that bites off 4Gb chunks of these images and calls fit over each of these pieces. I'm worried that I'm only training on the last piece on not the entire dataset. Effectively, my pseudo-code looks like this: I know that the API and the Keras forums say that this will train over the entire dataset, but I can't intuitively understand why the network wouldn't relearn over just the last training chunk. Some help understanding this would be much appreciated. Best,
Joe This question was raised at the Keras github repository in Issue #4446: Quick Question: can a model be fit for multiple times?  It was closed by François Chollet with the following statement: Yes, successive calls to fit will incrementally train the model. So, yes, you can call fit multiple times. For datasets that do not fit into memory, there is an answer in the Keras Documentation FAQ section You can do batch training using model.train_on_batch(X, y) and
  model.test_on_batch(X, y). See the models documentation. Alternatively, you can write a generator that yields batches of
  training data and use the method model.fit_generator(data_generator, samples_per_epoch, nb_epoch). You can see batch training in action in our CIFAR10 example. So if you want to iterate your dataset the way you are doing, you should probably use model.train_on_batch and take care of the batch sizes and iteration yourself. One more thing to note is that you should make sure the order in which the samples you train your model with is shuffled after each epoch. The way you have written the example code seems to not shuffle the dataset. You can read a bit more about shuffling here and hereI understand neural networks with any number of hidden layers can approximate nonlinear functions, however, can it approximate: I can't think of how it could. It seems like a very obvious limitation of neural networks that can potentially limit what it can do. For example, because of this limitation, neural networks probably can't properly approximate many functions used in statistics like Exponential Moving Average, or even variance. Speaking of moving average, can recurrent neural networks properly approximate that? I understand how a feedforward neural network or even a single linear neuron can output a moving average using the sliding window technique, but how would recurrent neural networks do it without X amount of hidden layers (X being the moving average size)? Also, let us assume we don't know the original function f, which happens to get the average of the last 500 inputs, and then output a 1 if it's higher than 3, and 0 if it's not. But for a second, pretend we don't know that, it's a black box. How would a recurrent neural network approximate that? We would first need to know how many timesteps it should have, which we don't. Perhaps a LSTM network could, but even then, what if it's not a simple moving average, it's an exponential moving average? I don't think even LSTM can do it. Even worse still, what if f(x,x1) that we are trying to learn is simply That seems very simple and straightforward. Can a neural network learn it? I don't see how. Am I missing something huge here or are machine learning algorithms extremely limited? Are there other learning techniques besides neural networks that can actually do any of this? The key point to understand is compact:  Neural networks (as any other approximation structure like, polynomials, splines, or Radial Basis Functions) can approximate any continuous function only within a compact set.  In other words the theory states that, given: then there exists a neural network that approximates f(x) with an approximation error less than ε, everywhere within [a,b].  Regarding your example of f(x) = x2, yes you can approximate it with a neural network within any finite range: [-1,1], [0, 1000], etc. To visualise this, imagine that you approximate f(x) within [-1,1] with a Step Function. Can you do it on paper? Note that if you make the steps narrow enough you can achieve any desired accuracy. The way neural networks approximate f(x) is not much different than this.  But again, there is no neural network (or any other approximation structure) with a finite number of parameters that can approximate f(x) = x2 for all x in [-∞, +∞]. The question is very legitimate and unfortunately many of the answers show how little practitioners seem to know about the theory of neural networks. The only rigorous theorem that exists about the ability of neural networks to approximate different kinds of functions is the Universal Approximation Theorem.  The UAT states that any continuous function on a compact domain can be approximated by a neural network with only one hidden layer provided the activation functions used are BOUNDED, continuous and monotonically increasing. Now, a finite sum of bounded functions is bounded by definition.  A polynomial is not bounded so the best we can do is provide a neural network approximation of that polynomial over a compact subset of R^n. Outside of this compact subset, the approximation will fail miserably as the polynomial will grow without bound. In other words, the neural network will work well on the training set but will not generalize! The question is neither off-topic nor does it represent the OP's opinion. I am not sure why there is such a visceral reaction, I think it is a legitimate question that is hard to find by googling it, even though I think it is widely appreciated and repeated outloud. I think in this case you are looking for the actually citations showing that a neural net can approximate any function. This recent paper explains it nicely, in my opinion. They also cite the original paper by Barron from 1993 that proved a less general result. The conclusion: a two-layer neural network can represent any bounded degree polynomial, under certain (seemingly non-restrictive) conditions. Just in case the link does not work, it is called "Learning Polynomials with Neural Networks" by Andoni et al., 2014. I understand neural networks with any number of hidden layers can approximate nonlinear functions, however, can it approximate: f(x) = x^2 The only way I can make sense of that question is that you're talking about extrapolation. So e.g. given training samples in the range -1 < x < +1 can a neural network learn the right values for x > 100? Is that what you mean? If you had prior knowledge, that the functions you're trying to approximate are likely to be low-order polynomials (or any other set of functions), then you could surely build a neural network that can represent these functions, and extrapolate x^2 everywhere. If you don't have prior knowledge, things are a bit more difficult: There are infinitely many smooth functions that fit x^2 in the range -1..+1 perfectly, and there's no good reason why we would expect x^2 to give better predictions than any other function. In other words: If we had no prior knowledge about the function we're trying to learn, why would we want to learn x -> x^2? In the realm of artificial training sets, x^2 might be a likely function, but in the real world, it probably isn't. To give an example: Let's say the temperature on Monday (t=0) is 0°, on Tuesday it's 1°, on Wednesday it's 4°. We have no reason to believe temperatures behave like low-order polynomials, so we wouldn't want to infer from that data that the temperature next Monday will probably be around 49°. Also, let us assume we don't know the original function f, which happens to get the average of the last 500 inputs, and then output a 1 if it's higher than 3, and 0 if it's not. But for a second, pretend we don't know that, it's a black box. How would a recurrent neural network approximate that? I think that's two questions: First, can a neural network represent that function? I.e. is there a set of weights that would give exactly that behavior? It obviously depends on the network architecture, but I think we can come up with architectures that can represent (or at least closely approximate) this kind of function. Question two: Can it learn this function, given enough training samples? Well, if your learning algorithm doesn't get stuck in a local minimum, sure: If you have enough training samples, any set of weights that doesn't approximate your function gives a training error greater that 0, while a set of weights that fit the function you're trying to learn has a training error=0. So if you find a global optimum, the network must fit the function. A network can learn x|->x * x if it has a neuron that calculates x * x. Or more generally, a node that calculates x**p and learns p. These aren't commonly used, but the statement that "no neural network can learn..." is too strong. A network with ReLUs and a linear output layer can learn x|->2*x, even on an unbounded range of x values. The error will be unbounded, but the proportional error will be bounded. Any function learnt by such a network is piecewise linear, and in particular asymptotically linear.  However, there is a risk with ReLUs: once a ReLU is off for all training examples it ceases learning. With a large domain, it will turn on for some possible test examples, and give an erroneous result. So ReLUs are only a good choice if test cases are likely to be within the convex hull of the training set. This is easier to guarantee if the dimensionality is low. One work around is to prefer LeakyReLU. One other issue: how many neurons do you need to achieve the approximation you want? Each ReLU or LeakyReLU implements a single change of gradient. So the number needed depends on the maximum absolute value of the second differential of the objective function, divided by the maximum error to be tolerated. There are theoretical limitations of Neural Networks. No neural network can ever learn the function f(x) = x*x
Nor can it learn an infinite number of other functions, unless you assume the impractical: 1- an infinite number of training examples
2- an infinite number of units
3- an infinite amount of time to converge NNs are good in learning low-level pattern recognition problems (signals that in the end have some statistical pattern that can be represented by some "continuous" function!), but that's it! 
No more! Here's a hint:
Try to build a NN that takes n+1 data inputs (x0, x1, x2, ... xn) and it will return true (or 1) if (2 * x0) is in the rest of the sequence. And, good luck.
Infinite functions especially those that are recursive cannot be learned. They just are!I am trying to implement multivariate linear regression in Python using TensorFlow, but have run into some logical and implementation issues. My code throws the following error: Ideally the weights output should be [2, 3] Run this: Or (depending on the version of TF that you have): It's not 100% clear from the code example, but if the list initial_parameters_of_hypothesis_function is a list of tf.Variable objects, then the line session.run(init) will fail because TensorFlow isn't (yet) smart enough to figure out the dependencies in variable initialization. To work around this, you should change the loop that creates parameters to use initial_parameters_of_hypothesis_function[i].initialized_value(), which adds the necessary dependency: There is another the error happening which related to the order when calling initializing global variables. I've had the sample of code has similar error FailedPreconditionError (see above for traceback): Attempting to use uninitialized value W You should change to following Normally there are two ways of initializing variables, 1) using the sess.run(tf.global_variables_initializer()) as the previous answers noted; 2) the load the graph from checkpoint.  You can do like this:  And the third method is to use the tf.train.Supervisor. The session will be  Create a session on 'master', recovering or initializing the model as needed, or wait for a session to be ready. I want to give my resolution, it work when i replace the line [session = tf.Session()] with [sess = tf.InteractiveSession()]. Hope this will be useful to others. run both:  sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer())Update: a better formulation of the issue. I'm trying to understand the backpropagation algorithm with an XOR neural network as an example.  For this case there are 2 input neurons + 1 bias, 2 neurons in the hidden layer + 1 bias, and 1 output neuron. 
(source: wikimedia.org)  I'm using stochastic backpropagation. After reading a bit more I have found out that the error of the output unit is propagated to the hidden layers... initially this was confusing, because when you get to the input layer of the neural network, then each neuron gets an error adjustment from both of the neurons in the hidden layer.  In particular, the way the error is distributed is difficult to grasp at first. Step 1 calculate the output for each instance of input.
Step 2 calculate the error between the output neuron(s) (in our case there is only one) and the target value(s):

Step 3 we use the error from Step 2 to calculate the error for each hidden unit h:
 The 'weight kh' is the weight between the hidden unit h and the output unit k, well this is confusing because the input unit does not have a direct weight associated with the output unit.  After staring at the formula for a few hours I started to think about what the summation means, and I'm starting to come to the conclusion that each input neuron's weight that connects to the hidden layer neurons is multiplied by the output error and summed up.  This is a logical conclusion, but the formula seems a little confusing since it clearly says the 'weight kh' (between the output layer k and hidden layer h). Am I understanding everything correctly here? Can anybody confirm this?   What's O(h) of the input layer? My understanding is that each input node has two outputs: one that goes into the the first node of the hidden layer and one that goes into the second node hidden layer. Which of the two outputs should be plugged into the O(h)*(1 - O(h)) part of the formula?
 The tutorial you posted here is actually doing it wrong. I double checked it against Bishop's two standard books and two of my working implementations. I will point out below where exactly. An important thing to keep in mind is that you are always searching for derivatives of the error function with respect to a unit or weight. The former are the deltas, the latter is what you use to update your weights. If you want to understand backpropagation, you have to understand the chain rule. It's all about the chain rule here. If you don't know how it works exactly, check up at wikipedia - it's not that hard. But as soon as you understand the derivations, everything falls into place. Promise! :) ∂E/∂W can be composed into ∂E/∂o ∂o/∂W via the chain rule. ∂o/∂W is easily calculated, since it's just the derivative of the activation/output of a unit with respect to the weights. ∂E/∂o is actually what we call the deltas. (I am assuming that E, o and W are vectors/matrices here) We do have them for the output units, since that is where we can calculate the error. (Mostly we have an error function that comes down to delta of (t_k - o_k), eg for quadratic error function in the case of linear outputs and cross entropy in case for logistic outputs.)  The question now is, how do we get the derivatives for the internal units? Well, we know that the output of a unit is the sum of all incoming units weighted by their weights and the application of a transfer function afterwards. So o_k = f(sum(w_kj * o_j, for all j)).  So what we do is, derive o_k with respect to o_j. Since delta_j = ∂E/∂o_j = ∂E/∂o_k ∂o_k/∂o_j = delta_k ∂o_k/o_j. So given delta_k, we can calculate delta_j! Let's do this. o_k = f(sum(w_kj * o_j, for all j)) => ∂o_k/∂o_j = f'(sum(w_kj * o_j, for all j)) * w_kj = f'(z_k) * w_kj. For the case of the sigmoidal transfer function, this becomes z_k(1 - z_k) * w_kj. (Here is the error in the tutorial, the author says o_k(1 - o_k) * w_kj!) I'm not sure what your question is but I actually went through that tutorial myself and I can assure you, other than a one obvious typo, there is nothing incorrect about it. I will make the assumption that your question is because you are confused about how the backpropagation hidden delta is derived. If this is indeed your question then please consider 
(source: pandamatak.com)  You are probably confused as to how the author derived this equation. This is actually a straightforward application of the multivariate chain rule. Namely, (what follows is taken from wikipedia) "Suppose that each argument of z = f(u, v) is a two-variable function such that u = h(x, y) and v = g(x, y), and that these functions are all differentiable. Then the chain rule would look like:  
" Now imagine extending the chain rule by an induction argument to E(z'1,z'2,..,z'n)
where z'k is the output of the  kth output layer pre-activation,
and z'k(wji) that is to say that E is a function of the z' and z' itself is a function of wji (if this doesn't make sense to you at first think very carefully about how a NN is setup.) Applying the chain rule directly extended to n variables: δE(z'1,z'2,..,z'n)/δwji = Σk δE/δz'k δz'k/δwji  that is the most important step, the author then applies the chain rule again, this time within the sum to expand the δz'k/δwji term, that is  δz'k/δwji = δz'k/δoj
δoj/δzj
δzj/δwji. If you have difficulties understanding the chain rule, you may need to take a course on multivariate calculus, or read such a section in a textbook. Good luck. What I read from Step 3's equation is: Each unit has only one output, but each link between the output and the next layer is weighted. So the output is the same, but on the receiving end, each unit will receive a different value if the weight of the links is different. O_h always refers to the value of this neuron for the last iteration. Error does not apply to the input layer, as by definition, the input has no 'error' per se. The error needs to be calculated layer by layer, starting at the output side, since we need the error values of layer N+1 to calculate layer N. You are right, there is no direct connection between input and output in backpropagation.  I believe the equation is correct, if counterintuitive. What is probably confusing is that in forward propagation for each unit we have to consider all the units and links on the left of the unit (input values), but for error propagation (backpropagation) was have to consider the units on the right (output value) of the unit being processed.Scikit-learn utilizes a very convenient approach based on fit and predict methods. I have time-series data in the format suited for fit and predict. For example I have the following Xs: and the corresponding ys: These data have the following meaning. The values stored in ys form a time series. The values in Xs are corresponding time dependent "factors" that are known to have some influence on the values in ys (for example: temperature, humidity and atmospheric pressure). Now, of course, I can use fit(Xs,ys). But then I get a model in which future values in ys depend only on factors and do not dependend on the previous Y values (at least directly) and this is a limitation of the model. I would like to have a model in which Y_n depends also on Y_{n-1} and Y_{n-2} and so on. For example I might want to use an exponential moving average as a model. What is the most elegant way to do it in scikit-learn ADDED As it has been mentioned in the comments, I can extend Xs by adding ys. But this way has some limitations. For example, if I add the last 5 values of y as 5 new columns to X, the information about time ordering of ys is lost. For example, there is no indication in X that values in the 5th column follows value in the 4th column and so on. As a model, I might want to have a linear fit of the last five ys and use the found linear function to make a prediction. But if I have 5 values in 5 columns it is not so trivial. ADDED 2 To make my problem even more clear, I would like to give one concrete example. I would like to have a "linear" model in which y_n = c + k1*x1 + k2*x2 + k3*x3 + k4*EMOV_n, where EMOV_n is just an exponential moving average. How, can I implement this simple model in scikit-learn? According to Wikipedia, EWMA works well with stationary data, but it does not work as expected in the presence of trends, or seasonality. In those cases you should use a second or third order EWMA method, respectively. I decided to look at the pandas ewma function to see how it handled trends, and this is what I came up with:  As you can see, the EWMA bucks the trend uphill and downhill. We can correct for this (without having to implement a second-order scheme ourselves) by taking the EWMA in both directions and then averaging. I hope your data was stationary! This might be what you're looking for, with regard to the exponentially weighted moving average: Here, com is a parameter that you can read about here. Then you can combine EMOV_n to Xs, using something like: And then you can look at various linear models, here, and do something like: Best of luck!I am using DBSCAN to cluster some data using Scikit-Learn (Python 2.7): However, I found that there was no built-in function (aside from "fit_predict") that could assign the new data points, Y, to the clusters identified in the original data, X. The K-means method has a "predict" function but I want to be able to do the same with DBSCAN. Something like this: So that the density can be inferred from X but the return values (cluster assignments/labels) are only for Y. From what I can tell, this capability is available in R so I assume that it is also somehow available in Python. I just can't seem to find any documentation for this. Also, I have tried searching for reasons as to why DBSCAN may not be used for labeling new data but I haven't found any justifications. While Anony-Mousse has some good points (Clustering is indeed not classifying) I think the ability of assigning new points has it's usefulness. * Based on the original paper on DBSCAN and robertlaytons ideas on github.com/scikit-learn, I suggest running through core points and assigning to the cluster of the first core point that is within eps of you new point. 
Then it is guaranteed that your point will at least be a border point of the assigned cluster according to the definitions used for the clustering. 
(Be aware that your point might be deemed noise and not assigned to a cluster) I've done a quick implementation: The labels obtained by clustering (dbscan_model = DBSCAN(...).fit(X) and the labels obtained from the same model on the same data  (dbscan_predict(dbscan_model, X)) sometimes differ. I'm not quite certain if this is a bug somewhere or a result of randomness.  EDIT: I Think the above problem of differing prediction outcomes could stem from the possibility that a border point can be close to multiple clusters. Please update if you test this and find an answer. Ambiguity might be solved by shuffling core points every time or by picking the closest instead of the first core point.  *) Case at hand: I'd like to evaluate if the clusters obtained from a subset of my data makes sense for other subset or is simply a special case.
If it generalises it supports the validity of the clusters and the earlier steps of pre-processing applied. Clustering is not classification. Clustering is unlabeled. If you want to squeeze it into a prediction mindset (which is not the best idea), then it essentially predicts without learning. Because there is no labeled training data available for clustering. It has to make up new labels for the data, based on what it sees. But you can't do this on a single instance, you can only "bulk predict". But there is something wrong with scipys DBSCAN: random_state : numpy.RandomState, optional : The generator used to initialize the centers. Defaults to numpy.random. DBSCAN does not "initialize the centers", because there are no centers in DBSCAN. Pretty much the only clustering algorithm where you can assign new points to the old clusters is k-means (and its many variations). Because it performs a "1NN classification" using the previous iterations cluster centers, then updates the centers. But most algorithms don't work like k-means, so you can't copy this. What the R version maybe is doing, is using a 1NN classificator for prediction; maybe with the extra rule that points are assigned the noise label, if their 1NN distance is larger than epsilon, mabye also using the core points only. Maybe not. Get the DBSCAN paper, it does not discuss "prediction" IIRC. Here a slightly different and more efficient implementation. Also, instead of taking the first best core point that is within the eps radius, the core point that is closest to the sample is taken. Although it's not the exact same algorithm you can preform approximate predictions for new points with sklearn HDBSCAN. See here. It works like this: Great answers are already posted to this question here. My suggestion is to give HDBSCAN a try. It provides an approximate_predict() method which might be what you need.   Let's first try to understand a few basic things about DBSCAN density-based clustering, the following figure summarizes the basic concepts. 
Let's first create a sample 2D dataset that will be clustered with DBSCAN. The following figure shows how the dataset looks.  Now let's use scikit-learn's implementation of DBSCAN to cluster: Notice from the above results that Let's visualize the clusters using the following code snippet:  Finally, let's implement the predict() method to predict the cluster of a new data point. The implementation is based on the following: in order that the new point x belongs to a cluster, it must be directly density reachable from a core point in the cluster. We shall compute the nearest core point to the cluster, if it's within ϵ distance from x, we shall return the label of the core point, otherwise the point x will be declared a noise point (outlier). Notice that this differs from the training algorithm, since we no longer allow any more point to become a new core point (i.e., number of core points are fixed). the next code snippet implements the predict() function based on the above idea The next animation shows how a few new test points are labeled using the predict() function defined above.  The K-means algo doesn't do prediction, it just tries to best place the K clusters. sklearn.cluster.KMeans.predict compares the Euclidian distance of each cluster to the new instance and labels it with the closest cluster. DBSCAN doesn't have cluster centers, but it does have one or more "core instances" per cluster. Therefore, some prediction options after using DBSCAN are: --An aside-- To help avoid confusion (as I've seen some answers say some wrong things), it is true that DBSCAN is non-parametric, but that is not why sklearn's implementation has no predict function. As a counter example, k-Nearest Neighbors is non-parametric and is capable of making predictions. Non-parametric clustering models like DBSCAN, Spectral clustering, and Hierarchical clustering, cannot directly predict labels of new points, because they are non-parametric. That means we cannot get a set of parameters, and predict the label of a new point based on the parameters. As a comparison, K-means and Gaussian mixture model are parametric clustering models. The Clustering with Neural Network and Index (CNNI) model is another parametric clustering model. To predict labels of new points after analysis of DBSCAN, as other answers' suggestion,
we may need to train a supervised model with the clustering result, then predict the label with the supervised model. Or just using the KNN classifier. Min-Max-Jump distance provides an alternative method for predicting labels of new points. https://doi.org/10.31219/osf.io/fbxrz To predict label of a new point, we just compare the point's Min-Max-Jump distance to the center (One-SCOM) of each cluster. A mechanism that is similar to K-means. Following is an example of using Min-Max-Jump distance to  predict labels of 10,000 new points, of three toy data-sets. Predicting labels of new points Here is an example illustrating difference between KNN and Min-Max-Jump distance to  predict labels of new points, when the clusters are not well separated. Read more information from https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.htmlWhen we have a high degree linear polynomial that is used to fit a set of points in a linear regression setup, to prevent overfitting, we use regularization, and we include a lambda parameter in the cost function. This lambda is then used to update the theta parameters in the gradient descent algorithm. My question is how do we calculate this lambda regularization parameter? The regularization parameter (lambda) is an input to your model so what you probably want to know is how do you select the value of lambda.  The regularization parameter reduces overfitting, which reduces the variance of your estimated regression parameters; however, it does this at the expense of adding bias to your estimate.  Increasing lambda results in less overfitting but also greater bias.  So the real question is "How much bias are you willing to tolerate in your estimate?" One approach you can take is to randomly subsample your data a number of times and look at the variation in your estimate.  Then repeat the process for a slightly larger value of lambda to see how it affects the variability of your estimate.  Keep in mind that whatever value of lambda you decide is appropriate for your subsampled data, you can likely use a smaller value to achieve comparable regularization on the full data set. Hi! nice explanations for the intuitive and top-notch mathematical approaches there. I just wanted to add some specificities that, where not "problem-solving", may definitely help to speed up and give some consistency to the process of finding a good regularization hyperparameter. I assume that you are talking about the L2 (a.k. "weight decay") regularization, linearly weighted by the lambda term, and that you are optimizing the weights of your model either with the closed-form Tikhonov equation (highly recommended for low-dimensional  linear regression models), or with some variant of gradient descent with backpropagation. And that in this context, you want to choose the value for lambda that provides best generalization ability. If you are able to go the Tikhonov way with your model (Andrew Ng says under 10k dimensions, but this suggestion is at least 5 years old) Wikipedia - determination of the Tikhonov factor offers an interesting closed-form solution, which has been proven to provide the optimal value. But this solution probably raises some kind of implementation issues (time complexity/numerical stability) I'm not aware of, because there is no mainstream algorithm to perform it. This 2016 paper looks very promising though and may be worth a try if you really have to optimize your linear model to its best. In this new innovative method, we have derived an iterative approach to solving the general Tikhonov regularization problem, which converges to the noiseless solution, does not depend strongly on the choice of lambda, and yet still avoids the inversion problem. And from the GitHub README of the project:
InverseProblem.invert(A, be, k, l) #this will invert your A matrix, where be is noisy be, k is the no. of iterations, and lambda is your dampening effect (best set to 1) All links of this part are from Michael Nielsen's amazing online book "Neural Networks and Deep Learning", recommended reading! For this approach it seems to be even less to be said: the cost function is usually non-convex, the optimization is performed numerically and the performance of the model is measured by some form of cross validation (see Overfitting and Regularization and why does regularization help reduce overfitting if you haven't had enough of that). But even when cross-validating, Nielsen suggests something: you may want to take a look at this detailed explanation  on how does the L2 regularization provide a weight decaying effect, but the summary is that it is inversely proportional to the number of samples n, so when calculating the gradient descent equation with the L2 term, just use backpropagation, as usual, and then add (λ/n)*w to the partial derivative of all the weight terms. And his conclusion is that, when wanting a similar regularization effect with a different number of samples, lambda has to be changed proportionally: we need to modify the regularization parameter. The reason is because the size n of the training set has changed from n=1000 to n=50000, and this changes the weight decay factor 1−learning_rate*(λ/n). If we continued to use λ=0.1 that would mean much less weight decay, and thus much less of a regularization effect. We compensate by changing to λ=5.0. This is only useful when applying the same model to different amounts of the same data, but I think it opens up the door for some intuition on how it should work, and, more importantly, speed up the hyperparametrization process by allowing you to finetune lambda in smaller subsets and then scale up. For choosing the exact values, he suggests in his conclusions on how to choose a neural network's hyperparameters the purely empirical approach: start with 1 and then progressively multiply&divide by 10 until you find the proper order of magnitude, and then do a local search within that region. In the comments of this SE related question, the user Brian Borchers suggests also a very well known method that may be useful for that local search:  Hope this helps! Cheers,
Andres The cross validation described above is a method used often in Machine Learning. However, choosing a reliable and safe regularization parameter is still a very hot topic of research in mathematics. 
If you need some ideas (and have access to a decent university library) you can have a look at this paper:
http://www.sciencedirect.com/science/article/pii/S0378475411000607I'm trying to use SGD to classify a large dataset. As the data is too large to fit into memory, I'd like to use the partial_fit method to train the classifier. I have selected a sample of the dataset (100,000 rows) that fits into memory to test fit vs. partial_fit: I then test both classifiers with an identical test set. In the first case I get an accuracy of 100%. As I understand it, SGD by default passes 5 times over the training data (n_iter = 5). In the second case, I have to pass 60 times over the data to reach the same accuracy. Why this difference (5 vs. 60)? Or am I doing something wrong? I have finally found the answer. You need to shuffle the training data between each iteration, as setting shuffle=True when instantiating the model will NOT shuffle the data when using partial_fit (it only applies to fit). Note: it would have been helpful to find this information on the sklearn.linear_model.SGDClassifier page. The amended code reads as follows:I have been exploring scikit-learn, making decision trees with both entropy and gini splitting criteria, and exploring the differences. My question, is how can I "open the hood" and find out exactly which attributes the trees are splitting on at each level, along with their associated information values, so I can see where the two criterion make different choices? So far, I have explored the 9 methods outlined in the documentation.  They don't appear to allow access to this information.  But surely this information is accessible?  I'm envisioning a list or dict that has entries for node and gain. Thanks for your help and my apologies if I've missed something completely obvious. Directly from the documentation ( http://scikit-learn.org/0.12/modules/tree.html ): StringIO module is no longer supported in Python3, instead import io module. There is also the tree_ attribute in your decision tree object, which allows the direct access to the whole structure. And you can simply read it for more details look at the source code of export method In general you can use the inspect module to get all the object's elements  If you just want a quick look at which what is going on in the tree, try: where X is the data frame of independent variables and clf is the decision tree object. Notice that clf.tree_.children_left and clf.tree_.children_right together contain the order that the splits were made (each one of these would correspond to an arrow in the graphviz visualization). Scikit learn introduced a delicious new method called export_text in version 0.21 (May 2019) to view all the rules from a tree. Documentation here.  Once you've fit your model, you just need two lines of code. First, import export_text: Second, create an object that will contain your rules. To make the rules look more readable, use the feature_names argument and pass a list of your feature names. For example, if your model is called model and your features are named in a dataframe called X_train, you could create an object called tree_rules:  Then just print or save tree_rules. Your output will look like this:I have been working with the CountVectorizer class in scikit-learn. I understand that if used in the manner shown below, the final output will consist of an array containing counts of features, or tokens. These tokens are extracted from a set of keywords, i.e. The next step is: Where we get This is fine, but my situation is just a little bit different.   I want to extract the features the same way as above, but I don't want the rows in data to be the same documents that the features were extracted from. In other words, how can I get counts of another set of documents, say,  And get: I have read the documentation for the CountVectorizer class, and came across the vocabulary argument, which is a mapping of terms to feature indices.  I can't seem to get this argument to help me, however. Any advice is appreciated.
PS:  all credit due to Matthias Friedrich's Blog for the example I used above. You're right that vocabulary is what you want.  It works like this: So you pass it a dict with your desired features as the keys. If you used CountVectorizer on one set of documents and then you want to use the set of features from those documents for a new set, use the vocabulary_ attribute of your original CountVectorizer and pass it to the new one.  So in your example, you could do to create a new tokenizer using the vocabulary from your first one. You should call fit_transform or just fit on your original vocabulary source so that the vectorizer learns a vocab. Then you can use this fit vectorizer on any new data source via the transform() method. You can obtain the vocabulary produced by the fit (i.e. mapping of word to token ID) via vectorizer.vocabulary_ (assuming you name your CountVectorizer the name vectorizer. To verify that CountVectorizer is using the vocabulary learned from tags on new_docs: print vect.vocabulary_ again or compare the output of new_docs.toarray() to that of tags.toarray()I'm using a random forest model with 9 samples and about 7000 attributes.  Of these samples, there are 3 categories that my classifier recognizes.  I know this is far from ideal conditions but I'm trying to figure out which attributes are the most important in feature predictions.  Which parameters would be the best to tweak for optimizing feature importance?  I tried different n_estimators and noticed that the amount of "significant features" (i.e. nonzero values in the feature_importances_ array) increased dramatically.  I've read through the documentation but if anyone has any experience in this, I would like to know which parameters are the best to tune and a brief explanation why.  From my experience, there are three features worth exploring with the sklearn RandomForestClassifier, in order of importance: n_estimators max_features criterion n_estimators is not really worth optimizing. The more estimators you give it, the better it will do. 500 or 1000 is usually sufficient. max_features is worth exploring for many different values. It may have a large impact on the behavior of the RF because it decides how many features each tree in the RF considers at each split. criterion may have a small impact, but usually the default is fine. If you have the time, try it out. Make sure to use sklearn's GridSearch (preferably GridSearchCV, but your data set size is too small) when trying out these parameters. If I understand your question correctly, though, you only have 9 samples and 3 classes? Presumably 3 samples per class? It's very, very likely that your RF is going to overfit with that little amount of data, unless they are good, representative records. The crucial parts are usually three elements: This wonderful article has a detailed explanation of tunable parameters, how to track performance vs speed trade-off, some practical tips, and how to perform grid-search. n_estimators is good one as others said. It is also good at dealing with the overfitting when increasing it. But I think min_sample_split is also helpful when dealing with overfitting occurred in a small-sample but big-features dataset.I am new to the field of neural networks and I would like to know the difference between Deep Belief Networks and Convolutional Networks. 
Also, is there a Deep Convolutional Network which is the combination of Deep Belief and Convolutional Neural Nets? This is what I have gathered till now. Please correct me if I am wrong. For an image classification problem, Deep Belief networks have many layers, each of which is trained using a greedy layer-wise strategy. 
For example, if my image size is 50 x 50, and I want a Deep Network with 4 layers namely My input layer will have 50 x 50 = 2500 neurons, HL1 = 1000 neurons (say) , HL2 = 100 neurons (say) and output layer = 10 neurons,
 in order to train the weights (W1) between Input Layer and HL1, I use an AutoEncoder (2500 - 1000 - 2500) and learn W1 of size 2500 x 1000 (This is unsupervised learning). Then I feed forward all images through the first hidden layers to obtain a set of features and then use another autoencoder ( 1000 - 100 - 1000) to get the next set of features and finally use a softmax layer (100 - 10) for classification. (only learning the weights of the last layer (HL2 - Output which is the softmax layer) is supervised learning). (I could use RBM instead of autoencoder). If the same problem was solved using Convolutional Neural Networks, then for 50x50 input images, I would develop a network using only 7 x 7 patches (say). My layers would be And for learning the weights, I take 7 x 7 patches from images of size 50 x 50, and feed forward through convolutional layer, so I will have 25 different feature maps each of size (50 - 7 + 1) x (50 - 7 + 1) = 44 x 44. I then use a window of say 11x11 for pooling hand hence get 25 feature maps of size (4 x 4) for as the output of the pooling layer. I use these feature maps for classification. While learning the weights, I don't use the layer wise strategy as in Deep Belief Networks (Unsupervised Learning), but instead use supervised learning and learn the weights of all the layers simultaneously. Is this correct or is there any other way to learn the weights? Is what I have understood correct?  So if I want to use DBN's for image classification, I should resize all my images to a particular size (say 200x200) and have that many neurons in the input layer, whereas in case of CNN's, I train only on a smaller patch of the input (say 10 x 10 for an image of size 200x200) and convolve the learnt weights over the entire image? Do DBNs provide better results than CNNs or is it purely dependent on the dataset? Thank You. Generally speaking, DBNs are generative neural networks that stack Restricted Boltzmann Machines (RBMs) . You can think of RBMs as being generative autoencoders; if you want a deep belief net you should be stacking RBMs and not plain autoencoders as Hinton and his student Yeh proved that stacking RBMs results in sigmoid belief nets. Convolutional neural networks have performed better than DBNs by themselves in current literature on benchmark computer vision datasets such as MNIST. If the dataset is not a computer vision one, then DBNs can most definitely perform better. In theory, DBNs should be the best models but it is very hard to estimate joint probabilities accurately at the moment. You may be interested in Lee et. al's (2009) work on Convolutional Deep Belief Networks which looks to combine the two. I will try to explain the situation through learning shoes. If you use DBN to learn those images here is the bad thing that will happen in your learning algorithm  there will be shoes on different places.  all the neurons will try to learn not only shoes but also the place of the shoes in the images because it will not have the concept of 'local image patch' inside weights. DBN makes sense if all your images are aligned by means of size, translation and rotation. the idea of convolutional networks is that, there is a concept called weight sharing. If I try to extend this 'weight sharing' concept  first you looked at 7x7 patches, and according to your example - as an example of 3 of your neurons in the first layer you can say that they learned shoes 'front', 'back-bottom' and 'back-upper' parts as these would look alike for a 7x7 patch through all shoes. Normally the idea is to have multiple convolution layers one after another to learn  You can think of these 3 different things I told you as 3 different neurons. And such areas/neurons in your images will fire when there are shoes in some part of the image.  Pooling will protect your higher activations while sub-sampling your images and creating a lower-dimensional space to make things computationally easier and feasible.  So at last layer when you look at your 25X4x4, in other words 400 dimensional vector, if there is a shoe somewhere in the picture your 'shoe neuron(s)' will be active whereas non-shoe neurons will be close to zero.  And to understand which neurons are for shoes and which ones are not you will put that 400 dimensional vector to another supervised classifier(this can be anything like multi-class-SVM or as you said a soft-max-layer) I can advise you to have a glance at Fukushima 1980 paper to understand what I try to say about translation invariance and line -> arc -> semicircle -> shoe front -> shoe idea (http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf). Even just looking at the images in the paper will give you some idea.I am using sklearn for multi-classification task. I need to split alldata into train_set and test_set. I want to take randomly the same sample number from each class.
Actually, I amusing this function but it gives unbalanced dataset! Any suggestion. Although Christian's suggestion is correct, technically train_test_split should give you stratified results by using the stratify param. So you could do: The trick here is that it starts from version 0.17 in sklearn. From the documentation about the parameter stratify: stratify : array-like or None (default is None)
  If not None, data is split in a stratified fashion, using this as the labels array.
  New in version 0.17: stratify splitting You can use StratifiedShuffleSplit to create datasets featuring the same percentage of classes as the original one: If the classes are not balanced but you want the split to be balanced, then stratifying isn't going to help. There doesn't seem to be a method for doing balanced sampling in sklearn but it's kind of easy using basic numpy, for example a function like this might help you: Note that if you use this and sample more points per class than in the input data, then those will be upsampled (sample with replacement). As a result, some data points will appear multiple times and this may have an effect on the accuracy measures etc. And if some class has only one data point, there will be an error. You can easily check the numbers of points per class for example with np.unique(target, return_counts=True) Another approach is to over- or under- sample from your stratified test/train split. The imbalanced-learn library is quite handy for this, specially useful if you are doing online learning & want to guarantee balanced train data within your pipelines. This is my implementation that I use to get train/test data indexes This is the function I am using. You can adapt it and optimize it.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 1 year ago. What is the best way to perform hyperparameter optimization for a Pytorch model? Implement e.g. Random Search myself? Use Skicit Learn? Or is there anything else I am not aware of? Many researchers use RayTune. It's a scalable hyperparameter tuning framework, specifically for deep learning. You can easily use it with any deep learning framework (2 lines of code below), and it provides most state-of-the-art algorithms, including HyperBand, Population-based Training, Bayesian Optimization, and BOHB. [Disclaimer: I contribute actively to this project!] What I found is following: More young projects: UPDATE
something new: Ax: Adaptive Experimentation Platform by facebook  BoTorch: Bayesian Optimization in PyTorch Also, I found a useful table at post by @Richard Liaw:  You can use Bayesian optimization (full disclosure, I've contributed to this package) or Hyperband.  Both of these methods attempt to automate the hyperparameter tuning stage.  Hyperband is supposedly the state of the art in this space.  Hyperband is the only parameter-free method that I've heard of other than random search.  You can also look into using reinforcement learning to learn the optimal hyperparameters if you prefer. The simplest parameter-free way to do black box optimisation is random search, and it will explore high dimensional spaces faster than a grid search. There are papers on this but tl;dr with random search you get different values on every dimension each time, while with grid search you don't. Bayesian optimisation has good theoretical guarantees (despite the approximations), and implementations like Spearmint can wrap any script you have; there are hyperparameters but users don't see them in practice. Hyperband got a lot of attention by showing faster convergence than Naive Bayesian optimisation. It was able to do this by running different networks for different numbers of iterations, and Bayesian optimisation doesn't support that naively. While it is possible to do better with a Bayesian optimisation algorithm that can take this into account, such as FABOLAS, in practice hyperband is so simple you're probably better using it and watching it to tune the search space at intervals.I found that scaling in SVM (Support Vector Machine) problems really improve its performance.
I have read this explanation: The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Unfortunately this didn't help me. Can somebody provide a better explanation? Feature scaling is a general trick applied to optimization problems (not just SVM). The underline algorithm to solve the optimization problem of SVM is gradient descend. Andrew Ng has a great explanation in his coursera videos here. I will illustrate the core ideas here (I borrow Andrew's slides). Suppose you have only two parameters and one of the parameters can take a relatively large range of values. Then the contour of the cost function 
can look like very tall and skinny ovals (see blue ovals below). Your gradients (the path of gradient is drawn in red) could take a long time and go back and forth to find the optimal solution.
 Instead if your scaled your feature, the contour of the cost function might look like circles; then the gradient can take a much more straight path and achieve the optimal point much faster. 
 The true reason behind scaling features in SVM is the fact, that this classifier is not affine transformation invariant. In other words, if you multiply one feature by a 1000 than a solution given by SVM will be completely different. It has nearly nothing to do with the underlying optimization techniques (although they are affected by these scales problems, they should still converge to global optimum). Consider an example: you have man and a woman, encoded by their sex and height (two features). Let us assume a very simple case with such data: 0 -> man
1 -> woman And let us do something silly. Train it to predict the sex of the person, so we are trying to learn f(x,y)=x (ignoring second parameter). It is easy to see, that for such data largest margin classifier will "cut" the plane horizontally somewhere around height "175", so once we get new sample "0 178" (a woman of 178cm height) we get the classification that she is a man.  However, if we scale down everything to [0,1] we get sth like and now largest margin classifier "cuts" the plane nearly vertically (as expected) and so given new sample "0 178" which is also scaled to around "0 0.56" we get that it is a woman (correct!) So in general - scaling ensures that just because some features are big it won't lead to using them as a main predictor.  Just personal thoughts from another perspective.
1. why feature scaling influence?
There's a word in applying machine learning algorithm, 'garbage in, garbage out'. The more real reflection of your features, the more accuracy your algorithm will get. That applies too for how machine learning algorithms treat relationship between features. Different from human's brain, when machine learning algorithms do the classify for example, all the features are expressed and calculated by the same coordinate system, which in some sense, establish a priori assumption between the features(not really reflection of data itself). And also the nature of most algorithms is to find the most appropriate weight percentage between the features to fittest the data. So when these algorithms' input is unscaled features, large scale data has more influence on the weight. Actually it's not the reflection of data iteself.
2. why usually feature scaling improve the accuracy?
The common practice in unsupervised machine learning algorithms about the hyper-parameters(or hyper-hyper parameters) selection(for example, hierachical Dirichlet process, hLDA) is that you should not add any personal subjective assumption about data. The best way is just to assume that they have the equality probability to appear. I think it applies here too. The feature scaling just try to make the assumption that all the features has the equality opportunity to influence the weight, which more really reflects the information/knowledge you know about the data. Commonly also result in better accuracy. BTW, about the affine transformation invariant and converge faster, there's are interest link here on stats.stackexchange.com. We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.
This is from Andrews NG coursera course. So, it is done to do something like standardizing the data.
Sometimes researchers want to know if a specific observation is common or exceptional. express a score in terms of the number of standard deviations it is removed from the mean. This number is what we call a z-score. If we recode original scores into z-scores, we say that we standardize a variable. From what i have learnt from the Andrew Ng course on coursera is that feature scaling helps us to achieve the gradient decent more quickly,if the data is more spread out,that means if it has a higher standerd deviation,it will relatively take more time to calculate the gradient decent compared to the situation when we scale our data via feature scaling  The Idea of scaling is to remove exess computes on a particular variable by standardising all the variable on to a same scale with this we tend to calculate the slope a lot more easier ( y = mx + c) where we are normalizing the M parameter to converge as quickly as possible. Yes if normalisation is not there then contour will be skinny thus with normalisation:I'm using R package randomForest to do a regression on some biological data. My training data size is 38772 X 201. I just wondered---what would be a good value for the number of trees ntree and the number of variable per level mtry? Is there an approximate formula to find such parameter values? Each row in my input data is a 200 character representing the amino acid sequence, and I want to build a regression model to use such sequence in order to predict the distances between the proteins.    The default for mtry is quite sensible so there is not really a need to muck with it. There is a function tuneRF for optimizing this parameter. However, be aware that it may cause bias.  There is no optimization for the number of bootstrap replicates. I often start with ntree=501 and then plot the random forest object. This will show you the error convergence based on the OOB error. You want enough trees to stabilize the error but not so many that you over correlate the ensemble, which leads to overfit.  Here is the caveat: variable interactions stabilize at a slower rate than error so, if you have a large number of independent variables you need more replicates. I would keep the ntree an odd number so ties can be broken.  For the dimensions of you problem I would start ntree=1501. I would also recommended looking onto one of the published variable selection approaches to reduce the number of your independent variables.              The short answer is no. The randomForest function of course has default values for both ntree and mtry. The default for mtry is often (but not always) sensible, while generally people will want to increase ntree from it's default of 500 quite a bit. The "correct" value for ntree generally isn't much of a concern, as it will be quite apparent with a little tinkering that the predictions from the model won't change much after a certain number of trees. You can spend (read: waste) a lot of time tinkering with things like mtry (and sampsize and maxnodes and nodesize etc.), probably to some benefit, but in my experience not a lot. However, every data set will be different. Sometimes you may see a big difference, sometimes none at all. The caret package has a very general function train that allows you to do a simple grid search over parameter values like mtry for a wide variety of models. My only caution would be that doing this with fairly large data sets is likely to get time consuming fairly quickly, so watch out for that. Also, somehow I forgot that the ranfomForest package itself has a tuneRF function that is specifically for searching for the "optimal" value for mtry. Could this paper help ? 
Limiting the Number of Trees in Random Forests  Abstract. The aim of this paper is to propose a simple procedure that
  a priori determines a minimum number of classifiers to combine in order
  to obtain a prediction accuracy level similar to the one obtained with the
  combination of larger ensembles. The procedure is based on the McNemar
  non-parametric test of significance. Knowing a priori the minimum
  size of the classifier ensemble giving the best prediction accuracy, constitutes
  a gain for time and memory costs especially for huge data bases
  and real-time applications. Here we applied this procedure to four multiple
  classifier systems with C4.5 decision tree (Breiman’s Bagging, Ho’s
  Random subspaces, their combination we labeled ‘Bagfs’, and Breiman’s
  Random forests) and five large benchmark data bases. It is worth noticing
  that the proposed procedure may easily be extended to other base
  learning algorithms than a decision tree as well. The experimental results
  showed that it is possible to limit significantly the number of trees. We
  also showed that the minimum number of trees required for obtaining
  the best prediction accuracy may vary from one classifier combination
  method to another They never use more than 200 trees.   One nice trick that I use is to initially start with first taking square root of the number of predictors and plug that value for "mtry". It is usually around the same value that tunerf funtion in random forest would pick.  I use the code below to check for accuracy as I play around with ntree and mtry (change the parameters):Restore original text from Keras’s imdb dataset I want to restore imdb’s original text from Keras’s imdb dataset. First, when I load Keras’s imdb dataset, it returned sequence of word index. 
 I found imdb.get_word_index method(), it returns word index dictionary like {‘create’: 984, ‘make’: 94,…}. For converting, I create index word dictionary.


 Then, I tried to restore original text like following. 
 I’m not good at English, but I know this sentence is something strange. Why is this happened? How can I restore original text? Your example is coming out as gibberish, it's much worse than just some missing stop words. If you re-read the docs for the start_char, oov_char, and index_from parameters of the [keras.datasets.imdb.load_data](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification
) method they explain what is happening: start_char: int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character. oov_char: int. words that were cut out because of the num_words or skip_top limit will be replaced with this character. index_from: int. Index actual words with this index and higher. That dictionary you inverted assumes the word indices start from 1. But the indices returned my keras have <START> and <UNKNOWN> as indexes 1 and 2. (And it assumes you will use 0 for <PADDING>). This works for me: The punctuation is missing, but that's all: You can get the original dataset without stop words removed using get_file from  keras.utils.data_utils: Credit - Jeremy Howards fast.ai course lesson 5 This happened because of a basic NLP data preparation. Loads of the so called stop words were removed from text in order to make learning feasible. Usually - also the most of puntuation and less frequent words are removed from text during preprocessing. I think that the only way to restore original text is to find the most matching texts at IMDB using e.g. a Google's browser API. This encoding will work along with the labels: Upvote if helps. :) The indices are offset by 3 because 0, 1 and 2 are reserved indices for "padding", "start of sequence" and "unknown". The following should work. This works for me: To get an equivalent array of all the reviews: Try below code. this code worked. Get the index details: Build the Key-Value pair:I am using Linear regression to predict data. But, I am getting totally contrasting results when I Normalize (Vs) Standardize variables.  Normalization               = x -xmin/ xmax – xmin
 
Zero Score Standardization  = x - xmean/ xstd
  Thanks,
Santosh Note that the results might not necessarily be so different. You might simply need different hyperparameters for the two options to give similar results. The ideal thing is to test what works best for your problem. If you can't afford this for some reason, most algorithms will probably benefit from standardization more so than from normalization. See here for some examples of when one should be preferred over the other: For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; but more about PCA in my previous article). However, this doesn’t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale. One disadvantage of normalization over standardization is that it loses some information in the data, especially about outliers. Also on the linked page, there is this picture:  As you can see, scaling clusters all the data very close together, which may not be what you want. It might cause algorithms such as gradient descent to take longer to converge to the same solution they would on a standardized data set, or it might even make it impossible. "Normalizing variables" doesn't really make sense. The correct terminology is "normalizing / scaling the features". If you're going to normalize or scale one feature, you should do the same for the rest. That makes sense because normalization and standardization do different things. Normalization transforms your data into a range between 0 and 1 Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1 Normalization/standardization are designed to achieve a similar goal, which is to create features that have similar ranges to each other. We want that so we can be sure we are capturing the true information in a feature, and that we dont over weigh a particular feature just because its values are much larger than other features. If all of your features are within a similar range of each other then theres no real need to standardize/normalize. If, however, some features naturally take on values that are much larger/smaller than others then normalization/standardization is called for If you're going to be normalizing at least one variable/feature, I would do the same thing to all of the others as well First question is why we need Normalisation/Standardisation? => We take a example of dataset where we have salary variable and age variable.
Age can take range from 0 to 90 where salary can be from 25thousand to 2.5lakh. We compare difference for 2 person then age difference will be in range of below 100 where salary difference will in range of thousands.  So if we don't want one variable to dominate other then we use either Normalisation or Standardization. Now both age and salary will be in same scale
 but when we use standardiztion or normalisation, we lose original values and it is transformed to some values. So loss of interpretation but extremely important when we want to draw inference from our data. Normalization rescales the values into a range of [0,1]. also called min-max scaled. Standardization rescales data to have a mean (μ) of 0 and standard deviation (σ) of 1.So it gives a normal graph.  Example below:  Another example:  In above image, you can see that our actual data(in green) is spread b/w 1 to 6, standardised data(in red) is spread around -1 to 3 whereas normalised data(in blue) is spread around 0 to 1. Normally many algorithm required you to first standardise/normalise data before passing as parameter. Like in PCA, where we do dimension reduction by plotting our 3D data into 1D(say).Here we required standardisation. But in Image processing, it is required to normalise pixels before processing.
But during normalisation, we lose outliers(extreme datapoints-either too low or too high) which is slight disadvantage. So it depends on our preference what we chose but standardisation is most recommended as it gives a normal curve. None of the mentioned transformations shall matter for linear regression as these are all affine transformations. Found coefficients would change but explained variance will ultimately remain the same. So, from linear regression perspective, Outliers remain as outliers (leverage points). And these transformations also will not change the distribution. Shape of the distribution remains the same. lot of people use Normalisation and Standardisation interchangeably. The purpose remains the same is to bring features into the same scale. The approach is to subtract each value from min value or mean and divide by max value minus min value or SD respectively. The difference you can observe that when using min value u will get all value + ve and mean value u will get bot + ve and -ve values. This is also one of the factors to decide which approach to use.I got an keras(h5) file. I need to convert it to tflite??
I researched, First i need to go via h5 -> pb -> tflite
(because h5 - tflite sometimes results in some issue) You can use the TFLiteConverter to directly convert .h5 files to .tflite file.
This does not work on Windows. For Windows, use this Google Colab notebook to convert. Upload the .h5 file and it will convert it .tflite file. Follow, if you want to try it yourself : Create a code cell and insert this code. Run the cell. You will get a model.tflite file. Right click on the file and select "DOWNLOAD" option. This worked for me on Windows 10 using Tensorflow 2.1.0 and Keras 2.3.1 Just did this from CoLab using this code in a notebook: I had difficulty uploading the h5 model via CoLab so I mounted my Google Drive, uploaded it there, and then moved it over to the notebook content folder. If You are using Google Colab Notebook try this: This works for me. I am using keras==2.6.0 and tensorflow-cpu==2.5.0
version. For more information, you can visit https://www.tensorflow.org/guide/keras/save_and_serialize . If you are using Tensorflow-2 then you can follow these steps: In TF-2, it requires to load the Keras model instance and returns a converted instance. Check out this link for more details. There is one factor, which you must to consider. You need to change the learning phase, before converting. It's super important, when you have Dropout or Batch Normalization. You can take a look at 'Keras model to tflite' or 'Problem after converting keras model into Tensorflow pb' discussions Only some specific version of Tensorflow and Keras works properly in all the os. I even tried toco command line but it has issues too. Use tensorflow==1.13.0-rc1 and keras==2.1.3 and then after this will work For Tensorflow GPU 2.6.2 and Keras 2.6.0:What exactly does the LogisticRegression.predict_proba function return? In my example I get a result like this: From other calculations, using the sigmoid function, I know, that the second column are probabilities. The documentation says, that the first column are n_samples, but that can't be, because my samples are reviews, which are texts and not numbers. The documentation also says, that the second column are n_classes. That certainly can't be, since I only have two classes (namely +1 and -1) and the function is supposed to be about calculating probabilities of samples really being of a class, but not the classes themselves. What is the first column really and why it is there? The first column is the probability that the entry has the -1 label and the second column is the probability that the entry has the +1 label. Note that classes are ordered as they are in self.classes_. If you would like to get the predicted probabilities for the positive label only, you can use logistic_model.predict_proba(data)[:,1]. This will yield you the [9.95342389e-01, 2.41487300e-02, 1.66258341e-05] result.I am currently reading the Machine Learning book by Tom Mitchell. When talking about neural networks, Mitchell states: "Although the perceptron rule finds a successful weight vector when
  the training examples are linearly separable, it can fail to converge
  if the examples are not linearly separable. " I am having problems understanding what he means with "linearly separable"? Wikipedia tells me that "two sets of points in a two-dimensional space are linearly separable if they can be completely separated by a single line." But how does this apply to the training set for neural networks? How can inputs (or action units) be linearly separable or not?  I'm not the best at geometry and maths - could anybody explain it to me as though I were 5? ;) Thanks! Suppose you want to write an algorithm that decides, based on two parameters, size and price, if an house will sell in the same year it was put on sale or not. So you have 2 inputs, size and price, and one output, will sell or will not sell. Now, when you receive your training sets, it could happen that the output is not accumulated to make our prediction easy (Can you tell me, based on the first graph if X will be an N or S? How about the second graph): Where: As you can see in the first graph, you can't really separate the two possible outputs (sold/not sold) by a straight line, no matter how you try there will always be both S and N on the both sides of the line, which means that your algorithm will have a lot of possible lines but no ultimate, correct line to split the 2 outputs (and of course to predict new ones, which is the goal from the very beginning). That's why linearly separable (the second graph) data sets are much easier to predict. This means that there is a hyperplane (which splits your input space into two half-spaces) such that all points of the first class are in one half-space and those of the second class are in the other half-space. In two dimensions, that means that there is a line which separates points of one class from points of the other class. EDIT: for example, in this image, if blue circles represent points from one class and red circles represent points from the other class, then these points are linearly separable.  In three dimensions, it means that there is a plane which separates points of one class from points of the other class. In higher dimensions, it's similar: there must exist a hyperplane which separates the two sets of points. You mention that you're not good at math, so I'm not writing the formal definition, but let me know (in the comments) if that would help. Look at the following two data sets: The left data set is not linearly separable (without using a kernel). The right one is separable into two parts for A' andB` by the indicated line. I.e. You cannot draw a straight line into the left image, so that all the X are on one side, and all the O are on the other. That is why it is called "not linearly separable" == there exist no linear manifold separating the two classes. Now the famous kernel trick (which will certainly be discussed in the book next) actually allows many linear methods to be used for non-linear problems by virtually adding additional dimensions to make a non-linear problem linearly separable.All this time (specially in Netflix contest), I always come across this blog (or leaderboard forum) where they mention how by applying a simple SVD step on data helped them in reducing sparsity in data or in general improved the performance of their algorithm in hand.
I am trying to think (since long time) but I am not able to guess why is it so.
In general, the data in hand I get is very noisy (which is also the fun part of bigdata) and then I do know some basic feature scaling stuff like log-transformation stuff , mean normalization.
But how does something like SVD helps.
So lets say i have a huge matrix of user rating movies..and then in this matrix, I implement some version of recommendation system (say collaborative filtering): how does it helps SVD is not used to normalize the data, but to get rid of redundant data, that is, for dimensionality reduction. For example, if you have two variables, one is humidity index and another one is probability of rain, then their correlation is so high, that the second one does not contribute with any additional information useful for a classification or regression task. The eigenvalues in SVD help you determine what variables are most informative, and which ones you can do without.  The way it works is simple. You perform SVD over your training data (call it matrix A), to obtain U, S and V*. Then set to zero all values of S less than a certain arbitrary threshold (e.g. 0.1), call this new matrix S'. Then obtain A' = US'V* and use A' as your new training data. Some of your features are now set to zero and can be removed, sometimes without any performance penalty (depending on your data and the threshold chosen). This is called k-truncated SVD. SVD doesn't help you with sparsity though, only helps you when features are redundant. Two features can be both sparse and informative (relevant) for a prediction task, so you can't remove either one. Using SVD, you go from n features to k features, where each one will be a linear combination of the original n. It's a dimensionality reduction step, just like feature selection is. When redundant features are present, though, a feature selection algorithm may lead to better classification performance than SVD depending on your data set (for example, maximum entropy feature selection). Weka comes with a bunch of them. See: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition https://stats.stackexchange.com/questions/33142/what-happens-when-you-apply-svd-to-a-collaborative-filtering-problem-what-is-th The Singular Value Decomposition is often used to approximate a matrix X by a low rank matrix X_lr: The matrix X_lr is then the best approximation of rank k of the matrix X, for the Frobenius norm (the equivalent of the l2-norm for matrices). It is computationally efficient to use this representation, because if your matrix X is n by n and k << n, you can store its low rank approximation with only (2n + 1)k coefficients (by storing U, D' and V). This was often used in matrix completion problems (such as collaborative filtering) because the true matrix of user ratings is assumed to be low rank (or well approximated by a low rank matrix). So, you wish to recover the true matrix by computing the best low rank approximation of your data matrix. However, there are now better ways to recover low rank matrices from noisy and missing observations, namely nuclear norm minimization. See for example the paper The power of convex relaxation: Near-optimal matrix completion by E. Candes and T. Tao.  (Note: the algorithms derived from this technique also store the SVD of the estimated matrix, but it is computed differently). PCA or SVD, when used for dimensionality reduction, reduce the number of inputs. This, besides saving computational cost of learning and/or predicting, can sometimes produce more robust models that are not optimal in statistical sense, but have better performance in noisy conditions. Mathematically, simpler models have less variance, i.e. they are less prone to overfitting. Underfitting, of-course, can be a problem too. This is known as bias-variance dilemma. Or, as said in plain words by Einstein: Things should be made as simple as possible, but not simpler.What is the difference between cross-entropy and log loss error? The formulae for both seem to be very similar. They are essentially the same; usually, we use the term log loss for binary classification problems, and the more general cross-entropy (loss) for the general case of multi-class classification, but even this distinction is not consistent, and you'll often find the terms used interchangeably as synonyms. From the Wikipedia entry for cross-entropy: The logistic loss is sometimes called cross-entropy loss. It is also known as log loss From the fast.ai wiki entry on log loss [link is now dead]: Log loss and cross-entropy are slightly different depending on the context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing. From the ML Cheatsheet: Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.Simple machine learning question. Probably numerous ways to solve this: There is an infinite stream of  4 possible events: 'event_1', 'event_2', 'event_4', 'event_4' The events do not come in in completely random order. We will assume that there are some complex patterns to the order that most events come in, and the rest of the events are just random. We do not know the patterns ahead of time though. After each event is received, I want to predict what the next event will be based on the order that events have come in in the past. So my question is: What machine learning algorithm should I use for this predictor? The predictor will then be told what the next event actually was: The question arises of how long of a history that the predictor should maintain, since maintaining infinite history will not be possible. I'll leave this up to you to answer. The answer can't be infinte though for practicality. So I believe that the predictions will have to be done with some kind of rolling history. Adding a new event and expiring an old event should therefore be rather efficient, and not require rebuilding the entire predictor model, for example. Specific code, instead of research papers, would add for me immense value to your responses. Python or C libraries are nice, but anything will do. Update: And what if more than one event can happen simultaneously on each round. Does that change the solution? This is essentially a sequence prediction problem, so you want Recurrent neural networks or hidden Markov models. If you only have a fixed time to look back, time window approaches might suffice. You take the sequence data and split it into overlapping windows of length n. (eg. you split a sequence ABCDEFG into ABC, BCD, CDE, DEF, EFG). Then you train a function approximator (e.g. neural network or linear regression) to map the first n-1 parts of that window onto the nth part. Your predictor will not be able to look back in time longer than the size of your window. RNNs and HMMs can do so in theory, but are hard to tune or sometimes just don't work. (State of the art RNN implementations can be found in PyBrain http://pybrain.org) Update: Here is the pybrain code for your problem. (I haven't tested it, there might be some typos and stuff, but the overall structure should work.) This will train the recurrent network for 1000 epochs and print out the error after every epochs. Afterwards you can check for correct predictions like this: This will print an array of booleans for every event. Rather than keeping a full history, one can keep aggregated information about the past (along with a relatively short sliding history, to be used as input to the Predictor logic). A tentative implementation could go like this:
In a nutshell: Managing a set of Markov chains of increasing order, and grading and averaging their predictions There can be several variations on the general logic described above. In particular in the choice of the particular metric used to "grade" the quality of prediction of the individual N-Gram lengths. 
Other considerations should be put with regards to detecting and adapting to possible shifts in the events distribution (the above assumes a generally ergodic event source).  One possible approach is to use two sets of tables (combining the probabilities accordingly), and periodically dropping the contents of all tables of one of the sets.  Choosing the right period for these resets is a tricky business, essentially balancing the need for statistically significant volumes of history and the need for short enough period lest me miss on the shorter modulations... The question arises of how long of a history that the predictor should maintain The only answer is "it depends". It depends on how accurate this needs to be. I don't believe this strategy could ever be 100% accurate even with an infinite history. Try out a history of 10 and you'll get x% accuracy, then try 100 and you'll get y% accuracy, etc etc... Eventually you should find either the system is as accurate as you desire it to be or you will find the rise in accuracy will not be worth the increase in history length (and increased memory usage, processing time etc...). At this point either job done, or you need to find a new strategy. For what it is worth i think looking into a simple "soft" neural net might be a better plan. We just studied about branch-predictors in computer architecture (Because the processor would take too long to actually evaluate a condition if(EXPRESSION), it tries to 'guess' and save some time that way). I am sure more research has been done in this area, but that's all I can think of at the moment. I haven't seen a unique setup like yours, so I think you might need to do some preliminary experimentation on your own. Try running your solution for X number of seconds with a history of N slots, what is the correctness ratio? And compare that with the same fixed X and varying N history slots to try to find the best memory-history ratio (graphing them out ).  If more than one event can happen simulataneously... that's a little mind bending, there has to be some constraints there : what if infinite number of events happen at a time? Uhoh, that's computationally impossible for you. I'd try the same approach as just one event at a time, except where the predictor is enabled predict multiple events at a time. Processors use a few really lightweight tricks to predict whether a branch statement will branch or not. This helps them with efficient pipe-lining. They may not be as general as Markov models for instance, but they are interesting because of their simplicity. Here is the Wikipedia article on branch prediction. See the Saturating Counter, and the Two-Level Adaptive PredictorMy machine has the following spec:  CPU: Xeon E5-1620 v4 GPU: Titan X (Pascal)  Ubuntu 16.04 Nvidia driver 375.26 CUDA tookit 8.0 cuDNN 5.1 I've benchmarked on the following Keras examples with Tensorflow as the backed reference:  My gpu is clearly out performing my cpu in non-lstm models.  Has anyone else experienced this?  If you use Keras, use CuDNNLSTM in place of LSTM or CuDNNGRU in place of GRU. In my case (2 Tesla M60), I am seeing 10x boost of performance. By the way I am using batch size 128 as suggested by @Alexey Golyshev. Too small batch size. Try to increase. Results for my GTX1050Ti: It's just a tip. Using GPU is powerful when 1. your neural network model is big. 
2. batch size is big. It's what I found from googling. I have got similar issues here: CPU: Intel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz Ubuntu 14.04 imdb_bidirectional_lstm.py: 155s GPU: GTX 860m Nvidia Driver: 369.30 CUDA Toolkit: v8.0 cuDNN: v6.0 imdb_bidirectional_lstm.py:450s When I observe the GPU load curve, I found one interesting thing:  GPU load This is mainly due to the sequential computation in LSTM layer. Remember that LSTM requires sequential input to calculate hidden layer weights iteratively, in other words, you must wait for hidden state at time t-1 to calculate hidden state at time t.  That's not a good idea for GPU cores, since they are many small cores who like doing computations in parallel, sequential compuatation can't fully utilize their computing powers. That's why we are seeing GPU load around 10% - 20% most of the time. But in the phase of backpropagation, GPU could run derivative computation in parallel, so we can see GPU load peak around 80%.I have a network which I want to train on some dataset (as an example, say CIFAR10). I can create data loader object via My question is as follows: Suppose I want to make several different training iterations. Let's say I want at first to train the network on all images in odd positions, then on all images in even positions and so on. In order to do that, I need to be able to access to those images. Unfortunately, it seems that trainset does not allow such access. That is, trying to do trainset[:1000] or more generally trainset[mask] will throw an error. I could do instead  and then However, that will force me to create a new copy of the full dataset in each iteration (as I already changed trainset.train_data so I will need to redefine trainset). Is there some way to avoid it? Ideally, I would like to have something "equivalent" to torch.utils.data.Subset is easier, supports shuffle, and doesn't require writing your own sampler: You can define a custom sampler for the dataset loader avoiding recreating the dataset (just creating a new loader for each different sampling). PS: You can find more info here: http://pytorch.org/docs/master/_modules/torch/utils/data/sampler.html#SamplerIn scikit-learn, all estimators have a fit() method, and depending on whether they are supervised or unsupervised, they also have a predict() or transform() method. I am in the process of writing a transformer for an unsupervised learning task and was wondering if there is a rule of thumb where to put which kind of learning logic. The official documentation is not very helpful in this regard: fit_transform(X, y=None, **fit_params)
  Fit to data, then transform it. In this context, what is meant by both fitting data and transforming data? Fitting finds the internal parameters of a model that will be used to transform data. Transforming  applies the parameters to data. You may fit a model to one set of data, and then transform it on a completely different set. For example, you fit a linear model to data to get a slope and intercept. Then you use those parameters to transform (i.e., map) new or existing values of x to y. fit_transform is just doing both steps to the same data. A scikit example: You fit data to find the principal components. Then you transform your data to see how it maps onto these components: As other answers explain it, fit does not need to be doing anything (except from returning the transformer object). It is there so that all transformers have the same interface and work nicely with stuff like pipelines.
Of course some transformers need a fit method (think tf-idf, PCA...) that actually does things.
The transform method needs to return the transformed data. fit_transform is a convenience method that chains the fit and transform operations. You can get it for free (!) by deriving your custom transformer class from TransformerMixin and implementing fit and transform. In this case, calling the fit method does not do anything. As you can see in this example, not all transformers need to actually do something with fit or transform methods. My guess is that every class in scikit-learn should implement the fit, transform and/or predict in order for it to be consistent with the rest of the package. But I guess this is indeed quite an overkill.I'm working on a multivariate (100+ variables) multi-step (t1 to t30) forecasting problem where the time series frequency is every 1 minute. The problem requires to forecast one of the 100+ variables as target.
I'm interested to know if it's possible to do it using FB Prophet's Python API. I was able to do it in a univariate fashion using only the target variable and the datetime variable. Any help and direction is appreciated. Please let me know if any further input or clarity is needed on the question. You can add additional variables in Prophet using the add_regressor method. For example if we want to predict variable y using also the values of the additional variables add1 and add2. Let's first create a sample df: and split train and test: Before training the forecaster, we can add regressors that use the additional variables. Here the argument of add_regressor is the column name of the additional variable in the training df. The predict method will then use the additional variables to forecast: Note that the additional variables should have values for your future (test) data. If you don't have them, you could start by predicting add1 and add2 with univariate timeseries, and then predict y with add_regressor and the predicted add1 and add2 as future values of the additional variables. From the documentation I understand that the forecast of y for t+1 will only use the values of add1 and add2 at t+1, and not their values at t, t-1, ..., t-n as it does with y. If that is important for you, you could create new additional variables with the lags. See also this notebook, with an example of using weather factors as extra regressors in a forecast of bicycle usage. To do forecasting for more than one dependent variable you need to implement that time series using Vector Auto Regression.  In  VAR model, each variable is a linear function of the past values of itself and the past values of all the other variables. for more information on VAR go to https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/ I am confused, it seems like there is no agreement if Prophet works in multivariate way, see the github issues here and here. Judging by some comments, queise's answer and a nice youtube tutorial you can somehow make a work around to multivariate functionality, see the video here: https://www.youtube.com/watch?v=XZhPO043lqU This might be late, however if you are reading this in 2019, you can implement multivariate time series using LSTM, Keras. You can do this with one line using the timemachines package that wraps prophet in a functional form. See prophet skaters to be precise. Here's an example of use: Note that you can set k to be whatever you want. That is the number of steps ahead to use. Now be careful, because when prophet says multivariate they are really referring to variables known in advance (the a argument). It doesn't really address multivariate prediction. But you can use the facebook skater called _recursive to use prophet to predict the exogenous variables before it predicts the one you really care about. Having said all that, I strongly advise you to read this critique of prophet and also check its position on the Elo ratings before using it in anger. The answer to the original question is yes! Here is a link to specific Neural prophet documentation with several examples of how to use multivariate inputs. For neuralprophet, these are referred to as 'lagged regressors'. https://neuralprophet.com/html/lagged_covariates_energy_ercot.html yes indeed we can now apply multivariate time series forecasting,
here is the solution.
https://medium.com/@bobrupakroy/yes-our-favorite-fbprophet-is-back-with-multivariate-forecasting-785fbe412731
 VAR is a pure econometric model, but after reading a lot of literature on forecasting , i see that VAR also suffers from not able to capture the trend. So then we are not having a good forecast, but still VAR is a workhorse model for multivariate analysis. I think prophet is not for a multivariate, rather use the ML models like RF,XGBOOST, NNET ... but keep in mind that if you want to capture the trend then be sure which model is better. Else go for deep learningI am copying the pyspark.ml example from the official document website:
http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer However, the example above wouldn't run and gave me the following errors: What additional configuration/variable needs to be set to get the example running? You can add to the begining of your code to define a SparkSession, then the spark.createDataFrame() should work. Answer by 率怀一 is good and will work for the first time.
But the second time you try it, it will throw the following exception : There are two ways to avoid it. 1) Using SparkContext.getOrCreate() instead of SparkContext(): 2) Using sc.stop() in the end, or before you start another SparkContext. Since you are calling createDataFrame(), you need to do this: instead of this: spark stands there as the sqlContext.  In general, some people have that as sc, so if that didn't work, you could try: You have to import the spark as following if you are using python then it will create
a spark session but remember it is an old method though it will work. If it errors you regarding other open session do this:Here's a puzzle... I have two databases of the same 50000+ electronic products and I want to match products in one database to those in the other. However, the product names are not always identical. I've tried using the Levenshtein distance for measuring the string similarity however this hasn't worked. For example, These items are the same, yet their product names vary quite a lot.  On the other hand... These are different products with very similar product names. How should I tackle this problem?  My first thought is to try to parse the names into a description of features (company LG, size 42 Inch, resolution 1080p, type LCD HDTV). Then you can match these descriptions against each other for compatibility; it's okay to omit a product number but bad to have different sizes. Simple are-the-common-attributes-compatible might be enough, or you might have to write / learn rules about how much different attributes are allowed to differ and so on. Depending on how many different kinds of products you have and how different the listed names are, I might actually start by manually defining a set of attributes and possibly even just adding specific words / regexes to match them, iteratively seeing what isn't been parsed so far and adding rules for that. I'd imagine there's not a lot of ambiguity in terms of one vocabulary item possibly belonging to multiple attributes, though without seeing your database I guess I don't know. If that's not going to be feasible, this extraction is kind of analogous to semi-supervised part-of-speech tagging. It's somewhat different, though, in that I imagine the vocabulary is much more limited than typical parsing, and in that the space of product names is more heirarchical: the resolution tag only applies to certain kinds of products. I'm not very familiar with that literature; there might be some ideas you could use. Use a large set of training examples. For each possible pair in this example set: Now, when you get a pair of strings for which you want to decide if they are same or not, extract the features like you did in the training set and create the tuple of numbers for the distance between the various components of the string. Feed the tuple to the trained SVM and classify if they are same or not. The advantage of using a learning approach like this is that you don't have to keep modifying the rules over and over again, and also the system learns the differences between a large pair of products that are same and different. You could use LibSVM package in WEKA to do this. I don't know that much about machine learning, but I do know Levenshtein distance is not the best approach for this type of problem. I am working on an extremely similar problem currently, and have found much more accurate matches using Largest Consecutive Sub Sequence (https://www.geeksforgeeks.org/longest-consecutive-subsequence). You may also find Longest Common Substring helpful too (https://www.geeksforgeeks.org/longest-common-substring-dp-29/). ... Or maybe even a combination of both! Levenshtein is not great because it allows for substitutions, which can easily discount similar strings which have extra characters.
For example, "Hello AAAAAA", "Hello", and "BBBBB". "Hello" and "BBBBB" are closer by Levenshtein distance, even though you would probably like "Hello" to match with "Hello AAAAAA". LCS and LSS do not allow substitutions, so with both of these methods, "Hello" would match with "Hello AAAAAA".I have a trained model that I've exported the weights and want to partially load into another model.
My model is built in Keras using TensorFlow as backend. Right now I'm doing as follows: I'm sure it's a terrible way to do it, although it works. How do I load just the first 9 layers? If your first 9 layers are consistently named between your original trained model and the new model, then you can use model.load_weights() with by_name=True.  This will update weights only in the layers of your new model that have an identically named layer found in the original trained model. The name of the layer can be specified with the name keyword, for example: This call: will return a list of all weight tensors in the model, as Numpy arrays. All what you have to do next is to iterate over this list and apply: where model.layers is a flattened list of the layers comprising the model. In this case, you reload the weights of the first 9 layers. More information is available here: https://keras.io/layers/about-keras-layers/ https://keras.io/models/about-keras-models/I would like to get a confidence score of each of the predictions that it makes, showing on how sure the classifier is on its prediction that it is correct.   I want something like this: How sure is the classifier on its prediction? Class 1: 81% that this is class 1
Class 2: 10%
Class 3: 6%
Class 4: 3%   Samples of my code: I suspect that I would use the score() function, but I seem to keep implementing it correctly. I don't know if that's the right function or not, but how would one get the confidence percentage of a classifier's prediction? Per the SVC documentation, it looks like you need to change how you construct the SVC: and then use the predict_proba method: For those estimators implementing predict_proba() method, like Justin Peel suggested, You can just use predict_proba() to produce probability on your prediction. For those estimators which do not implement predict_proba() method, you can construct confidence interval by yourself using bootstrap concept (repeatedly calculate your point estimates in many sub-samples). Let me know if you need any detailed examples to demonstrate either of these two cases.I ran PCA on a data frame with 10 features using this simple code: The result of pca.explained_variance_ratio_ shows: I believe that means that the first PC explains 52% of the variance, the second component explains 29% and so on... What I dont undestand is the output of pca.components_. If I do the following:  I get the data frame bellow where each line is a principal component. 
What I'd like to understand is how to interpret that table. I know that if I square all the features on each component and sum them I get 1, but what does the -0.56 on PC1 mean? Dos it tell something about "Feature E" since it is the highest magnitude on a component that explains 52% of the variance?  Thanks Terminology: First of all, the results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score). PART1: I explain how to check the importance of the features and how to plot a biplot. PART2: I explain how to check the importance of the features and how to save them into a pandas dataframe using the feature names. Summary in an article: Python compact guide: https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=friends_link&sk=65bf5440e444c24aff192fedf9f8b64f In your case, the value -0.56 for Feature E is the score of this feature on the PC1. This value tells us 'how much' the feature influences the PC (in our case the PC1). So the higher the value in absolute value, the higher the influence on the principal component. After performing the PCA analysis, people usually plot the known 'biplot' to see the transformed features in the N dimensions (2 in our case) and the original variables (features). I wrote a function to plot this. Example using iris data: Results  The important features are the ones that influence more the components and thus, have a large absolute value on the component. TO  get the most important features on the PCs with names and save them into a pandas dataframe use this: This prints: So on the PC1 the feature named e is the most important and on PC2 the d. Summary in an article: Python compact guide: https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=friends_link&sk=65bf5440e444c24aff192fedf9f8b64f Basic Idea The Principle Component breakdown by features that you have there basically tells you the "direction" each principle component points to in terms of the direction of the features.  In each principle component, features that have a greater absolute weight "pull" the principle component more to that feature's direction.  For example, we can say that in PC1, since Feature A, Feature B, Feature I, and Feature J have relatively low weights (in absolute value), PC1 is not as much pointing in the direction of these features in the feature space. PC1 will be pointing most to the direction of Feature E relative to other directions.  Visualization in Lower Dimensions For a visualization of this, look at the following figures taken from here and here: The following shows an example of running PCA on correlated data. 
 We can visually see that both eigenvectors derived from PCA are being "pulled" in both the Feature 1 and Feature 2 directions. Thus, if we were to make a principle component breakdown table like you made, we would expect to see some weightage from both Feature 1 and Feature 2 explaining PC1 and PC2.  Next, we have an example with uncorrelated data.  Let us call the green principle component as PC1 and the pink one as PC2. It's clear that PC1 is not pulled in the direction of feature x', and as isn't PC2 in the direction of feature y'. 
Thus, in our table, we must have a weightage of 0 for feature x' in PC1 and a weightage of 0 for feature y' in PC2.  I hope this gives an idea of what you're seeing in your table.I've got about 300k documents stored in a Postgres database that are tagged with topic categories (there are about 150 categories in total).  I have another 150k documents that don't yet have categories.  I'm trying to find the best way to programmaticly categorize them. I've been exploring NLTK and its Naive Bayes Classifier.  Seems like a good starting point (if you can suggest a better classification algorithm for this task, I'm all ears). My problem is that I don't have enough RAM to train the NaiveBayesClassifier on all 150 categoies/300k documents at once (training on 5 categories used 8GB).  Furthermore, accuracy of the classifier seems to drop as I train on more categories (90% accuracy with 2 categories, 81% with 5, 61% with 10). Should I just train a classifier on 5 categories at a time, and run all 150k documents through the classifier to see if there are matches?  It seems like this would work, except that there would be a lot of false positives where documents that don't really match any of the categories get shoe-horned into on by the classifier just because it's the best match available...  Is there a way to have a "none of the above" option for the classifier just in case the document doesn't fit into any of the categories? Here is my test class http://gist.github.com/451880 You should start by converting your documents into TF-log(1 + IDF) vectors: term frequencies are sparse so you should use python dict with term as keys and count as values and then divide by total count to get the global frequencies. Another solution is to use the abs(hash(term)) for instance as positive integer keys. Then you an use scipy.sparse vectors which are more handy and more efficient to perform linear algebra operation than python dict. Also build the 150 frequencies vectors by averaging the frequencies of all the labeled documents belonging to the same category. Then for new document to label, you can compute the cosine similarity  between the document vector and each category vector and choose the most similar category as label for your document. If this is not good enough, then you should try to train a logistic regression model using a L1 penalty as explained in this example of scikit-learn (this is a wrapper for liblinear as explained by @ephes). The vectors used to train your logistic regression model should be the previously introduced TD-log(1+IDF) vectors to get good performance (precision and recall). The scikit learn lib offers a sklearn.metrics module with routines to compute those score for a given model and given dataset. For larger datasets: you should try the vowpal wabbit which is probably the fastest rabbit on earth for large scale document classification problems (but not easy to use python wrappers AFAIK). How big (number of words) are your documents? Memory consumption at 150K trainingdocs should not be an issue. Naive Bayes is a good choice especially when you have many categories with only a few training examples or very noisy trainingdata. But in general, linear Support Vector Machines do perform much better. Is your problem multiclass (a document belongs only to one category exclusivly) or multilabel (a document belongs to one or more categories)?  Accuracy is a poor choice to judge classifier performance. You should rather use precision vs recall, precision recall breakeven point (prbp), f1, auc and have to look at the precision vs recall curve where recall (x) is plotted against precision (y) based on the value of your confidence-threshold (wether a document belongs to a category or not). Usually you would build one binary classifier per category (positive training examples of one category vs all other trainingexamples which don't belong to your current category). You'll have to choose an optimal confidence threshold per category. If you want to combine those single measures per category into a global performance measure, you'll have to micro (sum up all true positives, false positives, false negatives and true negatives and calc combined scores) or macro (calc score per category and then average those scores over all categories) average. We have a corpus of tens of million documents, millions of training examples and thousands of categories (multilabel). Since we face serious training time problems (the number of documents are new, updated or deleted per day is quite high), we use a modified version of liblinear. But for smaller problems using one of the python wrappers around liblinear (liblinear2scipy or scikit-learn) should work fine. Is there a way to have a "none of the
  above" option for the classifier just
  in case the document doesn't fit into
  any of the categories? You might get this effect simply by having a "none of the above" pseudo-category trained each time.  If the max you can train is 5 categories (though I'm not sure why it's eating up quite so much RAM), train 4 actual categories from their actual 2K docs each, and a "none of the above" one with its 2K documents taken randomly from all the other 146 categories (about 13-14 from each if you want the "stratified sampling" approach, which may be sounder). Still feels like a bit of a kludge and you might be better off with a completely different approach -- find a multi-dimensional doc measure that defines your 300K pre-tagged docs into 150 reasonably separable clusters, then just assign each of the other yet-untagged docs to the appropriate cluster as thus determined.  I don't think NLTK has anything directly available to support this kind of thing, but, hey, NLTK's been growing so fast that I may well have missed something...;-)What is the difference between MinMaxScaler() and StandardScaler(). mms = MinMaxScaler(feature_range = (0, 1)) (Used in a machine learning model) sc = StandardScaler() (In another machine learning model they used standard-scaler and not min-max-scaler) MinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion). StandardScaler() will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalised by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal. If there are outliers, use RobustScaler(). Alternatively you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed) Additional Note: If scaler is used before train_test_split, data leakage will happen. Do use scaler after train_test_split From ScikitLearn site:  StandardScaler removes the mean and scales the data to unit variance.
  However, the outliers have an influence when computing the empirical
  mean and standard deviation which shrink the range of the feature
  values as shown in the left figure below. Note in particular that
  because the outliers on each feature have different magnitudes, the
  spread of the transformed data on each feature is very different: most
  of the data lie in the [-2, 4] range for the transformed median income
  feature while the same data is squeezed in the smaller [-0.2, 0.2]
  range for the transformed number of households. StandardScaler therefore cannot guarantee balanced feature scales in
  the presence of outliers. MinMaxScaler rescales the data set such that all feature values are in
  the range [0, 1] as shown in the right panel below. However, this
  scaling compress all inliers in the narrow range [0, 0.005] for the
  transformed number of households. Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.
Scaling the data means it helps to  Normalize the data within a particular range. When MinMaxScaler is used the it is also known as Normalization and it transform all the values in range between (0 to 1)
formula is x = [(value - min)/(Max- Min)] StandardScaler comes under Standardization and its value ranges between (-3 to +3)
formula is z = [(x - x.mean)/Std_deviation] Before implementing MinMaxScaler or Standard Scaler you should know about the distribution of your dataset. StandardScaler rescales a dataset to have a mean of 0 and a standard deviation of 1. Standardization is very useful if data has varying scales and the algorithm assumption about data having a gaussian distribution. Normalization or MinMaxScaler rescale a dataset so that each value fall between 0 and 1. It is useful when data has varying scales and the algorithm does not make assumptions about the distribution. It is a good technique when we did not know about the distribution of data or when we know the distribution is not gaussian.I am fairly new to Tensorflow and ML in general, so I hereby apologize for a (likely) trivial question.  I use the dropout technique to improve learning rates of my network, and it seems to work just fine. Then, I would like to test the network on some data to see if it works like this: Obviously, it yields different results each time as the dropout is still in place. One solution I can think of is to create two separate models - one for a training and the other one for an actual later use of the network, however, such a solution seems impractical to me. What's the common approach to solving this problem?  The easiest way is to change the keep_prob parameter using a placeholder_with_default: in this way when you train you can set the parameter like this: and when you evaluate the default value of 1.0 is used. With the new tf.estimator API you specify a model function, that returns different models, based on whether you are training or testing, but still allows you to reuse your model code.
In your model function you would do something similar to: The mode argument is automatically passed depending on whether you call estimator.train(...) or estimator.predict(...). you should set the keep_prob in tensorflow dropout layer, that is the probability to keep the weight, I think you set that variable with values between 0.5 and 0.8.
When testing the network you must simply feed keep_prob with 1. You should define something like that: Then change the values in the session: if you don't want to use Estimator API, you can create the dropout this way: So, you feed the session with {'tf_is_training': False} when doing evaluation instead of changing the dropout rate. With the update of Tensorflow, the class tf.layer.dropout should be used instead of tf.nn.dropout. This supports an is_training parameter. Using this allows your models to define keep_prob once, and not rely on your feed_dict to manage the external parameter. This allows for better refactored code. More info: https://www.tensorflow.org/api_docs/python/tf/layers/dropout When you test you are supposed to multiply the output of the layer by 1/drop_prob.  In that case you would have to put an additional multiplication step in the test phase.I am creating a pipeline in scikit learn,  and  computing the accuracy using cross validation How can I report confusion matrix instead of 'accuracy'? You could use cross_val_predict(See the scikit-learn docs) instead of cross_val_score. instead of doing : you can do : Short answer is "you cannot". You need to understand difference between cross_val_score and cross validation as model selection method. cross_val_score as name suggests, works only on scores. Confusion matrix is not a score, it is a kind of summary of what happened during evaluation. A major distinction is that a score is supposed to return an orderable object, in particular in scikit-learn - a float. So, based on score you can tell whether method b is better from a by simply comparing if b has bigger score. You cannot do this with confusion matrix which, again as name suggests, is a matrix.  If you want to obtain confusion matrices for multiple evaluation runs (such as cross validation) you have to do this by hand, which is not that bad in scikit-learn - it is actually a few lines of code. I think what you really want is average of confusion matrices obtained from each cross-validation run. @lejlot already nicely explained why, I'll just upgrade his answer with calculation of mean of confusion matrices: Calculate confusion matrix in each run of cross validation. You can use something like this: On the end you can calculate your mean of list of numpy arrays (confusion matrices) with: What you can do though is to define a scorer that uses certain values from the confusion matrix. See here [link]. Just citing the code: This will perform the cross validation for each of these four scorers and return the scoring dictionary cv_results, e.g., with keys test_tp, test_tn, etc. containing the confusion matrices' values from each cross-validation split. From this you could reconstruct an average confusion matrix, but the cross_val_predict of Xema seems more elegant for this. Note that this will actually not work with cross_val_score; you'll need cross_validate (introduced in scikit-learn v0.19). Side note: you could use one of these scorers (i.e. one element of the matrix) for  hyper-parameter optimization via grid search. I am new to machine learning. If I understand correctly, the confusion matrix can obtain from 4 value, which are TP, FN, FP and TN. Those 4 value cannot obtain directly from scoring, but it is implied in accuracy, precision and recall. Now it has 4 unknown TP, FN, FP and TN. Eq1 : tp/(tp+fp)=P   Eq2 : tp/(tp+fn)=R   Eq3 : (tp+tn)/(tp+fn+fp+tn)=A   Assuming one of the unknown is 1, then it becomes 3 unknown and 3 equations. The relative value can be solved using system of equation. P R A can obtain from scoring cross_validate can get all 3 source at one timeI wrote a vanilla autoencoder using only Dense layer. 
Below is my code: 1) softmax provides probability distribution. Understood. This means, I would have a vector of 784 values with probability between 0 and 1. For example [ 0.02, 0.03..... upto 784 items], summing all 784 elements provides 1.  2) I don't understand how the binary crossentropy works with these values. Binary cross entropy is for two values of output, right? In the context of autoencoders the input and output of the model is the same. So, if the input values are in the range [0,1] then it is acceptable to use sigmoid as the activation function of last layer. Otherwise, you need to use an appropriate activation function for the last layer (e.g. linear which is the default one). As for the loss function, it comes back to the values of input data again. If the input data are only between zeros and ones (and not the values between them), then binary_crossentropy is acceptable as the loss function. Otherwise, you need to use other loss functions such as 'mse' (i.e. mean squared error) or 'mae' (i.e. mean absolute error). Note that in the case of input values in range [0,1] you can use binary_crossentropy, as it is usually used (e.g. Keras autoencoder tutorial and this paper). However, don't expect that the loss value becomes zero since binary_crossentropy does not return zero when both prediction and label are not either zero or one (no matter they are equal or not). Here is a video from Hugo Larochelle where he explains the loss functions used in autoencoders (the part about using binary_crossentropy with inputs in range [0,1] starts at 5:30) Concretely, in your example, you are using the MNIST dataset. So by default the values of MNIST are integers in the range [0, 255]. Usually you need to normalize them first: Now the values would be in range [0,1]. So sigmoid can be used as the activation function and either of binary_crossentropy or mse as the loss function. Why binary_crossentropy can be used even when the true label values (i.e. ground-truth) are in the range [0,1]? Note that we are trying to minimize the loss function in training. So if the loss function we have used reaches its minimum value (which may not be necessarily equal to zero) when prediction is equal to true label, then it is an acceptable choice. Let's verify this is the case for binray cross-entropy which is defined as follows: where y is the true label and p is the predicted value. Let's consider y as fixed and see what value of p minimizes this function: we need to take the derivative with respect to p (I have assumed the log is the natural logarithm function for simplicity of calculations): As you can see binary cross-entropy have the minimum value when y=p, i.e. when the true label is equal to predicted label and this is exactly what we are looking for.Hi can anyone help why I am getting AttributeError: 'SMOTE' object has no attribute 'fit_sample' error? I don't think this code should cause any error? Thanks If you import like this you need to do fit_resample()This post was edited and submitted for review 5 months ago and failed to reopen the post: Original close reason(s) were not resolved I just read about the Keras weight initializers in here. In the documentation, only different initializers has been introduced. Such as: I want to know what is the default weight when I don't specify the kernel_initializer argument. 
Is there a way to access it? Each layer has its own default value for initializing the weights. For most of the layers, such as Dense, convolution and RNN layers, the default kernel initializer is 'glorot_uniform' and the default bias intializer is 'zeros' (you can find this by going to the related section for each layer in the documentation; for example here is the Dense layer doc). You can find the definition of glorot_uniform initializer here in the Keras documentation. As for accessing the weights of each layer, it has already been answered here.I know xgboost need first gradient and second gradient, but anybody else has used "mae"  as obj function? A little bit of theory first, sorry! You asked for the grad and hessian for MAE, however, the MAE is not continuously twice differentiable so trying to calculate the first and second derivatives becomes tricky. Below we can see the "kink" at x=0 which prevents the MAE from being continuously differentiable. Moreover, the second derivative is zero at all the points where it is well behaved. In XGBoost, the second derivative is used as a denominator in the leaf weights, and when zero, creates serious math-errors. Given these complexities, our best bet is to try to approximate the MAE using some other, nicely behaved function. Let's take a look.  We can see above that there are several functions that approximate the absolute value. Clearly, for very small values, the Squared Error (MSE) is a fairly good approximation of the MAE. However, I assume that this is not sufficient for your use case. Huber Loss is a well documented loss function. However, it is not smooth so we cannot guarantee smooth derivatives. We can approximate it using the Psuedo-Huber function. It can be implemented in python XGBoost as follows, Other function can be used by replacing the obj=huber_approx_obj. Fair Loss is not well documented at all but it seems to work rather well. The fair loss function is:  It can be implemented as such, This code is taken and adapted from the second place solution in the Kaggle Allstate Challenge. Log-Cosh Loss function. Finally, you can create your own custom loss functions using the above functions as templates. Warning: Due to API changes newer versions of XGBoost may require loss functions for the form: For the Huber loss above, I think the gradient is missing a negative sign upfront. Should be as  I am running the huber/fair metric from above on ~normally distributed Y, but for some reason with alpha <0 (and all the time for fair) the result prediction will equal to zero...Is there a way to predict how long it will take to run a classifier from sci-kit learn based on the parameters and dataset?  I know, pretty meta, right? Some classifiers/parameter combinations are quite fast, and some take so long that I eventually just kill the process.  I'd like a way to estimate in advance how long it will take. Alternatively, I'd accept some pointers on how to set common parameters to reduce the run time. There are very specific classes of classifier or regressors that directly report remaining time or progress of your algorithm (number of iterations etc.). Most of this can be turned on by passing verbose=2 (any high number > 1) option to the constructor of individual models. Note: this behavior is according to sklearn-0.14. Earlier versions have a bit different verbose output (still useful though). The best example of this is ensemble.RandomForestClassifier or ensemble.GradientBoostingClassifier` that print the number of trees built so far and remaining time. Or This progress information is fairly useful to estimate the total time.  Then there are other models like SVMs that print the number of optimization iterations completed, but do not directly report the remaining time. Models like linear models don't provide such diagnostic information as far as I know.  Check this thread to know more about what the verbosity levels mean: scikit-learn fit remaining time If you are using IPython, you can consider to use the built-in magic commands such as %time and %timeit %time - Time execution of a Python statement or expression. The CPU and wall clock times are printed, and the value of the expression (if any) is returned. Note that under Win32, system time is always reported as 0, since it can not be measured. %timeit - Time execution of a Python statement or expression using the timeit module. Example: References:  https://ipython.readthedocs.io/en/stable/interactive/magics.html http://scikit-learn.org/stable/developers/performance.html We're actually working on a package that gives runtime estimates of scikit-learn fits.  You would basically run it right before running the algo.fit(X, y) to get the runtime estimation. Here's a simple use case: Feel free to take a look!TLDR: scikit's roc_curve function is only returning 3 points for a certain dataset. 
Why could this be, and how do we control how many points to get back? I'm trying to draw a ROC curve, but consistently get a "ROC triangle".  Just to make sure the lengths are ok:  Returns 13759 on both.  Then running:  returns [ 0.          0.28240129  1.        ] If I call threasholds, I get array([ 0.4822225 , -0.5177775 , -0.84595197]) (always only 3 points).  It is therefore no surprise that my ROC curve looks like a triangle.  What I cannot understand is why scikit's roc_curve is only returning 3 points. Help hugely appreciated.     The number of points depend on the number of unique values in the input. Since the input vector has only 2 unique values, the function gives correct output. I had the same problem with a different example. The mistake I made was to input the outcomes for a given threshold and not the probabilities in the argument y_score of roc_curve. It also gives a plot with three points but it is a mistake ! I ran into same problem, and after reading the documentaion carefully I realized that the mistake is in:  Although, there were hints pointed by others by checking the uniqueness. It should be instead: It's not necessary to get 1 point except (0,0) and (1,1).
I'm using mushrooms dataset from kaggle for a binary classification problem.
Procuring fpr and tpr from roc_curve, I'm getting 4 more points, though their value is more or less same. fpr = {0, 0, 0.02290076, 0.0267176, 0.832061, 1} tpr = {0, 0.0315361, 0.985758, 0.996948, 1, 1} I'm not sure if we can consider this as 1 point because plotting the curve using this looks like the one shown in question.What is the difference between a Bayesian network and a Naive Bayes classifier? I noticed one is just implemented in Matlab as classify the other has an entire net toolbox.  If you could explain in your answer which one is more likely to provide a better accuracy as well I would be grateful (not a pre-requisite).   Short answer, if you're only interested in solving a prediction task: use Naive Bayes. A Bayesian network (has a good wikipedia page) models relationships between features in a very general way. If you know what these relationships are, or have enough data to derive them, then it may be appropriate to use a Bayesian network. A Naive Bayes classifier is a simple model that describes particular class of Bayesian network - where all of the features are class-conditionally independent. Because of this, there are certain problems that Naive Bayes cannot solve (example below). However, its simplicity also makes it easier to apply, and it requires less data to get a good result in many cases. You have a learning problem with binary features x1 and x2 and a target variable y = x1 XOR x2. In a Naive Bayes classifier, x1 and x2 must be treated independently - so you would compute things like "The probability that y = 1 given that x1 = 1" - hopefully you can see that this isn't helpful, because x1 = 1 doesn't make y = 1 any more or less likely. Since a Bayesian network does not assume independence, it would be able to solve such a problem. Naive Bayes is just a restricted/constrained form of a general Bayesian network where you enforce the constraint that the class node should have no parents and that the nodes corresponding to the attribute variables should have no edges between them. As such, there is nothing that prevents a general Bayesian network from being used for classification - the predicted class is the one with the maximum probability when (conditioned on) all the other variables are set to the prediction instance values in the usual Bayesian inference fashion. A good paper to read on this is "Bayesian Network Classifiers, Machine Learning, 29, 131–163 (1997)". Of particular interest is section 3. Though Naive Bayes is a constrained form of a more general Bayesian network, this paper also talks about why Naive Bayes can and does outperform a general Bayesian network in classification tasks. For the Bayesian network as a classifier, the features are selected based on some scoring functions like Bayesian scoring function and minimal description length(the two are equivalent in theory to each other given that there are enough training data). The scoring functions mainly restrict the structure (connections and directions) and the parameters(likelihood) using the data. After the structure has been learned the class is only determined by the nodes in the Markov blanket(its parents, its children, and the parents of its children), and all variables given the Markov blanket are discarded.  For the Naive Bayesian Network which is more well-known nowadays, all features are considered as attributes and are independent given the class.  Bayesian networks and naive Bayesian network have their own advantages and disadvantages and we can see the performance comparison(done on 25 data sets mainly from the UCI repository) as depicted below:   We can see that there are some points below the diagonal line representing the Naive Bayes performs better than the Bayesian Network on those datasets and some points above the diagonal line representing the reverse on some other datasets.  Bayesian Network is more complicated than the Naive Bayes but they almost perform equally well, and the reason is that all the datasets on which the Bayesian network performs worse than the Naive Bayes have more than 15 attributes. That's during the structure learning some crucial attributes are discarded.  We can combine the two and add some connections between the features of the Naive Bayes and it becomes the tree augmented Naive Bayes or k-dependence Bayesian classifier.   References:
1. Bayesian Network ClassifiersI've checked the source code for both functions, and it seems that LSTM() makes the LSTM network in general, while LSTMCell() only returns one cell.  However, in most cases people only use one LSTM Cell in their program. Does this mean when you have only one LSTM Cell (ex. in simple Seq2Seq), calling LSTMCell() and LSTM() would make no difference? A recurrent layer contains a cell object. The cell contains the core code for the calculations of each step, while the recurrent layer commands the cell and performs the actual recurrent calculations.  Usually, people use LSTM layers in their code.
Or they use RNN layers containing LSTMCell. Both things are almost the same. An LSTM layer is a RNN layer using an LSTMCell, as you can check out in the source code.  About the number of cells: Alghout it seems, because of its name, that LSTMCell is a single cell, it is actually an object that manages all the units/cells as we may think. In the same code mentioned, you can see that the units argument is used when creating an instance of LSTMCell.libsvm and liblinear are both software libraries that implement Support Vector Machines. What's the difference? And how do the differences make liblinear faster than libsvm? In practice the complexity of the SMO algorithm (that works both for kernel and linear SVM) as implemented in libsvm is O(n^2) or O(n^3) whereas liblinear is O(n) but does not support kernel SVMs. n is the number of samples in the training dataset. Hence for medium to large scale forget about kernels and use liblinear (or maybe have a look at approximate kernel SVM solvers such as LaSVM). Edit: in practice libsvm becomes painfully slow at 10k samples. SVM is support vector machine, which is basically a linear classifier, but using many kernel transforms to turn a non-linear problem into a linear problem beforehand. From the link above, it seems like liblinear is very much the same thing, without those kernel transforms. So, as they say, in cases where the kernel transforms are not needed (they mention document classification), it will be faster. From : http://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf It supports L2-regularized logistic regression (LR), L2-loss and L1-loss linear support vector machines (SVMs) (Boser et al., 1992). It inherits many features of the popular SVM library LIBSVM And you might also see some useful information here from one of the creators: http://agbs.kyb.tuebingen.mpg.de/km/bb/showthread.php?tid=710 The main idea, I would say, is that liblinear is optimized to deal with linear classification (i.e. no kernels necessary), whereas linear classification is only one of the many capabilities of libsvm, so logically it may not match up to liblinear in terms of classification accuracy. Obviously, I'm making some broad generalizations here, and the exact details on the differences are probably covered in the paper I linked above as well as with the corresponding user's guide to libsvm from the libsvm website.Caffe can not only print overall accuracy, but also per-class accuracy. In Keras log, there's only overall accuracy. It's hard for me to calculate the separate class accuracy. Anybody who knows how to output per-class accuracy in keras? Precision & recall are more useful measures for multi-class classification (see definitions). Following the Keras MNIST CNN example (10-class classification), you can get the per-class measures using classification_report from sklearn.metrics: Here is the result: You are probably looking to use a callback, which you can easily add to the model.fit() call. For example, you can define your own class using the keras.callbacks.Callback interface. I recommend using the on_epoch_end() function since it will format nicely inside of your training summary if you decide to print with that verbosity setting. Please note that this particular code block is set to use 3 classes, but you can of course change it to your desired number. Then, you are going to want to configure your new callback to your model fit. Assuming your validation data (val_data) is some tuple pair, you can use the following: Please note that the _ indicates values likely to change based on your configuration For train per-class accuracy: implement below on training dataset - after (and/or before) training on the dataset.  Example: >>{
'0': [0.25, 0.50], 
'1': [0.50, 0.25], 
'2': [0.75, 1.00], 
'3': [1.00, 0.00]  
} Update to the solution provided by Solution by desertnaut: 
Now in Keras, you will get an error AttributeError: 'Sequential' object has no attribute
'predict_classes'" To fix the error use the following code:Is it possible to use XGBoost for multi-label classification? Now I use OneVsRestClassifier over GradientBoostingClassifier from sklearn. It works, but use only one core from my CPU. In my data I have ~45 features and the task is to predict about 20 columns with binary (boolean) data. Metric is mean average precision (map@7). If you have a short example of code to share, that would be great. One possible approach, instead of using OneVsRestClassifier which is for multi-class tasks, is to use MultiOutputClassifier from the sklearn.multioutput module. Below is a small reproducible sample code with the number of input features and target outputs requested by the OP There are a couple of ways to do that, one of which is the one you already suggested: 1. clf_multilabel will fit one binary classifier per class, and it will use however many cores you specify in params (fyi, you can also specify n_jobs in OneVsRestClassifier, but that eats up more memory). 2.
If you first massage your data a little by making k copies of every data point that has k correct labels, you can hack your way to a simpler multiclass problem. At that point, just to get classification margins/probabilities for each class and decide what threshold you want for predicting a label.
Note that this solution is not exact: if a product has tags (1, 2, 3), you artificially introduce two negative samples for each class. You can add a label to each class you want to predict.
For example if this is your data: You can simply reshape your data by adding a label to the input, according to the output, and xgboost should learn how to treat it accordingly, like so: This way you will have a 1-dimensional Y, but you can still predict many labels.I have a model trained using Keras with Tensorflow as my backend, but now I need to turn my model into a tensorflow graph for a certain application. I attempted to do this and make predictions to insure that it is working correctly, but when comparing to the results gathered from model.predict() I get very different values. For instance: returns: The values from keras predict are correct, but the tf graph results are not. If it helps to know the final intended application, I am creating a jacobian matrix with the tf.gradients() function, but currently it does not return the correct results when comparing to theano's jacobian function, which gives the correct jacobian. Here is my tensorflow jacobian code: EDIT: Model code @frankyjuang linked me to here  https://github.com/amir-abdi/keras_to_tensorflow and combining this with code from  https://github.com/metaflow-ai/blog/blob/master/tf-freeze/load.py and https://github.com/tensorflow/tensorflow/issues/675 I have found a solution to both predicting using a tf graph and creating the jacobian function: Call: Create function to load the tf model as a graph: Create a function to make model predictions using the tf graph Make predictions: Jacobian function: Compute Jacobian Matrix:Module's parameters get changed during training, that is, they are what is learnt during training of a neural network, but what is a buffer? and is it learnt during neural network training? Pytorch doc for register_buffer() method reads This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm’s running_mean is not a parameter, but is part of the persistent state. As you already observed, model parameters are learned and updated using SGD during the training process.
However, sometimes there are other quantities that are part of a model's "state" and should be
 - saved as part of state_dict.
 - moved to cuda() or cpu() with the rest of the model's parameters.
 - cast to float/half/double with the rest of the model's parameters.
Registering these "arguments" as the model's buffer allows pytorch to track them and save them like regular parameters, but prevents pytorch from updating them using SGD mechanism. An example for a buffer can be found in _BatchNorm module where the running_mean , running_var and num_batches_tracked are registered as buffers and updated by accumulating statistics of data forwarded through the layer. This is in contrast to weight and bias parameters that learns an affine transformation of the data using regular SGD optimization. Both parameters and buffers you create for a module (nn.Module). Say you have a linear layer nn.Linear. You already have weight and bias parameters. But if you need a new parameter you use register_parameter() to register a new named parameter that is a tensor. When you register a new parameter it will appear inside the module.parameters() iterator, but when you register a buffer it will not. The difference: Buffers are named tensors that do not update gradients at every step, like parameters.
For buffers, you create your custom logic (fully up to you). The good thing is when you save the model, all params and buffers are saved, and when you move the model to or off the CUDA params and buffers will go as well.I am trying to detect the outliers to my dataset and I find the sklearn's Isolation Forest. I can't understand how to work with it. I fit my training data in it and it gives me back a vector with -1 and 1 values. Can anyone explain to me how it works and provide an example? How can I know that the outliers are 'real' outliers? Tuning Parameters? Here is my code: It seems you have many questions, let me try to answer them one by one to the best of my knowledge. How it works? It works due to the fact that the nature of outliers in any data set, which is outliers, is few and different, which is quite different from the typical clustering-based or distance-based algorithm. At the top level, it works on the logic that outliers take fewer steps to 'isolate' compare to the 'normal' point in any data set.
To do so, this is what IF does; suppose you have training data set X with n data points, each having m features. In training, IF creates Isolation trees (Binary search trees) for different features. For training, you have 3 parameters for tuning during the train phase: max_samples is the number of random samples it will pick from the original data set for creating Isolation trees. During the test phase: sklearn_IF finds the path length of data point under test from all the trained Isolation Trees and finds the average path length. The higher the path length, the more normal the point, and vice-versa. Based on the average path length. It calculates the anomaly score, decision_function of sklearn_IF can be used to get this. For sklearn_IF, the lower the score, the more anomalous the sample. Based on the anomaly score, you can decide whether the given sample is anomalous or not by setting the proper value of contamination in the sklearn_IF object. The default value of contamination is 0.1, which you can tune for deciding the threshold. The amount of contamination of the data set, i.e., the proportion of outliers in the data set. Tuning parameters Training ->  n_estimators, max_samples, max_features. Testing  ->  contamination -1 represents the outliers (according to the fitted model). See IsolationForest example for a nice depiction of the process. If you have some prior knowledge, you could provide more parameters to get a more accurate fitting. For example, if you know the contamination (proportion of outliers in the data set) you could provide it as an input. By default it is assumed to be 0.1. See description of the parameters here. Let me add something, which I got stucked, when I read this question. Most of the time you are using it for binary classification (I would assume), where you have a majority class 0 and an outlier class 1. For exmaple if you want to detect fraud then your major class is non-fraud (0) and fraud is (1). Now if you have a train and test split: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,  random_state=42) and you run: The output for "normal" classifier scoring can be quite confusiong. As already mentioned the y_pred_testwill consists of [-1,1], where 1 is your majority class 0 and -1 is your minor class 1. So I can recommend you to convert it: Then you can use your normal scoring funtions etc.I am trying to run a PCA on a matrix of dimensions m x n where m is the number of features and n the number of samples. Suppose I want to preserve the nf features with the maximum variance. With scikit-learn I am able to do it in this way: Now, I get a new matrix X_new that has a shape of n x nf. Is it possible to know which features have been discarded or the retained ones? Thanks The features that your PCA object has determined during fitting are in pca.components_. The vector space orthogonal to the one spanned by pca.components_ is discarded. Please note that PCA does not "discard" or "retain" any of your pre-defined features (encoded by the columns you specify). It mixes all of them (by weighted sums) to find orthogonal directions of maximum variance. If this is not the behaviour you are looking for, then PCA dimensionality reduction is not the way to go. For some simple general feature selection methods, you can take a look at sklearn.feature_selection The projected features onto principal components will retain the important information (axes with maximum variances) and drop axes with small variances. This behavior is like to compression (Not discard).  And X_proj is the better name of X_new, because it is the projection of X onto principal components You can reconstruct the X_rec as Here, X_rec is close to X, but the less important information was dropped by PCA. So we can say X_rec is denoised. In my opinion, I can say the noise is discard. The answer marked above is incorrect. The sklearn site clearly states that the components_ array is sorted. so it can't be used to identify the important features. components_ : array, [n_components, n_features]
  Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.htmlI have class imbalance problem and want to solve this using cost sensitive learning.  Question  Scikit learn has 2 options called class weights and sample weights. Is sample weight actually doing option 2) and class weight options 1). Is option 2) the the recommended way of handling class imbalance.  It's similar concepts, but with sample_weights you can force estimator to pay more attention on some samples, and with class_weights you can force estimator to learn with attention to some particular class. sample_weight=0 or class_weight=0 basically means that estimator doesn't need to take into consideration such samples/classes in learning process at all. Thus classifier (for example) will never predict some class if class_weight = 0 for this class. If some sample_weight/class_weight bigger than sample_weight/class_weight on other samples/classes - estimator will try to minimize error on that samples/classes in the first place. You can use user-defined sample_weights and class_weights simultaneously. If you want to undersample/oversample your training set with simple cloning/removing - this will be equal to increasing/decreasing of corresponding sample_weights/class_weights. In more complex cases you can also try artificially generate samples, with  techniques like SMOTE. sample_weight and class_weight have a similar function, that is to make your estimator pay more attention to some samples.  Actual sample weights will be sample_weight * weights from class_weight. This serves the same purpose as under/oversampling but the behavior is likely to be different: say you have an algorithm that randomly picks samples (like in random forests), it matters whether you oversampled or not. To sum it up:
class_weight and sample_weight both do 2), option 2) is one way to handle class imbalance. I don't know of an universally recommended way, I would try 1), 2) and 1) + 2) on your specific problem to see what works best.I am training ML logistic classifier to classify two classes using python scikit-learn. They are in an extremely imbalanced data (about 14300:1). I'm getting almost 100% accuracy and ROC-AUC, but 0% in precision, recall, and f1 score. I understand that accuracy is usually not useful in very imbalanced data, but why is the ROC-AUC measure is close to perfect as well? The above is using logistic regression, below is using decision tree, the decision matrix looks almost identical, but the AUC is a lot different. One must understand crucial difference between AUC ROC and "point-wise" metrics like accuracy/precision etc. ROC is a function of a threshold. Given a model (classifier) that outputs the probability of belonging to each class, we predict the class that has the highest probability (support). However, sometimes we can get better scores by changing this rule and requiring one support to be 2 times bigger than the other to actually classify as a given class. This is often true for imbalanced datasets. This way you are actually modifying the learned prior of classes to better fit your data. ROC looks at "what would happen if I change this threshold to all possible values" and then AUC ROC computes the integral of such a curve.  Consequently:I'm setting up the new Tensorflow Object Detection API to find small objects in large areas of satellite imagery. It works quite well - it finds all 10 objects I want, but I also get 50-100 false positives [things that look a little like the target object, but aren't]. I'm using the sample config from the 'pets' tutorial, to fine-tune the faster_rcnn_resnet101_coco model they offer. I've started small, with only 100 training examples of my objects (just 1 class). 50 examples in my validation set. Each example is a 200x200 pixel image with a labeled object (~40x40) in the center. I train until my precision & loss curves plateau. I'm relatively new to using deep learning for object detection. What is the best strategy to increase my precision? e.g. Hard-negative mining? Increase my training dataset size? I've yet to try the most accurate model they offer faster_rcnn_inception_resnet_v2_atrous_coco as i'd like to maintain some speed, but will do so if needed. Hard-negative mining seems to be a logical step. If you agree, how do I implement it w.r.t setting up the tfrecord file for my training dataset? Let's say I make 200x200 images for each of the 50-100 false positives: I've revisited this topic recently in my work and thought I'd update with my current learnings for any who visit in the future. The topic appeared on Tensorflow's Models repo issue tracker. SSD allows you to set the ratio of how many negative:postive examples to mine (max_negatives_per_positive: 3), but you can also set a minimum number for images with no postives (min_negatives_per_image: 3). Both of these are defined in the model-ssd-loss config section.  That said, I don't see the same option in Faster-RCNN's model configuration. It's mentioned in the issue that models/research/object_detection/core/balanced_positive_negative_sampler.py contains the code used for Faster-RCNN. One other option discussed in the issue is creating a second class specifically for lookalikes. During training, the model will attempt to learn class differences which should help serve your purpose. Lastly, I came across this article on Filter Amplifier Networks (FAN) that may be informative for your work on aerial imagery. =================================================================== The following paper describes hard negative mining for the same purpose you describe:
Training Region-based Object Detectors with Online Hard Example Mining In section 3.1 they describe using a foreground and background class: Background RoIs. A region is labeled background (bg) if its maximum
  IoU with ground truth is in the interval [bg lo, 0.5). A lower
  threshold of bg lo = 0.1 is used by both FRCN and SPPnet, and is
  hypothesized in [14] to crudely approximate hard negative mining; the
  assumption is that regions with some overlap with the ground truth are
  more likely to be the confusing or hard ones. We show in Section 5.4
  that although this heuristic helps convergence and detection accuracy,
  it is suboptimal because it ignores some infrequent, but important,
  difficult background regions. Our method removes the bg lo threshold. In fact this paper is referenced and its ideas are used in Tensorflow's object detection losses.py code for hard mining: Based on your model config file, the HardMinerObject is returned by losses_builder.py in this bit of code: which is returned by model_builder.py and called by train.py. So basically, it seems to me that simply generating your true positive labels (with a tool like LabelImg or RectLabel) should be enough for the train algorithm to find hard negatives within the same images. The related question gives an excellent walkthrough. In the event you want to feed in data that has no true positives (i.e. nothing should be classified in the image), just add the negative image to your tfrecord with no bounding boxes.  I think I was passing through the same or close scenario and it's worth it to share with you. I managed to solve it by passing images without annotations to the trainer. On my scenario I'm building a project to detect assembly failures from my client's products, at real time.
I successfully achieved very robust results (for production env) by using detection+classification for components that has explicity a negative pattern (e.g. a screw that has screw on/off(just the hole)) and only detection for things that doesn't has the negative pattens (e.g. a tape that can be placed anywhere). On the system it's mandatory that the user record 2 videos, one containing the positive scenario and another containing the negative (or the n videos, containing n patterns of positive and negative so the algorithm can generalize). After a while testing I found out that if I register to detected only tape the detector was giving very confident (0.999) false positive detections of tape. It was learning the pattern where the tape was inserted instead of the tape itself. When I had another component (like a screw on it's negative format) I was passing the negative pattern of tape without being explicitly aware of it, so the FPs didn't happen. So I found out that, in this scenario, I had to necessarily pass the images without tape so it could differentiate between tape and no-tape. I considered two alternatives to experiment and try to solve this behavior: Concluded that both alternatives worked perfectly on my scenario.
The training loss got a little messy but the predictions work with robustness for my very controlled scenario (the system's camera has its own box and illumination to decrease variables). I had to make two little modifications for the first alternative to work: Part of the traning progress:  Currently I'm using tensorflow object detection along with tensorflow==1.15, using faster_rcnn_resnet101_coco.config. Hope it will solve someone's problem as I didn't found any solution on the internet. I read a lot of people telling that faster_rcnn is not adapted for negative training for FPs reduction but my tests proved the opposite.Is there any specific algorithm for handwriting recognition?
The algorithm should recognize the hand written letter. Any one could help would be greatly appreciated in advance. Thank you You can use a genetic algorithm: http://www.foibg.com/ibs_isc/ibs-02/IBS-02-p03.pdf You can use Greedy Point Match: http://www.cs.berkeley.edu/~fateman/msw/GreedyPointMatchWriteup.pdf I would suggest reading this paper: On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey OCR might be a good starting point. There's the dollar family of recognizers which you can use to recognize single or multistroke gestures (and potentially map that to your alphabet) The lastest member of this family is the $P Recognizer. Here's a starting point: http://depts.washington.edu/aimgroup/proj/dollar/pdollar.html Google has released an open source OCR engines Tesseract OCR, and It has an Python binding.
Hope this helps. This one seems to win all the major competitions lately: https://github.com/alexgraves/RNNLIB/wikiI just started working with keras and noticed that there are two layers with very similar names for max-pooling: MaxPool and MaxPooling. I was surprised that I couldn't find the difference between these two on Google; so I am wondering what the difference is between the two if any. They are basically the same thing (i.e. aliases of each other). For future readers who might want to know how this could be determined: go to the documentation page of the layer (you can use the list here) and click on "View aliases". This is then accompanied by a blue plus sign (+). For example, if you go to MaxPool2D documentation and do this, you will find MaxPooling2D in the list of aliases of this layer as follow:  They are the same... You can test it on your own I used 1D max-pooling but the same is valid for all the pooling operations (2D, 3D, avg, global pooling) Ther are the same. The library is soo many times updated that's why there are some functions with different names but with the same tasks. you can use any of them.I am in dire need of a classification task example using LibSVM in python. I don't know how the Input should look like and which function is responsible for training and which one for testing
Thanks The code examples listed here don't work with LibSVM 3.1, so I've more or less ported the example by mossplix: This example demonstrates a one-class SVM classifier; it's about as simple as possible while still showing the complete LIBSVM workflow. Step 1: Import NumPy & LIBSVM Step 2: Generate synthetic data: for this example, 500 points within a given boundary (note: quite a few real data sets are are provided on the  LIBSVM website)  Step 3: Now, choose some non-linear decision boundary for a one-class classifier: Step 4: Next, arbitrarily partition the data w/r/t this decision boundary: Class I: those that lie on or within an arbitrary circle Class II: all points outside the decision boundary (circle) The SVM Model Building begins here; all steps before this one were just to prepare some synthetic data. Step 5: Construct the problem description by calling svm_problem, passing in the decision boundary function and the data, then bind this result to a variable. Step 6: Select a kernel function for the non-linear mapping For this exmaple, i chose RBF (radial basis function) as my kernel function Step 7: Train the classifier, 
by calling svm_model, passing in the problem description (px) & kernel (pm) Step 8: Finally, test the trained classifier by calling predict on the trained model object ('v') For the example above, I used version 3.0 of LIBSVM (the current stable release at the time this answer was posted). Finally, w/r/t the part of your question regarding the choice of kernel function, Support Vector Machines are not specific to a particular kernel function--e.g., i could have chosen a different kernel (gaussian, polynomial, etc.).  LIBSVM includes all of the most commonly used kernel functions--which is a big help because you can see all plausible alternatives and to select one for use in your model, is just a matter of calling svm_parameter and passing in a value for kernel_type (a three-letter abbreviation for the chosen kernel). Finally, the kernel function you choose for training must match the kernel function used against the testing data. LIBSVM reads the data from a tuple containing two lists. The first list contains the classes and the second list contains the input data. create simple dataset with two possible classes
you also need to specify which kernel you want to use by creating svm_parameter. You might consider using http://scikit-learn.sourceforge.net/ That has a great python binding of libsvm and should be easy to install Adding to @shinNoNoir : param.kernel_type represents the type of kernel function you want to use,
0: Linear
1: polynomial
2: RBF
3: Sigmoid Also have in mind that, svm_problem(y,x) : here y is the class labels and x is the class instances and x and y can only be lists,tuples and dictionaries.(no numpy array) SVM via SciKit-learn: For more details here: http://scikit-learn.org/stable/modules/svm.html#svm Here is a dummy example I mashed up:  I don't know about the earlier versions but in LibSVM 3.xx the method svm_parameter('options') will takes just one argument. In my case C, G, p and nu are the dynamic values. You make changes according to your code. options: Source of documentation: https://www.csie.ntu.edu.tw/~cjlin/libsvm/It always amazed me how the Akinator app could guess a character by asking just several questions. So I wonder what kind of algorithm or method let it do that? Is there a name for that class of algorithms and where can I read more about them? Yes, there is a name for these class of algorithms - it is called classification algorithms in the field of machine learning. Decision trees is one example for classification algorithm. In this classification problem, the features for the algorithm are the answers to the question. Deciding which question should be asked next can be done in various ways - for example by trying to maximize the predicted (or mean) entropy from the next question. This game is sometimes known as 20 Questions. There are some questions on SO on it, e.g.: Main characteristics of algorithm: Akinator game algorithm model is called "Expert system based on Fuzzy logic". And this is NOT Decision trees, because Decision trees have no mistakes-indulgence. I had wrote one some time ago on C#, you can find it by link: https://github.com/ukushu/AkinatorEngine additional info you can read on wiki: https://en.wikipedia.org/wiki/Expert_system https://ru.wikipedia.org/wiki/Экспертная_система I don't know what exactly algorithm Akinator uses, but here I have put open-source an algorithm that achieves the same effect: https://github.com/srogatch/ProbQA Basically we use a cube of N(Questions) times N(Answer Options) times N(Targets) , see https://github.com/srogatch/ProbQA/blob/master/ProbQA/PqaCore/CpuEngine.decl.h . We train the cube by applying Bayesian formula with independence assumption, see https://github.com/srogatch/ProbQA/blob/master/ProbQA/PqaCore/CEEvalQsSubtaskConsider.cpp Because the code is highly optimized for AVX2 and multi-threading, it may be hard to read. It may be easier to read the CUDA code for the same: https://github.com/srogatch/ProbQA/blob/master/ProbQA/PqaCore/CudaEngineGpu.cu An application of this algorithm is also available as a website to recommend a game. I think this is like an expert system, with B-Tree structure. i have a small project to build a dynamic quiz for 'find your taste' and i want to share it with you :  Akinator uses more of binary trees. So, the root node is the first question. This goes on inner and inner. Probably, Akinator also gets the location where you are and links the questions. I have developed my own Akinator named as Ankusha. I have used a pattern matching algorithm. It's quite complicated to say the way Ankusha works. You can get the source code from this github link.
Ankusha-The Mind Reader
Ankusha finishes the guess within five questions. He confirms the movie he guessed with you. If you click on 'No', next five questions are asked. This iteration continues up to twenty one questions. It is a Python 3.7 program, which requires the PIL and pygame module to be installed compulsorily. Please do try it out. NOTE: Make sure that you read the file README.md before proceeding with the installation of the app. All licences are done under the GNU GPLv3 (GNU General Public Licence version 3.0).I was trying to plot train and test learning curve in keras, however, the following code produces KeyError: 'val_acc error. The official document <https://keras.io/callbacks/> states that in order to use 'val_acc' I need to enable  validation and accuracy monitoring which I dont understand and dont know how to use in my code. Any help would be much appreciated. 
Thanks. Looks like in Keras + Tensorflow 2.0 val_acc was renamed to val_accuracy if u print keys of history_dict, you will get like this dict_keys(['loss', 'acc', 'val_loss', 'val_acc']).  and edit a code like this Keys and error You may need to enable the validation split of your trainset. Usually, the validation happens in 1/3 of the trainset. In your code, make the change as  given below: It works! The main point everyone misses to mention is that this Key Error is related to the naming of metrics during model.compile(...). You need to be consistent with the way you name your accuracy metric inside model.compile(....,metrics=['<metric name>']). Your history callback object will receive the dictionary containing key-value pairs as defined in metrics.  So, if your metric is metrics=['acc'], you can access them in history object with history.history['acc'] but if you define metric as metrics=['accuracy'], you need to change to history.history['accuracy'] to access the value, in order to avoid Key Error. I hope it helps.  N.B. Here's a link to the metrics you can use in Keras. If you upgrade keras older version (e.g. 2.2.5) to 2.3.0 (or newer) which is compatible with Tensorflow 2.0, you might have such error (e.g. KeyError: 'acc'). Both acc and val_acc has been renamed to accuracy and val_accuracy respectively. Renaming them in script will solve the issue. to get any val_* data (val_acc, val_loss, ...), you need to first set the validation. first method (will validate from what you give it): second method (will validate from a part of the training data): I have changed acc to accuracy and my problem solved. Tensorflow 2+ e.g. This error also happens when you specify the validation_data=(X_test, Y_test) and your X_test and/or Y_test are empty. To check this, print the shape of X_test and Y_test respectively. In this case, the model.fit(validation_data=(X_test, Y_test), ...) method ran but because the validation set was empty, it didn't create a dictionary key for val_loss in the history.history dictionary.  What worked for me was changing objective='val_accuracy'to objective=["val_accuracy"] in I have TensorFlow 2+.I'm a little bit confused about initial_epoch value in fit and fit_generator methods. Here is the doc: initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). I understand, it is not useful if you start training from scratch. It is useful if you trained your dataset and want to improve accuracy or other values (correct me if I'm wrong). But I'm not sure what it really does. So after all this, I have 2 questions: Since in some of the optimizers, some of their internal values (e.g. learning rate) are set using the current epoch value, or even you may have (custom) callbacks that depend on the current value of epoch, the initial_epoch argument let you specify the initial value of epoch to start from when training.  As stated in the documentation, this is mostly useful when you have trained your model for some epochs, say 10, and then saved it and now you want to load it and resume the training for another 10 epochs without disrupting the state of epoch-dependent objects (e.g. optimizer). So you would set initial_epoch=10 (i.e. we have trained the model for 10 epochs) and epochs=20 (not 10, since the total number of epochs to reach is 20) and then everything resume as if you were initially trained the model for 20 epochs in one single training session. However, note that when using built-in optimizers of Keras you don't need to use initial_epoch, since they store and update their state internally (without considering the value of current epoch) and also when saving a model the state of the optimizer will be stored as well. The answer above is correct however it is important to note that if you have trained for 10 epochs and set initial_epoch=10 and epochs=20 you train for 10 more epochs until you reach a total of 20 epochs. For example I trained for 2 epochs, then set initial_epoch=2 and epochs=4. The result is it trains for 4-2=2 more epochs. The new data in the history object starts at epoch 3. So the returned history object does start from epoch 1 as you might expect. Another words the state of the history object is not preserved from the initial training epochs. If you do not set initial_epoch and you train for 2 epochs, then rerun the fit_generator with epochs=4 it will train for 4 more epochs starting from the state preserved at the end of the second epoch (provided you use the built in optimizers). Again the history object state is NOT preserved from the initial training and only contains the data for the last 4 epochs. I noticed this because I plot the validation loss versus epochs. Here is an example of how to integrate the initial_epoch in your code Also don't forget to specify a particular random_state value while splitting the data into train and test, so that it encounters the same set of training data each time you reinitiate the training process, so that there is no data leakage of test data entering the training data.Java based Mahout's goal is to build scalable machine learning libraries. Are there any equivalent libraries in Python ? scikits learn is highly recommended http://scikit-learn.sourceforge.net/ Spark MLlib is recommmended. It is a scalable machine learning lib, can read data from HDFS and of course runs on top of Spark.  You can access it via PySpark (see the Programming Guide's Python examples).  Orange is supposedly pretty decent, from what I've heard, but I've never used it personally. PyML might be worth taking a look at as well. Also, Monte. pysuggest is a Python wrapper for SUGGEST, a Top-N recommendation engine that implements a variety of recommendation algorithms for collaborative filtering. An interesting library is crab. As of this post, the library only has stable implementations for collaborative filtering algorithms: user-based and item-based.  An SVD implementation is included but it's experimental and content-based algorithms are on the roadmap. Do check it out!I am learning the neural network and I want to write a function cross_entropy in python. Where it is defined as  where N is the number of samples, k is the number of classes, log is the natural logarithm, t_i,j is 1 if sample i is in class j and 0 otherwise, and p_i,j is the predicted probability that sample i is in class j.
To avoid numerical issues with logarithm, clip the predictions to [10^{−12}, 1 − 10^{−12}] range. According to the above description, I wrote down the codes by clipping the predictions to [epsilon, 1 − epsilon] range, then computing the cross_entropy based on the above formula. The following code will be used to check if the function cross_entropy are correct. The output of the above codes is False, that to say my codes for defining the function cross_entropy is not correct. Then I print the result of cross_entropy(predictions, targets). It gave 0.178389544455 and the correct result should be ans = 0.71355817782. Could anybody help me to check what is the problem with my codes? You're not that far off at all, but remember you are taking the average value of N sums, where N = 2 (in this case). So your code could read: Here, I think it's a little clearer if you stick with np.sum(). Also, I added 1e-9 into the np.log() to avoid the possibility of having a log(0) in your computation. Hope this helps! NOTE: As per @Peter's comment, the offset of 1e-9 is indeed redundant if your epsilon value is greater than 0.I wanted to write a program for asking questions about weather. What are the algorithms and techniques I should start looking at. ex: Will it be sunny this weekend in Chicago.
I wanted to know the intent = weather query, date = this weekend, location = chicago. User can express the same query in many forms.  I would like to solve some constrained form and looking for ideas on how to get started. The solution needs to be just good enough. Since your input is in the natural language form, best way to start looking into it, first by parsing the sentence structure. and running the sentence through NER (Named Entity Recognizer). Parsing the sentence lets you come up with rules such as, certain types of dependencies always give you the intent. Running the NER will let you identify places and dates. If it's not simple to come up with rules to classify the intent, you can as well use a classifier to do the same using feature vector formulated from the input sentence. In fact some of the parser out put can go into formulating the feature vector.  For both there exists software's from Stanford NLP Group  May be you can look into: Once you parse the sentence, you have intent and other information require to answer the question. Ex: I took your sentence "Will it be sunny this weekend in Chicago." and ran it through Online Stanford NER Tagger. Which gave me the following:  Now you have identified date and location. I hope this helps. I know the answer is quite generic, and may be helpful in just getting started.  I think this api is exactly what you are looking for.  It's easy and awesome to use. https://wit.ai/ Additionally, https://www.luis.ai/ is a good implementation of an NLP framework. They have an API as well as a nuget SDK. We've been using them for awhile now. They were cheaper than the other options we looked at. i.e. wit.ai.  So re your example -  ex: Will it be sunny this weekend in Chicago -> would map to a LUIS intent called WeatherQuery.
date -> would map to a pre-built LUIS dateTime entity
location -> chicago -> would map to a pre-built LUIS entity -> geography or address I think.People often throw around the terms IR, ML, and data mining, but I have noticed a lot of overlap between them. From people with experience in these fields, what exactly draws the line between these? This is just the view of one person (formally trained in ML); others might see things quite differently.  Machine Learning is probably the most homogeneous of these three terms, and the most consistently applied--it's limited to the pattern-extraction (or pattern-matching) algorithms themselves.  Of the terms you mentioned, "Machine Learning" is the one most used by Academic Departments to describe their Curricula, their academic departments, and their research programs, as well as the term most used in academic journals and conferences proceedings. ML is clearly the least context-dependent of the terms you mentioned.  Information Retrieval and Data Mining are much closer to describing complete commercial processes--i.e., from user query to retrieval/delivery of relevant results. ML algorithms might be somewhere in that process flow, and in the more sophisticated applications, often are, but that's not a formal requirement. In addition, the term Data Mining seems usually to refer to application of some process flow on big data (i.e, > 2BG) and therefore usually includes a distributed processing (map-reduce) component near the front of that workflow. So Information Retrieval (IR) and Data Mining (DM) are related to Machine Learning (ML) in an Infrastructure-Algorithm kind of way. In other words, Machine Learning is one source of tools used to solve problems in Information Retrieval. But it's only one source of tools. But IR doesn't depend on ML--for instance, a particular IR project might be storage and rapid retrieval of the fully-indexed data responsive to a user's search query IR, the crux of which is optimizing performance of the data flow, i.e., the round-trip from query to delivering the search results to the user. Prediction or pattern matching might not be useful here. Likewise, a DM project might use an ML algorithm for the predictive engine, yet a DM project is more likely to also be concerned with the entire processing flow--for instance, parallel computation techniques for efficient input of an enormous data volume (TB perhaps) which delivers a proto-result to a processing engine for computation of descriptive statistics (mean, standard deviation, distribution, etc. on the variables (columns). Lastly consider the Netflix Prize. This competition was directed solely to Machine Learning--the focus was on the prediction algorithm, as evidenced by the fact that there was a single success criterion: accuracy of the predictions returned by the algorithm. Imagine if the 'Netflix Prize' were rebranded as a Data Mining competition. The success criteria would almost certainly be expanded to more accurately access the algorithm's performance in the actual commercial setting--so for instance overall execution speed (how quickly are the recommendations delivered to the user) would probably be considered along with accuracy. The terms "Information Retrieval" and "Data Mining" are now in mainstream use, though for a while I only saw these terms in my job description or in vendor literature (usually next to the word "solution.") At my employer, we recently hired a "Data Mining" analyst. I don't know what he does exactly, but he wears a tie to work every day. I'd try to draw the line as follows: Information retrieval is about finding something that already is part of your data, as fast as possible. Machine learning are techniques to generalize existing knowledge to new data, as accurate as possible. Data mining is primarly about discovering something hidden in your data, that you did not know before, as "new" as possible. They intersect and often use techniques of one another. DM and IR both use index structures to accelerate processes. DM uses a lot of ML techniques, for example a pattern in the data set that is useful for generalization might be a new knowledge. They are often hard to separate. Do yourself a favor and don't just go for the buzzwords. In my opinion the best way of distinguishing them is by their intention, as given above: find data, generalize to new data, find new properties of existing data. You can also add pattern recognition and (computational?) statistics as another couple of areas that overlap with the three you mentioned. I'd say there is no well-defined line between them. What separates them is their history and their emphases. Statistics emphasizes mathematical rigor, data mining emphasizes scaling to large datasets, ML is somewhere in between. Data mining is about discovering hidden patterns or unknown knowledge, which can be used 
for decision making by people. Machine learning is about learning a model to classify new objects.Want to improve this question? Add details and clarify the problem by editing this post. Closed 6 years ago. All four functions seem really similar to me. In some situations some of them might give the same result, some not. Any help will be thankfully appreciated! Now I know and I assume that internally, factorize and LabelEncoder work the same way and having no big differences in terms of results. I am not sure whether they will take up similar time with large magnitudes of data. get_dummies and OneHotEncoder will yield the same result but OneHotEncoder can only handle numbers but get_dummies will take all kinds of input. get_dummies will generate new column names automatically for each column input, but OneHotEncoder will not (it rather will assign new column names 1,2,3....). So get_dummies is better in all respectives. Please correct me if I am wrong! Thank you! These four encoders can be split in two categories: The main difference between pandas and scikit-learn encoders is that scikit-learn encoders are made to be used in scikit-learn pipelines with fit and transform methods. Pandas factorize and scikit-learn LabelEncoder belong to the first category. They can be used to create categorical variables for example to transform characters into numbers. Pandas get_dummies and scikit-learn OneHotEncoder belong to the second category. They can be used to create binary variables. OneHotEncoder can only be used with categorical integers while get_dummies can be used with other type of variables. I've also written a more detailed post based on this answer.I am currently training my data using neural network and using fit function.  Now I have used validation_split as 20%. What I understood is that my training data will be 80% and testing data will be 20%. I am confused how this data is dealt on back end. Is it like top 80% samples will be taken for training and below 20% percent for testing or the samples are randomly picked from inbetween? If I want to give separate training and testing data, how will I do that using fit()?? Moreover, my second concern is how to check if data is fitting well on model? I can see from the results that training accuracy is around 90% while the validation accuracy is around 55%. Does this mean it is the case of over-fitting or Under-fitting? My last question is what does evaluate returns? Document says it returns the loss but I am already getting loss and accuracy during each epoch (as a return of fit() (in history)). What does accuracy and score returned by evaluate shows? If the accuracy returned by evaluate returns 90%, can I say my data is fitting well, regardless of what individual accuracy and loss was for each epoch? Below is my Code: The keras documentation says:"The validation data is selected from the last samples in the x and y data provided, before shuffling.", this means that the shuffle occurs after the split, there is also a boolean parameter called "shuffle" which is set true as default, so if you don't want your data to be shuffled you could just set it to false Getting good results on your training data and then getting bad or not so good results on your evaluation data usually means that your model is overfitting, overfit is when your model learns in a very specific scenario and can't achieve good results on new data evaluation is to test your model on new data that it has "never seen before", usually you divide your data on training and test, but sometimes you might also want to create a third group of data, because if you just adjust your model to obtain better and better results on your test data this in some way is like cheating because in some way you are telling your model how is the data you are going to use for evaluation and this could cause overfitting Also, if you want to split your data without using keras, I recommend you to use the sklearn train_test_split() function. it's easy to use and it looks like this:In order to access a model's parameters in pytorch, I saw two methods: using state_dict and using parameters() I wonder what's the difference, or if one is good practice and the other is bad practice. Thanks The parameters() only gives the module parameters i.e. weights and biases. Returns an iterator over module parameters. You can check the list of the parameters as follows: On the other hand, state_dict  returns a dictionary containing a whole state of the module. Check its source code that contains not just the call to parameters but also buffers, etc. Both parameters and persistent buffers (e.g. running averages) are included. Keys are the corresponding parameter and buffer names. Check all keys that state_dict contains using: For example, in state_dict, you'll find entries like bn1.running_mean and running_var, which are not present in .parameters(). If you only want to access parameters, you can simply use .parameters(), while for purposes like saving and loading model as in transfer learning, you'll need to save state_dict not just parameters. Besides the differences in @kHarshit 's answer, the attribute requires_grad of trainable tensors in net.parameters() is True, while False in net.state_dict()This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I'm gathering results from my image detector algorithm. So basically what I do is that, from a set of images (with the size of 320 x 480), I would run a sliding window of 64x128 thru it, and also under a number of predefined scales. I understand that: But what about True Negatives ? Are these true negatives all the windows that my classifier gives me negative results ? That sounds weird, since I'm sliding a small window (64x128) by 4 pixels at a time, and I've around 8 different scales used in detection. If I were to do that, then I'd have lots of true negatives per image. Or do I prepare a set of pure negative images (no objects / human at all), where I just slide thru, and if there's one or more positive detections in each of these images, I'd count it as False Negative, and vice versa ? Here's an example image (with green rects as the ground truth)  I've always seen the four terms as the following: In your case, if I understand correctly, you are trying to detect if there are objects in your image. False negative would therefore mean that there was a object (result should be positive) but the algorithm did not detect it (and therefore returned negative). A true negative is simply the algorithm correctly stating that the area it checked does not hold an object. You can choose to ignore negative values, but these could be used to further train your algorithm (Eg; using an algorithm that looks for both, instead of setting everything that is not recognised to false). All possible rectangles that are not fn, fp, or tp are tn.  Therefore, The amount of true negatives (tn) is vast but exhaustive.  Context: Generally, in object detection all metrics that contain tn are ignored, because there would be to many tn, which makes the metric difficult to use. True Negatives : It's kind of background prediction, ground truth do not have boxes for those location and are not present in predictions also. False Negatives: Ground truth have boxes their but the prediction do not contain any box at that location. Generally, in object detection task we do not look for True Negative (TN) cases, for as the algorithm tell us we like to detect object (not non-object candidate); On the contrary, in classification tasks we aim to decide for each instance to be consider as negative or positive. Thus, naturally we will have True Negative (TN) cases in classification tasks.
Besides, there are better and more compatible measures for object detection tasks. You can refer to the mean Average Precision(mAP) to evaluate your object detection algorithm.
Note that, mAP is different with simple Averaging of Precision. You can find more info on this topic on: 
https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 There is a nice explanation here. F1 score explained in wiki and here is helpful for measuring success.  I have an attempt to write a function that calculates F1 score: AFAIK, A True Negative would be a scenario where an object is present in the image but has not been marked either in the ground truth annotation or the model prediction. Usually 2D object detection systems use only two data i.e ground truth annotations and model predictions. However, to find the True Negative cases we would require a sought of superset of the ground truth annotations which contains information about all the class instances present in the image (not just those specific to our model). For example taking the given image; if we are interested in doing object detection for autonomous driving purposes we can consider the two ground truth annotations as below: Super Set GT Annotations Autonomous Driving GT Annotations With the above two ground truth annotations it would be possible to calculate the True Negatives for burger and window. However I doubt if True Negatives can be calculated without the superset annotation.Want to improve this question? Add details and clarify the problem by editing this post. Closed 6 years ago. All four functions seem really similar to me. In some situations some of them might give the same result, some not. Any help will be thankfully appreciated! Now I know and I assume that internally, factorize and LabelEncoder work the same way and having no big differences in terms of results. I am not sure whether they will take up similar time with large magnitudes of data. get_dummies and OneHotEncoder will yield the same result but OneHotEncoder can only handle numbers but get_dummies will take all kinds of input. get_dummies will generate new column names automatically for each column input, but OneHotEncoder will not (it rather will assign new column names 1,2,3....). So get_dummies is better in all respectives. Please correct me if I am wrong! Thank you! These four encoders can be split in two categories: The main difference between pandas and scikit-learn encoders is that scikit-learn encoders are made to be used in scikit-learn pipelines with fit and transform methods. Pandas factorize and scikit-learn LabelEncoder belong to the first category. They can be used to create categorical variables for example to transform characters into numbers. Pandas get_dummies and scikit-learn OneHotEncoder belong to the second category. They can be used to create binary variables. OneHotEncoder can only be used with categorical integers while get_dummies can be used with other type of variables. I've also written a more detailed post based on this answer.By setting the bottom and the top blob to be the same we can tell Caffe to do "in-place" computation to preserve memory consumption. Currently I know I can safely use in-place "BatchNorm", "Scale" and "ReLU" layers (please let me know if I'm wrong). While it seems to have some issues for other layers (this issue seems to be an example). When to use in-place layers in Caffe?
How does it work with back-propagation? As you well noted, in-place layers don't usually work "out of the box".
For some layers, it is quite trivial ("ReLU" and other neuron activation layers).
However, for others it requires special handling in code. For example, the implementation of "PReLU" layer has specific cache bottom_memory_ member variable that stores information needed for backprop.
You can see similar code for other layers that specifically test for if (top[0] == bottom[0]) to see if the layer is used in an "in-place" case. Moreover, it makes little sense to have an in-place layer for which the input and output are of different shapes, thus layers such as "Convolution", "InnerProduct", "Pool" are not considered as candidates for "in-place" layers.I came across a problem with comparing the predictions of my model with the labels of training set. The arrays I'm using have shapes: Training set (200000, 28, 28) (200000,)
Validation set (10000, 28, 28) (10000,)
Test set (10000, 28, 28) (10000,) However, when checking the accuracy with the function: It's giving me: C:\Users\Arslan\Anaconda3\lib\site-packages\ipykernel_launcher.py:5: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.
""" And it gives the accuracy as 0% for all datasets. I think we cannot compare the arrays using '=='. How could I compare the arrays in the right way instead? I assume the error occurs in this expression: can you tell us something about the 2 arrays, predictions, labels?  The usual stuff - dtype, shape, some sample values.  Maybe go the extra step and show the np.argmax(...) for each. In numpy you can compare arrays of the same size, but it has become pickier about comparing arrays that don't match in size: This error is telling you that the comparisson you're performing doesn't really make sense, since both arrays have different shapes, hence it can't perform elementwise comparisson. Here's an example: Where attempting to do x==y will yield: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.
x==y The right way to do this, would be to use np.array_equal, which checks equality of both shape and elements: In the case of floats, np.allclose is more suited, since it allows to control both the relative and absolute tolerance of the comparisson result. Here's an example: I solved this by upgrading python to 3.6.4 (latest)I am using scikit learn for Gaussian process regression (GPR) operation to predict data. My training data are as follows: The test points (2-D) where mean and variance/standard deviation need to be predicted are: Now, after running the GPR (GausianProcessRegressor) fit (Here, the product of ConstantKernel and RBF is used as Kernel in GaussianProcessRegressor), mean and variance/standard deviation can be predicted by following the line of code: While printing the predicted mean (y_pred_test) and variance (sigma), I get following output printed in the console:  In the predicted values (mean), the 'nested array' with three objects inside the inner array is printed. It can be presumed that the inner arrays are the predicted mean values of each data source at each 2-D test point locations. However, the printed variance contains only a single array with 16 objects (perhaps for 16 test location points). I know that the variance provides an indication of the uncertainty of the estimation. Hence, I was expecting the predicted variance for each data source at each test point. Is my expectation wrong? How can I get the predicted variance for each data source at each test points? Is it due to wrong code? Well, you have inadvertently hit on an iceberg indeed... As a prelude, let's make clear that the concepts of variance & standard deviation are defined only for scalar variables; for vector variables (like your own 3d output here), the concept of variance is no longer meaningful, and the covariance matrix is used instead (Wikipedia, Wolfram). Continuing on the prelude, the shape of your sigma is indeed as expected according to the scikit-learn docs on the predict method (i.e. there is no coding error in your case): Returns: y_mean : array, shape = (n_samples, [n_output_dims]) Mean of predictive distribution a query points y_std : array, shape = (n_samples,), optional Standard deviation of predictive distribution at query points. Only returned when return_std is True. y_cov : array, shape = (n_samples, n_samples), optional Covariance of joint predictive distribution a query points. Only returned when return_cov is True. Combined with my previous remark about the covariance matrix, the first choice would be to try the predict function with the argument return_cov=True instead (since asking for the variance of a vector variable is meaningless); but again, this will lead to a 16x16 matrix, instead of a 3x3 one (the expected shape of a covariance matrix for 3 output variables)... Having clarified these details, let's proceed to the essence of the issue. At the heart of your issue lies something rarely mentioned (or even hinted at) in practice and in relevant tutorials: Gaussian Process regression with multiple outputs is highly non-trivial and still a field of active research. Arguably, scikit-learn cannot really handle the case, despite the fact that it will superficially appear to do so, without issuing at least some relevant warning. Let's look for some corroboration of this claim in the recent scientific literature: Gaussian process regression with multiple response variables (2015) - quoting (emphasis mine): most GPR implementations model only a single response variable, due to
the difficulty in the formulation of covariance function for
correlated multiple response variables, which describes not only the
correlation between data points, but also the correlation between
responses. In the paper we propose a direct formulation of the
covariance function for multi-response GPR, based on the idea that [...] Despite the high uptake of GPR for various modelling tasks, there
still exists some outstanding issues with the GPR method. Of
particular interest in this paper is the need to model multiple
response variables. Traditionally, one response variable is treated as
a Gaussian process, and multiple responses are modelled independently
without considering their correlation. This pragmatic and
straightforward approach was taken in many applications (e.g. [7, 26,
27]), though it is not ideal. A key to modelling multi-response
Gaussian processes is the formulation of covariance function that
describes not only the correlation between data points, but also the
correlation between responses. Remarks on multi-output Gaussian process regression (2018) - quoting (emphasis in the original): Typical GPs are usually designed for single-output scenarios wherein
the output is a scalar. However, the multi-output problems have
arisen in various fields, [...]. Suppose that we attempt to approximate T outputs {f(t}, 1 ≤t ≤T , one intuitive idea is to use the single-output GP (SOGP) to approximate them individually using the associated training data D(t) = { X(t), y(t) }, see Fig. 1(a). Considering that the outputs are correlated in some way, modeling them individually may result in the loss of valuable information. Hence, an increasing diversity of engineering applications are embarking on the use of multi-output GP (MOGP), which is conceptually depicted in Fig. 1(b), for surrogate modeling. The study of MOGP has a long history and is known as multivariate
Kriging or Co-Kriging in the geostatistic community; [...] The MOGP handles problems with the basic assumption that the outputs are correlated in some way. Hence, a key issue in MOGP is to exploit the output correlations such that the outputs can leverage information from one another in order to provide more accurate predictions in comparison to modeling them individually.  Physics-Based Covariance Models for Gaussian Processes with Multiple Outputs (2013) - quoting: Gaussian process analysis of processes with multiple outputs is
limited by the fact that far fewer good classes of covariance
functions exist compared with the scalar (single-output) case. [...] The difficulty of finding “good” covariance models for multiple
outputs can have important practical consequences. An incorrect
structure of the covariance matrix can significantly reduce the
efficiency of the uncertainty quantification process, as well as the
forecast efficiency in kriging inferences [16]. Therefore, we argue,
the covariance model may play an even more profound role in co-kriging
[7, 17]. This argument applies when the covariance structure is
inferred from data, as is typically the case. Hence, my understanding, as I said, is that sckit-learn is not really capable of handling such cases, despite the fact that something like that is not mentioned or hinted at in the documentation (it may be interesting to open a relevant issue at the project page). This seems to be the conclusion in this relevant SO thread, too, as well as in this CrossValidated thread regarding the GPML (Matlab) toolbox. Having said that, and apart from reverting to the choice of simply modeling each output separately (not an invalid choice, as long as you keep in mind that you may be throwing away useful information from the correlation between your 3-D output elements), there is at least one Python toolbox which seems capable of modeling multiple-output GPs, namely the runlmc (paper, code, documentation). First of all, if the parameter used is "sigma", that's referring to standard deviation, not variance (recall, variance is just standard deviation squared). It's easier to conceptualize using variance, since variance is defined as the Euclidean distance from a data point to the mean of the set. In your case, you have a set of 2D points. If you think of these as points on a 2D plane, then the variance is just the distance from each point to the mean. The standard deviation than would be the positive root of the variance. In this case, you have 16 test points, and 16 values of standard deviation. This makes perfect sense, since each test point has its own defined distance from the mean of the set. If you want to compute the variance of the SET of points, you can do that by summing the variance of each point individually, dividing that by the number of points, then subtracting the mean squared. The positive root of this number will yield the standard deviation of the set. ASIDE: this also means that if you change the set through insertion, deletion, or substitution, the standard deviation of EVERY point will change. This is because the mean will be recomputed to accommodate the new data. This iterative process is the fundamental force behind k-means clustering.I am using Word2Vec with a dataset of roughly 11,000,000 tokens looking to do both word similarity (as part of synonym extraction for a downstream task) but I don't have a good sense of how many dimensions I should use with Word2Vec. Does anyone have a good heuristic for the range of dimensions to consider based on the number of tokens/sentences? Typical interval is between 100-300. I would say you need at least 50D to achieve lowest accuracy. If you pick lesser number of dimensions, you will start to lose properties of high dimensional spaces. If training time is not a big deal for your application, i would stick with 200D dimensions as it gives nice features. Extreme accuracy can be obtained with 300D. After 300D word features won't improve dramatically, and training will be extremely slow.   I do not know theoretical explanation and strict bounds of dimension selection in high dimensional spaces (and there might not a application-independent explanation for that), but I would refer you to Pennington et. al, Figure2a where x axis shows vector dimension and y axis shows the accuracy obtained. That should provide empirical justification to above argument.  I think that the number of dimensions from word2vec depends on your application. The most empirical value is about 100. Then it can perform well. The number of dimensions reflects the over/under fitting. 100-300 dimensions is the common knowledge. Start with one number and check the accuracy of your testing set versus training set. The bigger the dimension size the easier it will be overfit on the training set and had bad performance on the test. Tuning this parameter is required in case you have high accuracy on training set and low accuracy on the testing set, this means that the dimension size is too big and reducing it might solve the overfitting problem of your model.I have some categorical features in my data along with continuous ones. Is it a good or absolutely bad idea to hot encode category features to find correlation of it to labels along with other continuous creatures? There is a way to calculate the correlation coefficient without one-hot encoding the category variable. Cramers V statistic is one method for calculating the correlation of categorical variables. It can be calculated as follows. The following link is helpful. Using pandas, calculate Cramér's coefficient matrix For variables with other continuous values, you can categorize by using cut of pandas. please note the .as_matrix() is deprecated in pandas since verison 0.23.0 . use .values instead I found phik library quite useful in calculating correlation between categorical and interval features. This is also useful for binning numerical features. Try this once: phik documentation I was looking to do same thing in BigQuery.
For numeric features you can use built in CORR(x,y) function.
For categorical features, you can calculate it as:
cardinality (cat1 x cat2) / max (cardinality(cat1),  cardinality(cat2).
Which translates to following SQL: Higher number means lower correlation. I used following python script to generate SQL: It should be straightforward to do same thing in numpy.I am new to Tensorflow and deep leaning. I am trying to see how the loss decreases over 10 epochs in my RNN model that I created to read a dataset from kaggle which contains credit card fraud data. I am trying to classify the transactions as fraud(1) and not fraud(0). When I try to run the below code I keep getting the below error: Can anyone point out what I am doing wrong in my code and also any problem in my code if possible. Thank you in advance. Shown below is my code: Make sure that the number of labels in the final classification layer is equal to the number of classes you have in your dataset. InvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[1,2] labels_size=[1,24] as shown in your question might suggest that you are have just two classes in your final classification layer while you actually need 24. In my case, I had 7 classes in my dataset, but I mistakenly used 4 labels in the final classification layer. Therefore, I had to change from tf.keras.layers.Dense(4, activation="softmax") to tf.keras.layers.Dense(7, activation="softmax") When you say the prediction and labels have incompatible shapes. You need to change how the predictions are computed to get one per example in your minibatch. This error occurs because there is a mismatch in the count of Predicted class and the input. I copied a code and encountered this error This was the original code where output is 5 classes In my case, I had 30 classes and correcting the classes count fixed it A little late to the party but I had the same error with a CNN, I messed around with different types of cross entropy and the error was resolved by using sparce_softmax_cross_entropy_with_logits(). I met the similar problems when using the CNN, the problem occurs because I changed the labels data type as np.uint8 in the Generator function while did nothing for the labels type in the rest of my code. I solved the problem by changing the labels type to uint8 in all of my code. This happened to me when using Tensorflow DataGenerator with small batch size. Try increasing your batch size. Check the input shape in the first convolutional layer and the input shape fed into the network using data.shape. There is high chance you have a different shape fed into your CNN.I have implemented a tied weights Auto-encoder in Keras and have successfully trained it.  My goal is to use only the decoder part of the Auto-encoder as the last layer of another network, to fine tune both the network and the decoder. Thing is, as you can see below from the summary, the decoder has no parameters with my tied weights implementation, so there is nothing to be fine tuned. (decoder.get_weights() returns []) My question is: Should I change the implementation of the tied weights, so that the tied layer can still hold weights, that is the transposed weights of the encoder? If yes, how?  Or am I just way off? Below is the summary of the autoencoder model as well as the class of the tied Dense layer (slightly modified from https://github.com/nanopony/keras-convautoencoder/blob/master/autoencoder_layers.py.) It's been more than 2 years since this question was asked, but this answer might still be relevant for some. The function Layer.get_weights() retrieves from self.trainable_weights and self.non_trainable_weights (see keras.engine.base_layer.Layer.weights). In your custom layer, your weights self.W and self.b are not being added to any of these collections and that's why the layer has 0 parameters. You could tweak your implementation as follows: NOTE: I am excluding the regularizers and constraints for simplicity. If you want those, please refer to keras.engine.base_layer.Layer.add_weight.I was following the codelabs tensorflow for poets and the training worked fine but when I runned the script to evaluate a image: I got the following error:  I looked around and it has something to do with chosing the input and output layer, the script label_image.py has 'input' and 'output' set as default. The architecture I'm using is 'inception_v3'. I changed ~/scripts/label_image.py line 77 and it works: from  to Use --input_layer name as Placeholder. It will work because the retrain.py script has set default value of input_layer as "Placeholder". Not everyone is getting this error. I'm guessing if you used any other architecture apart from MobileNet this error turns up.
In your label_image.py file 
change the values to:  This should solve it. As @Mimii and @Celio mentioned: change ~/scripts/label_image.py, at the line input_layer = "input" to input_layer = "Mul" AND change the input dimensions: input_height = 299 and input_width= 299 Use this You have to make some changes in label_image.py in the scripts folder input_height = 299 Change input_height to 299 from 224 
input_width = 299  Change input_width to 299 from 224
input_mean = 128 
input_std = 128 
input_layer = "Mul" Change input_layer to Mul from input 
output_layer = "final_result"  Output: Evaluation time (1-image): 1.901s daisy (score=0.98584)
sunflowers (score=0.01136)
dandelion (score=0.00210)
tulips (score=0.00066)
roses (score=0.00004) For more info, refer this page You should add --output_layer=final_result:0 as parameter. Sorry for late answer.
I run python script below with a retrained model. Can you try this one? Requirements:
labels.txt  and  output.pb(retrained model) should be at same directory with my python scipt.
Save code below as test.py
And call it as: python test.py xxx.jpg Inside the "retrain.py" code you'll see an argument called '--final_tensor_name'. If you don't pass that argument it will keep 'final_result' or 'Mul' (depending on the version your using) as the default.  The only way to view the input and output names without the actual training output files is to view the graph in TensorBoard of the 'frozen_graph.pb' or in your case the 'retrained_graph.pb' file. This is a nice way of outputting the required files to view it in TensorBoard.
https://gist.github.com/jubjamie/2eec49ca1e4f58c5310d72918d991ef6 Once you run that code and have the output going to your chosen directory, you can start up TensorBoard and view it in Chrome. Viewing the graph helps me alot since I'm a noob in this area. Or you can run by command lines with the options without changing codes: Setting input layer to Mul works for me.  However, it seems to be ignoring my input size settings and doesn't do any magic to resize the image to 299x299 which I guess Mul is expecting.  I did this: and got this: And ohhh, looking at the code, input_width and input_height are what to normalize to, not to normalize from.  So it's all good.  Also I needed to add my labels.I write programs to play board game variants sometimes. The basic strategy is standard alpha-beta pruning or similar searches, sometimes augmented by the usual approaches to endgames or openings.  I've mostly played around with chess variants, so when it comes time to pick my evaluation function, I use a basic chess evaluation function. However, now I am writing a program to play a completely new board game.  How do I choose a good or even decent evaluation function? The main challenges are that the same pieces are always on the board, so a usual material function won't change based on position, and the game has been played less than a thousand times or so, so humans don't necessarily play it enough well yet to give insight.  (PS.  I considered a MoGo approach, but random games aren't likely to terminate.) Game details: The game is played on a 10-by-10 board with a fixed six pieces per side.  The pieces have certain movement rules, and interact in certain ways, but no piece is ever captured.  The goal of the game is to have enough of your pieces in certain special squares on the board.  The goal of the computer program is to provide a player which is competitive with or better than current human players. I will start with some basics and move to harder stuff later. Basic agent and a testing framework No matter what approach you take you need to start with something really simple and dumb. The best approach for a dumb agent is a random one (generate all possible moves, select one at random). This will serve as a starting point to compare all your other agents. You need a strong framework for comparison. Something that takes various agents, allows to play some number of games between them and returns the matrix of the performance. Based on the results, you calculate the fitness for each agent. For example your function tournament(agent1, agent2, agent3, 500) will play 500 games between each pair of agent (playing the first/second) and returns you something like: Here for example I use 2 points for a win, 1 point for draw scoring function, and at the end just summing everything to find the fitness. This table immediately tells me that agent3 is the best, and agent1 is not really different from agent2. So once these two important things are set up you are ready to experiment with your evaluation functions. Let's start with selecting features First of all you need to create not a terrible evaluation function. By this I mean that this function should correctly identify 3 important aspects (win/draw/loss). This sounds obvious, but I have seen significant amount of bots, where the creators were not able to correctly set up these 3 aspects. Then you use your human ingenuity to find some features of the game state. The first thing to do is to speak with a game expert and ask him how he access the position. If you do not have the expert, or you even just created the rules of your game 5 minutes ago, do not underestimate the human's ability to search for patters. Even after playing a couple of games, a smart person can give you ideas how he should have played (it does not mean that he can implement the ideas). Use these ideas as features. At this point you do not really need to know how these features affect the game. Example of features: value of the pieces, pieces mobility, control of important positions, safety, total number of possible moves, closeness to a finish. After you coded up these features and used them separately to see what works best (do not hurry up to discard features that do not perform reasonable by itself, they might be helpful in conjunction with others), you are ready to experiment with combinations. Building better evaluations by combining and weighting simple features. There are a couple of standard approaches. Create an uber function based on various combinations of your features. It can be linear eval = f_1 * a_1 + ... f_n * a_n (f_i features, a_i coefficients), but it can be anything. Then instantiate many agents with absolutely random weights for this evaluation function and use genetic algorithm to play them agains each other. Compare the results using the testing framework, discard a couple of clear losers and mutate a couple of winners. Continue the same process. (This is a rough outline, read more about GA) Use the back-propagation idea from a neural networks to back propagate the error from the end of the game to update the weights of your network. You can read more how it was done with backgammon (I have not written anything similar, so sorry for the shortness). You can work without evaluation function! This might sound insane for a person who only heard about minimax/alpha-beta, but there are methods which do not require an evaluation at all. One of them is called Monte Carlo Tree Search and as a Monte Carlo in a name suggests it uses a lot of random (it should not be random, it can use your previous good agents) game plays to generate a tree. This is a huge topic by itself, so I will give you mine really high-level explanation. You start with a root, create your frontier, which you try to expand. Once you expand something, you just randomly go to the leaf. Getting the result from the leaf, you backpropagate the result. Do this many many times, and collect the statistics about each child of the current frontier. Select the best one. There is significant theory there which relates to how do you balance between exploration and exploitation and a good thing to read there is UCT (Upper Confidence Bound algorithm) Find a few candidates for your evaluation function, like mobility (# of possible moves) minus opponent's mobility, then try to find the optimal weight for each metric. Genetic algorithms seem to work pretty well for optimizing weights in an evaluation function. Create a population with random weights, fight them against each other with a limited depth and turns, replace the losers with random combinations from the winners, shuffle, and repeat, printing out the population average after every generation. Let it run until you're satisfied with the result, or until you see a need to adjust the range for some of the metrics and try again, if it appears that the optimal value for one metric might be outside your initial range. Late edit: A more accepted, studied, understood approach that I didn't know at the time is something called "Differential Evolution". Offspring are created from 3 parents instead of 2, in such a way that avoids the problem of premature convergence towards the average. I would look at a supervised machine learning algorithm such as reinforcement learning.  Check out Reinforcement learning in board games.  I think that will give you some good directions to look into. Also, check out Strategy Acquisition for the Game Othello Based on Reinforcement Learning (PDF link) where given the rules of the game, a good "payoff function" can be learned.  This is closely related to TD-Gammon ... During training, the neural network
  itself is used to select moves for
  both sides ... The rather surprising
  finding was that a substantial amount
  of learning actually took place, even
  in the zero initial knowledge
  experiments utilizing a raw board
  encoding. If nobody understands the game yet, there's no way you can get a decent evaluation function. Don't tell me that standard alpha-beta with material count is good or even decent for chess or its variants (maybe losers' chess is an exception). You could try neural networks with feedback or similar machine learning algorithms but they usually suck until they have tons of training, which in this case is probably not available. And even then, if they don't suck, you can't gain knowledge from them. I think there's no way short of understanding the game the best you can and, for starters, leave the unknowns as random on the evaluation function (or just out of the picture until the unknowns become better known). Of course, if you'd share more info about the game you could get better ideas from the community. As I understand it, you want a good static evaluation function to use at the leaves of your min-max tree. If so, it is best to remember that the purpose of this static evaluation function is to provide a rating as to how good that board is for the computer player. So is f(board1) > f(board2) then it must be true that board1 is better for the computer (it is more likely to eventually win) than in board2. Of course, no static function is ever completely correct for all boards. So, you say that "The goal of the game is to have enough of your pieces in certain special squares on the board", so a first stab at f(board) would simply be to count the number of pieces the computer has on those special squares. You can then finesse it more. Without knowing the specifics of the game its impossible to give better guesses. If you gave us the game rules I am sure the stackoverflow users would be able to come with tons of original ideas for such functions. While you could use various machine learning methods to come up with an evaluation function (TD-Learning, used in such projects such as gnubackgammon, is one such example), the results are definitely dependent on the game itself. For backgammon, it works really well, because the stochastic nature of the game (rolling dice) forces the learner to explore territory it may not want to do. Without such a crucial component, you will probably end up with an evaluation function which is good against itself, but not against others. Since material difference may not be applicable, is the concept of mobility important -- i.e. how many possible moves you have available? Is controlling a certain area of the board usually better than not? Talk to the people who play the game to find out some clues. While it's preferable to have as good of an evaluation function as you can, you also need to tune your search algorithm so you can search as deeply as possible. Sometimes, this is actually more of a concern, since a deep searcher with a medicore evaluation function can outplay shallow searches with a good evaluation function. It all depends on the domain. (gnubackgammon plays an expert game with a 1-ply search, for example) There are other techniques you can use to improve the quality of your search, most importantly, to have a transposition table to cache search results to have sound forward pruning. I highly recommend looking over these slides. You also need to be careful on your choice. If your algorithm does not have a known relation to the actual value, the standard AI functions will not work properly. To be valid, your evaluation function, or heuristic has to be the same as, or below the actual value consistently or it will guide your decisions in an odd way (which one could argue for chess, even though I think the standard points are fine). What I typically do is find out what is capable and what is required. For some games, like sokoban, I have used the minimum number of box moves required to get one box (in isolation) from its current location to any of the goal locations. This is not an accurate answer for the number of required moves, but I think it is a pretty good heuristic since it can never overestimate and it can be pre-calculated for the entire board. When summing the score for a board it is just the sum of the values for each current box location. In an artificial life simulation that I wrote to evolve pack hunting and pack defense, the scoring system that I used was only to guide evolution and not to perform any pruning. I gave each creature one point for being born. For each point of energy that they consumed in their life, I gave them one additional point. I then used the sum of the points of their generation to determine how likely each was to reproduce. In my case, I simply used the proportion of the total points of their generation that they had acquired. If I had wanted to evolve creatures that were great at evading, I would have scored down for getting points eaten off of them. You should also be careful that your function is not too hard of a goal to hit. If you are trying to evolve something, you want to make sure the solution space has a decent slope. You want to guide the evolution in a direction, not just declare a victory if it happens to randomly hit. Without knowing more about your game I would be hard pressed to tell you how to build a function. Are there clear values of something that indicate a win or a loss? Do you have a way of estimating a minimum cost to close the gap? If you provide more information, I would be happy to try and provide more insight. There are lots of excellent books on the topic as well. Jacob Take in mind that it's not nescessarily true that a decent evaluation function even exists. For this statement I assume that, an evaluation function has to be of low complexity (P).I'm tryin to use scikit-learn to cluster text documents. On the whole, I find my way around, but I have my problems with specific issues. Most of the examples I found illustrate clustering using scikit-learn with k-means as clustering algorithm. Adopting these example with k-means to my setting works in principle. However, k-means is not suitable since I don't know the number of clusters. From what I read so far -- please correct me here if needed -- DBSCAN or MeanShift seem the be more appropriate in my case. The scikit-learn website provides examples for each cluster algorithm. The problem is now, that with both DBSCAN and MeanShift I get errors I cannot comprehend, let alone solve. My minimal code is as follows: (My documents are already processed, i.e., stopwords have been removed and an Porter Stemmer has been applied.) When I run this code, I get the following error when instatiating DBSCAN and calling fit(): Clicking on the line in dbscan_.py that throws the error, I noticed the following line When I use these to lines directly in my code for testing, I get the same error. I don't really know what np.asarray(X) is doing here, but after the command X.shape = (). Hence X.shape[0] bombs -- before, X.shape[0] correctly refers to the number of documents. Out of curiosity, I removed X = np.asarray(X) from dbscan_.py. When I do this, something is computing heavily. But after some seconds, I get another error: In short, I have no clue how to get DBSCAN working, or what I might have missed, in general. It looks like sparse representations for DBSCAN are supported as of Jan. 2015.  I upgraded sklearn to 0.16.1 and it worked for me on text. The implementation in sklearn seems to assume you are dealing with a finite vector space, and wants to find the dimensionality of your data set. Text data is commonly represented as sparse vectors, but now with the same dimensionality. Your input data probably isn't a data matrix, but the sklearn implementations needs them to be one. You'll need to find a different implementation. Maybe try the implementation in ELKI, which is very fast, and should not have this limitation. You'll need to spend some time in understanding similarity first. For DBSCAN, you must choose epsilon in a way that makes sense for your data. There is no rule of thumb; this is domain specific. Therefore, you first need to figure out which similarity threshold means that two documents are similar. Mean Shift may actually need your data to be vector space of fixed dimensionality.I'm doing the Toxic Comment Text Classification Kaggle challenge.  There are 6 classes: ['threat', 'severe_toxic', 'obscene', 'insult', 'identity_hate', 'toxic'].  A comment can be multiple of these classes so it's a multi-label classification problem. I built a basic neural network with Keras as follows: I run this line: and get 99.11% accuracy after 3 epochs. However, 99.11% accuracy is a good bit higher than the best Kaggle submission.  This makes me think I'm either (possibly both) a) overfitting or b) misusing Keras's accuracy. 1) Seems a bit hard to overfit when I'm using 50% of my data as a validation split and only 3 epochs. 2) Is accuracy here just the percentage of the time the model gets each class correct? So if I output [0, 0, 0, 0, 0, 1] and the correct output was [0, 0, 0, 0, 0, 0], my accuracy would be 5/6? After a bit of thought, I sort of think the accuracy metric here is just looking at the class my model predicts with highest confidence and comparing vs. ground truth.  So if my model outputs [0, 0, 0.9, 0, 0, 0], it will compare the class at index 2 ('obscene') with the true value.  Do you think this is what's happening? Thanks for any help you can offer! For multi-label classification, I think it is correct to use sigmoid as the activation and binary_crossentropy as the loss. If the output is sparse multi-label, meaning a few positive labels and a majority are negative labels, the Keras accuracy metric will be overflatted by the correctly predicted negative labels. If I remember correctly, Keras does not choose the label with the highest probability.  Instead, for binary classification, the threshold is 50%.  So the prediction would be [0, 0, 0, 0, 0, 1].  And if the actual labels were [0, 0, 0, 0, 0, 0], the accuracy would be 5/6.  You can test this hypothesis by creating a model that always predicts negative label and look at the accuracy. If that's indeed the case, you may try a different metric such as top_k_categorical_accuracy. Another remote possibility I can think of is your training data. Are the labels y somehow "leaked" into x? Just a wild guess. You can refer to Keras Metrics documentation to see all metrics available (e.g. binary_accuracy). You can also create your own custom metric (and make sure it does exactly what you expect). I wanted to make sure neurite was right about how the accuracy is computed, so this is what I did (note: activation="sigmoid") : Running the training you will see that the custom_acc is always equal to the binary_accuracy (and therefore to the custom_acc).  Now you can refer to the Keras code on Github to see how it is computed:  Which confirm what neurite said (i.e. If the prediction is [0, 0, 0, 0, 0, 1] and the actual labels were [0, 0, 0, 0, 0, 0], the accuracy would be 5/6).I see that the imageDataGenerator allows me to specify different styles of data normalization, e.g. featurewise_center, samplewise_center, etc. I see from the examples that if I specify one of these options, then I need to call the fit method on the generator in order to allow the generator to compute statistics like the mean image on the generator. My question is, how does prediction work if I have specified data normalization during training? I can't see how in the framework I would even pass knowledge of the training set mean/std deviation along to predict to allow me to normalize my test data myself, but I also don't see in the training code where this information is stored. Are the image statistics needed for normalization stored in the model so that they can be used during prediction? Yes - this is a really huge downside of Keras.ImageDataGenerator that you couldn't provide the standarization statistics on your own. But - there is an easy method on how to overcome this issue. Assuming that you have a function normalize(x) which is normalizing an image batch (remember that generator is not providing a simple image but an array of images - a batch with shape (nr_of_examples_in_batch, image_dims ..) you could make your own generator with normalization by using: Then you might simply use gen_with_norm(datagen.flow, normalize) instead of datagen.flow. Moreover - you might recover the mean and std computed by a fit method by getting it from appropriate fields in datagen (e.g. datagen.mean and datagen.std). Use the standardize method of the generator for each element. Here is a complete example for CIFAR 10: I also had the same issue and I solved it using the same functionality, that the ImageDataGenerator used: Note that this is only possible if you have a reasonable small dataset, like CIFAR-10. Otherwise the solution proposed by Marcin sounds good more reasonable. I am using the datagen.fit function itself.  Ideally with this,  test_datagen fitted on training dataset will learn the training datasets statistics. Then it will use these statistics to normalize testing data.What is an example of how to use a TensorFlow TFRecord with a Keras Model and tf.session.run() while keeping the dataset in tensors w/ queue runners? Below is a snippet that works but it needs the following improvements: Here is the snippet, there are several TODO lines indicating what is needed: Why is this question relevant? Here is some starter information for a semantic segmentation problem example: I don't use tfrecord dataset format so won't argue on the pros and cons, but I got interested in extending Keras to support the same.  github.com/indraforyou/keras_tfrecord is the repository. Will briefly explain the main changes. Dataset creation and loading data_to_tfrecord and read_and_decode here takes care of creating tfrecord dataset and loading the same. Special care must be to implement the read_and_decode otherwise you will face cryptic errors during training.  Initialization and Keras model  Now both tf.train.shuffle_batch and Keras Input layer returns tensor. But the one returned by tf.train.shuffle_batch don't have metadata needed by Keras internally. As it turns out, any tensor can be easily turned into a tensor with keras metadata by calling Input layer with tensor param.  So this takes care of initialization: Now with x_train_inp any keras model can be developed.  Training (simple) Lets say train_out is the output tensor of your keras model. You can easily write a custom training loop on the lines of: Training (keras style) One of the features of keras that makes it so lucrative is its generalized training mechanism with the callback functions.  But to support tfrecords type training there are several changes that are need in the fit function But all this can be easily supported by another flag parameter. What makes things messing are the keras features sample_weight and class_weight they are used to weigh each sample and weigh each class. For this in compile() keras creates placeholders (here) and placeholders are also implicitly created for the targets (here) which is not needed in our case the labels are already fed in by tfrecord readers. These placeholders needs to be fed in during session run which is unnecessary in our cae.  So taking into account these changes, compile_tfrecord(here) and fit_tfrecord(here) are the extension of compile and fit and shares say 95% of the code.  They can be used in the following way: You are welcome to improve on the code and pull requests. Update 2018-08-29 this is now directly supported in keras, see the following example: https://github.com/keras-team/keras/blob/master/examples/mnist_tfrecord.py Original Answer: TFRecords are supported by using an external loss. Here are the key lines constructing an external loss: Here is an example for Keras 2. It works after applying the small patch #7060: I've also been working to improve the support for TFRecords in the following issue and pull request: Finally, it is possible to use tf.contrib.learn.Experiment to train Keras models in TensorFlow.I am building a prediction model for the sequence data using conv1d layer provided by Keras. This is how I did However, the debugging information has The training data and validation data shape are as follows I think the input_shape in the first layer was not setup right. How to set it up? Update: After using input_shape=(64,1), I got the following error message, even though the model summary runs through You should either change input_shape to ... or use batch_input_shape: This discussion explains the difference between the two in keras in detail.  I had the same issue. I found expanding the dimensions of the input data fixed it using tf.expand_dims I my case I wanted to use Conv2D on a single 20*32 feature map, and did: which gives the expected ndim=4, found ndim=3. Full shape received: [None, 20, 32]. However you need to tell Conv2D that there is only 1 feature map, and add an extra dimension to the input vector. This worked:I have been trying to cluster multiple datasets of URLs (around 1 million each), to find the original and the typos of each URL. I decided to use the levenshtein distance as a similarity metric, along with dbscan as the clustering algorithm as k-means algorithms won't work because I do not know the number of clusters. I am facing some problems using Scikit-learn's implementation of dbscan. This snippet below works on small datasets in the format I an using, but since it is precomputing the entire distance matrix, that takes O(n^2) space and time and is way too much for my large datasets. I have run this for many hours but it just ends up taking all the memory of my PC.  So I figured I needed some way to compute the similarity on the fly and hence tried this method. But this method ends up giving me an error: Which I realize means that its trying to convert the inputs to the similarity function to floats. But I don't want it to do that. 
As far as I understand, it just needs a function that can take two arguments and return a float value that it can then compare to eps, which the levenshtein distance should do. I am stuck at this point, as I do not know the implementation details of sklearn's dbscan to find why it is trying to convert it to float, and neither do I have any better idea on how to avoid the O(n^2) matrix computation.  Please let me know if there is any better or faster way to cluster these many strings that I may have overlooked. From the scikit-learn faq you can do this by making a custom metric: Try ELKI instead of sklearn. It is the only tool I know that allows index accelerated DBSCAN with any metric. It includes Levenshtein distance. You need to add an index to your database with -db.index. I always use the cover tree index (you need to choose the same distance for the index and for the algorithm, of course!) You could use "pyfunc" distances and ball trees in sklearn, but performance was really bad because of the interpreter. Also, DBSCAN in sklearn is much more memory intensive.I'm using Python scikit-learn for simple linear regression on data obtained from csv. The min and max values are showed as 0.0
0.6
41998.0
2593.9 Yet I'm getting this error ValueError: Input contains NaN, infinity or a value too large for dtype('float64'). How should I remove this error?
Because from the above result it is true that it doesn't contain infinites or Nan values. What's the solution for this? Edit: all-stocks-cleaned.csv is avaliabale at http://www.sharecsv.com/s/cb31790afc9b9e33c5919cdc562630f3/all-stocks-cleaned.csv The problem with your regression is that somehow NaN's have sneaked into your data. This could be easily checked with the following code snippet: If you try imputing missing values like below: your regression will run smoothly without a problem: In short: you have missing values in your data, as the error message said. EDIT:: perhaps an easier and more straightforward approach would be to check if you have any missing data right after you read the data with pandas: and then impute the data with any of the two lines below: orI've built a pipeline in Scikit-Learn with two steps: one to construct features, and the second is a RandomForestClassifier. While I can save that pipeline, look at various steps and the various parameters set in the steps, I'd like to be able to examine the feature importances from the resulting model. Is that possible? Ah, yes it is. You list identify the step where you want to check the estimator: For instance: Which returns: You can then access the model step directly: I wrote an article on doing this in general you can find here. In general for a pipeline you can access the named_steps parameter.  This will give you each transformer in a pipeline.  So for example for this pipeline: we could access the individual feature steps by doing model.named_steps["transformer"].get_feature_names() This will return the list of feature names from the TfidfTransformer.  This is all fine and good but doesn't really cover many use cases since we normally want to combine a few features.  Take this model for example: Here we combine a few features using a feature union and a subpipeline.  To access these features we'd need to explicitly call each named step in order.  For example getting the TF-IDF features from the internal pipeline we'd have to do: That's kind of a headache but it is doable.  Usually what I do is use a variation of the following snippet to get it. The below code just treats sets of pipelines/feature unions as a tree and performs DFS combining the feature_names as it goes. You'll also need this method.  Which operates on individual transformations, things like the TfidfVectorizer, to get the names.  In SciKit-Learn there isn't a universal get_feature_names so you have to kind of fudge it for each different case.  This is my attempt at doing something reasonable for most use cases.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Is there a chance to get the value of information gain be negative? IG(Y|X) = H(Y) - H(Y|X) >= 0 , since H(Y) >= H(Y|X) worst case is that X and Y are independent, thus H(Y|X)=H(Y) Another way to think about it is that by observing the random variable X taking some value, we either gain no or some information about Y (you don't lose any). EDIT Let me clarify information gain in the context of decision trees (which actually I had in mind in the first place as I came from a machine learning background). Assume a classification problem where we are given a set of instances and labels (discrete classes). The idea of choosing which attribute to split by at each node of the tree, is to select the feature that splits the class attribute into the two purest possible groups of instances (i.e. lowest entropy). This is in turn equivalent to picking the feature with the highest information gain since where the entropy after the split is the sum of entropies of each branch weighted by the number of instances down that branch. Now there exist no possible split of class values that will generate a case with an even worse purity (higher entropy) than before splitting. Take this simple example of a binary classification problem. At a certain node we have 5 positive instances and 4 negative ones (total of 9). Therefore the entropy (before the split) is: Now lets consider some cases of splits. The best case scenario is that the current attribute splits the instances perfectly (i.e. one branch is all positive, the other all negative): then Imagine that the second attribute is the worst case possible, where one of the branches created doesn't get any instances: rather all instances go down to the other branch (could happen if for example the attribute is constant across instances, thus useless): and Now somewhere in between these two cases, you will see any number of cases like: and so no matter how you split those 9 instances, you always get a positive gain in information. I realize this is no mathematical proof (go to MathOverflow for that!), I just thought an actual example could help. (Note: All calculations according to Google) First, the answer is no, it cannot be negative.  The absolute worst possibility is no change, or an IG of zero.  If you want proof, go look up the full proof on MathOverFlow like Amro pointed out. Now for the advice.  If you only do the first level of a decision tree, it seems obvious that it would never come up negative.  However, when building my first tree using Information Gain, I found myself with a negative gain by my third branching.  This didn't seem useful or possible, so I scrambled to check my math.  The math was fine.  The part I had wrong was the first part of the base formula.  I was using the answer from the level above as my starting entropy, but this is wrong because it includes information from other datasets.  You need to make sure that for your starting entropy you determine the entropy for that branch alone!  This means your "starting entropy" could actually be higher than it was at the level before this one. In other words, when calculating IG, make sure you are only using the current dataset. Sure it can.  Information gain is just the change in information entropy from one state to another: IG(Ex, a) = H(Ex) - H(Ex | a) That state change can go in either direction--it can be positive or negative.   This is easy to see by example: Decision Tree algorithms works like this: at a given node, you calculate its information entropy (for the independent variable).  You can think of it like this: information entropy is to categorical/discrete variables as variance is to continuous variables). Variance, of course, is just the square of the standard deviation. So for instance, if we are looking predicting price based on various criteria, and we have arbitrarily group our data set into two groups, in which the prices for group A are (50, 60, and 70), and the prices for group B are (50, 55, 60), group B has the lowest variance--i.e., they are close together. Of course variance cannot be negative (because after you sum the distances of each point from the mean, you square it) but the difference in variance certainly can.  To see how this relates to Information Entropy/Information Gain, suppose we aren't predicting price but something else, like whether the visitor to our Site will become a registered user or a premium subscriber, or neither. The indepdendent variable here is discrete, not continuous like price, so you can't calculate variance in a meaningful way. Information entropy is what is used instead. (If you doubt the close analogy here between variance and IE, you should know that most decision tree algorithms capable of handling both discrete and continuous variables, in the latter case, the algorithm will use variance as the splitting criterion, rather than using IG.) In any event, after you calculate the information entropy for a given node, you then split the data at that node (which is the entire data set if you are at the root node) on every value for every variable, then for each split, calculate the IE for both groups, and take the weighted average IE. Next take the split that results in the lowest weighted average IE and compare it with the node IE (which is obviously just a single group). If that weighted average IE for the split is lower than the node IE, then you split the data at that node (form a branch), if not, then you stop, i.e., that node can't be further split--you are at the bottom. In sum, at the heart of the decision tree algorithm is the criterion to determine whether to split a node--that's how they are constructed. That criterion is whether IG is positive or negative.A geometric margin is simply the euclidean distance between a certain x (data point) to the hyperlane.  What is the intuitive explanation to what a functional margin is? Note: I realize that a similar question has been asked here:
How to understand the functional margin in SVM ? However, the answer given there explains the equation, but not its meaning (as I understood it). "A geometric margin is simply the euclidean distance between a certain x (data point) to the hyperlane. " I don't think that is a proper definition for the geometric margin, and I believe that is what is confusing you. The geometric margin is just a scaled version of the functional margin. You can think the functional margin, just as a testing function that will tell you whether a particular point is properly classified or not. And the geometric margin is functional margin scaled by ||w|| If you check the formula:  You can notice that independently of the label, the result would be positive for properly classified points (e.g sig(1*5)=1 and sig(-1*-5)=1) and negative otherwise. If you scale that by ||w|| then you will have the geometric margin.  Why does the geometric margin exists? Well to maximize the margin you need more that just the sign, you need to have a notion of magnitude, the functional margin would give you a number but without a reference you can't tell if the point is actually far away or close to the decision plane. The geometric margin is telling you not only if the point is properly classified or not, but the magnitude  of that distance in term of units of |w|    The functional margin represents the correctness and confidence of the prediction if the magnitude of the vector(w^T) orthogonal to the hyperplane has a constant value all the time. By correctness, the functional margin should always be positive, since if wx + b is negative, then y is -1 and if wx + b is positive, y is 1. If the functional margin is negative then the sample should be divided into the wrong group. By confidence, the functional margin can change due to two reasons: 1) the sample(y_i and x_i) changes or 2) the vector(w^T) orthogonal to the hyperplane is scaled (by scaling w and b).
If the vector(w^T) orthogonal to the hyperplane remains the same all the time, no matter how large its magnitude is, we can determine how confident the point is grouped into the right side. The larger that functional margin, the more confident we can say the point is classified correctly. But if the functional margin is defined without keeping the magnitude of the vector(w^T) orthogonal to the hyperplane the same, then we define the geometric margin as mentioned above. The functional margin is normalized by the magnitude of w to get the geometric margin of a training example. In this constraint, the value of the geometric margin results only from the samples and not from the scaling of the vector(w^T) orthogonal to the hyperplane. The geometric margin is invariant to the rescaling of the parameter, which is the only difference between geometric margin and functional margin. EDIT: The introduction of functional margin plays two roles: 1) intuit the maximization of geometric margin and 2) transform the geometric margin maximization issue to the minimization of the magnitude of the vector orthogonal to the hyperplane. Since scaling the parameters w and b can result in nothing meaningful and the parameters are scaled in the same way as the functional margin, then if we can arbitrarily make the ||w|| to be 1(results in maximizing the geometric margin) we can also rescale the parameters to make them subject to the functional margin being 1(then minimize ||w||). Check Andrew Ng's Lecture Notes from Lecture 3 on SVMs (notation changed to make it easier to type without mathjax/TeX on this site): "Letâ€™s formalize the notions of the functional and geometric margins
  . Given a
  training example (x_i, y_i) we define the functional margin of (w, b) with
  respect to the training example  gamma_i = y_i( (w^T)x_i + b ) Note that if y_i > 0 then for the functional margin to be large (i.e., for
  our prediction to be confident and correct), we need (w^T)x + b to be a large
  positive number. Conversely, if y_i < 0, then for the functional margin
  to be large, we need (w^T)x + b to be a large negative number. Moreover, if y_i( (w^T)x_i + b) > 0 then our prediction on this example is correct. (Check this yourself.) Hence, a large functional margin represents a confident and a correct prediction."  Page 3 from the Lecture 3 PDF linked at the materials page linked above. Not going into unnecessary complications about this concept, but in the most simple terms here is how one can think of and relate functional and geometric margin.I have a scikit-learn pipline with kerasRegressor in it: After, training the pipline, I am trying to save to disk using joblib... But I am getting an error: RuntimeError: maximum recursion depth exceeded How would you save the pipeline to disk? I struggled with the same problem as there are no direct ways to do this. Here is a hack which worked for me. I saved my pipeline into two files. The first file stored a pickled object of the sklearn pipeline and the second one was used to store the Keras model: And here is how the model could be loaded back: Keras is not compatible with pickle out of the box. You can fix it if you are willing to monkey patch: https://github.com/tensorflow/tensorflow/pull/39609#issuecomment-683370566. You can also use the SciKeras library which does this for you and is a drop in replacement for KerasClassifier: https://github.com/adriangb/scikeras Disclosure: I am the author of SciKeras as well as that PR.I run a clustering algorithm and want to evaluate the result by using silhouette score in scikit-learn. But in the scikit-learn, it needs to calculate the distance matrix: distances = pairwise_distances(X, metric=metric, **kwds) Due to the fact that my data is order of 300K, and my memory is 2GB, and the result is out of memory. And I can not evaluate the clustering result. Does anyone know how to overcome this problem? Set the sample_size parameter in the call to silhouette_score to some value smaller than 300K. Using this parameter will sample datapoints from X and calculate the silhouette_score on those instead of the entire array.Do you know of any good c++ svm libraries out there
I tried libsvm http://www.csie.ntu.edu.tw/~cjlin/libsvm/ but so far I'm not flabbergasted. I have also heard of SVMLight and TinySVM. Have you tried them ? Any new players ? Thanks ! A comprehensive list of SVM libraries can be found here. I've used SVMLight before and found it to be very stable and fast. I had a good experience using it and would recommend it. However, I think there is probably less documentation on SVMLight than libSVM; just the papers by Thorsten Joachims and the comments in the source code. I didn't find the source too hard to follow in general, but you need to read the papers beforehand to understand the background. It's also written in pure C, not C++, if that matters to you. As for 'new players', the new research is mostly into making the SVM optimisation algorithms more efficient. For example, using stochastic gradient descent as in svmsgd and pegasos. I haven't looked at the implementations of these algorithms, but it's research code so I wouldn't expect that they are particularly easy to follow, if that's your primary concern. Here's another monster list of SVM packages, libraries and SVM applications. the best way to get started is to read the libsvm guide provided in the website, also, a good starting video tutorial on how to install libsvm, and do ur first trainig/classification task can be found here:
http://www.youtube.com/watch?v=gePWtNAQcK8
good luck with that, i am also just starting it these days, pretty good results that i got, but still tuning it. There is also dlib, which is quiet complete.  In particular, there are algorithms for performing classification, regression, clustering, sequence labeling, anomaly detection, and feature ranking, as well as algorithms for doing more specialized computations.  shark SHARK is a modular C++ library for the design and optimization of adaptive systems. It provides methods for linear and nonlinear optimization, in particular evolutionary and gradient-based algorithms, kernel-based learning algorithms and neural networks, and various other machine learning techniques. SHARK serves as a toolbox to support real world applications as well as research in different domains of computational intelligence and machine learning. The sources are compatible with the following platforms: Windows, Solaris, MacOS X, and Linux.In the chapter seven of this book "TensorFlow Machine Learning Cookbook" the author in pre-processing data uses fit_transform function of scikit-learn to get the tfidf features of text for training. The author gives all text data to the function before separating it into train and test. Is it a true action or we must separate data first and then perform fit_transform on train and transform on test? According to the documentation of scikit-learn, fit() is used in order to Learn vocabulary and idf from training set. On the other hand, fit_transform() is used in order to Learn vocabulary and idf, return term-document matrix. while transform() Transforms documents to document-term matrix. On the training set you need to apply both fit() and transform() (or just fit_transform() that essentially joins both operations) however, on the testing set you only need to transform() the testing instances (i.e. the documents). Remember that training sets are used for learning purposes (learning is achieved through fit()) while testing set is used in order to evaluate whether the trained model can generalise well to new unseen data points. For more details you can refer to the article fit() vs transform() vs fit_transform() Author gives all text data before separating train and test to
  function. Is it a true action or we must separate data first then
  perform tfidf fit_transform on train and transform on test? I would consider this as already leaking some information about the test set into the training set. I tend to always follow the rule that before any pre-processing first thing to do is to separate the data, create a hold-out set. As we are talking about text data, we have to make sure that the model is trained only on the vocabulary of the training set as when we will deploy a model in real life, it will encounter words that it has never seen before so we have to do the validation on the test set keeping that in mind.
We have to make sure that the new words in the test set are not a part of the vocabulary of the model.
Hence we have to use fit_transform on the training data and transform on the test data.
If you think about doing cross validation, then you can use this logic across all the folds.I am thinking of training word2vec on huge large scale data of more than 10 TB+ in size on web crawl dump.  I personally trained c implementation GoogleNews-2012 dump (1.5gb) on my iMac took about 3 hours to train and generate vectors (impressed with speed). I did not try python implementation though :( I read somewhere that generating vectors on wiki dump (11gb) of 300 vector length takes about 9 days to generate.  How to speed up word2vec? Do i need to use distributed models or what type of hardware i need to do it within 2-3 days? i have iMac with 8gb ram. Which one is faster? Gensim python or C implemention? I see that word2vec implementation does not support GPU training. There are a number of opportunities to create Word2Vec models at scale. As you pointed out, candidate solutions are distributed (and/or multi-threaded) or GPU. This is not an exhaustive list but hopefully you get some ideas as to how to proceed. Distributed / Multi-threading options: A number of Word2Vec GPU implementations exist. Given the large dataset size, and limited GPU memory you may have to consider a clustering strategy. There are a number of other CUDA implementations of Word2Vec, at varying degrees of maturity and support: I believe the SparkML team has recently got going a prototype cuBLAS-based Word2Vec implementation. You may want to investigate this.I've trained dataset using XGB Classifier, but I got this error in local. It worked on Colab and also my friends don't have any problem with same code.
I don't know what that error means... Invalid classes inferred from unique values of y.  Expected: [0 1 2 3 4 5], got [1 2 3 4 5 6] this is my code, but I guess it's not the reason. That happens because the class column has to start from 0 (as required since version 1.3.2). An easy way to solve that is using LabelEncoder from sklearn.preprocssing library. Solution (works for version 1.6): And then you try/run your code again: Try to adding stratify to the train_test_split code: Downgrading to 1.5.0 worked for me Also got this warning message during execution UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. using the Label encoder in 1.6 returns this error for me: MultiClassEvaluation: label must be in [0, num_class), num_class=6 but found 6 in label The erros comes with the new version of xgboost, Uninstall current Xgboost and install xgboost 0.90 It's because the y_train must be encoded in a newer update XGBoost model before training it, i.e., you must use some categorical transformation like label encoders: Then apply it to XGBoost model for training: After training to find out its confusion matrix you must inverse transform the predicted y values, as shown: Use python version 3.7 as used in colab If it helps, i just rolled back to version 1.2.1In the example below, I am using StandardScaler(), is this the correct way to apply it to test set as well? Yes, this is the right way to do this but there is a small mistake in your code. Let me break this down for you. When you use the StandardScaler as a step inside a Pipeline then scikit-learn will internally do the job for you. What happens can be described as follows: Note: You should be using grid.fit(X, y) and NOT grid.fit(X_train, y_train) because the GridSearchCV will automatically split the data into training and testing data (this happen internally). Use something like this: Once you run this code (when you call grid.fit(X, y)), you can access the outcome of the grid search in the result object returned from grid.fit(). The best_score_ member provides access to the best score observed during the optimization procedure and the best_params_ describes the combination of parameters that achieved the best results. IMPORTANT EDIT 1: if you want to keep a validation dataset of the original dataset use this: Then use: Quick answer: Your methodology is correct. Although the above answer is very good, I just would like to point out some subtleties: best_score_ [1] is the best cross-validation metric, and not the generalization performance of the model [2]. To evaluate how well the best found parameters generalize, you should call the score on the test set, as you've done. Therefore it is needed to start by splitting the data into training and test set, fit the grid search only in the X_train, y_train, and then score it with X_test, y_test [2]. Deep Dive: A threefold split of data into training set, validation set and test set is one way to prevent overfitting in the parameters during grid search. On the other hand, GridSearchCV uses Cross-Validation in the training set, instead of having both training and validation set, but this does not replace the test set. This can be verified in [2] and [3]. References: [1] GridSearchCV [2] Introduction to Machine Learning with Python [3] 3.1 Cross-validation: evaluating estimator performanceMy problem is conceptually similar to solving anagrams, except I can't just use a dictionary lookup. I am trying to find plausible words rather than real words. I have created an N-gram model (for now, N=2) based on the letters in a bunch of text. Now, given a random sequence of letters, I would like to permute them into the most likely sequence according to the transition probabilities. I thought I would need the Viterbi algorithm when I started this, but as I look deeper, the Viterbi algorithm optimizes a sequence of hidden random variables based on the observed output. I am trying to optimize the output sequence. Is there a well-known algorithm for this that I can read about? Or am I on the right track with Viterbi and I'm just not seeing how to apply it?  Update I have added a bounty to ask for more insight into this problem. (Analysis explaining why an efficient approach isn't possible, other heuristics/approximations besides simulated annealing, etc.) As an exercise, I wrote a simple implementation of Markov Chains in MATLAB. Basically its a letter-based probabilistic model to generating words. We will need some text to train the model. We use 'The Wonderful Wizard of Oz' from Project Gutenberg. Finally, we use the model to either sample random words or sample words from a set of letters: Here's a bunch of examples generated from the letters 'markovchains', along with log-probability of the word given the model: You can see that although none are correct words, they are still better than just a random sequence of letters. Obviously using only the previous character to generate the next one is not enough, still it can be easily extended to more sophisticated cases (N-gram). The nice thing about such an approach is that its not restricted to one language, and can be adapted to any other by simply feeding it documents from your language of choice. Consider the set of K letters as vertices in a graph.  Add directed edges to represent the 2-grams from each letter to all the others, with weights that correspond to their probabilities. A "word", then, is a path through the (complete, directed) graph. You are looking for the best (least- or most-weighted) "word" (path) that uses all the letters (visits all the vertices). This is the asymmetric Traveling Salesman Problem.  It's NP-complete.  I don't think it's going to get easier if you use N-grams bigger than N=2, so you're not likely to find an efficient algorithm, but let us know if you do  (Simulated Annealing or something like it is probably the way to go) If I understand your problem correctly, you are searching all permutations of letters in a word for the one with the lowest product of 2-gram probabilities. If your word is too long to simply brute force all combinations, I've found that stochastic optimization algorithms produce good results in a short time. I (having a mathematical background) have done some work on the algorithm "Simulated Annealing", which I think would fit nicely to your problem. And it is pretty easy to implement.  You could also do it stochastically with a Markov chain. For starters, make sure that your N-gram table includes a "beginning of word" symbol; then find the available transitions from that state, and filter them so that they only include available letters from your pool, and choose randomly among them using weighted probabilities. Then find the transitions from the next state, filtering down to the still-available letters, and end when there are no more letters in the pool (or, if you reach a state that you can't transition out of, go back to the beginning and try again). You may actually find it useful that this is more random than some of the other available options, and if it's too random you have the option of massaging the probabilities, or simply generating some number n (say 100) of random words, sorting them by their "likelihood", and then choosing randomly from among the top m (perhaps 10), which gives you relatively fine control over whether the words you generate from any bag of letters are more consistent or more random.This question does not appear to be about programming within the scope defined in the help center. Closed 11 months ago. I am trying to get a feel for the difference between the various classes of machine-learning algorithms.   I understand that the implementations of evolutionary algorithms are quite different from the implementations of neural networks.  However, they both seem to be geared at determining a correlation between inputs and outputs from a potentially noisy set of training/historical data.   From a qualitative perspective, are there problem domains that are better targets for neural networks as opposed to evolutionary algorithms? I've skimmed some articles that suggest using them in a complementary fashion.  Is there a decent example of a use case for that? Here is the deal: in machine learning problems, you typically have two components:  a)  The model (function class, etc) b)  Methods of fitting the model (optimizaiton algorithms) Neural networks are a model: given a layout and a setting of weights, the neural net produces some output.  There exist some canonical methods of fitting neural nets, such as backpropagation, contrastive divergence, etc.  However, the big point of neural networks is that if someone gave you the 'right' weights, you'd do well on the problem. Evolutionary algorithms address the second part -- fitting the model.  Again, there are some canonical models that go with evolutionary algorithms: for example, evolutionary programming typically tries to optimize over all programs of a particular type.  However, EAs are essentially a way of finding the right parameter values for a particular model.  Usually, you write your model parameters in such a way that the crossover operation is a reasonable thing to do and turn the EA crank to get a reasonable setting of parameters out.   Now, you could, for example, use evolutionary algorithms to train a neural network and I'm sure it's been done.  However, the critical bit that EA require to work is that the crossover operation must be a reasonable thing to do -- by taking part of the parameters from one reasonable setting and the rest from another reasonable setting, you'll often end up with an even better parameter setting.  Most times EA is used, this is not the case and it ends up being something like simulated annealing, only more confusing and inefficient. Problems that require "intuition" are better suited to ANNs, for example hand writing recognition. You train a neural network with a huge amount of input and rate it until you're done (this takes a long time), but afterwards you have a blackbox algorithm/system that can "guess" the hand writing, so you keep your little brain and use it as a module for many years or something. Because training a quality ANN for a complex problem can take months I'm worst case, and luck. Most other evolutionary algorithms "calculate" an adhoc solution on the spot, in a sort of hill climbing pattern. Also as pointed out in another answer, during runtime an ANN can "guess" faster than most other evolutionary algorithms can "calculate". However one must be careful, since the ANN is just "guessing" an it might be wrong. Evolutionary, or more generically genetic algorithms, and neural networks can both be used for similar objectives, and other answers describe well the difference. However, there is one specific case where evolutionary algorithms are more indicated than neural networks: when the solution space is non-differentiable. Indeed, neural networks use gradient descent to learn from backpropagation (or similar algorithm). The calculation of a gradient relies on derivatives, which needs a continuous and derivative space, in other words that you can shift gradually and progressively from one solution to the next. If your solution space is non-differentiable (ie, either you can choose solution A, or B, or C, but nothing in the middle like 0.5% A + 0.5% B, so that some solutions are impossible), then you are trying to fit a non-differentiable function, and then neural networks cannot work. (Side note: discrete state space partially share the same issue and so are a common issue for most algorithms but there are usually some work done to workaround these issues, for example decision trees can work easily on categorical variables, while other models like svm have more difficulties and generally require encoding categorical variables into continuous values). In this case, evolutionary and genetic algorithms are perfect, one could even say a god send, since they can "jump" from one solution to the next without any issue. They don't care that some solutions are impossible, nor that the gaps are big or small between subset of the possible state space, evolutionary algorithms can jump randomly far away or close by until they find appropriate solutions. Also worth mentioning is that evolutionary algorithms are not subject to the curse of dimensionality as much as any other machine learning algorithm, including neural networks. This might seem a bit counter intuitive, since the convergence to a global maximum is not guaranteed, and the procedure might seem to be slow to evolve to a good solution, but in practice the selection procedure works fast and converges to a good local maximum. This makes evolutionary algorithms a very versatile and generic tool to approach naively any problem, and one of the very few tools to deal with either non-differentiable functions, discrete functions, or with astronomically high dimensional datasets. Look at Neuro Evolution. (NE) The current best methods is NEAT and HyperNEAT by Kenneth Stanley. Genetic Algorithms only find a genome of some sort; It's great to create the genome of a neural network, because you get the reactive nature of the neural network, rather than just a bunch of static genes. There's not many limits to what it can learn. But it takes time of course. Neural topology have to be evolved through the usual mutation and crossover, as well as weights updated. There can be no back propagation. Also you can train it with a fitness function, which is thus superior to back propagation when you do not know what the output should be. Perfect for learning complex behaviour for systems that you do not know any optimal strategies for. Only problem is that it'll learn behaviour you didn't anticipate. Often that behaviour can be very alien, although it does exactly what you rewarded it for in the fitness function. Thus you'll be using as much time deriving fitness functions as you would have creating output sets for backpropagation :P Evolutionary algorithms (EAs) are slow because they rely on unsupervised learning: EAs are told that some solutions are better than others, but not how to improve them. Neural networks are generally faster, being an instance of supervised learning: they know how to make a solution better by using gradient descent within a function space over certain parameters; this allows them to reach a valid solution faster. Neural networks are often used when there isn't enough knowledge about the problem for other methods to work. In terms of problem domains, I compare artificial neural networks trained by backpropagation to an evolutionary algorithm. An evolutionary algorithm deploys a randomized beamsearch, that means your evolutionary operators develop candidates to be tested and compared by their fitness. Those operators are usually non deterministic and you can design them so they can both find candidates in close proximity and candidates that are further away in the parameter space to overcome the problem of getting stuck in local optima. However the success of a EA approach greatly depends on the model you develop, which is a tradeoff between high expression potential (you might overfit) and generality (the model might not be able to express the target function). Because neural networks usually are multilayered the parameter space is not convex and contains local optima, the gradient descent algorithms might get stuck in. The gradient descent is a deterministic algorithm, that searches through close proximity. That's why neural networks usually are randomly initialised and why you should train many more than one model. Moreover you know each hidden node in a neural network defines a hyperplane you can design a neural network so it fits your problem well. There are some techniques to prevent neural networks from overfitting. All in all, neural networks might be trained fast and get reasonable results with few efford (just try some parameters). In theory a neural network that is large enough is able to approximate every target function, which on the other side makes it prone to overfitting. Evolutionary algorithms require you to make a lot of design choices to get good results, the hardest probably being which model to optimise. But EA are able to search through very complex problem spaces (in a manner you define) and get good results quickly. AEs even can stay successful when the problem (the target function) is changing over time. Tom Mitchell's Machine Learning Book:
http://www.cs.cmu.edu/~tom/mlbook.html Evolutionary algorithms (EA) represent a manner of training a model, where as neuronal nets (NN) ARE a model. Most commonly throughout the literature, you will find that NNs are trained using the backpropagation algorithm. This method is very attractive to mathematicians BUT it requires that you can express the error rate of the model using a mathematical formula. This is the case for situations in which you know lots of input and output values for the function that you are trying to approximate. This problem can be modeled mathematically, as the minimization of a loss function, which can be achieved thanks to calculus (and that is why mathematicians love it). But neuronal nets are also useful for modeling systems which try to maximize or minimize some outcome, the formula of which is very difficult to model mathematically. For instance, a neuronal net could control the muscles of a cyborg to achieve running. At each different time frame, the model would have to establish how much tension should be present in each muscle of the cyborg's body, based on the input from various sensors. It is impossible to provide such training data. EAs allow training by only providing a manner of evaluation of the model. For our example, we would punish falling and reward the traveled distance across a surface (in a fixed timeframe). EA would just select the models which do their best in this sense. First generations suck but, surprisingly, after a few hundred generations, such individuals achieve very "natural" movements and manage to run without falling off. Such models may also be capable of dealing with obstacles and external physical forces.I have data with differing weights for each sample. In my application, it is important that these weights are accounted for in estimating the model and comparing alternative models. I'm using sklearn to estimate models and to compare alternative hyperparameter choices. But this unit test shows that GridSearchCV does not apply sample_weights to estimate scores. Is there a way to have sklearn use sample_weight to score the models? Unit test: Which produces output: Explanation: Since manually computing the loss without weighting produces the same scoring as GridSearchCV, we know that the sample weights are not being used. The GridSearchCV takes a scoring as input, which can be callable. You can see the details of how to change the scoring function, and also how to pass your own scoring function here. Here's the relevant piece of code from that page for the sake of completeness:   EDIT: The fit_params is passed only to the fit functions, and not the score functions. If there are parameters which are supposed to be passed to the scorer, they should be passed to the make_scorer. But that still doesn't solve the issue here, since that would mean that the whole sample_weight parameter would be passed to log_loss, whereas only the part which corresponds to y_test at the time of calculating the loss should be passed. sklearn does NOT support such a thing, but you can hack your way through, using a padas.DataFrame. The good news is, sklearn understands a DataFrame, and keeps it that way. Which means you can exploit the index of a DataFrame as you see in the code here: As you see, the score_f uses the index of y_true to find which parts of sample_weight to use. For the sake of completeness, here's the whole code: The output of the code is then: EDIT 2: as the comment bellow says:  the difference in my score and the sklearn score using this solution
  originates in the way that I was computing a weighted average of
  scores. If you omit the weighted average portion of the code, the two
  outputs match to machine precision. Currently in sklearn, GridSearchCV(and any classes inherit BaseSearchCV) only allow sample_weight in **fit_params but not using it in scoring, which is not correct, since CV pick the "best estimator" via unweighted score. Notes, when you grid.fit(X, y, sample_weight=w) only use sample weights in fit, not score. There are two ways to solve this problems: Just pointing out that there is an ongoing effort to support this important feature: https://github.com/scikit-learn/scikit-learn/pull/13432 But it seems that because of backward compatibility issues and the desire to tackle the more general problem of passing arbitrary sample related information it is taking a bit too long. The last attempt seems to be: https://github.com/scikit-learn/scikit-learn/pull/16079 Here is a good review of the issue: http://deaktator.github.io/2019/03/10/the-error-in-the-comparator/What is the difference between word2vec and glove? 
Are both the ways to train a word embedding? if yes then how can we use both? Yes, they're both ways to train a word embedding. They both provide the same core output: one vector per word, with the vectors in a useful arrangement. That is, the vectors' relative distances/directions roughly correspond with human ideas of overall word relatedness, and even relatedness along certain salient semantic dimensions. Word2Vec does incremental, 'sparse' training of a neural network, by repeatedly iterating over a training corpus. GloVe works to fit vectors to model a giant word co-occurrence matrix built from the corpus. Working from the same corpus, creating word-vectors of the same dimensionality, and devoting the same attention to meta-optimizations, the quality of their resulting word-vectors will be roughly similar. (When I've seen someone confidently claim one or the other is definitely better, they've often compared some tweaked/best-case use of one algorithm against some rough/arbitrary defaults of the other.) I'm more familiar with Word2Vec, and my impression is that Word2Vec's training better scales to larger vocabularies, and has more tweakable settings that, if you have the time, might allow tuning your own trained word-vectors more to your specific application. (For example, using a small-versus-large window parameter can have a strong effect on whether a word's nearest-neighbors are 'drop-in replacement words' or more generally words-used-in-the-same-topics. Different downstream applications may prefer word-vectors that skew one way or the other.) Conversely, some proponents of GLoVe tout that it does fairly well without needing metaparameter optimization. You probably wouldn't use both, unless comparing them against each other, because they play the same role for any downstream applications of word-vectors. Word2vec is a predictive model: trains by trying to predict a target word given a context (CBOW method) or the context words from the target (skip-gram method). It uses trainable embedding weights to map words to their corresponding embeddings, which are used to help the model make predictions. The loss function for training the model is related to how good the model’s predictions are, so as the model trains to make better predictions it will result in better embeddings. The Glove is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently (matrix values) we see this word in some “context” (the columns) in a large corpus.  The number of “contexts” would be very large, since it is essentially combinatorial in size. So  we factorize this matrix to yield a lower-dimensional (word x features) matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data. Before GloVe, the algorithms of word representations can be divided into two main streams, the statistic-based (LDA) and learning-based (Word2Vec). LDA produces the low dimensional word vectors by singular value decomposition (SVD) on the co-occurrence matrix, while Word2Vec employs a three-layer neural network to do the center-context word pair classification task where word vectors are just the by-product. The most amazing point from Word2Vec is that similar words are located together in the vector space and arithmetic operations on word vectors can pose semantic or syntactic relationships, e.g., “king” - “man” + “woman” -> “queen” or “better” - “good” + “bad” -> “worse”. However, LDA cannot maintain such linear relationship in vector space. The motivation of GloVe is to force the model to learn such linear relationship based on the co-occurreence matrix explicitly. Essentially, GloVe is a log-bilinear model with a weighted least-squares objective. Obviously, it is a hybrid method that uses machine learning based on the statistic matrix, and this is the general difference between GloVe and Word2Vec. If we dive into the deduction procedure of the equations in GloVe, we will find the difference inherent in the intuition. GloVe observes that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. Take the example from StanfordNLP (Global Vectors for Word Representation), to consider the co-occurrence probabilities for target words ice and steam with various probe words from the vocabulary: However, Word2Vec works on the pure co-occurrence probabilities so that the probability that the words surrounding the target word to be the context is maximized. In the practice, to speed up the training process, Word2Vec employs negative sampling to substitute the softmax fucntion by the sigmoid function operating on the real data and noise data. This emplicitly results in the clustering of words into a cone in the vector space while GloVe’s word vectors are located more discretely.I am trying to use a hidden Markov model (HMM) for a problem where I have M different observed variables (Yti) and a single hidden variable (Xt) at each time point, t.  For clarity, let us assume all observed variables (Yti) are categorical, where each Yti conveys different information and as such may have different cardinalities. 
An illustrative example is given in the figure below, where M=3.  My goal is to train the transition,emission and prior probabilities of an HMM, using the Baum-Welch algorithm, from my observed variable sequences (Yti). Let's say, Xt will initially have 2 hidden states. I have read a few tutorials (including the famous Rabiner paper) and went through the codes of a few HMM software packages, namely 'HMM Toolbox in MatLab' and 'hmmpytk package in Python'. Overall, I did an extensive web search and all the resources -that I could find- only cover the case, where there is only a single observed variable (M=1) at each time point. This increasingly makes me think HMM's are not suitable for situations with multiple observed variables. Thanks. Edit:
In this paper, the situation depicted in the figure is described as a Dynamic Naive Bayes, which -in terms of the training and estimation algorithms- requires a slight extension to Baum-Welch and Viterbi algorithms for a single-variable HMM. The simplest way to do this, and have the model remain generative, is to make the y_is conditionally independent given the x_is. This leads to trivial estimators, and relatively few parameters, but is a fairly restrictive assumption in some cases (it's basically the HMM form of the Naive Bayes classifier).  EDIT: what this means. For each timestep i, you have a multivariate observation y_i = {y_i1...y_in}. You treat the y_ij as being conditionally independent given x_i, so that: you're then effectively learning a naive Bayes classifier for each possible value of the hidden variable x. (Conditionally independent is important here: there are dependencies in the unconditional distribution of the ys). This can be learned with standard EM for an HMM. You could also, as one commenter said, treat the concatenation of the y_ijs as a single observation, but if the dimensionality of any of the j variables is beyond trivial this will lead to a lot of parameters, and you'll need way more training data. Do you specifically need the model to be generative? If you're only looking for inference in the x_is, you'd probably be much better served with a conditional random field, which through its feature functions can have far more complex observations without the same restrictive assumptions of independence. I found that this can be achieved by modelling the system as a Dynamic Naive Bayes classifier (DNB), which is a slight extension of an ordinary (single-variable) HMM that can cater for multi-observation scenarios as shown in the figure.  Caution is advised in that DNB still has a hidden state and should therefore not be regarded as a direct sequential expansion of the original Naive Bayes classifier. The 'naive' in the algorithm's name originates from the fact that all observed variables are independent of each other, given the hidden state variable. Similar to an HMM, the parameter estimations of this model can be achieved via the Baum Welch (or EM, whichever you prefer to name it) algorithm. Since the emission distribution at each time step is now the product of P(Yti|Xt) of each observed variable Yti, the forward, backward, and joint variable equations need to be slightly modified as described in section 3 of this paper by Aviles-Arriaga et al. The thing that you are looking for is called Structured Perceptron. Take a look at the following slid on page 42.
http://www.cs.umd.edu/class/fall2015/cmsc723/slides/inclass_09.pdf you could model the problem using tensors
structure a tensor using the two time series and then identify the HMM parameters.
"Hidden Markov Model Identifiability via Tensors" is a good reference for this. Matlab provides tensor toolbox. fyi, I am working on a related problem so feel free to email me if you want to discuss in a more private manner You can try hidden semi-Markov model which is an extension of hmm. It allows each state lasting for multiple time periods. This paper proposed an algorithm to solve the problemI currently have two numpy arrays: This is the code I have written to attempt to build a linear classification model of these features. First of all I adapted the arrays to a Tensorflow dataset: I then tried to fit an SVM model: But this just returns the error: What am I doing wrong? Here's an SVM usage example which does not throw an error: Examples passed to the SVM Estimator need string IDs. You can probably substitute back infer_real_valued_columns_from_input, but you would need to pass it a dictionary so it picks up the right name for the column. In this case it's conceptually simpler to just construct the feature column yourself. As the error says self.name is  an empty string and that empty string is not present in your dictionary that you are passing to infer_real_valued_columns_from_input that creates _RealValuedColumn object So What I found by debugging the error is that the tf.contrib.learn.infer_real_valued_columns_from_input(X) the X that you pass has to be a dictionary so that the self.name of _RealValuedColumn object is initialized by the key of the dictionary that you pass So this is what I did Now this removes the above error an but it gives a new error TypeError: Input 'input' of 'SdcaFprint' Op has type int64 that does not match expected type of string.I am working with a very memory demanding CNN model for a task of classification.
This poses a big limit on the batch size that I can use during training. One solution is to accumulate the gradients during training, meaning that the weights of the model are not updated after every single batch. Instead the same weights are used for several batches, while the gradients from each batch are accumulated and than averaged for a single weight-update action. I'm using a Tensorflow backend Keras and I'm pretty sure that Keras has no off-the-shelf function/method to achieve this. How can it be done for a Keras/tensorflow model? As was mentioned in the question, there is no off-the-shelf function/method to achieve this with Keras/Tensorflow. However this can be done by writing a custom optimizer for Keras. The main idea is to use a flag to determine whether to update the weights during each batch. The following implementation is based on this github post by "alexeydevederkin" and it is an accumulating Adam optimizer: It can be used in the following way: In this example, the model processes 10 samples in every iteration ("batch_size"), but the update to the weights only happens after accumulating 5 such batches ("accum_iters"). So the actual batch size for updating the weights is 50. We have published an open-source tool to automatically add gradient accumulation support in Keras models we implemented at Run:AI to help us with batch sizing issues. Using gradient accumulation in our models allowed us to use large batch sizes while being limited by GPU memory. It specifically allowed us running neural networks with large batch sizes using only a single GPU. The project is available at https://github.com/run-ai/runai/tree/master/runai/ga along with explanations and examples you can use right out of the box. Using this tool, all you have to do is add a single line of code to your Python script, and you can add gradient accumulation support to your optimizer. The Python package is available at PyPI and can be installed using the command: pip install runai. Adding gradient accumulation support to Keras models is extremely easy. First, import the package to your code: import runai.ga. Then, you have to create a gradient accumulation optimizer. There are two ways to do so: 1. Wrap an existing Keras optimizer You can take any Keras optimizer - whether it's a built-in one (SGD, Adam, etc...) or a custom optimizer with your algorithm implementation - and add gradient accumulation support to it using the next line: Where optimizer is your optimizer, and STEPS is the number of steps you want to accumulate gradients over. 2. Create a gradient accumulation version of any of the built-ins optimizers There are gradient accumulation versions of all built-in optimizers (SGD, Adam, etc...) available in the package. They can be created using this line: Here, we create a gradient accumulation version of Adam optimizer, and we accumulate gradients over STEPS steps. More information, explanations, and examples are available in GitHub. In addition to the open-source tool itself, we have published a series of 3 articles on Towards Data Science (Medium), where we explained issues when using large batch sizes, what is gradient accumulation and how can it help in solving these issues, how it works, and how we implemented it. Here are links to the articles: The problem of batch sizing and limited GPU memory What is Gradient Accumulation and how does it help? How-to guide to using the gradient accumulation mechanism and how we implemented it Let us know if the tool helped you in using gradient accumulation in your own Keras models.
We are here to give any support and help with the problems you encounter when using it in your own models. A more convenient way is to inject some changes into the existing optimizer. usage: reference: https://github.com/bojone/accum_optimizer_for_kerasI read from somewhere that if you choose a batch size that is a power 2, training will be faster. What is this rule? Is this applicable to other applications? Can you provide a reference paper? The notion comes from aligning computations (C) onto the physical
  processors (PP) of the GPU. Since the number of PP is often a power of 2, using a number of C different from a power of 2 leads to poor performance. You can see the mapping of the C onto the PP as a pile of slices of size the number of PP.
Say you've got 16 PP.
You can map 16 C on them : 1 C is mapped onto 1 PP.
You can map 32 C on them : 2 slices of 16 C , 1 PP will be responsible for 2 C. This is due to the SIMD paradigm used by GPUs. This is often called Data Parallelism : all the PP do the same thing at the same time but on different data. Algorithmically speaking, using larger mini-batches allows you to reduce the variance of your stochastic gradient updates (by taking the average of the gradients in the mini-batch), and this in turn allows you to take bigger step-sizes, which means the optimization algorithm will make progress faster. However, the amount of work done (in terms of number of gradient computations) to reach a certain accuracy in the objective will be the same: with a mini-batch size of n, the variance of the update direction will be reduced by a factor n, so the theory allows you to take step-sizes that are n times larger, so that a single step will take you roughly to the same accuracy as n steps of SGD with a mini-batch size of 1. As for tensorFlow, I found no evidence of your affirmation, and its a question that has been closed on github : https://github.com/tensorflow/tensorflow/issues/4132 Note that image resized to power of two makes sense (because pooling is generally done in 2X2 windows), but that’s a different thing altogether. I've heard this, too. Here's a white paper about training on CIFAR-10 where some Intel researchers make the claim: In general, the performance of processors is better if the batch size is a power of 2. (See: https://software.intel.com/en-us/articles/cifar-10-classification-using-intel-optimization-for-tensorflow.) However, it's unclear just how big the advantage may be because the authors don't provide any training duration data :/I'm trying to convert some old code from using sklearn to Keras implementation. Since it is crucial to maintain the same way of operation, I want to understand if I'm doing it correctly. I've converted most of the code already, however I'm having trouble with sklearn.svm SVC classifier conversion. Here is how it looks right now: Super easy, right. However, I couldn't find the analog of SVC classifier in Keras. So, what I've tried is this: But, I think that it is not correct by any means. Could you, please, help me find an alternative of the SVC classifier from sklearn in Keras? Thank you. If you are making a classifier, you need squared_hinge and regularizer, to get the complete SVM loss function as can be seen here. So you will also need to break your last layer to add regularization parameter before performing activation, I have added the code here. These changes should give you the output Also hinge is implemented in keras for binary classification, so if you are working on a binary classification model, use the code below. If you cannot understand the article or have issues with the code, feel free to comment.
I had this same issue a while back, and this GitHub thread helped me understand, maybe go through it too, some of the ideas here are directly from here https://github.com/keras-team/keras/issues/2588 If you are using Keras 2.0  then you need to change the following lines of anand v sing's answer. W_regularizer -> kernel_regularizer Github link Or You can use follow You can use SVM with Keras implementation suing scikeras. It is a Scikit-Learn API wrapper for Keras. It was first release in May 2020. Below I have attached the official documentation link for it. I hope you will find your answer over there. https://pypi.org/project/scikeras/#descriptionI want to color my clusters with a color map that I made in the form of a dictionary (i.e. {leaf: color}).   I've tried following https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ but the colors get messed up for some reason.  The default plot looks good, I just want to assign those colors differently.  I saw that there was a link_color_func but when I tried using my color map (D_leaf_color dictionary) I got an error b/c it wasn't a function.  I've created D_leaf_color to customize the colors of the leaves associated with particular clusters.  In my actual dataset, the colors mean something so I'm steering away from arbitrary color assignments.  I don't want to use color_threshold b/c in my actual data, I have way more clusters and SciPy repeats the colors, hence this question. . .   How can I use my leaf-color dictionary to customize the color of my dendrogram clusters?  I made a GitHub issue https://github.com/scipy/scipy/issues/6346 where I further elaborated on the approach to color the leaves in  Interpreting the output of SciPy's hierarchical clustering dendrogram? (maybe found a bug...) but I still can't figure out how to actually either: (i)  use dendrogram output to reconstruct my dendrogram with my specified color dictionary or (ii) reformat my D_leaf_color dictionary for the link_color_func parameter.   I also tried how do I get the subtrees of dendrogram made by scipy.cluster.hierarchy Here a solution that uses the return matrix Z of linkage() (described early but a little hidden in the docs) and link_color_func: Here the output:
 Two-liner for applying custom colormap to cluster branches: You can then replace rainbow by any cmap and change 10 for the number of cluster you want.
 I found a hackish solution, and does require to use the color threshold (but I need to use it in order to obtain the same original coloring, otherwise the colors are not the same as presented in the OP), but could lead you to a solution. However, you may not have enough information to know how to set the color palette order. The result:The main goals are as follows: Apply StandardScaler to continuous variables Apply LabelEncoder and OnehotEncoder to categorical variables The continuous variables need to be scaled, but at the same time, a couple of categorical variables are also of integer type. Applying StandardScaler would result in undesired effects. On the flip side, the StandardScaler would scale the integer based categorical variables, which is also not what we want. Since continuous variables and categorical ones are mixed in a single Pandas DataFrame, what's the recommended workflow to approach this kind of problem? The best example to illustrate my point is the Kaggle Bike Sharing Demand dataset, where season and weather are integer categorical variables Check out the sklearn_pandas.DataFrameMapper meta-transformer. Use it as the first step in your pipeline to perform column-wise data engineering operations: Also, you should be using sklearn.preprocessing.LabelBinarizer instead of a list of [LabelEncoder(), OneHotEncoder()]. Checkout the ColumnTransformer in scikit-learnLet's say you have access to an email account with the history of received emails from the last years (~10k emails) classified into 2 groups How would you approach the task of creating a neural network solution that could be used for spam detection - basically classifying any email either as spam or not spam? Let's assume that the email fetching is already in place and we need to focus on classification part only. The main points which I would hope to get answered would be: Also any resource recommendations, or existing implementations (preferably in C#) are more than welcome Thank you EDIT If you insist on NNs... I would calculate some features for every email  Both Character-Based, Word-based, and Vocabulary features (About 97 as I count these): You could also add some more features based on the formatting: colors, fonts, sizes, ... used. Most of these measures can be found online, in papers, or even Wikipedia (they're all simple calculations, probably based on the other features). So with about 100 features, you need 100 inputs, some number of nodes in a hidden layer, and one output node. The inputs would need to be normalized according to your current pre-classified corpus. I'd split it into two groups, use one as a training group, and the other as a testing group, never mixing them. Maybe at a 50/50 ratio of train/test groups with similar spam/nonspam ratios. Are you set on doing it with a Neural Network? It sounds like you're set up pretty well to use Bayesian classification, which is outlined well in a couple of essays by Paul Graham: The classified history you have access to would make very strong corpora to feed to a Bayesian algorithm, you'd probably end up with quite an effective result. Generally, my experience has led me to believe that neural networks will show mediocre performance at best in this task, and I'd definitely recommend something Bayesian as Chad Birch suggests, if this is something other than a toy problem for exploring neural nets. Chad, the answers you've gotten so far are reasonable, but I'll respond to your update that: I am set on using neural networks as the main aspect on the project is to test how the NN approach would work for spam detection. Well, then you have a problem: an empirical test like this can't prove unsuitability. You're probably best off learning a bit about what NN actually do and don't do, to see why they are not a particularly good idea for this sort of classification problem.  Probably a helpful way to think about them is as universal function approximators.  But for some idea of how this all fits together in the area of classification (which is what the spam filtering problem is), browsing an intro text like pattern classification might be helpful. Failing that if you are dead set on seeing it run, just use any general NN library for the network itself.  Most of your issue is going to be how to represent the input data anyway. The `best' structure is non-obvious, and it probably doesn't matter that much.  The inputs are going to have to be a number of (normalized) measurements (features) on the corpus itself.  Some are obvious (counts of 'spam' words, etc), some much less so.  This is the part you can really play around with, but you should expect to do poorly compared to Bayesian filters (which have their own problems here) due to the nature of the problem.I would like to understand the rational behind the Spark's OneHotEncoder dropping the last category by default. For example: By default, the OneHotEncoder will drop the last category: Of course, this behavior can be changed: Question::  The last category is not included by default (configurable via dropLast) because it makes the vector entries sum up to one, and hence linearly dependent. According to the doc it is to keep the column independents: A one-hot encoder that maps a column of category indices to a column
of binary vectors, with at most a single one-value per row that
indicates the input category index. For example with 5 categories, an
input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0,
0.0]. The last category is not included by default (configurable via OneHotEncoder!.dropLast because it makes the vector entries sum up to
one, and hence linearly dependent. So an input value of 4.0 maps to
[0.0, 0.0, 0.0, 0.0]. Note that this is different from scikit-learn's
OneHotEncoder, which keeps all categories. The output vectors are
sparse. https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/ml/feature/OneHotEncoder.htmlWant to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 5 years ago. I've looked around the web and cant seem to find a way to use React Native with TensorFlow. I don't think TF supports react-native (at least not officially) integration but i hope someone in the community has found a way. How can one use TensorFlow in a React Native project? Thanks. CONTRIBUTOR UPDATE: Now is possible  This article explains using tensorflow with native android development.
https://omid.al/posts/2017-02-20-Tutorial-Build-Your-First-Tensorflow-Android-App.html I also needs a tf supporting library for react-native. But it seems as you said there is no library yet. Here they have built an adroid app with react native with tensorflow support, have a look,
https://github.com/NorthFoxz/whatsthis Came across the following Platform Adapter for React Native. This package provides a TensorFlow.js platform adapter for react
  native. It provides GPU accelerated execution of TensorFlow.js
  supporting all major modes of tfjs usage There is also a library react-native-tensorflow bridging tensorflow with react-native. A TensorFlow inference library for react native. They have recently added support for iOS, so it's available for both Android and iOS platforms. To get started: Perform library link: For iOS you will also need to use CocoaPods with content: There are performance limitations due to the nature of react-native bridge when transferring large data between native side and JS. For other known issues and usage check out the library's documentation.I was running the sketch_rnn.ipynb on my jupyter notebook, upon loading the environment to load the trained dataset, it returned an error 'Object arrays cannot be loaded when allow_pickle=False' This is the code already used by google developers in developing the sketch_rnn algorithm that was even run in the google colab. In the past i have ran it myself on the google colab it worked but seems not to be working on my own jupyter notebook i expected the output to be  But it gave me Use allow_pickle=True as one of the arguments to np.load(). This code solved the problem at my side. I just downgrade numpy as the problem is due to some internal conflict.  So I believe this has just surfaced due to a change in numpy to load(), if you observe the line that the error occurs it references something like  but the Keras source code, for example here at line 58: https://github.com/keras-team/keras/blob/master/keras/datasets/imdb.py now uses  where np.load(path) becomes np.load(path, boolean) From brief reading, the addition of pickles has to do with security, since pickles can contain arbitrary Python code that would be run when something is loaded. (Possibly similar to the way SQL injections are performed) After updating np.load with the new param list, it's working for my projectI am interested in test the SVM performance to classify several individuals into four groups/classes. When using the svmtrain LibSVM function from MATLAB, I am able to get the three equations used to classify those individuals among the 4 groups, based on the values of this equation. An scheme could be as follows: Is there any way to get these equations using the svm function in the e1071 R package? svm in e1071 uses the "one-against-one" strategy for multiclass classification (i.e. binary classification between all pairs, followed by voting). So to handle this hierarchical setup, you probably need to do a series of binary classifiers manually, like group 1 vs. all, then group 2 vs. whatever is left, etc.. Additionally, the basic svm function does not tune the hyperparameters, so you will typically want to use a wrapper like tune in e1071, or train in the excellent caret package. Anyway, to classify new individuals in R, you don't have to plug numbers into an equation manually. Rather, you use the predict generic function, which has methods for different models like SVM. For model objects like this, you can also usually use the generic functions plot and summary. Here is an example of the basic idea using a linear SVM:  Tabulate actual class labels vs. model predictions: Extract feature weights from svm model object (for feature selection, etc.). Here, Sepal.Length is obviously more useful. To understand where the decision values come from, we can calculate them manually as the dot product of the feature weights and the preprocessed feature vectors, minus the intercept offset rho. (Preprocessed means possibly centered/scaled and/or kernel transformed if using RBF SVM, etc.) This should equal what is calculated internally:I'm working on classification problem where i need to add different levels of gaussian noise to my dataset and do classification experiments until my ML algorithms can't classify the dataset. 
unfortunately i have no idea how to do that. any advise or coding tips on how to add the gaussian noise?  You can follow these steps:  Here's a reproducible example:  Overall code without the comments and print statements:  To save the file back to csvI have been looking at the nlp tag on SO for the past couple of hours and am confident I did not miss anything but if I did, please do point me to the question.  In the mean time though, I will describe what I am trying to do. A common notion that I observed on many posts is that semantic similarity is difficult. For instance, from this post, the accepted solution suggests the following: My high-level requirement is to utilize k-means clustering and categorize the text based on semantic similarity so all I need to know is whether they are an approximate match. For instance, in the above example, I am OK with classifying 1,2,4,5 into one category and 3 into another (of course, 3 will be backed up with some more similar sentences). Something like, find related articles, but they don't have to be 100% related.  I am thinking I need to ultimately construct vector representations of each sentence, sort of like its fingerprint but exactly what this vector should contain is still an open question for me. Is it n-grams, or something from the wordnet or just the individual stemmed words or something else altogether? This thread did a fantastic job of enumerating all related techniques but unfortunately stopped just when the post got to what I wanted. Any suggestions on what is the latest state-of-the-art in this area? Latent Semantic Modeling could be useful.  It's basically just yet another application of the Singular Value Decomposition.  The SVDLIBC is a pretty nice C implementation of this approach, which is an oldie but a goodie, and there are even python binding in the form of sparsesvd.   I suggest you try a topic modelling framework such as Latent Dirichlet Allocation (LDA). The idea there is that documents (in your case sentences, which might prove to be a problem) are generated from a set of latent (hidden) topics; LDA retrieves those topics, representing them by word clusters. An implementation of LDA in Python is available as part of the free Gensim package. You could try to apply it to your sentences, then run k-means on its output.i have MNIST dataset and i am trying to visualise it using pyplot. The dataset is in cvs format where each row is one image of 784 pixels. i want to visualise it in pyplot or opencv in the 28*28 image format. I am trying directly using : but i its not working? any ideas on how should i approach this. Assuming you have a CSV file with this format, which is a format the MNIST dataset is available in Here's how you can visulize it in Python with Matplotlib and then OpenCV  You can take the pixels numpy array from above which is of dtype='uint8' (unsigned 8-bits integer) and shape 28 x 28 , and plot with cv2.imshow() Importing necessary packages Reading mnist train dataset ( which is csv formatted ) as a pandas dataframe Converting the pandas dataframe to a numpy matrix The first column contains the label, so store it in a separate array And delete the first column from the data matrix The first row represents the first image, it is 28X28 image (stored as 784 pixels)  For all like me who want a quick and dirty solution, simply to get a rough idea what a given input is about, in-console and without fancy libraries: (expects the input to be shaped like [784] and with float values from 0 to 1. If either is not the case, you can easily convert (e.g. pixels = pixels.reshape((784,)) or pixels \= 255)  The output is a bit distorted but you get the idea.I have written a tensorflow CNN and it is already trained. I wish to restore it to run it on a few samples but unfortunately its spitting out: ValueError: No variables to save My eval code can be found here: The tf.train.Saver must be created after the variables that you want to restore (or save). Additionally it must be created in the same graph as those variables. Assuming that Process.forward_propagation(…) also creates the variables in your model, adding the saver creation after this line should work: In addition, you must pass the new tf.Graph that you created to the tf.Session constructor so you'll need to move the creation of sess inside that with block as well. The resulting function will be something like: Simply, there should be at least one tf.variable that is defined before you create your saver object.  You can get the above code running by adding the following line of code before the saver object definition.  The code that you need to add has come between the two ###. Note that since TF 0.11 — a long time ago yet after the currently accepted answer — tf.train.Saver gained a defer_build argument in its constructor that allows you to define variables after it has been constructed. However you now need to call its build member function when all variables have been added, typically just before finilizeing your graph.Im triying to obtain the most informative features from a textual corpus. From this well answered question I know that this task could be done as follows: Then: For this classfier: The problem is the output of most_informative_feature_for_class: It is not returning the label nor the words. Why this is happening and how can I print the words and the labels?. Do you guys this is happening since I am using pandas to read the data?. Another thing I tried is the following, form this question: But I get this traceback: Traceback (most recent call last): Any idea of how to solve this, in order to get the features with the highest coefficient values?. To solve this specifically for linear SVM, we first have to understand the formulation of the SVM in sklearn and the differences that it has to MultinomialNB. The reason why the most_informative_feature_for_class works for MultinomialNB is because the output of the coef_ is essentially the log probability of features given a class (and hence would be of size [nclass, n_features], due to the formulation of the naive bayes problem. But if we check the documentation for SVM, the coef_ is not that simple. Instead coef_ for (linear) SVM is [n_classes * (n_classes -1)/2, n_features] because each of the binary models are fitted to every possible class.  If we do possess some knowledge on which particular coefficient we're interested in, we could alter the function to look like the following: This would work as intended and print out the labels and the top n features according to the coefficient vector that you're after.  As for getting the correct output for a particular class, that would depend on the assumptions and what you aim to output. I suggest reading through the multi-class documentation within the SVM documentation to get a feel for what you're after.  So using the train.txt file which was described in this question, we can get some kind of output, though in this situation it isn't particularly descriptive or helpful to interpret. Hopefully this helps you. with output:For my project I have large amounts of data, about 60GB spread into npy files, each holding about 1GB, each containing about 750k records and labels. Each record is a 345 float32  and the labels are 5 float32. I read the tensorflow dataset documentation and the queues / threads documentation as well but I can't figure out how to best handle the input for training and then how save the model and weights for future predicting. My model is pretty straight forward, it looks like this: The way I was training my neural net was reading the files one at a time in a random order then using a shuffled numpy array to index each file and manually creating each batch to feed the train_op using feed_dict. From everything I read this is very inefficient and I should somehow replace it with datasets or queue and threads but as I said the documentation was of no help. So, what is the best way to handle large amounts of data in tensorflow? Also, for reference, my data was saved to a numpy file in a 2 operation step: The utilities for npy files indeed allocate the whole array in memory. I'd recommend you to convert all of your numpy arrays to TFRecords format and use these files in training. This is one of the most efficient ways to read large dataset in tensorflow. Convert to TFRecords A complete example that deals with images can be found here. Read TFRecordDataset The data manual can be found here.Can anybody explain why is loc used in python pandas with examples like shown below? The use of .loc is recommended here because the methods df.Age.isnull(), df.Gender == i and df.Pclass == j+1 may return a view of slices of the data frame or may return a copy. This can confuse pandas. If you don't use .loc you end up calling all 3 conditions in series which leads you to a problem called chained indexing. When you use .loc however you access all your conditions in one step and pandas is no longer confused. You can read more about this along with some examples of when not using .loc will cause the operation to fail in the pandas documentation. The simple answer is that while you can often get away with not using .loc and simply typing (for example) you'll always get the SettingWithCopy warning and your code will be a little messier for it. In my experience .loc has taken me a while to get my head around and it's been a bit annoying updating my code. But it's really super simple and very intuitive: df.loc[row_index,col_indexer]. For more information see the pandas documentation on Indexing and Selecting Data.I want to predict the next frame of a (greyscale) video given N previous frames - using CNNs or RNNs in Keras. Most tutorials and other information regarding time series prediction and Keras use a 1-dimensional input in their network but mine would be 3D (N frames x rows x cols) I'm currently really unsure what a good approach for this problem would be. My ideas include: Using one or more LSTM layers. The problem here is that I'm not sure whether they're suited to take a series of images instead a series of scalars as input. Wouldn't the memory consumption explode? If it is okay to use them: How can I use them in Keras for higher dimensions? Using 3D convolution on the input (the stack of previous video frames). This raises other questions: Why would this help when I'm not doing a classification but a prediction? How can I stack the layers in such a way that the input of the network has dimensions (N x cols x rows) and the output (1 x cols x rows)? I'm pretty new to CNNs/RNNs and Keras and would appreciate any hint into the right direction. So basically every approach has its advantages and disadvantages. Let's go throught the ones you provided and then other to find the best approach: LSTM: Among their biggest advantages is an ability to learn a long-term dependiencies patterns in your data. They were designed in order to be able to analyse long sequences like e.g. speech or text. This is also might cause problems because of number parameters which could be really high. Other typical recurrent network architectures like GRU might overcome this issues. The main disadvantage is that in their standard (sequential implementation) it's infeasible to fit it on a video data for the same reason why dense layers are bad for an imagery data - loads of time and spatial invariances must be learnt by a topology which is completely not suited for catching them in an efficient manner. Shifting a video by a pixel to the right might completely change the output of your network. Other thing which is worth to mention is that training LSTM is belived to be similiar to finding equilibrium between two rivalry processes - finding good weights for a dense-like output computations and finding a good inner-memory dynamic in processing sequences. Finding this equilibrium might last for a really long time but once its finded - it's usually quite stable and produces a really good results. Conv3D: Among their biggest advantages one may easily find an ability to catch spatial and temporal invariances in the same manner as Conv2D in an imagery case. This make the curse of dimensionality much less harmful. On the other hand - in the same way as Conv1D might not produce good results with a longer sequences - in the same way - a lack of any memory might make learning a long sequence harder. Of course one may use different approaches like: TimeDistributed + Conv2D: using a TimeDistributed wrapper - one may use some pretrained convnet like e.g. Inception framewise and then analyse the feature maps sequentially. A really huge advantage of this approach is a possibility of a transfer learning. As a disadvantage - one may think about it as a Conv2.5D - it lacks temporal analysis of your data. ConvLSTM: this architecture is not yet supported by the newest version of Keras (on March 6th 2017) but as one may see here it should be provided in the future. This is a mixture of LSTM and Conv2D and it's belived to be better then stacking Conv2D and LSTM.  Of course these are not the only way to solve this problem, I'll mention one more which might be usefull: PS: One more thing that is also worth to mention is that shape of video data is actually 4D with (frames, width, height, channels). PS2: In case when your data is actually 3D with (frames, width, hieght) you actually could use a classic Conv2D (by changing channels to frames) to analyse this data (which actually might more computationally effective). In case of a transfer learning you should add additional dimension because most of CNN models were trained on data with shape (width, height, 3). One may notice that your data doesn't have 3 channels. In this case a technique which is usually used is repeating spatial matrix three times. PS3: An example of this 2.5D approach is: After doing lots of research, I finally stumbled upon the Keras Example for the ConvLSTM2D layer (Already mentioned by Marcin Możejko), which does exactly what I need.  In the current version of Keras (v1.2.2), this layer is already included and can be imported using To use this layer, the video data has to be formatted as follows:For instance, after I have created my operations, fed the batch data through the operation and run the operation, does tf.train.batch automatically feed in another batch of data to the session? I ask this because tf.train.batch has an attribute of allow_smaller_final_batch which makes it possible for the final batch to be loaded as a size lesser than the indicated batch size. Does this mean even without a loop, the next batch could be automatically fed? From the tutorial codes I am rather confused. When I load a single batch, I get literally a single batch size of shape [batch_size, height, width, num_channels], but the documentation says it Creates batches of tensors in tensors. Also, when I read the tutorial code in the tf-slim walkthrough tutorial, where there is a function called load_batch, there are only 3 tensors returned: images, images_raw, labels. Where are 'batches' of data as explained in the documentation? Thank you for your help. ... does tf.train.batch automatically feeds in another batch of data to the session? No. Nothing happens automatically. You must call sess.run(...) again to load a new batch. Does this mean even without a loop, the next batch could be automatically fed? No. tf.train.batch(..) will always load batch_size tensors. If you have for example 100 images and a batch_size=30 then you will have 3*30 batches as in you can call sess.run(batch) three times before the input queue will start from the beginning (or stop if epoch=1). This means that you miss out 100-3*30=10 samples from training. In case you do not want to miss them you can do tf.train.batch(..., allow_smaller_final_batch=True) so now you will have 3x 30-sample-batches and 1x 10-sample-batch before the input queue will restart. Let me also elaborate with a code sample: You need to call sess.run and pass the batch to it everytime when you want to load the next batch. See the code below. the answer would be something like this:  [4,1,8] [4,1,8] [2,3,7] [2,3,7] [2,6,8] [2,6,8] I made the modification to the code from https://github.com/tensorflow/models/blob/master/research/slim/slim_walkthrough.ipynb and bodokaiser answer from the above post. Please note that this is from the evaluation scrip on https://github.com/tensorflow/models/tree/master/research/slim, eval_image_classifier.py. The most important modification to the eval_image_classifier.py code is to add num_epochs=1 to the DatasetDataProvider line. That way, all the images would be accessed once for inference.I have a problem with loading the previously saved model. This is my save: After this I can see that auditor_model is saved in model directory. now I would like to load this model with: but I get: ValueError: Unable to restore custom object of type _tf_keras_metric
  currently. Please make sure that the layer implements get_configand
  from_config when saving. In addition, please use the
  custom_objects arg when calling load_model(). I have read about custom_objects in TensorFlow docs but I don't understand how to implement it while I use no custom layers but the predefined ones. Could anyone give me a hint how to make it work? I use TensorFlow 2.2 and Python3 Your example is missing the definition of f1, precision and recall functions. If the builtin metrics e.g. 'f1' (note it is a string) do not fit your usecase you can pass the custom_objects as follows: You have a custom metric f1.
If you want to evaluate your model on test data after saving it and if you need the f1 score for test data you can try this:I've ran the brown-clustering algorithm from https://github.com/percyliang/brown-cluster and also a python implementation https://github.com/mheilman/tan-clustering. And they both give some sort of binary and another integer for each unique token. For example: What does the binary and the integer mean? From the first link, the binary is known as a bit-string, see http://saffron.deri.ie/acl_acl/document/ACL_ANTHOLOGY_ACL_P11-1053/ But how do I tell from the output that dog and mouse and cat is one cluster and the and chased is not in the same cluster? If I understand correctly, the algorithm gives you a tree and you need to truncate it at some level to get clusters. In case of those bit strings, you should just take first L characters. For example, cutting at the second character gives you two clusters At the third character you get The cutting strategy is a different subject though. In Percy Liang's implementation (https://github.com/percyliang/brown-cluster), the -C parameter allows you to specify the number of word clusters. The output contains all the words in the corpus, together with a bit-string annotating the cluster and the word frequency in the following format: <bit string> <word> <word frequency>. The number of distinct bit strings in the output equals the number of desired clusters and the words with the same bit string belong to the same cluster. Change your running : ./wcluster --text input.txt --c 3  --c number    this number means the number of cluster, and the default is 50. You can't distinguish the different cluster of words because the default input has only three sentences. Change 50 clusters to 3 clusters and you can tell the difference. I enter three tweets into the input and give 3 as the cluster parameter  The integers are counts of how many times the word is seen in the document. (I have tested this in the python implementation.) From the comments at the top of the python implementation: Instead of using a window (e.g., as in Brown et al., sec. 4), this
  code computed PMI using the probability that two randomly selected
  clusters from the same document will be c1 and c2. Also, since the
  total numbers of cluster tokens and pairs are constant across pairs,
  this code use counts instead of probabilities. From the code in the python implementation we see that it outputs the word, the bit string and the word counts. My guess is:  According to Figure 2 in Brown et al 1992, the clustering is hierarchical and to get from the root to each word "leaf" you have to make an up/down decision. If up is 0 and down is 1, you can represent each word as a bit string. From https://github.com/mheilman/tan-clustering/blob/master/class_lm_cluster.py :For my project I have large amounts of data, about 60GB spread into npy files, each holding about 1GB, each containing about 750k records and labels. Each record is a 345 float32  and the labels are 5 float32. I read the tensorflow dataset documentation and the queues / threads documentation as well but I can't figure out how to best handle the input for training and then how save the model and weights for future predicting. My model is pretty straight forward, it looks like this: The way I was training my neural net was reading the files one at a time in a random order then using a shuffled numpy array to index each file and manually creating each batch to feed the train_op using feed_dict. From everything I read this is very inefficient and I should somehow replace it with datasets or queue and threads but as I said the documentation was of no help. So, what is the best way to handle large amounts of data in tensorflow? Also, for reference, my data was saved to a numpy file in a 2 operation step: The utilities for npy files indeed allocate the whole array in memory. I'd recommend you to convert all of your numpy arrays to TFRecords format and use these files in training. This is one of the most efficient ways to read large dataset in tensorflow. Convert to TFRecords A complete example that deals with images can be found here. Read TFRecordDataset The data manual can be found here.I was looking at the example of Spark site for Word2Vec: How do I do the interesting vector such as king - man + woman = queen. I can use model.getVectors, but not sure how to proceed further.  Here is an example in pyspark, which I guess is straightforward to port to Scala - the key is the use of model.transform. First, we train the model as in the example: k is the dimensionality of the word vectors - the higher the better (default value is 100), but you will need memory, and the highest number I could go with my machine was 220. (EDIT: Typical values in the relevant publications are between 300 and 1000) After we have trained the model, we can define a simple function as follows: Now, here are some examples with countries and their capitals: The results are not always correct - I'll leave it to you to experiment, but they get better with more training data and increased vector dimensionality k. The for loop in the function removes entries that belong to the input query itself, as I noticed that frequently the correct answer was the second one in the returned list, with the first usually being one of the input terms. and the running result as below: Here is the pseudo code. For the full implementation, read the documentation: https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/mllib/feature/Word2VecModel.html edit: https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/mllib/feature/Word2VecModel.html#findSynonyms(org.apache.spark.mllib.linalg.Vector,%20int)I look for a NMF implementation that has a python interface, and handles both missing data and zeros. I don't want to impute my missing values before starting the factorization, I want them to be ignored in the minimized function. It seems that neither scikit-learn, nor nimfa, nor graphlab, nor mahout propose such an option. Thanks! Using this Matlab to python code conversion sheet I was able to rewrite NMF from Matlab toolbox library. 
I had to decompose a 40k X 1k matrix with sparsity of 0.7%. Using 500 latent features my machine took 20 minutes for 100 iteration. Here is the method: Here I was using Scipy sparse matrix as input and missing values were converted to 0 using toarray() method. Therefore, the mask was created using numpy.sign() function. However, if you have nan values you could get same results by using numpy.isnan() function. Scipy has a method to solve non-negative least squares problem (NNLS). In this answer, I am reproducing my blogpost on using scipy's NNLS for non-negative matrix factorisation. You may also be interested in my other blog posts that use autograd, Tensorflow and CVXPY for NNMF.  Our solution consists of two steps. First, we fix W and learn H, given A. Next, we fix H and learn W, given A. We repeat this procedure iteratively. Fixing one variable and learning the other (in this setting) is popularly known as alternating least squares, as the problem is reduced to a least squares problem. However, an important thing to note is that since we want to constraint W and H to be non-negative, we us NNLS instead of least squares.  Using the illustration above, we can learn each column of H, using the corresponding column from A and the matrix W. In the problem of collaborative filtering, A is usually the user-item matrix and it has a lot of missing entries. These missing entries correspond to user who have not rated items. We can modify our formulation to account for these missing entries.
Consider that  M' ≤ M  entries in A have observed data, we would now modify the above equation as: where, the mask is found by considering only the  M′  entries.  However, since A has missing entries, we have to define the cost in terms of the entries present in A Let us just try to see the cost of the initial set of values of W and H we randomly assigned. Let's view the values of the masked entries. Down there is a decently fast and simple solution based on gradient descent with a quasi newton method:What is the right way to use TensorFlow for real time predictions in a high traffic application. Ideally I would have a server/cluster running tensorflow listening on a port(s) where I can connect from app servers and get predictions similar to the way databases are used.
Training should be done by cron jobs feeding the training data through the network to the same server/cluster. How does one actually use tensorflow in production? Should I build a setup where the python is running as a server and use the python scripts to get predictions? I'm still new to this but I feel that such script will need to open sessions etc.. which is not scalable. (I'm talking about 100s of predictions/sec). Any pointer to relevant information will be highly appreciated. I could not find any. This morning, our colleagues released TensorFlow Serving on GitHub, which addresses some of the use cases that you mentioned. It is a distributed wrapper for TensorFlow that is designed to support high-performance serving of multiple models. It supports both bulk processing and interactive requests from app servers. For more information, see the basic and advanced tutorials.Can anybody explain why is loc used in python pandas with examples like shown below? The use of .loc is recommended here because the methods df.Age.isnull(), df.Gender == i and df.Pclass == j+1 may return a view of slices of the data frame or may return a copy. This can confuse pandas. If you don't use .loc you end up calling all 3 conditions in series which leads you to a problem called chained indexing. When you use .loc however you access all your conditions in one step and pandas is no longer confused. You can read more about this along with some examples of when not using .loc will cause the operation to fail in the pandas documentation. The simple answer is that while you can often get away with not using .loc and simply typing (for example) you'll always get the SettingWithCopy warning and your code will be a little messier for it. In my experience .loc has taken me a while to get my head around and it's been a bit annoying updating my code. But it's really super simple and very intuitive: df.loc[row_index,col_indexer]. For more information see the pandas documentation on Indexing and Selecting Data.Im triying to obtain the most informative features from a textual corpus. From this well answered question I know that this task could be done as follows: Then: For this classfier: The problem is the output of most_informative_feature_for_class: It is not returning the label nor the words. Why this is happening and how can I print the words and the labels?. Do you guys this is happening since I am using pandas to read the data?. Another thing I tried is the following, form this question: But I get this traceback: Traceback (most recent call last): Any idea of how to solve this, in order to get the features with the highest coefficient values?. To solve this specifically for linear SVM, we first have to understand the formulation of the SVM in sklearn and the differences that it has to MultinomialNB. The reason why the most_informative_feature_for_class works for MultinomialNB is because the output of the coef_ is essentially the log probability of features given a class (and hence would be of size [nclass, n_features], due to the formulation of the naive bayes problem. But if we check the documentation for SVM, the coef_ is not that simple. Instead coef_ for (linear) SVM is [n_classes * (n_classes -1)/2, n_features] because each of the binary models are fitted to every possible class.  If we do possess some knowledge on which particular coefficient we're interested in, we could alter the function to look like the following: This would work as intended and print out the labels and the top n features according to the coefficient vector that you're after.  As for getting the correct output for a particular class, that would depend on the assumptions and what you aim to output. I suggest reading through the multi-class documentation within the SVM documentation to get a feel for what you're after.  So using the train.txt file which was described in this question, we can get some kind of output, though in this situation it isn't particularly descriptive or helpful to interpret. Hopefully this helps you. with output:I have written a tensorflow CNN and it is already trained. I wish to restore it to run it on a few samples but unfortunately its spitting out: ValueError: No variables to save My eval code can be found here: The tf.train.Saver must be created after the variables that you want to restore (or save). Additionally it must be created in the same graph as those variables. Assuming that Process.forward_propagation(…) also creates the variables in your model, adding the saver creation after this line should work: In addition, you must pass the new tf.Graph that you created to the tf.Session constructor so you'll need to move the creation of sess inside that with block as well. The resulting function will be something like: Simply, there should be at least one tf.variable that is defined before you create your saver object.  You can get the above code running by adding the following line of code before the saver object definition.  The code that you need to add has come between the two ###. Note that since TF 0.11 — a long time ago yet after the currently accepted answer — tf.train.Saver gained a defer_build argument in its constructor that allows you to define variables after it has been constructed. However you now need to call its build member function when all variables have been added, typically just before finilizeing your graph.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 6 years ago. I've been looking into implementations of Hidden Markov Models in C++ lately. I was wondering If I could use any of the existing HMM libraries written in C++ out there to use
with Action Recognition (with OpenCV)? I'm tying to AVOID "re-inventing the wheel"! Is it possible to use Torch3Vision even though(looks like) it was designed to
work for speech recognition? My idea is that, if we can convert the feature vectors into Symbols/Observations
(using Vector Quantization - Kmeans clustering), we can use those symbols for
decoding, inference, parameter learning (Baum–Welch algorithm). This way it
would work with Torch3Vision in OpenCV. Any help on this will be truly appreciated. You can take a look at http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf for the theory behind HMMs. It's not hard to implement the algorithms yourself. For a C-based version, you can take a look at my implementation, http://code.google.com/p/accelges/, which I've done for a Google Summer of Code project. There is also this implementation that I wrote several days ago. It is a class for discrete HMM using OpenCV. You may take a look here: https://sourceforge.net/projects/cvhmm/ After quantization of your features, you can convert each feature vector to one label and use the sequence of labels to train a discrete HMM.BACKGROUND I have vectors with some sample data and each vector has a category name (Places,Colors,Names). My objective is to train a model that take a new input string and predict which category it belongs to. For example if a new input is "purple" then I should be able to predict 'Colors' as the correct category. If the new input is "Calgary" it should predict 'Places' as the correct category. APPROACH I did some research and came across Word2vec. This library has a "similarity" and "mostsimilarity" function which i can use. So one brute force approach I thought of is the following: So for instance for input "pink" I can calculate its similarity with words in vector "names" take a average and then do that for the other 2 vectors also. The vector that gives me the highest similarity average would be the correct vector for the input to belong to. ISSUE Given my limited knowledge in NLP and machine learning I am not sure if that is the best approach and hence I am looking for help and suggestions on better approaches to solve my problem. I am open to all suggestions and also please point out any mistakes I may have made as I am new to machine learning and NLP world. If you're looking for the simplest / fastest solution then I'd suggest you take the pre-trained word embeddings (Word2Vec or GloVe) and just build a simple query system on top of it. The vectors have been trained on a huge corpus and are likely to contain good enough approximation to your domain data. Here's my solution below: In order to run it, you'll have to download and unpack the pre-trained GloVe data from here (careful, 800Mb!). Upon running, it should produce something like this: ... which looks pretty reasonable. And that's it! If you don't need such a big model, you can filter the words in glove according to their tf-idf score. Remember that the model size only depends on the data you have and words you might want to be able to query. Also, what its worth, PyTorch has a good and faster implementation of Glove these days.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Thanks to Google for providing a few pre-trained models with tensorflow API. I would like to know how to retrain a pre-trained model available from the above repository, by adding new classes to the model.
For example, the trained COCO dataset model has 90 classes, I would like to add 1 or 2 classes to the existing one and get one 92 class object detection model as a result. Running Locally is provided by the repository but it is completely replacing those pre-trained classes with newly trained classes. There, only train and eval are mentioned. So, is there any other way to retrain the model and get 92 classes as a result? Question : How do we add a few more classes to my already trained network? Specifically, we want to keep all the network as-is other than the output of the new classes.  This means that for something like ResNet, we want to keep everything other than the last layer frozen, and somehow expand the last layer to have our new classes. Answer : Combine the existing last layer with a new one you train Specifically, we will replace the last layer with a fully connected layer that is large enough for your new classes and the old ones.  Initialize it with random weights and then train it on your classes and just a few of the others.  After training, copy the original weights of the original last fully connected layer into your new trained fully connected layer. If, for example, the previous last layer was a 1024x90 matrix, and your new last layer is a 1024x92 matrix, copy the 1024x90 into the corresponding space in your new 1024x92.  This will destructively replace all your training of the old classes with the pre-trained values but leave your training of your new classes.  That is good, because you probably didn't train it with the same number of old classes.  Do the same thing with the bias, if any. Your final network will have only 1024x2 new weight values (plus any bias), corresponding to your new classes. A word of caution, although this will train fast and provide quick results, it will not perform as well as retraining on a full and comprehensive data set. That said, it'll still work well ;) Here is a reference to how to replace the last layer How to remove the last layer from trained model in Tensorflow that someone else answeredI have a CSV file that contains the average temperature over almost 5 years. After decomposition using seasonal_decompose function from statsmodels.tsa.seasonal, I got the following results. Indeed, the results do not show any seasonal! However, I see a clear sin in the trend! I am wondering why is that and how can I correct it? Thank you.  It looks like your freq is off.Following the Spark MLlib Guide  we can read that Spark has two machine learning libraries: According to this and this question on StackOverflow, Dataframes are better (and newer) than RDDs and should be used whenever possible. The problem is that I want to use common machine learning algorithms (e.g: Frequent Pattern Mining,Naive Bayes, etc.) and spark.ml (for dataframes) don't provide such methods, only spark.mllib(for RDDs) provides this algorithms. If Dataframes are better than RDDs and the referred guide recommends the use of spark.ml, why aren't common machine learning methods implemented in that lib? Spark 2.0.0 Currently Spark moves strongly towards DataFrame API with ongoing deprecation of RDD API. While number of native "ML" algorithms is growing the main points highlighted below are still valid and internally many stages are implemented directly using RDDs. See also: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0 Spark < 2.0.0 I guess that the main missing point is that spark.ml algorithms in general don't operate on DataFrames. So in practice it is more a matter of having a ml wrapper than anything else. Even native ML implementation (like ml.recommendation.ALS use RDDs internally). Why not implement everything from scratch on top of DataFrames? Most likely because only a very small subset of machine learning algorithms can actually benefit from the optimizations which are currently implemented in Catalyst not to mention be efficiently and naturally implemented using DataFrame API / SQL. There is one more problem with DataFrames, which is not really related to machine learning. When you decide to use a DataFrame in your code you give away almost all benefits of static typing and type inference. It is highly subjective if you consider it to be a problem or not but one thing for sure, it doesn't feel natural in Scala world.  Regarding better, newer and faster I would take a look at Deep Dive into Spark SQL’s Catalyst Optimizer, in particular the part related to quasiquotes: The following figure shows that quasiquotes let us generate code with performance similar to hand-tuned programs.  * This has been changed in Spark 1.6 but it is still limited to default HashPartitioningI am curious about the Tensorflow implementation of tf.nn.conv2d(...). To call it, one simply runs tf.nn.conv2d(...). However, I'm going down the rabbit hole trying to see where it is executed. The code is as follows (where the arrow indicates the function it ultimately calls): I am familiar with Tensorflow's implementation of LSTMs and the ability to easily manipulate them as one deems fit. Is the function that performs the conv2d() calculation written in Python, and if so, where is it? Can I see where and how the strides are executed? TL;DR: The implementation of tf.nn.conv2d() is written in C++, which invokes optimized code using either Eigen (on CPU) or the cuDNN library (on GPU). You can find the implementation here. The chain of functions that you mentioned in the question (from tf.nn.conv2d() down) are Python functions for building a TensorFlow graph, but these do not invoke the implementation. Recall that, in TensorFlow, you first build a symbolic graph, then execute it. The implementation of tf.nn.conv2d() is only executed happens when you call Session.run() passing a Tensor whose value depends on the result of some convolution. For example: Invoking sess.run(...) tells TensorFlow to run all the ops that are neeeded to compute the value of conv, including the convolution itself. The path from here to the implementation is somewhat complicated, but goes through the following steps: The "Conv2D" OpKernel is implemented here, and its Compute() method is here. Because this op is performance critical for many workloads, the implementation is quite complicated, but the basic idea is that the computation is offloaded to either the Eigen Tensor library (if running on CPU), or cuDNN's optimized GPU implementation. TensorFlow programs as consisting of two discrete sections: tf.nn.conv2d(...) -> tf.nn_ops.conv2d(...) -> tf.gen_nn_ops.conv2d(...) -> _op_def_lib.apply_op("Conv2D", ...) -> graph.create_op -> register op into graph sess = tf.Session(target) -> sess.run(conv2d) -> master prune full graph to client graph -> master split client graph by task to graph partition -> register graph partition to worker -> worker split subgraph by device to graph partition -> then master notify all workers to run graph partitions -> worker notify all devices to run graph partitions -> executor will run ops by topological sort on device. For one of op, the executor will invoke kernel implement to compute for the op. The kernel implement of tf.nn.conv2d() is written in C++, which invokes optimized code using either Eigen (on CPU) or the cuDNN library (on GPU).I am currently working on a small binary classification project using the new keras API in tensorflow. The problem is a simplified version of the Higgs Boson challenge posted on Kaggle.com a few years back. The dataset shape is 2000x14, where the first 13 elements of each row form the input vector, and the 14th element is the corresponding label. Here is a sample of said dataset: I am relatively new to machine learning and tensorflow, but I am familiar with the higher level concepts such as loss functions, optimizers and activation functions. I have tried building various models inspired by examples of binary classification problems found online, but I am having difficulties with training the model. During training, the loss somethimes increases within the same epoch, leading to unstable learning. The accuracy hits a plateau around 70%. I have tried changing the learning rate and other hyperparameters but to no avail. In comparison, I have hardcoded a fully-connected feed forward neural net that reaches around 80-85% accuracy on the same problem. Here is my current model: As mentionned, some of the epochs start with a higher accuracy than they finish with, leading to unstable learning.  What could be the cause of these oscillations in learning in such a simple model? 
Thanks EDIT: I have followed some suggestions from the comments and have modified the model accordingly. It now looks more like this: Those are most definitely connected to the size of your network; each batch coming through changes your neural network considerably as it does not have enough neurons to represent the relationships.  It works fine for one batch, updates the weights for another and changes previously learned connections effectively "unlearning". That's why the loss is also jumpy as the network tries to accommodate to the task you have given it. Sigmoid activation and it's saturation may be causing you troubles as well (as the gradient is squashed into small region and most gradient updates are zero). Quick fix - use ReLU activation as described below. Additionally, neural network does not care about accuracy, only about minimizing the loss value (which it tries to do most of the time). Say it predicts probabilities: [0.55, 0.55, 0.55, 0.55, 0.45] for classes [1, 1, 1, 1, 0] so it's accuracy is 100% but it's pretty uncertain. Now, let's say the next update pushes the network into probabilities predictions: [0.8, 0.8, 0.8, 0.8, 0.55]. In such case, loss would drop, but so would accuracy, from 100% to 80%. BTW. You may want to check the scores for logistic regression and see how it performs on this task (so a single layer with output only). It's always good to start with simple model and grow it bigger if needed (wouldn't advise the other way around). You may want to check on a really small subsample of data (say two/three batches, 160 elements or so) whether your model can learn the relationship between input and output. In your case I doubt the model will be able to learn those relationships with the size of layers you are providing. Try increasing the size, especially in the earlier layers (maybe 50/100 for starters) and see how it behaves. Sigmoid easily saturates (small region where changes occur, most of the values are almost 0 or 1). It is rarely used nowadays as activation before bottleneck (final layer). Most common nowadays is ReLU which is not prone to saturation (at least when the input is positive) or it's variations. This might help as well. For each dataset and each neural network model optimal choice of learning rate is different. Defaults usually work so-so, but when the learning rate is too small it might get stuck in the local minima (and it's generalization will be worse), while the value being too big will make your network unstable (loss will highly oscillate). You may want to read up on Cyclical Learning Rate (or in the original research paper by Leslie N. Smith. In there you can find info on how to choose a good learning rate heuristically and setup some simple learning rate schedulers. Those techniques were used by fast.ai teams in CIFAR10 competitions with really good results. On their site or in documentation of their library you can find One Cycle Policy and learning rate finder (based on the work of aforementioned researcher). This should get you started in this realm I think. Not sure, but this normalization looks pretty non-standard to me (never seen it done like that). Good normalization is the basis for neural network convergence (unless the data is already pretty close to normal distribution). Usually one subtracts the mean and divides by standard deviation for each feature. You can check some schemes in scikit-learn library for example. This shouldn't be an issue but if your input is complicated you should consider adding more layers to your neural network (right now it's almost definitely too thin). This would allow it to learn more abstract features and transform the input space more. When the network overfits to the data you may employ some regularization techniques (hard to tell what might help, you should test it on your own), some of those include: Plus there are tons of other techniques you may find. Check what makes intuitive sense and which one you like the most and test how it performs. I once trained a siamese network where I realised that if I use higher learning rates the training loss was going down smooth (as expected since that is what the neural network is learning) ,but saw huge ups and downs with the val loss. This had never happened before when I was using lower learning rate (in the order of 1e-05). I believe that the train loss is actually false since recent papers have proved that large neural networks (I mean neural networks with more complexity) can learn random data flawlessly in the training set, though they performed extremely worse while validating them, I have attached the paper for your reference below which clearly explains this phenomena related to overfitting. So one can't conclude the overall model's performance by just observing the training data. Though other parameters mentioned above also matter, but I guess one should start tweaking the learning rates initially in such a case before tweaking the model itself. Link for the paper : https://arxiv.org/pdf/1611.03530 Please correct me if I am wrong... All of Symon's points are great, but another possible cause: are you shuffling your dataset? If not and your data contains some ordered bias, your model may be tuning itself to one 'end' of the dataset, only to do poorly at the other 'end'.I am trying to use Keras to make a neural network. The data I am using is https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics. My code is as follows: From here I have tried using model.fit(X, Y), but the accuracy of the model appears to remain at 0. I am new to Keras so this is probably an easy solution, apologies in advance. My question is what is the best way to add regression to the model so that the accuracy increases? Thanks in advance. First of all, you have to split your dataset into training set and test set using train_test_split class from sklearn.model_selection library. Also, you have to scale your values using StandardScaler class. Then, you should add more layers in order to get better results.  Note Usually it's a good practice to apply following formula in order to find out the total number of hidden layers needed. where  So our classifier becomes: The metric that you use- metrics=['accuracy'] corresponds to a classification problem. If you want to do regression, remove metrics=['accuracy']. That is, just use Here  is a list of keras metrics for regression and classification Also, you have to define the batch_size and epochs values for fit method.  After you trained your network you can predict the results for X_test using model.predict method. Now, you can compare the y_pred that we obtained from neural network prediction and y_test which is real data. For this, you can create a plot using matplotlib library. It seems that our neural network learns very good Here is how the plot looks.
 Here is the full codeI'd like to use silhouette score in my script, to automatically compute number of clusters in k-means clustering from sklearn. Someone can help me with question marks? I don't understand what to put instead of question marks. I have taken the code from an example.
The commented part is the previous versione, where I do k-means clustering with a fixed number of clusters set to 4. The code in this way is correct, but in my project I need to automatically chose the number of clusters. I am assuming you are going to silhouette score to get the optimal no. of clusters.  First declare a seperate object of KMeans and then call it's fit_predict functions over your data df like this See this official example for more clarity. The ? is the data set or Data frame that you are applying K-means to.
Thank you.I am using a multi-dimensional SVM classifier (SVM.NET, a wrapper for libSVM) to classify a set of features. Given an SVM model, is it possible to incorporate new training data without having to recalculate on all previous data? I guess another way of putting it would be: is an SVM mutable? Actually, it's usually called incremental learning. The question has come up before and is pretty well answered here : A few implementation details for a Support-Vector Machine (SVM).  In brief, it's possible but not easy, you would have to change the library you are using or implement the training algorithm yourself. I found two possible solutions, SVMHeavy and LaSVM, that supports incremental training. But I haven't used either and don't know anything about them. Online and incremental although similar but differ slightly. In online, its generally a single pass(epoch=1) or number of epochs could be configured. Where as, incremental would mean that you already have a model; no matter how it is built, but then model can be mutable by new examples. Also, a combination of online and incremental is often what is required.  Here is a list of tools with some remarks on the online and/or incremental SVM : https://stats.stackexchange.com/questions/30834/is-it-possible-to-append-training-data-to-existing-svm-models/51989#51989I'm using sklearn.pipeline.Pipeline to chain feature extractors and a classifier. Is there a way to combine multiple feature selection classes (for example the ones from sklearn.feature_selection.text) in parallel and join their output? My code right now looks as follows: It results in the following: I want to be able to specify a pipeline that looks as follows: This has been implemented recently in the master branch of scikit-learn under the name FeatureUnion: http://scikit-learn.org/dev/modules/pipeline.html#feature-unionKeras 2.x killed off a bunch of useful metrics that I need to use, so I copied the functions from the old metrics.py file into my code, then included them as follows. ... and this results in  What am I doing wrong? I can't see anything I'm doing wrong according to Keras documentation. edit: Here is the full Traceback: From the traceback it seems that the problem occurs when you try to load the saved model: Take a look at this issue: https://github.com/keras-team/keras/issues/10104 You need to add your custom objects when loading the model. For example: My suggestion would be to implement your metrics in Keras callback. Because: It can achieve the same thing as metrics does. It can also provide you model saving strategy. after that, you can add your callback to your: I tested your code in Python 3.6.5, TensorFlow==1.9 and Keras==2.2.2 and it worked. I think the error could be due to Python 2 usage. Output:Background The reference manual for the gbm package states the interact.gbm function computes Friedman's H-statistic to assess the strength of variable interactions. the H-statistic is on the scale of [0-1]. The reference manual for the dismo package does not reference any literature for how the gbm.interactions function detects and models interactions. Instead it gives a list of general procedures used to detect and model interactions. The dismo vignette "Boosted Regression Trees for ecological modeling" states that the dismo package extends functions in the gbm package.  Question How does dismo::gbm.interactions actually detect and model interactions? Why I am asking this question because gbm.interactions in the dismo package yields results >1, which the gbm package reference manual says is not possible.  I checked the tar.gz for each of the packages to see if the source code was similar. It is different enough that I cannot determine if these two packages are using the same method to detect and model interactions. To summarize, the difference between the two approaches boils down to how the "partial dependence function" of the two predictors is estimated.  The dismo package is based on code originally given in  Elith et al., 2008 and you can find the original source in the supplementary material. The paper very briefly describes the procedure. Basically the model predictions are obtained over a grid of two predictors, setting all other predictors at their means. The model predictions are then regressed onto the grid. The mean squared errors of this model are then multiplied by 1000. This statistic indicates departures of the model predictions from a linear combination of the predictors, indicating a possible interaction.  From the dismo package, we can also obtain the relevant source code for gbm.interactions. The interaction test boils down to the following commands (copied directly from source): pred.frame contains a grid of the two predictors in question, and prediction is the prediction from the original gbm fitted model where all but two predictors under consideration are set at their means.  This is different than Friedman's H statistic (Friedman & Popescue, 2005), which is estimated via formula (44) for any pair of predictors. This is essentially the departure from additivity for any two predictors averaging over the values of the other variables, NOT setting the other variables at their means. It is expressed as a percent of the total variance of the partial dependence function of the two variables (or model implied predictions) so will always be between 0-1.Hello fellow Number crunchers As the headline suggests, I am looking for a library for learning and inference of Bayesian Networks. I have already found some, but I am hoping for a recommendation. Requirements in a quick overview: Which one do you recommend ? Have a look at Weka. It's kind of popular in my neck of the woods... It's open source and written in Java. This will tell you about bayesian networks in Weka, from the abstract: So here I give my subjective answer. From my experience everything related to statistics is best solved with R. I have seen this often that in fields related to statistics, R has the most libraries and very often the most state-of-the-art algorithms/methods implemented. Most programmers like me like to stay with the languages that they know, and learning something new is a trade off, mainly because it's time consuming. So if learning a new language is a viable option, R is a good choice, in my opinion the best. Take a brief  look at the R libraries related to Bayesian Networks and Bayesian Interference. Baysian:
http://cran.r-project.org/web/views/Bayesian.html Graphical Models:
http://cran.r-project.org/web/views/gR.html Machine Learning:
http://cran.r-project.org/web/views/MachineLearning.html The main advantages of R:
  - easy to install a library: install.packages("RWeka")
  - the help format and style is the same for all libraries
  - if you know R, it's easy to switch from one library to the next. So it's easy to test all available libraries and then use the one that fits best Never used it, but perhaps the MALLET library fits the bill?Can someone please tell how to use ensembles in sklearn using partial fit.
I don't want to retrain my model. 
Alternatively, can we pass pre-trained models for ensembling ?
I have seen that voting classifier for example does not support training using partial fit. The Mlxtend library has an implementation of VotingEnsemble which allows you to pass in pre-fitted models. For example if you have three pre-trained models clf1, clf2, clf3. The following code would work. When set to false the fit_base_estimators argument in EnsembleVoteClassifier ensures that the classifiers are not refit. In general, when looking for more advanced technical features that sci-kit learn does not provide, look to mlxtend as a first point of reference. Workaround: VotingClassifier checks that estimators_ is set in order to understand whether it is fitted, and is using the estimators in estimators_ list for prediction.
If you have pre trained classifiers, you can put them in estimators_ directly like the code below. However, it is also using LabelEnconder, so it assumes labels are like 0,1,2,... and you also need to set le_ and classes_ (see below). Unfortunately, currently this is not possible in scikit VotingClassifier.  But you can use http://sebastianraschka.com/Articles/2014_ensemble_classifier.html (from which VotingClassifer is implemented) to try and implement your own voting classifier which can take pre-fitted models. Also we can look at the source code here and modify it to our use: It's not too hard to implement the voting. Here's my implementation:  The Mlxtend library has an implementation works, you still need to call the fit function for the EnsembleVoteClassifier. Seems the fit function doesn't really modify any parameters rather checking the possible label values. In the example below, you have to give an array contains all the possible values appear in original y(in this case 1,2) to eclf2.fit It doesn't matter for X.In order to do proper CV it is advisable to use pipelines so that same transformations can be applied to each fold in the CV. I can define custom  transformations by using either sklearn.preprocessing.FunctionTrasformer or by subclassing sklearn.base.TransformerMixin. Which one is the recommended approach? Why? Well it is totally upto you, both will achieve the same results more or less, only the way you write the code differs. For instance, while using sklearn.preprocessing.FunctionTransformer you can simply define the function you want to use and call it directly like this (code from official documentation) On the other hand, while using subclassing sklearn.base.TransformerMixin you will have to define the whole class along with the fit and transform functions of the class.
So you will have to create a class like this(Example code take from this blog post) So as you can see, TransformerMixin gives you more flexibility as compared to FunctionTransformer with regard to transform function. You can apply multiple trasnformations, or partial transformation depending on the value, etc. An example can be like, for the first 50 values you want to log while for the next 50 values you wish to take inverse log and so on. You can easily define your transform method to deal with data selectively. If you just want to directly use a function as it is, use sklearn.preprocessing.FunctionTrasformer, else if you want to do more modification or say complex transformations, I would suggest subclassing sklearn.base.TransformerMixin Here, take a look at the following links to get a more better idea The key difference between FunctionTransformer and a subclass of TransformerMixin is that with the latter, you have the possibility that your custom transformer can learn by applying the fit method. E.g. the StandardScaler learns the means and standard deviations of the columns during the fit method, and in the transform method these attributes are used for transformation. This cannot be achieved by a simple FunctionTransformer, at least not in a canonical way as you have to pass the train set somehow. This possibility to learn is in fact the reason to use custom transformers and pipelines - if you just apply an ordinary function by the usage of a FunctionTransformer, nothing is gained in the cross validation process. It makes no difference whether you transform before the cross validation once or in each step of the cross validation (except that the latter will take more time).So, I have this doubt and have been looking for answers. So the question is when I use, After which I will train and test the model (A,B as features, C as Label) and get some accuracy score. Now my doubt is, what happens when I have to predict the label for new set of data. Say, Because when I normalize the column the values of A and B will be changed according to the new data, not the data which the model will be trained on.
So, now my data after the data preparation step that is as below, will be. Values of A and B will change with respect to the Max and Min value of df[['A','B']]. The data prep of df[['A','B']] is with respect to Min Max of df[['A','B']]. How can the data preparation be valid with respect to different numbers relate? I don't understand how the prediction will be correct here.  In summary: Example using your data: Example using iris data: Hope this helps. See also by post here: https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79 Best way is train and save MinMaxScaler model and load the same when it's required. Saving model: Loading saved model:Going through this book, I am familiar with the following: For each training instance the backpropagation algorithm first makes a
  prediction (forward pass), measures the error, then goes through each
  layer in reverse to measure the error contribution from each
  connection (reverse pass), and finally slightly tweaks the connection
  weights to reduce the error. However I am not sure how this differs from the reverse-mode autodiff implementation by TensorFlow.  As far as I know reverse-mode autodiff first goes through the graph in the forward direction and then in the second pass computes all partial derivatives for the outputs with respect to the inputs. This is very similar to the propagation algorithm. How does backpropagation differ from reverse-mode autodiff ? Thanks to the answer by David Parks for the valid contribution and useful links, however I have found the answer to this question by the author of the book himself, which may provide a more concise answer: Bakpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode auto diff is simply a technique used to compute gradients efficiently and it happens to be used by backpropagation. The most important distinction between backpropagation and reverse-mode AD is that reverse-mode AD computes the vector-Jacobian product of a vector valued function from R^n -> R^m, while backpropagation computes the gradient of a scalar valued function from R^n -> R. Backpropagation is therefore a subset of reverse-mode AD.  When we train neural networks, we always have a scalar-valued loss function, so we are always using backpropagation. Since backprop is a subset of reverse-mode AD, then we are also using reverse-mode AD when we train a neural network. Whether or not backpropagation takes the more general definition of reverse-mode AD as applied to a scalar loss function, or the more specific definition of reverse-mode AD as applied to a scalar loss function for training neural networks is a matter of personal taste. It's a word that has slightly different meaning in different contexts, but is most commonly used in the machine learning community to talk about computing gradients of neural network parameters using a scalar loss function. For completeness: Sometimes reverse-mode AD can compute the full Jacobian on a single reverse pass, not just the vector-Jacobian product. Also, the vector Jacobian product for a scalar function where the vector is the vector [1.0] is the same as the gradient.  Automatic differentiation differs from the method taught in standard calculus classes on how gradients are computed, and in some features such as its native ability to take the gradient of a data structure and not just a well defined mathematical function. I'm not expert enough to go into further detail, but this is a great reference that explains it in much more depth: https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/ Here's another guide that looks quite nice that I just found now. https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation I believe backprop may formally refer to the by-hand calculus algorithm for computing gradients, at least that's how it was originally derived and is how it's taught in classes on the subject. But in practice, backprop is used quite interchangeably with the automatic differentiation approach described in the above guides. So splitting those two terms is probably as much an effort in linguistics as it is mathematics. I also noted this nice article on the backpropagation algorithm to compare against the above guides on automatic differentiation. https://brilliant.org/wiki/backpropagation/As a newbie in Machine Learning, I have a set of trajectories that may be of different lengths. I wish to cluster them, because some of them are actually the same path and they just SEEM different due to the noise.  In addition, not all of them are of the same lengths. So maybe although Trajectory A is not the same as Trajectory B, yet it is part of Trajectory B. I wish to present this property after the clustering as well. I have only a bit knowledge of K-means Clustering and Fuzzy N-means Clustering. How may I choose between them two? Or should I adopt other methods? Any method that takes the "belongness" into consideration? 
(e.g. After the clustering, I have 3 clusters A, B and C. One particular trajectory X belongs to cluster A. And a shorter trajectory Y, although is not clustered in A, is identified as part of trajectory B.) =================== UPDATE ====================== The aforementioned trajectories are the pedestrians' trajectories. They can be either presented as a series of (x, y) points or a series of step vectors (length, direction). The presentation form is under my control. It might be a little late but I am also working on the same problem.
I suggest you take a look at TRACLUS, an algorithm created by Jae-Gil Lee, Jiawei Han and Kyu-Young Wang, published on SIGMOD’07.
http://web.engr.illinois.edu/~hanj/pdf/sigmod07_jglee.pdf This is so far the best approach I have seen for clustering trajectories because: Basically is a 2 phase approach: Phase one - Partition: Divide trajectories into segments, this is done using MDL Optimization with complexity of O(n) where n is the numbers of points in a given trajectory. Here the input is a set of trajectories and output is a set of segments. Phase two - Group: This phase discovers the clusters using some version of density-based clustering like in DBSCAN. Input in this phase is the set of segments obtained from phase one and some parameters of what constitutes a neighborhood and the minimum amount of lines that can constitute a cluster. Output is a set of clusters. Clustering is done over segments. They define their own distance measure made of 3 components: Parallel distance, perpendicular distance and angular distance. This phase has a complexity of O(n log n) where n is the number of segments. Finally they calculate a for each cluster a representative trajectory, which is nothing else that a discovered common sub-trajectory in each cluster. They have pretty cool examples and the paper is very well explained. Once again this is not my algorithm, so don't forget to cite them if you are doing research. PS: I made some slides based on their work, just for educational purposes:
http://www.slideshare.net/ivansanchez1988/trajectory-clustering-traclus-algorithm Every clustering algorithm needs a metric. You need to define distance between your samples. In your case simple Euclidean distance is not a good idea, especially if the trajectories can have different lengths. If you define a metric, than you can use any clustering algorithm that allows for custom metric. Probably you do not know the correct number of clusters beforehand, then hierarchical clustering is a good option. K-means doesn't allow for custom metric, but there are modifications of K-means that do (like K-medoids) The hard part is defining distance between two trajectories (time series). Common approach is DTW (Dynamic Time Warping). To improve performance you can approximate your trajectory by smaller amount of points (many algorithms for that). Neither will work. Because what is a proper mean here? Have a look at distance based clustering methods, such as hierarchical clustering (for small data sets, but you probably don't have thousands of trajectories) and DBSCAN. Then you only need to choose an appropriate distance function that allows e.g. differences in time and spatial resolution of trajectories. Distance functions such as dynamic time warping (DTW) distance can accomodate this. This is good concept and having possibility for real-time applications. In my view, one can adopt any clustering but need to select appropriate dissimilarity measure, later need to think about computational complexity. 
Paper (http://link.springer.com/chapter/10.1007/978-81-8489-203-1_15) used Hausdorff and suggest the technique for reducing complexity, and paper (http://www.cit.iit.bas.bg/CIT_2015/v-15-2/4-5-TCMVS%20A-edited-md-Gotovop.pdf) described the use of "Trajectory Clustering Technique Based on Multi-View Similarity"How to compare column names of 2 different Pandas data frame. I want to compare train and test data frames where there are some columns missing in test Data frames?? pandas.Index objects, including dataframe columns, have useful set-like methods, such as intersection and difference. For example, given dataframes train and test:I wanted to save multiple models for my experiment but I noticed that tf.train.Saver() constructor could not save more than 5 models. Here is a simple code:  When I ran this code, I saw only 5 models on my Desktop. Why is this? How can I save more than 5 models with the same tf.train.Saver() constructor? The tf.train.Saver() constructor takes an optional argument called max_to_keep, which defaults to keeping the 5 most recent checkpoints of your model. To save more models, simply specify a value for that argument: To keep all checkpoints, pass the argument max_to_keep=None to the saver constructor. In order to keep the intermediate checkpoints and not the last 5, you need to change 2 parameters in the tf.train.Saver(): So if you do the following, you will store a checkpoint every 2 hours and if the total number of saved checkpoints reaches 10, then the oldest checkpoint will be deleted and a new one will replace it: If you use tf.estimator.Estimator() then the saving of the checkpoint is done by the Estimator itself. That's why you need to pass it a tf.estimator.RunConfig() with some of the following parameters: So if you do the following, you will store a checkpoint every 100 iterations and if the total number of saved checkpoints reaches 10, then the oldest checkpoint will be deleted and a new one will replace it:What is the default method of variable initialization used when tf.get_variable() is called without any specification for the initializer? The Docs just says 'None'. From the documentation: If initializer is None (the default), the default initializer passed in the variable scope will be used. If that one is None too, a glorot_uniform_initializer will be used. The glorot_uniform_initializer function initializes values from a uniform distribution.  This function is documented as: The Glorot uniform initializer, also called Xavier uniform initializer. It draws samples from a uniform distribution within [-limit, limit],
  where limit is sqrt(6 / (fan_in + fan_out))
  where fan_in is the number of input units in the weight tensor
  and fan_out is the number of output units in the weight tensor. Reference: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdfI'm using opencv's har cascade face detector (cv.HaarDetectObjects) in python. for example: This will print a list of detections in this form: Where each line represent a detection. The first 4 numbers are the x,y location of the top-left point, and the height, width of the bounding box. The last number is (quoting from the openCV documentation) the number of neighbors. I guess I have two questions: 1) What does the last number mean? I couldn't find any reference to that when googling. 2) (more important)Is there a way to get a confidence score for each detection? How much is the face classifier certain that the detection corresponds to a real face?  Thanks 1) The detection code produces more than one detection for an object - e.g. in different scales, slightly shifted, etc. The detections are then grouped and the number of neighbours in such a group is the number returned. See also Viola Jones paper, paragraph 5.6 (http://research.microsoft.com/en-us/um/people/viola/Pubs/Detect/violaJones_IJCV.pdf) and OpenCV source. 2) You can possibly use the number of neighbours as some measure of confidence. Thanks a lot for your question and answer, I have been looking for a opencv face detection with confidence scores for a day. Your question and answer give me some guidance to solve the problem.  Like Palmstrom said, the last number means the number of object positions in that cluster. and you can use that as confidence score.  As far as I know, there is only such kind of API in the old python API. New API does not have this (number of object in the cluster) value. I put my code here in case it will help some other people. This is a old python API whose tutorial is hard to find.I'm trying to learn about  Baum-Welch algorithm(to be used with a hidden markov model).  I understand the basic theory of forward-backward models, but it would be nice for someone to help explain it with some code(I find it easier to read code because I can play around to understand it). I checked github and bitbucket and didn't find anything that was easily understandable. There are many HMM tutorials on the net but the probabilities are either already provided or, in the case of spell checkers, add occurrences of words to make the models. It would be cool if someone had examples of creating a Baum-Welch model with only the observations. For example, in http://en.wikipedia.org/wiki/Hidden_Markov_model#A_concrete_example if you only had: This is just an example, I think any example that explains it and we can play with the good to understand better is great.  I have a specific problem I am trying to solve but I was thinking it would maybe more valuable to show code that people can learn from and apply to their own problems(if its not acceptable I can post my own problem).  If possible though, It would be nice to have it in python(or java). Thanks in advance! Here's some code that I wrote several years ago for a class, based on the presentation in Jurafsky/Martin (2nd edition, chapter 6, if you have access to the book). It's really not very good code, doesn't use numpy which it absolutely should, and it does some crap to have the arrays be 1-indexed instead of just tweaking the formulae to be 0-indexed, but, well, maybe it'll help. Baum-Welch is referred to as "forward-backward" in the code. The example/test data is based on Jason Eisner's spreadsheet that implements some HMM-related algorithms. Note that the implemented version of the model uses an absorbing END state which other states have transition probabilities to, rather than assuming a pre-existing fixed sequence length. (Also available as a gist if you prefer.) hmm.py, half of which is testing code based on the following files: observations.txt, a sequence of observations for testing: example.hmm, the model used to generate the dataI am trying to make a prediction using Tensorflow Object Detection API on Google COLAB. Already I successfully completed the training process and  Export Inference Graph task. But Problem is when I am going to make a new prediction it's throwing some error log. cannot connect to X server Now I am unable to make a new prediction. Some portion of my error log:  How can I figure out the reason for this problem? An X server is a program in the X Window System that runs on local machines (i.e. the computers used directly by users) and handles all access to the graphics cards, display screens and input devices (typically a keyboard and mouse) on those computers. With that said Colab runs as a terminal instance in the server, if you are using GPU runtime, then the problem is not with X server accessing your Graphics card, neither with Input devices, generally this occurs when you try to parse some data that should be displayed as separate window on your desktop, commands like cv2.imshow(), there can be other similar functions that can cause this problem, if you have to use graphical ouput you might want to look into %matplotlib notebook and displaying the data in the interactable matplot plots. If this is not your issue, just post a link to your modified code and I might be able to help more. I had the same problem in Colab for a simple OpenCV program to track a tennis ball in a tennis match video and had to comment out these lines, as mentioned by @anand: Note: the above lines were not clustered but spread apart. I could fix this issue by using below python code in Google Colab noteI've looked at Algorithms of the Intelligent Web that describes (page 55) an interesting algorithm - called DocRank - for creating a PageRank like score for business documents (i.e. documents without links like PDF, MS Word documents, etc...). In short it analyzes term frequency intersection between each document in a collection.  Can anyone else identify interesting algorithms described elsewhere, or wants to share something novel here, to apply against these types of documents to improve search results?  Please forgo answers involving things like click tracking or other actions NOT about analyzing the actual documents.  First Technique: step-wise similarity I can offer one example--that i've actually tested/validated against real data. If you were to gather a number of techniques and rank them along two axes--inherent complexity or ease of implementation AND performance (resolution, or predictive accuracy), this technique would be high on the first axis, and somewhere near the middle on the second; a simple and effective technique, but which might underperform against state-of-the-art techniques. We found that the combination of low-frequency keyword intersection combined with similarity among readers/viewers of a document, is a fairly strong predictor of the document's content. Put another way: if two documents have the a similar set of very low-frequency terms (e.g., domain-specific terms, like 'decision manifold', etc.) and they have similar inbound traffic profiles, that combination is strongly probative of similarity of the documents. The relevant details:  first filter: low-frequency terms. We parsed a large set of documents to get the term frequency for each. We used this word frequency spectrum as a 'fingerprint', which is common, but we applied an inverse weighting, so that the common terms ('a', 'of', 'the') counted very little in the similarity measure, whereas the rare terms counted a lot (this is quite common, as you probably know). Trying to determine whether two documents were similar based on this along was problematic; for instance, two documents might share a list of rare terms relating to MMOs, and still the documents weren't similar because one is directed to playing MMOs and the other to designing them.  second filter: readers. Obviously we don't know who had read these documents, so we inferred readership from traffic sources. You can see how that helped in the example above. The inbound traffic for the MMO player site/document reflected the content, likewise for the document directed to MMO design.
 Second Technique: kernel Principal Component Analysis (kPCA) kPCA is unsupervised technique (the class labels are removed from the data before the data is passed in). At the heart of the technique is just an eigenvector-based decomposition of a matrix (in this case a covariance matrix). This technique handles non-linearity via the kernel trick, which just maps the data to a higher dimensional features space then performs the PCA in that space. In Python/NumPy/SciPy it is about 25 lines of code. The data is collected from very simple text parsing of literary works--in particular, most of the published works of these four authors: Shakespeare, Jane Austen, Jack London, Milton. (I believe, though i'm not certain, that normal college students take course in which they are assigned to read novels by these authors.)  This dataset is widely used in ML and is available from a number of places on the Web. So these works were divided into 872 pieces (corresponding roughly to chapters in novels); in other words, about 220 different substantial pieces of text for each of the four authors.  Next a word-frequency scan was performed on the combined corpus text, and the 70 most common words were selected for the study, the remainder of the results of the frequency scan were discarded.  Those 70 words were: These became the field (column) names. Finally, one data row corresponding to the 872 texts was prepared (from the truncated word frequency scan). Here's one of those data points: In sum, the data is comprised of 70 dimensions (each dimension is the frequency or total count of a particular word, in a given text of one of these four authors.  Again, although this data is primarily used for supervised classification (the class labels are there for a reason), the technique i used was unsupervised--put another way, i never showed the class labels to the algorithm. The kPCA algorithm has absolutely no idea what those four different clusters (shown in the plot below) correspond to nor how each differs from the other--the algorithm did not even know how many groups (classes) that data was comprised of. I just gave it the data, and it partitioned it very neatly into four distinct groups based on an inherent ordering. The results: Again, the algorithm i used here is kPCA. Using Python, NumPy, and Matplotlib, the script that produced these results is about 80 lines of code--for the IO, data processing, applying kPCA, and plotting the result.  Not much, but too much for an SO post. In any event, anyone who wants this code can get it from my repo. At the same time, there is also a complete, well-documented kPCA algorithm coded in python + numpy in each of these python packages (all available from mloss.org): shogun ('A Large Scale Machine Learning Toolbox'), 'sdpy (a set of modules directed to computer vision and machine learning), and mlpy ('Machine Learning in PYthon'). Another interesting algorithm - TextRank - sounds very similar to DocRank referenced in original question. Graph based, unsupervised, iterate until convergence.  Java implementation.  I've done some additional research on the topic and found the Wikipedia entry for the Okapi BM25 algorithm. It also has a successor BM25F that takes document structure into account, but this appears to be more relevant to HTML/XML.  BM25 Incorporates: Finally, the Wikipedia entry links to a Lucene implementation.  Compared to @Doug's answers above this appears to be a more complex algorithm to implement.  Here are some algorithms for ranking, though I haven't seen any implementations yet.I'm able to train a U-net with labeled images that have a binary classification.  But I'm having a hard time figuring out how to configure the final layers in Keras/Theano for multi-class classification (4 classes). I have 634 images and corresponding 634 masks that are unit8 and 64 x 64 pixels. My masks, instead of being black (0) and white (1), have color labeled objects in 3 categories plus background as follows: Before training runs, the array containing masks is one-hot encoded as follows: This makes mask_train.shape go from (634, 1, 64, 64) to (2596864, 4). My model closely follows the Unet architecture, however the final layers seem problematic, as I'm unable to flatten the structure so as to match the one-hot encoded array. Do you have any suggestions on how to modify the final portions of the model so this trains successfully? I get a variety of shape mismatch errors, and the few times I managed to make it run, the loss did not change throughout epochs. You should have your target as (634,4,64,64) if you're using channels_first.
Or (634,64,64,4) if channels_last. Each channel of your target should be one class. Each channel is an image of 0's and 1's, where 1 means that pixel is that class and 0 means that pixel is not that class. Then, your target is 634 groups, each group containing four images, each image having 64x64 pixels, where pixels 1 indicate the presence of the desired feature.  I'm not sure the result will be ordered correctly, but you can try: If the ordering doesn't work properly, you can do it manually: Bit late but you should try That will result in (634, 4, 64, 64) for mask_train.shape and a binary mask for each individual class (one-hot encoded).  Last conv layer, activation and loss looks good for multiclass segmentation.I am trying to implement an application that uses AdaBoost algorithm. I know that AdaBoost uses set of weak classifiers, but I don't know what these weak classifiers are. Can you explain it to me with an example and tell me if I have to create my own weak classifiers or I'm suppoused to use some kind of algorithm? Weak classifiers (or weak learners) are classifiers which perform only slightly better than a random classifier. These are thus classifiers which have some clue on how to predict the right labels, but not as much as strong classifiers have like, e.g., Naive Bayes, Neurel Networks or SVM. One of the simplest weak classifiers is the Decision Stump, which is a one-level Decision Tree. It selects a threshold for one feature and splits the data on that threshold. AdaBoost will then train an army of these Decision Stumps which each focus on one part of the characteristics of the data. When I used AdaBoost, my weak classifiers were basically thresholds for each data attribute. Those thresholds need to have a performance of more than 50%, if not it would be totally random. Here is a good presentation about Adaboost and how to calculate those weak classifiers:
https://user.ceng.metu.edu.tr/~tcan/ceng734_f1112/Schedule/adaboost.pdfI am confused about the difference between the cross_val_score scoring metric 'roc_auc' and the roc_auc_score that I can just import and call directly.  The documentation (http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) indicates that specifying scoring='roc_auc' will use the sklearn.metrics.roc_auc_score. However, when I implement GridSearchCV or cross_val_score with scoring='roc_auc' I receive very different numbers that when I call roc_auc_score directly. Here is my code to help demonstrate what I see: I feel like I am missing something very simple here -- most likely a mistake in how I am implementing/interpreting one of the scoring metrics. Can anyone shed any light on the reason for the discrepancy between the two scoring metrics? This is because you supplied predicted y's instead of the probability in roc_auc_score. This function takes a score, not the classified label. Try instead to do this: It should give a similar result to previous result from cross_val_score. Refer to this post for more info. I just ran into a similar issue here. The key takeaway there was that cross_val_score uses the KFold strategy with default parameters for making the train-test splits, which means splits into consecutive chunks rather than shuffling. train_test_split on the other hand does a shuffled split. The solution is to make the split strategy explicit and specify shuffling, like this: Ran into this problem myself and after digging a bit found the answer. Sharing for the love. There is actually two and a half problems. Here's a full example: Using two estimators Split the train/test set. But keep it into a variable we can reuse. Feed it to GridSearchCV and save scores. Note we are passing fourfold. Feed it to cross_val_score and save scores. Sometimes, you want to loop and compute several different scores, so this is what you use. Do we have the same scores across the board?My data: Var1 and Var2 are aggregated percentage values at the state level. N is the number of participants in each state. I would like to run a linear regression between Var1 and Var2 with the consideration of N as weight with sklearn in Python 2.7. The general line is: Say the data is loaded into df using Pandas and the N becomes df["N"], do I simply fit the data into the following line or do I need to process the N somehow before using it as sample_weight in the command? The weights enable training a model that is more accurate for certain values of the input (e.g., where the cost of error is higher). Internally, weights w are multiplied by the residuals in the loss function [1]:  Therefore, it is the relative scale of the weights that matters. N can be passed as is if it already reflects the priorities. Uniform scaling would not change the outcome. Here is an example. In the weighted version, we emphasize the region around last two samples, and the model becomes more accurate there. And, scaling does not affect the outcome, as expected.   (this transformation also seems necessary for passing Var1 and Var2 to fit)In my dataset I have a number of continuous and dummy variables. For analysis with glmnet, I want the continuous variables to be standardized but not the dummy variables. I currently do this manually by first defining a dummy vector of columns that have only values of [0,1] and then using the scale command on all the non-dummy columns. Problem is, this isn't very elegant. But glmnet has a built in standardize argument. By default will this standardize the dummies too? If so, is there an elegant way to tell glmnet's standardize argument to skip dummies? In short, yes - this will standardize the dummy variables, but there's a reason for doing so. The glmnet function takes a matrix as an input for its X parameter, not a data frame, so it doesn't make the distinction for factor columns which you may have if the parameter was a data.frame. If you take a look at the R function, glmnet codes the standardize parameter internally as Which converts the R boolean to a 0 or 1 integer to feed to any of the internal FORTRAN functions (elnet, lognet, et. al.) If you go even further by examining the FORTRAN code (fixed width - old school!), you'll see the following block: Take a look at the lines marked 1000 - this is basically applying the standardization formula to the X matrix. Now statistically speaking, one does not generally standardize categorical variables to retain the interpretability of the estimated regressors. However, as pointed out by Tibshirani here, "The lasso method requires initial standardization of the regressors, so that the penalization scheme is fair to all regressors. For categorical regressors, one codes the regressor with dummy variables and then standardizes the dummy variables" - so while this causes arbitrary scaling between continuous and categorical variables, it's done for equal penalization treatment. glmnet doesn't know anything about dummy variables, because it doesn't have a formula interface (and hence doesn't touch model.frame and model.matrix.) If you want them to be treated specially, you'll have to do it yourself.I have a data_df that looks like: Tried to convert classification columns (vehicleType) to dummies ("one hot encoding"): But the original data is missing: So, how to replace a given column with the dummies? you can use a more compact way: this line will drop your old column 'vehicleType', and automatically join the created columns to your datasetrf_mod$forest doesn't seem to have this information, and the docs don't mention it.   In R's randomForest package, the average variable importance for the entire forest of CART models is given by importance(rf_mod).   We can also extract individual tree structure with getTree. Here's the first tree.   One workaround is to grow many CARTs (i.e. - ntree = 1), get the variable importance of each tree, and average the resulting %IncMSE:   The extraction, averaging, and comparison step.  I'd like to be able to extract the variable importance of each tree directly from a randomForest object, without this roundabout method that involves completely re-running the RF in order to facilitate reproducible cumulative variable importance plots like this one, and the one below shown for mtcars. Minimal example here.   I'm aware that a single tree's variable importance is not statistically meaningful, and it's not my intention to interpret trees in isolation. I want them for the purpose of visualization and communicating that as trees increase in a forest, the variable importance measures jump around before stabilizing.    When training a randomForest model, the importance scores are computed for the entire forest and stored directly inside the object. Tree-specific scores are not kept and so cannot be directly retrieved from a randomForest object. Unfortunately, you are correct about having to incrementally construct a forest. The good news is that a randomForest object is self-contained, and you don't need to implement your own run_rf. Instead, you can use stats::update to re-fit the random forest model with a single tree and randomForest::grow to add additional trees one at a time: The data frame shows how feature importance changes with each additional tree. This is the right panel of your plot example. The trees themselves (for the left panel) can be retrieved from the final forest, which is given by dplyr::last( rfs ). Disclaimer: This is not really an answer, but too long to post as a comment. Will remove if deemed not appropriate. While I (think I) understand your question, to be honest I am unsure whether your question makes sense from a statistics/ML point-of-view. The following is based on my obviously limited understanding of RF and CART. Perhaps my comment-post will lead to some insights. Let's start with some general random forest (RF) theory on variable importance from Hastie, Tibshirani, Friedman, The Elements of Statistical Learning, p. 593 (bold-face mine): At each split in each tree, the improvement in the split-criterion is the
  importance measure attributed to the splitting variable, and is accumulated
  over all the trees in the forest separately for each variable. [...]
  Random forests also use the oob samples to construct a different variable-importance measure, apparently to measure the prediction strength of each variable. So the variable importance measure in RF is defined as a measure accumulated over all trees. In traditional single classification trees (CARTs), variable importance is characterised through the Gini index that measures node impurity (see e.g. How to measure/rank “variable importance” when using CART? (specifically using {rpart} from R) and Carolin Strobl's PhD thesis) More complex measures to characterise variable importance in CART-like models exist; for example in rpart: An overall measure of variable importance is the sum of the goodness of split
  measures for each split for which it was the primary variable, plus goodness * (adjusted
  agreement) for all splits in which it was a surrogate. In the printout these are scaled to sum
  to 100 and the rounded values are shown, omitting any variable whose proportion is less
  than 1%. So the bottom line here is the following: At the very least it won't be easy (and in the worst case it won't make sense) to compare variable measures from single classifaction trees with variable importance measures applied to ensemble-based methods like RF. Which leads me to ask: Why do you want to extract variable importance measures for individual trees from an RF model? Even if you came up with a method to calculate variable importances from individual trees, I believe they wouldn't be very meaningful, and they wouldn't have to "converge" to the ensemble-accumulated values.  We can simplify it byI try to apply this code : But I have this error : Update your scikit-learn, cv_results_ has been introduced in 0.18.1, earlier it was called grid_scores_ and had slightly different structure http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV  from sklearn.model_selection import GridSearchCV use this clf.cv_results_ Solved ! 
Uninstall and install conda scikit learn in 0.18.1 How to upgrade scikit-learn package in anaconda. When I import GridSearch :  First, you should update your sklearn, using: After that, check if you are include the wrong module: Change to new path: (this is the right way)Because online learning does not work well with Keras when you are using an adaptive optimizer (the learning rate schedule resets when calling .fit()), I want to see if I can just manually set it. However, in order to do that, I need to find out what the learning rate was at the last epoch. That said, how can I print the learning rate at each epoch? I think I can do it through a callback but it seems that you have to recalculate it each time and I'm not sure how to do that with Adam. I found this in another thread but it only works with SGD: I am using the following approach, which is based on @jorijnsmit answer: It works with Adam. I found this question very helpful. A minimal workable example that answers your question would be: For everyone that is still confused on this topic: The solution from @Andrey works but only if you set a decay to your learning rate, you have to schedule the learning rate to lower itself after 'n' epoch, otherwise it will always print the same number (the starting learning rate), this is because that number DOES NOT change during training, you can't see how the learning rates adapts, because every parameter in Adam has a different learning rate that adapts itself during the training, but the variable lr NEVER changes Follow this thread. This piece of code might help you. It is based on Keras implementation of Adam optimizer (beta values are Keras defaults)I would like to use the yolo architecture for object detection. Before training the network with my custom data, I followed these steps to train it on the Pascal VOC data: https://pjreddie.com/darknet/yolo/ The instructions are very clear.
But after the final step  ./darknet detector train cfg/voc.data cfg/yolo-voc.cfg darknet19_448.conv.23 darknet immediately stops training and announces that weights have been written to the backups/ directory. At first I thought that the pretraining was simply too good and that the stopping criteria would be reached at once.
So I've used the ./darknet detect command with these weights on one of the test images data/dog. Nothing is found. If I don't use any pretrained weights, the network does train.
I've edited cfg/yolo-voc.cfg to use Now the training process has been runnning for many hours and is keeping my gpu warm. Is this the intended way to train darknet ?
How can I use pretrained weights correctly, without training just breaking off ? Is there any setting to create checkpoints, or get an idea of the progress ? Adding -clear 1 at the end of your training command will clear the stats of how many images this model has seen in previous training. Then you can fine-tune your model on new data(set).  You can find more info about the usage in the function signature 
void train_detector(char *datacfg, char *cfgfile, char *weightfile, int *gpus, int ngpus, int clear)
at https://github.com/pjreddie/darknet/blob/b13f67bfdd87434e141af532cdb5dc1b8369aa3b/examples/detector.c I doubt it that increasing the max number of iterations is a good idea, as the learning rates are usually associated with current # of iteration. We usually increase the max # of iterations, when we want to resume a previous training task that ended because of reaching the max # of iterations, but we believe that with more iterations, it will give better results. FYI, when you have a small dataset, training on it from scratch or from a classification network may not be a great idea. You may still want to re-use the weights from a detection network trained on large dataset like Coco or ImageNet. This is an old question so I hope you have your answer by now, but here is mine just in case it helps. After working with darknet for about a month, I've run into most of the roadblocks that people have asked/posted about on forums. In your case, I'm pretty certain it's because the weights have been trained for the max number of batches already, and when the pre-trained weights were read in darknet assumed training was done. Relevant personal experience: when I used one of the pretrained weights files, it started from iteration 40101 and ran until 40200 before cutting off. I would stick to training from scratch if you have custom data, but if you want to try the pre-trained weights again, you might find that changing max batches in the cfg file helps. Also if using AlexeyAB/darknet they might have a problem with -clear option, 
in detector.c: should really be: otherwise the training loop will exit immediately. Modify OpenCV number in your darknet/Makefile to 0 OpenCV=0We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 2 years ago. Is anyone aware of a nearest neighbor algorithm implemented in Python that can be updated incrementally? All the ones I've found, such as this one, appear to be batch processes. Is it possible to implement an incremental NN algorithm? This is way late, but for posterity: There is actually a technique for converting batch-processed algorithms like KD-Tree into incremental algorithms: it's called a static-to-dynamic transformation. To generate an incremental variant of a KD-Tree, you store a set of trees instead of just one tree. When there are N elements in your nearest-neighbor structure, your structure will have a tree for each "1" bit in the binary representation of N. Moreover, if tree T_i corresponds to the i-th bit of N, then tree T_i contains 2^i elements. So, if you have 11 elements in your structure, then N = 11, or 1011 in binary, and therefore you have three trees - T_3, T_1, and T_0 - with 8 elements, 2 elements, and 1 element, respectively. Now, let's insert an element e into our structure. After insertion, we'll have 12 elements, or 1100 in binary. Comparing the new and the previous binary string, we see that T_3 doesn't change, we have a new tree T_2 with 4 elements, and trees T_1 and T_0 get deleted. We construct the new tree T_2 by doing a batch insertion of e along with all the elements in the trees "below" T_2, which are T_1 and T_0. In this way, we create an incremental point query structure from a static base structure. There is, however, an asymptotic slowdown in "incrementalizing" static structures like this in the form of an extra log(N) factor: I think the problem with incremental construction of a KD-tree or KNN-tree is, as you've alluded to in a comment, that the tree will eventually become unbalanced and you can't do simple tree rotation to fix balance problems and keep consistency. At the minimum, the re-balancing task is not trivial and one would definitely not want to do it at each insertion. Often, one will choose to build a tree with a batch method, insert a bunch of new points and allow the tree to become unbalanced up to a point, and then re-balance it. A very similar thing to do is to build the data structure in batch for M points, use it for M' points, and then re-build the data structure in batch with M+M' points. Since re-balancing is not normal, fast algorithm we are familiar with for trees, rebuilding is not necessarily slow in comparison and in some cases can be faster (depending on how the sequence of the points entering your incremental algorithm).  That being said, the amount of code you write, debugging difficulty, and the ease of others' understanding of your code can be significantly smaller if you take the rebuild approach. If you do so, you can use a batch method and keep an external list of points not yet inserted into the tree. A brute force approach can be used to ensure none of these is closer than the ones in the tree. Some links to Python implementations/discussions are below, but I haven't found any that explicitly claim to be incremental. Good luck. http://www.scipy.org/Cookbook/KDTree http://cgi.di.uoa.gr/~compgeom/pycgalvisual/kdppython.shtml http://sites.google.com/site/mikescoderama/Home/kd-tree-knn http://en.wikipedia.org/wiki/Kd-tree Note: My comments here apply to high-dimensional spaces. If you're working in 2D or 3D, what I've said may not be appropriate. (If you working in very high dimensional spaces, use brute force or approximate nearest neighbor.) There is. The Scipy Cookbook WebSite includes a complete implementation of a kNN algorithm that can be updated incrementally. Maybe a few lines of background would be helpful for anyone interested but not familiar with the terminology. A kNN engine is powered by either of two data representations--the pairwise distances between all points in the dataset stored in a multi-dimensional array (a distance matrix), or a kd-tree, which just stores the data points themselves in a multi-dimensional binary tree. These are only two operations that a kd-tree-based KNN algorithm needs: you create the tree from the dataset (analogous to the training step performed in batch mode in other ML algorithms), and you search the tree to find 'nearest neighbors' (analogous to the testing step). Online or incremental training in the context of a KNN algorithm (provided it's based on a kd-tree) means to insert nodes to an already-built kd-tree. Back to the kd-Tree implementation in the SciPy Cookbook: The specific lines of code responsible for node insertion appear after the comment line "insert node in kd-tree" (in fact, all of the code after that comment are directed to node insertion). Finally, there is a kd-tree implementation in the spatial module of the SciPy library (scipy.spatial module) called KDTree (scipy.spatial.KDTree) but i don't believe it supports node insertion, at least such a function is not in the Docs (i haven't looked at the source).Is it possible to get classification report from cross_val_score through some workaround? I'm using nested cross-validation and I can get various scores here for a model, however, I would like to see the classification report of the outer loop. Any recommendations?  I would like to see a classification report here along side the score values. 
http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html We can define our own scoring function as below: Now, just call cross_val_score with our new scoring function, using make_scorer:   It will print the classification report as text at the same time return the nested_score as a number. http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html example when run with this new scoring function, the last few lines of the output will be as follows: Its just an addition to Sandipan's answer as I couldn't edit it. If we want to calculate the average classification report for a complete run of the cross-validation instead of individual folds, we can use the following code: Now the result for the example in Sandipan's answer would look like this: Why not choose the easiest path! I would go for this - Input: Output:For batch normalization during testing, how does one calculate the mean and variance of each activation input (in each layer and input dimension)? Does one record the means and variances from training, calculate the means and variances of the entire training set, or calculate the means and variances of the entire test set? Many people say you have to precalculate the means and variances, but if you use the method of calculating the means and variances of the entire test set, wouldn't you need to calculate the means and variances of the entire test set while performing forward propagation (not "pre")? Thank you so much for all your help! When you are predicting on test, you always use train's statistics - be it simple transformation or batch normalization. I'd recommend trying cs231n course to know more about this. Here is how I coded batch normalization while doing this code: github link. If test statistics significantly differ from train, this means that test is different in general and the model won't work well. In this case you'll need to find different training data anyway. But to be more precise - when you train model on data, processed in a certain way, it won't work well on data, which is processed in a different way. Let's imagine that there is only 1 test sample - i. e. you want to make a prediction for one client or whatever. You simply can't calculate test statistics in this case. Secondly, let's take batch normalization. Data is normalized and values now show by how many standard deviations original data differes from a certain average. So the model will use this information for training). If you normalize test data using test statistics, then values will show deviation from a different average. A record of the empirical mean and variance is taken at training time, such as a running average, which is later used for the test set, instead of calculating the means and variances for each test batch.I'm interested in incorporating TensorFlow into a C++ server application built in Visual Studio on Windows 10 and I need to know if that's possible. 
Google recently announced Windows support for TensorFlow: https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html 
but from what I can tell this is just a pip install for the more commonly used Python package, and to use the C++ API you need to build the repo from source yourself: How to build and use Google TensorFlow C++ api
I tried building the project myself using bazel, but ran into issues trying to configure the build. Is there a way to get TensorFlow C++ to work in native Windows (not using Docker or the new Windows 10 Linux subsystem, as I've seen others post about)? Thanks, Ian It is certainly possible to use TensorFlow's C++ API on Windows, but it is not currently very easy. Right now, the easiest way to build against the C++ API on Windows would be to build with CMake, and adapt the CMake rules for the tf_tutorials_example_trainer project (see the source code here). Building with CMake will give you a Visual Studio project in which you can implement your C++ TensorFlow program. Note that the tf_tutorials_example_trainer project builds a Console Application that statically links all of the TensorFlow runtime into your program. At present we have not written the necessary rules to create a reusable TensorFlow DLL, although this would be technially possible: for example, the Python extension is a DLL that includes the runtime, but does not export the necessary symbols to use TensorFlow's C or C++ APIs directly.  There is a detailed guide by Joe Antognini and a similar TensorFlow ReadMe at GitHub explaining the building of TensorFlow source via CMake. You also need to have SWIG installed on your machine which allows connecting C/C++ source with the Python scripting language. I did use Visual CMAKE (cmake-gui) with the screen capture shown below.    In the CMake configuration, I used Visual Studio 15 2017 compiler. Once this stage successfully completes, you can click on the Generate button to go ahead with the actual build process.  However, on Visual Studio 2015, when I attempted building via the "ALL_BUILD" project, the setup gave me "build tools for v141 cannot be found" error. This did not go away even when I attempted to retarget my solution. Finally, the solution got built successfully with Visual Studio 2017. You also need to manually set the SWIG_EXECUTABLE path in CMake before it successfully configures. As indicated in the Antognini link, for me the build took about half an hour on a 16GB RAM, Core i7 machine. Once done, you might want to validate your build by attempting to run the tf_tutorials_example_trainer.exe file. Hope this helps! For our latest work on building TensorFlow C++ API on Windows, please look at this github page. This works on Windows 10, currently without CUDA support (only CPU). PS:
Only the bazel build method works, because CMake is not supported and not maintained anymore, resulting in CMake configuration errors. I had to use a downgraded version of my Visual Studio 2017 (from 15.7.5 to 15.4) by adding "VC++ 2017 version 15.4 v14.11 toolset" through the installer (Individual Components tab). The cmake command which worked for me was: After the build, open tensorflow.sln in Visual Studio and build ALL_BUILD. If you want to enable GPU computation, do check your Graphics Card here (Compute Capability > 3.5). Do remember to install all the packages (Cuda Toolkit 9.0, cuDNN, Python 3.7, SWIG, Git, CMake...) and add the paths to the environment variable in the beginning. I made a README detailing how to I built the Tensorflow dll and .lib file for the C++ API on Windows with GPU support building from source with Bazel. Tensorflow version 1.14 The tutorial is step by step and starts at the very beginning, so you may have to scroll down past steps you have already done, like checking your hardware, installing Bazel etc.
Here is the url: https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows Probably you will want to scroll all the way down to this part:
https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows#step-7-build-the-dll It shows how to pass command to create .lib and .dll. Then to test your .lib you should link it into your c++ project, Then it will show you how to identify and fix the missing symbols using the TF_EXPORT macro I am actively working on making this tutorial better so feel free to leave comments on this answer if you are having problems.Is it possible to get classification report from cross_val_score through some workaround? I'm using nested cross-validation and I can get various scores here for a model, however, I would like to see the classification report of the outer loop. Any recommendations?  I would like to see a classification report here along side the score values. 
http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html We can define our own scoring function as below: Now, just call cross_val_score with our new scoring function, using make_scorer:   It will print the classification report as text at the same time return the nested_score as a number. http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html example when run with this new scoring function, the last few lines of the output will be as follows: Its just an addition to Sandipan's answer as I couldn't edit it. If we want to calculate the average classification report for a complete run of the cross-validation instead of individual folds, we can use the following code: Now the result for the example in Sandipan's answer would look like this: Why not choose the easiest path! I would go for this - Input: Output:rf_mod$forest doesn't seem to have this information, and the docs don't mention it.   In R's randomForest package, the average variable importance for the entire forest of CART models is given by importance(rf_mod).   We can also extract individual tree structure with getTree. Here's the first tree.   One workaround is to grow many CARTs (i.e. - ntree = 1), get the variable importance of each tree, and average the resulting %IncMSE:   The extraction, averaging, and comparison step.  I'd like to be able to extract the variable importance of each tree directly from a randomForest object, without this roundabout method that involves completely re-running the RF in order to facilitate reproducible cumulative variable importance plots like this one, and the one below shown for mtcars. Minimal example here.   I'm aware that a single tree's variable importance is not statistically meaningful, and it's not my intention to interpret trees in isolation. I want them for the purpose of visualization and communicating that as trees increase in a forest, the variable importance measures jump around before stabilizing.    When training a randomForest model, the importance scores are computed for the entire forest and stored directly inside the object. Tree-specific scores are not kept and so cannot be directly retrieved from a randomForest object. Unfortunately, you are correct about having to incrementally construct a forest. The good news is that a randomForest object is self-contained, and you don't need to implement your own run_rf. Instead, you can use stats::update to re-fit the random forest model with a single tree and randomForest::grow to add additional trees one at a time: The data frame shows how feature importance changes with each additional tree. This is the right panel of your plot example. The trees themselves (for the left panel) can be retrieved from the final forest, which is given by dplyr::last( rfs ). Disclaimer: This is not really an answer, but too long to post as a comment. Will remove if deemed not appropriate. While I (think I) understand your question, to be honest I am unsure whether your question makes sense from a statistics/ML point-of-view. The following is based on my obviously limited understanding of RF and CART. Perhaps my comment-post will lead to some insights. Let's start with some general random forest (RF) theory on variable importance from Hastie, Tibshirani, Friedman, The Elements of Statistical Learning, p. 593 (bold-face mine): At each split in each tree, the improvement in the split-criterion is the
  importance measure attributed to the splitting variable, and is accumulated
  over all the trees in the forest separately for each variable. [...]
  Random forests also use the oob samples to construct a different variable-importance measure, apparently to measure the prediction strength of each variable. So the variable importance measure in RF is defined as a measure accumulated over all trees. In traditional single classification trees (CARTs), variable importance is characterised through the Gini index that measures node impurity (see e.g. How to measure/rank “variable importance” when using CART? (specifically using {rpart} from R) and Carolin Strobl's PhD thesis) More complex measures to characterise variable importance in CART-like models exist; for example in rpart: An overall measure of variable importance is the sum of the goodness of split
  measures for each split for which it was the primary variable, plus goodness * (adjusted
  agreement) for all splits in which it was a surrogate. In the printout these are scaled to sum
  to 100 and the rounded values are shown, omitting any variable whose proportion is less
  than 1%. So the bottom line here is the following: At the very least it won't be easy (and in the worst case it won't make sense) to compare variable measures from single classifaction trees with variable importance measures applied to ensemble-based methods like RF. Which leads me to ask: Why do you want to extract variable importance measures for individual trees from an RF model? Even if you came up with a method to calculate variable importances from individual trees, I believe they wouldn't be very meaningful, and they wouldn't have to "converge" to the ensemble-accumulated values.  We can simplify it byI am running into the following error when trying to train this on this dataset. Since this is the configuration published in the paper, I am assuming I am doing something incredibly wrong. This error arrives on a different image every time I try to run training. Any ideas? This kind of error generally occurs when using NLLLoss or CrossEntropyLoss, and when your dataset has negative labels (or labels greater than the number of classes). That is also the exact error you are getting Assertion t >= 0 && t < n_classes failed.  This won't occur for MSELoss, but OP mentions that there is a CrossEntropyLoss somewhere and thus the error occurs (the program crashes asynchronously on some other line). The solution is to clean the dataset and ensure that t >= 0 && t < n_classes is satisfied (where t represents the label). Also, ensure that your network output is in the range 0 to 1 in case you use NLLLoss or BCELoss (then you require softmax or sigmoid activation respectively). Note that this is not required for CrossEntropyLoss or BCEWithLogitsLoss because they implement the activation function inside the loss function. (Thanks to @PouyaB for pointing out).Because online learning does not work well with Keras when you are using an adaptive optimizer (the learning rate schedule resets when calling .fit()), I want to see if I can just manually set it. However, in order to do that, I need to find out what the learning rate was at the last epoch. That said, how can I print the learning rate at each epoch? I think I can do it through a callback but it seems that you have to recalculate it each time and I'm not sure how to do that with Adam. I found this in another thread but it only works with SGD: I am using the following approach, which is based on @jorijnsmit answer: It works with Adam. I found this question very helpful. A minimal workable example that answers your question would be: For everyone that is still confused on this topic: The solution from @Andrey works but only if you set a decay to your learning rate, you have to schedule the learning rate to lower itself after 'n' epoch, otherwise it will always print the same number (the starting learning rate), this is because that number DOES NOT change during training, you can't see how the learning rates adapts, because every parameter in Adam has a different learning rate that adapts itself during the training, but the variable lr NEVER changes Follow this thread. This piece of code might help you. It is based on Keras implementation of Adam optimizer (beta values are Keras defaults)I have a data_df that looks like: Tried to convert classification columns (vehicleType) to dummies ("one hot encoding"): But the original data is missing: So, how to replace a given column with the dummies? you can use a more compact way: this line will drop your old column 'vehicleType', and automatically join the created columns to your datasetI try to apply this code : But I have this error : Update your scikit-learn, cv_results_ has been introduced in 0.18.1, earlier it was called grid_scores_ and had slightly different structure http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV  from sklearn.model_selection import GridSearchCV use this clf.cv_results_ Solved ! 
Uninstall and install conda scikit learn in 0.18.1 How to upgrade scikit-learn package in anaconda. When I import GridSearch :  First, you should update your sklearn, using: After that, check if you are include the wrong module: Change to new path: (this is the right way)Quick answer: This is in fact really easy.
Here's the code (for those who don't want to read all that text): In this setup, the decode_model will use the same decode layer as the model.
If you train the model, the decode_model will be trained, too. Actual question: I'm trying to create a simple autoencoder for MNIST in Keras: This is the code so far: I'm training it to learn the identity function The reconstruction is quite interesting:  But I would also like to look at the representations of cluster.
What is the output of passing [1,0...0] to the decoding layer ?
This should be the "cluster-mean" of one class in MNIST. In order to do that I created a second model decode_model, which reuses the decoder layer.
But if I try to use that model, it complains: Exception: Error when checking : expected dense_input_5 to have shape (None, 784) but got array with shape (10, 10) That seemed strange. It's simply a dense layer, the Matrix wouldn't even be able to process 784-dim input.
I decided to look at the model summary: It is connected to dense_13.
It's difficult to keep track of the names of the layers, but that looks like the encoder layer. Sure enough, the model summary of the whole model is: Apparently the layers are permanently connected.
Strangely there is no input layer in my decode_model. How can I reuse a layer in Keras ?
I've looked at the functional API, but there too, layers are fused together. Oh, nevermind. I should have read the entire functional API:
https://keras.io/getting-started/functional-api-guide/#shared-layers Here's one of the predictions (maybe still lacking some training):
 I'm guessing this could be a 3 ?
Well at least it works now. And for those with similar problems,
here's the updated code: I only compiled one of the models.
For training you need to compile a model, for prediction that is not necessary. Shared layers can directly be accessed from one model to second through list model1.layers. What comes tricky is accessing the input layers tf.keras.Input (not sure of tf.keras.layers.InputLayer, and it's not recommended to use it either) instead as I saw recently these don't assign as simple. But you can access the inputs through model1.inputs in the same order the Input layers of the model1 were defined. And you can form a new model.I have a classic NLP problem, I have to classify a news as fake or real. I have created two sets of features: A) Bigram Term Frequency-Inverse Document Frequency B) Approximately 20 Features associated to each document obtained using pattern.en (https://www.clips.uantwerpen.be/pages/pattern-en) as subjectivity of the text, polarity, #stopwords, #verbs, #subject, relations grammaticals etc ... Which is the best way to combine the TFIDF features with the other features for a single prediction?
Thanks a lot to everyone. Not sure if your asking technically how to combine two objects in code or what to do theoretically after so I will try and answer both. Technically your TFIDF is just a matrix where the rows are records and the columns are features. As such to combine you can append your new features as columns to the end of the matrix. Probably your matrix is a sparse matrix (from Scipy) if you did this with sklearn so you will have to make sure your new features are a sparse matrix as well (or make the other dense). That gives you your training data, in terms of what to do with it then it is a little more tricky. Your features from a bigram frequency matrix will be sparse (im not talking data structures here I just mean that you will have a lot of 0s), and it will be binary. Whilst your other data is dense and continuous. This will run in most machine learning algorithms as is although the prediction will probably be dominated by the dense variables. However with a bit of feature engineering I have built several classifiers in the past using tree ensambles that take a combination of term-frequency variables enriched with some other more dense variables and give boosted results (for example a classifier that looks at twitter profiles and classifies them as companies or people). Usually I found better results when I could at least bin the dense variables into binary (or categorical and then hot encoded into binary) so that they didn't dominate.  What if you do use a classifier for the tfidf but use the pred to add a new feature say tfidf and the probabilities of it to give a better result, here is a pic from auto ml blueprint to show you the same The results were > 90 percent vs 80 percent for current vs the two separate classifier onesI am trying to freeze the weights of certain layer in a prediction model with Keras and mnist dataset, but it does not work. The code is like:  The program outputs 'the weights changed!!!!'. 
I do not understand why the weights of the layer named 'dense1' changes after setting model.get_layer(name=name).trainable = False. You can do it by using: You need to compile the graph after setting 'trainable'.
more info here let me keep my layers freezed upto 5th layer, rest i will keep trainable
Here is more simple & more efficient codehere is my code. I want to made a multi-classification using Keras.  the vcl_acc is better when training, But the predict value always is a same value. I confused,please help me  train.py training output as follow predict.py the output always is same even I use different picture to test as follow Well, after 1 month looking for a solution, i tried everything:
lowering the learning rate, changing the optimizer, using a bigger dataset, increasing and decreasing model complexity, changing the input shape to smaller and larger images, changin the imports from from keras import to from tensorflow.keras import and further to from tensorflow.python.keras import, changing the activation function of every layer, combining them, trying others datasets, etc... Nothing helped. Even if i used a net like VGG16/19 my results would be the same.
But, yesterday, i was reading a book (Deep Learning with Keras - Antonio Gulli, Sujit Pal) and i realized that the autors use imports like this: and not like this: the same for Conv, i was using: and the autors use: And when i changed the imports everything started to work finally!
I don't know if this is a bug or something like this, cause now my models always works even in datasets that it was predicting the same class.
So now i'm using the imports like this, for example: Try this, if doesn't work, look if your dataset is balanced, for example if you problem is to classify images of cats and dogs and you have 200 cat images and 800 dog images, try to use a number of images not so different cause it can cause problem: your model can 'think' well if i say that 10/10 images are dogs so i get 80% accuracy, but this is not what we want. You can use class_weight if you don't have more images to balance you dataset and everything will be ok, you can as well use data-augmentation. You can use callbacks too, like ReduceLROnPlateau which reduce your learning rate when your loss is not getting lower. You can increase your batch_size, don't forget to shuffle the data on your ImageDataGenerator and normalize your images, like this: All these things are very important, but nothing really helped me. The only thing that worked was importing keras.layers.core and keras.layers.convolutional, may this help you too! It seems that your problem is caused by a huge class inbalance in your dataset. One can see that assigning a 0 class to each example gives you over 90% of accuracy. In order to deal with that you may use following strategies: Rebalance your dataset: by either upsampling the less frequent class or downsampling the more frequent one. Adjust class weights: by setting the higher class weight for less frequent class you'll be promoting your network training for putting more attention on the downsampled class. I had the same issue but I tried this and it worked. Simply divide the image by 255 as my training and testing data were also divided by 255. I didn't use test_generator.reset()I want to implement a custom loss function in scikit learn. I use the following code snippet: What should be the arguments passed into my_custom_loss_func? My label matrix is called labm. I want to calculate the difference between the actual and the predicted output (by the model ) multiplied by the true output. If I use labm in place of y_true, what should I use in place of y_pred? Okay, there's 3 things going on here: 1) there is a loss function while training used to tune your models parameters 2) there is a scoring function which is used to judge the quality of your model 3) there is hyper-parameter tuning which uses a scoring function to optimize your hyperparameters. So... if you are trying to tune hyperparameters, then you are on the right track in defining a "loss fxn" for that purpose. If, however, you are trying to tune your whole model to perform well on, lets say, a recall test - then you need a recall optimizer to be part of the training process. It's tricky, but you can do it... 1) Open up your classifier. Let's use an RFC for example: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html 2) click [source] 3) See how it's inheriting from ForestClassifier? Right there in the class definition. Click that word to jump to it's parent definition. 4) See how this new object is inheriting from ClassifierMixin? Click that. 5) See how the bottom of that ClassifierMixin class says this? That's your model being trained on accuracy. You need to inject at this point if you want to train your model to be a "recall model" or a "precision model" or whatever model. This accuracy metric is baked into SKlearn. Some day, a better man than I will make this a parameter which models accept, however in the mean time, you gotta go into your sklearn installation, and tweak this accuracy_score to be whatever you want. Best of luck! The documentation for make_scorer goes like this: So, it dosen't need you to pass arguments while calling the function.
Is this what you were asking? The arguments of your my_custom_func_loss, does not have any connection with your true labels, which is labm. You can keep the way as it now.  Internally GridSearchCV will call the scoring function hence your true labels does not conflict there. y_pred would be the predicted values, generated from the model's output. y_true will be assigned with the values of labm.I'm trying to build a neural net with the neuralnet package and I'm having some trouble with it. I've been successful with the nnet package but no luck with the neuralnet one. I have read the whole documentation package and can't find the solution, or maybe I'm not able to spot it. The training command I'm using is  and for prediction The training takes a whole lot longer than the nnet training. I have tried using the same algorithm as nnet (backpropagation instead of resilent backpropagation) and nothing, changed the activation function too (and the linear.output=F) and pretty much everything else, and the result didn't improved. Predicted values are all the same. I don't understand why the nnet works for me, while the neuralnet one doesn't. I could really use some help, my (lack of) understanding of both things (neural nets and R) it's probably the cause, but can't find why. My dataset is from UCI. I want to use the neural network for a binary classification. A sample of the data would be: Converted into a matrix, with the factors as numerical values: Summary of the predicted values: Value of the Wilcoxon-Mann-Whitney test (area under the curve) shows that the prediction performance is virtualy the same as a random. The first reason to consider when you get weird results with neural networks is normalization. Your data must be normalized, otherwise, yes, the training will result in skewed NN which will produce the same outcome all the time, it is a common symptom. Looking at your data set, there are values >>1 which means they are all treated by NN essentially the same. The reason for it is that the traditionally used response functions are (almost) constant outside some range around 0. Always normalize your data before feeding it into a neural network. Similar to the answer from @sashkello, I faced a similar issue earlier when my data was not properly normalized. Once I normalized the data everything ran correctly. Recently, I faced this issue again and after debugging, I found that there can be another reason for neural networks giving the same output. If you have a neural network that has a weight decay term such as that in the RSNNS package, make sure that your decay term is not so large that all weights go to essentially 0.  I was using the caret package for in R. Initially, I was using a decay hyperparameter = 0.01. When I looked at the diagnostics, I saw that the RMSE was being calculated for each fold (of cross validation), but the Rsquared was always NA. In this case all predictions were coming out to the same value. Once I reduced the decay to a much lower value (1E-5 and lower), I got the expected results.  I hope this helps. I'm adding this here for anyone who might have the same problem I had. if any of the above didn't work and you're using TensorFlow with a custom training loop. make sure to set training=True as in: If anyone has the same problem, I solved it by using the parameter rep when defining the neural network. It seems that the training of the network is done only once if you don't set this parameter and that leads to the network returning a vector of identical values (or values which are very similar, e.g. 0.99872 and 0.97891). I believe that the problem could also be in the default value of err.fct parameter, which I set to ce for binary classification. This is the code which led to normal results: This is the output (maximum probability - 94.57%, minimum probability - 0.01%): The usage of rep leads to some weird behavior in RStudio when plotting, because there are multiple models in different training iterations. Therefore, if you don't want to make your environment crash from too much plotting, use an additional parameter:I'm doing a binary classification using Keras (with Tensorflow backend) and I've got about 76% precision and 70% recall. Now I want to try to play with decision threshold. As far as I know Keras uses decision threshold 0.5. Is there a way in Keras to use custom threshold for decision precision and recall? Thank you for your time! create custom metrics like this : Edited thanks to @Marcin : Create functions that returns the desired metrics with threshold_value as argument now you can use them in I hope this helps :)I am employing L1 regularization on my neural network parameters in Keras with keras.regularizers.l1(0.01) to obtain a sparse model. I am finding that, while many of my coefficients are close to zero, few of them are actually zero. Upon looking at the source code for the regularization, it suggests that Keras simply adds the L1 norm of the parameters to the loss function. This would be incorrect because the parameters would almost certainly never go to zero (within floating point error) as intended with L1 regularization. The L1 norm is not differentiable when a parameter is zero, so subgradient methods need to be used where the parameters are set to zero if close enough to zero in the optimization routine. See the soft threshold operator max(0, ..) here. Does Tensorflow/Keras do this, or is this impractical to do with stochastic gradient descent? EDIT: Also here is a superb blog post explaining the soft thresholding operator for L1 regularization. So despite @Joshua answer, there are three other things that are worth to mention: The problem of not having most of the values set to 0 might arise due to computational reasons due to the nature of a gradient-descent based algorithm (and setting a high l1 value) because of oscillations which might occur due to gradient discontinuity. To understand imagine that for a given weight w = 0.005 your learning rate is equal to 0.01 and a gradient of the main loss is equal to 0 w.r.t. to w. So your weight would be updated in the following manner: and after the second update: As you may see the absolute value of w hasn't decreased even though you applied l1 regularization and this happened due to the nature of the gradient-based algorithm. Of course, this is simplified situation but you could experience such oscillating behavior really often when using l1 norm regularizer. Keras correctly implements L1 regularization. In the context of neural networks, L1 regularization simply adds the L1 norm of the parameters to the loss function (see CS231). While L1 regularization does encourages sparsity, it does not guarantee that output will be sparse. The parameter updates from stochastic gradient descent are inherently noisy. Thus, the probability that any given parameter is exactly 0 is vanishingly small. However, many of the parameters of an L1 regularized network are often close to 0. A rudimentary approach would be to threshold small values to 0. There has been research to explore more advanced methods of generating sparse neural network. In this paper, the authors simultaneously prune and train a neural network to achieve 90-95% sparsity on a number of well known network architectures.  TL;DR:
The formulation in deep learning frameworks are correct, but currently we don't have a powerful solver/optimizer to solve it EXACTLY with SGD or its variants. But if you use proximal optimizers, you can obtain sparse solution. Your observation is right. Subgradient descent has very poor convergence properties for
  non-smooth functions, such as the Lasso objective, since it ignores
  problem structure completely (it doesn't distinguish between the least
  squares fit and the regularization term) by just looking at
  subgradients of the entire objective. Intuitively, taking small steps
  in the direction of the (sub)gradient usually won't lead to
  coordinates equal to zero exactly. Another simple work around is to zero out small weights (i.e.: absolute value <1e-4) after training or after each gradient descent step to force sparsity. This is just a handy heuristic approach and not theoretically rigorous. Keras implements L1 regularization properly, but this is not a LASSO. For the LASSO one would need a soft-thresholding function, as correctly pointed out in the original post. It would be very useful with a function similar to the keras.layers.ThresholdedReLU(theta=1.0), but with f(x) = x for x > theta or f(x) = x for x < -theta, f(x) = 0 otherwise. For the LASSO, theta would be equal to the learning rate times the regularization factor of the L1 function.I'm using Keras 1.0. My problem is identical to this one (How to implement a Mean Pooling layer in Keras), but the answer there does not seem to be sufficient for me. I want to implement this network:
 The following code does not work: If I don't set return_sequences=True, I get this error when I call AveragePooling1D():  Otherwise, I get this error when I call Dense(): I just attempted to implement the same model as the original poster, and I'm using Keras 2.0.3. The mean pooling after LSTM worked when I used GlobalAveragePooling1D, just make sure return_sequences=True in the LSTM layer. Give it a try! Adding TimeDistributed(Dense(1)) helped: I think the accepted answer is basically wrong. A solution was found at:
https://github.com/fchollet/keras/issues/2151
However, it only works for theano backend. I have modified the code so that it supports both theano and tensorflow. Thanks, I also meet the question, but I think TimeDistributed layer not working as you want, you can try Luke Guye's TemporalMeanPooling layer, it works for me. Here is the example: Quite late to the party, but tf.keras.layers.AveragePooling1D with suitable pool_size parameter also seems to return the correct result. Working on the example shared by bobchennan on this issue. The output is Now using AveragePooling1D, The output is, Some points to consider,I trained an IBK classifier with some training data that I created manually as following: Then I build up the classifier: I want to create a new instance with unlabeled class and classify this instance, I tried the following with no luck. I just get the following errors Looks like I am doing something wrong while creating a new instance. How can I create an unlabeled instance exactly ? Thanks in Advance You will see this error, when you classify a new instance which is not associated with a dataset. You have to associate every new instance you create to an Instances object using setDataset. After this you can classify newly created instance. http://www.cs.tufts.edu/~ablumer/weka/doc/weka.core.Instance.html Detailed Implementation Link The problem is with this line: When you try to classify newInst, Weka throws an exception because newInst has no Instances object (i.e., dataset) associated with it - thus it does not know anything about its class attribute. You should first create a new Instances object similar to the dataRaw, add your unlabeled instance to it, set class index, and only then try classifying it, e.g.: See pages 203 - 204 of the WEKA documentation. That helped me a lot! (The Weka Manual is a pdf file that is located in your weka installation folder. Just open the doucmentation.html and it will point you to the pdf manual.) Copy-pasting some snippets of the code listings of Chapter 17 (Using the WEKA API / Creating datasets in memory) should help you solve the task.When using something like: am I doing the prediction with the best weights calculated during training or model is using the last weights (which may not be the best ones)? So, is the above a safe approach or should I load best-weights.h5 into the model even if the predictions are done right after training? After the training stops by EarlyStopping callback, the current model may not be the best model with the highest/lowest monitored quantity. As a result a new argument, restore_best_weights, has been introduced in Keras 2.2.3 release for EarlyStopping callback if you would like to restore the best weights: restore_best_weights: whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the
  model weights obtained at the last step of training are used. EarlyStopping callback doesn't save anything on its own (you can double check it looking at its source code https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L458). Thus your code saves the last model that achieved the best result on dev set before the training was stopped by the early stopping callback.
I would say that, if you are saving only the best model according to dev, it is not useful to have also an early stopping callback (unless you don't want to save time and your are sure enough you are not going to find any better model if you continue the training) I would say model uses the latest weights, but I could not find any evidence in the docs. 
Fortunately you can check the behavior of model by yourself.  First you run:  After that, you can load best-weights.h5 and run the prediction on the same test set again.  If model contains the latest weights, you should get an improved result when loading best-weights.h5. If the results are the same, you can be sure that model automatically uses the best achieved results.  Always load the weights saved to the disk. In this case, best-weights.h5 on validation loss.Getting this memory error. But the book/link I am following doesn't get this error. A part of Code: Error:
MemoryError: Unable to allocate 359. MiB for an array with shape (60000, 784) and data type float64 I also get this error when I try to scale the data using StandardScaler's fit_transfrom But works fine in both if I decrease the size of training set (something like : x_train[:1000] ,y_train[:1000]) Link for the code in the book here. The error I get is in Line 60 and 63 (In [60] and In [63]) The book : Aurélien Géron - Hands-On Machine Learning with Scikit-Learn  Keras  and Tensorflow 2nd Ed (Page : 149 / 1130) Does this has anything to do with my ram? and what does "Unable to allocate 359" mean? is it the memory size ? Just in case my specs :
CPU - ryzen 2400g , ram - 8gb (3.1gb is free when using jupyter notebook) The message is straight forward, yes, it has to do with the available memory. 359 MiB = 359 * 2^20 bytes = 60000 * 784 * 8 bytes where MiB = Mebibyte = 2^20 bytes, 60000 x 784 are the dimensions of your array and 8 bytes is the size of float64. Maybe the 3.1gb free memory is very fragmented and it is not possible to allocate 359 MiB in one piece? A reboot may be helpful in that case. Upgrading python-64 bit seems to have solved all the "Memory Error" problem. did you try converting to smaller sized data types? float64 to float32 or if possible np.uint8 ? Pred['train'] = Pred['train'].astype(np.uint8,errors='ignore') Just Restart Your PC After Closing All Tabs And Softwares.This question came to my mind while working on 2 projects in AI and ML. What If I'm building a model (e.g. Classification Neural Network,K-NN, .. etc) and this model uses some function that includes randomness. If I don't fix the seed, then I'm going to get different accuracy results every time I run the algorithm on the same training data. However, If I fix it then some other setting might give better results. Is averaging a set of accuracies enough to say that the accuracy of this model is xx % ? I'm not sure If this is the right place to ask such a question/open such a discussion. Simple answer, yes, you randomize it and use statistics to show the accuracy. However, it's not sufficient to just average a handful of runs. You need, at a minimum, some notion of the variability as well. It's important to know whether "70%" accurate means "70% accurate for each of 100 runs" or "100% accurate once and 40% accurate once". If you're just trying to play around a bit and convince yourself that some algorithm works, then you can just run it 30 or so times and look at the mean and standard deviation and call it a day. If you're going to convince anyone else that it works, you need to look into how to do more formal hypothesis testing. There are models which are naturally dependent on randomness (e.g., random forests) and models which only use randomness as part of exploring the space (e.g., initialisation of values for neural networks), but actually have a well-defined, deterministic, objective function. For the first case, you will want to use multiple seeds and report average accuracy, std. deviation, and the minimum you obtained. It is often good if you have a way to reproduce this, so just use multiple fixed seeds. For the second case, you can always tell, just on the training data, which run is best (although it might actually not be the one which gives you the best test accuracy!). Thus, if you have the time, it is good to do say, 10 runs, and then evaluate on the one with the best training error (or validation error, just never evaluate on testing for this decision). You can go a level up and do multiple multiple runs and get a standard deviation too. However, if you find that this is significant, it probably means you weren't trying enough initialisations or that you are not using the right model for your data. Stochastic techniques are typically used to search very large solution spaces where exhaustive search is not feasible. So it's almost inevitable that you will be trying to iterate over a large number of sample points with as even a distribution as possible. As mentioned elsewhere, basic statistical techniques will help you determine when your sample is big enough to be representative of the space as a whole. To test accuracy, it is a good idea to set aside a portion of your input patterns and avoid training against those patterns (assuming you are learning from a data set). Then you can use the set to test whether your algorithm is learning the underlying pattern correctly, or whether it's simply memorizing the examples. Another thing to think about is the randomness of your random number generator. Standard random number generators (such as rand from <stdlib.h>) may not make the grade in many cases so look around for a more robust algorithm. I generalize the answer from what i get of your question,
I suppose Accuracy is always average accuracy of multiple runs and the standard deviation. So if you are considering accuracy you get using different seeds to the random generator, are you not actually considering a greater range of input (which should be a good thing). But you have to consider the Standard deviation to consider the accuracy. Or did i get your question it totally wrong ? I believe cross-validation may give you what you ask about: an averaged, and therefore more reliable, estimate of classification performance. It contains no randomness, except in permuting the data set initially. The variation comes from choosing different train/test splits.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Can I use reinforcement learning on classification? Such as human activity recognition? And how? There are two types of feedback. One is evaluative that is used in reinforcement learning method and second is instructive that is used in supervised learning mostly used for classification problems. When supervised learning is used, the weights of the neural network are adjusted based on the information of the correct labels provided in the training dataset. So, on selecting a wrong class, the loss increases and weights are adjusted, so that for the input of that kind, this wrong class is not chosen again. However, in reinforcement learning, the system explores all the possible actions, class labels for various inputs in this case and by evaluating the reward it decides what is right and what is wrong. It may be the case too that until it gets the correct class label it may be giving wrong class name as it is the best possible output it has found till now. So, it doesn't make use of the specific knowledge we have about the class labels, hence slows the convergence rate significantly as compared to supervised learning. You can use reinforcement learning for classification problems but it won't be giving you any added benefit and instead slow down your convergence rate. Short answer: Yes. Detailed answer: yes but it's an overkill. Reinforcement learning is useful when you don't have labeled dataset to learn the correct policy, so you need to develop correct strategy based on the rewards. This also allows to backpropagate through non-differentiable blocks (which I suppose is not your case). The biggest drawback of reinforcement learning methods is that thay are typically took a VERY large amount of time to converge. So, if you possess labels, it would be a LOT more faster and easier to use regular supervised learning. You may be able to develop an RL model that chooses which classifier to use. The gt labels being used to train the classifiers and the change in performance of those classifiers being the reward for the RL model. As others have said, it would probably take a very long time to converge, if it ever does. This idea may also require many tricks and tweaks to make it work. I would recommend searching for research papers on this topic.I'm running some supervised experiments for a binary prediction problem. I'm using 10-fold cross validation to evaluate performance in terms of mean average precision (average precision for each fold divided by the number of folds for cross validation - 10 in my case). I would like to plot PR-curves of the result of mean average precision over these 10 folds, however I'm not sure the best way to do this. A previous question in the Cross Validated Stack Exchange site raised this same problem. A comment recommended working through this example on plotting ROC curves across folds of cross validation from the Scikit-Learn site, and tailoring it to average precision. Here is the relevant section of code I've modified to try this idea:  The code runs, however in my case the mean average precision curve is incorrect. For some reason, the array I have assigned to store the mean_precision scores (mean_tpr variable in the ROC example) computes the first element to be near zero, and all other elements to be 1 after dividing by the number of folds. Below is a visualization of the mean_precision scores plotted against the mean_recall scores. As you can see, the plot jumps to 1 which is inaccurate. 

So my hunch is something is going awry in the update of mean_precision (mean_precision += interp(mean_recall, recall, precision) ) at in each fold of cross-validation, but it's unclear how to fix this. Any guidance or help would be appreciated. I had the same problem. Here is my solution: instead of averaging across the folds, I compute the precision_recall_curve across the results from all folds, after the loop. According to the discussion in https://stats.stackexchange.com/questions/34611/meanscores-vs-scoreconcatenation-in-cross-validation this is a generally preferable approach.  Adding to @Dietmar's answer, I agree that it's mostly correct, except instead of using sklearn.metrics.auc to compute area under precision recall curve, I think we should be using sklearn.metrics.average_precision_score. Supporting literature: For example, in PR space it is incorrect to linearly interpolate between points We provide evidence in favor of computing AUCPR using the lower trapezoid, average precision, or interpolated median estimators From sklearn's documentation on average_precision_score This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic. Here's a fully reproducible example which I hope can help others if they cross this thread:    I couldn't find an answer posted in other discussions, so hopefully this can help. The main thing was to reverse recall and precision before using interp: And making sure to reverse back when plotting:  The full code here:This is the code and I'm getting the error in the last line only which is y_pred = classifier.predict(X_test). The error I'm getting is AttributeError: 'KerasClassifier' object has no attribute 'model' Because you haven't fitted the classifier yet. For classifier to have the model variable available, you need to call  Although you have used cross_val_score() over the classifier, and found out accuracies, but the main point to note here is that the cross_val_score will clone the supplied model and use them for cross-validation folds. So your original estimator classifier is untouched and untrained.  You can see the working of cross_val_score in my other answer here So put the above mentioned line just above y_pred = classifier.predict(X_test) line and you are all set. Hope this makes it clear. You get the error because you didn´t actually train the returned model from KerasClassifier which is a Scikit-learn Wrapper to make use of Scikit-learn functions.  You could for example do a GridSearch (as you might know since the code seems to be from the Udemy ML/DL course):  If you don´t need Scikit-learn functionality I suggest to avoid the wrapper and simply build your model with:  and then train with:I am playing some demos about recurrent neural network.  I noticed that the scale of my data in each column differs a lot. So I am considering to do some preprocess work before I throw data batches into my RNN. The close column is the target I want to predict in the future. My question is, is preprocessing the data with, say StandardScaler in sklearn necessary in my case? And why? (You are welcome to edit my question) It will be beneficial to normalize your training data. Having different features with widely different scales fed to your model will cause the network to weight the features not equally. This can cause a falsely prioritisation of some features over the others in the representation. Despite that the whole discussion on data preprocessing is controversial either on when exactly it is necessary and how to correctly normalize the data for each given model and application domain there is a general consensus in Machine Learning that running a Mean subtraction as well as a general Normalization preprocessing step is helpful. In the case of Mean subtraction, the mean of every individual feature is being subtracted from the data which can be interpreted as centering the data around the origin from a geometric point of view. This is true for every dimensionality. Normalizing the data after the Mean subtraction step results in a normalization of the data dimensionality to approximately the same scale. Note that the different features will loose any prioritization over each other after this step as mentioned above. If you have good reasons to think that the different scales in your features bear important information that the network may need to truly understand the underlying patterns in your dataset, then a normalization will be harmful. A standard approach would be to scale the inputs to have mean of 0 and a variance of 1.  Further preprocessing operations may be helpful in specific cases such as performing PCA or Whitening on your data. Look into the awesome notes of CS231n (Setting up the data and the model) for further reference on these topics as well as for a more detailed explenation of the topics above. Definetly yes. Most of neural networks work best with data beetwen 0-1 or -1 to 1(depends on output function). Also when some inputs are higher then others network will "think" they are more important. This can make learning very long. Network must first lower weights in this inputs. I found this https://arxiv.org/abs/1510.01378 
If you normalize it may improve convergence so you will get lower training times.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 8 years ago. The community reviewed whether to reopen this question 10 months ago and left it closed: Original close reason(s) were not resolved Is there any Support Vector Machine library already implemented which I could use in my C# projects? I did some google searchhes and found links that could be interesting: libsvm svm Support_Vector_Machine libsvm implement Accord.NET I stumbled across your question when trying to find a good starting point example of a Support Vector Machine implementation in C#. If someone wants to write their own SVM, and has no desire to use a toolkit or can't use a toolkit for some reason... this served as a good reference for myself. kernel-support-vector-machines-example-C#I implemented a modified version of the Caffe C++ example and while it works really well, it's incredibly slow because it only accepts images one by one. Ideally I'd like to pass Caffe a vector of 200 images and return the best prediction for each one. I received some great help from Fanglin Wang and implemented some of his recommendations, but am still having some trouble working out how to retrieve the best result from each image.  The Classify method is now passed a vector of cv::Mat objects (variable input_channels) which is a vector of grayscale floating point images. I've eliminated the preprocessing method in the code because I don't need to convert these images to floating point or subtract the mean image. I've also been trying to get rid of the N variable because I only want to return the top prediction and probability for each image. UPDATE Thank-you so much for your help Shai, I made the changes you recommended but seem to be getting some strange compilation issues I can't work out (I managed to sort out a few of the issues). These are the changes I made: Header File: Class File: If I understand your problem correctly, you input n images, expecting n pairs of (label, prob), but getting only one such pair. I believe these modifications should do the trick for you: Classifier::Predict should return a vector< vector<float> >, that is a vector of probabilities per input image. That is a vector of size n of vectors of size output_layer->channels(): In Classifier::Classify you need to process each vector<float> through Argmax independantly: Unfortunately, I don't believe a parallelization of network Forward passes has been implemented. However, if you'd like you could simply implement your own wrapper to repeatedly run data through copies of your network, in parallel? Have a look at How many images can you pass to Caffe at a time? In the linked prototxt all you have to define is The existing implementation evaluates a batch of 64 images but not necessarily in parallel. However, if running on a GPU, processing a batch of 64 will be faster than 64 single-image batches.I'm trying to use the car evaluation dataset from the UCI repository and I wonder whether there is a convenient way to binarize categorical variables in sklearn. One approach would be to use the DictVectorizer of LabelBinarizer but here I'm getting k different features whereas you should have just k-1 in order to avoid collinearization. 
    I guess I could write my own function and drop one column but this bookkeeping is tedious, is there an easy way to perform such transformations and get as a result a sparse matrix?  if your data is a pandas DataFrame, then you can simply call get_dummies.
Assume that your data frame is df, and you want to have one binary variable per level of variable 'key'. You can simply call: and then delete one of the dummy variables, to avoid the multi-colinearity problem.
I hope this helps ... The basic method is Here is how to do in sparse format DictVectorizer is the recommended way to generate a one-hot encoding of categorical variables; you can use the sparse argument to create a sparse CSR matrix instead of a dense numpy array. I usually don't care about multicollinearity and I haven't noticed a problem with the approaches that I tend to use (i.e. LinearSVC, SGDClassifier, Tree-based methods). It shouldn't be a problem to patch the DictVectorizer to drop one column per categorical feature - you simple need to remove one term from DictVectorizer.vocabulary at the end of the fit method. (Pull requests are always welcome!)i have a csv file and has v3 column but that column has some 'nan' rows.
How can i except the rows. Edit: V3 columns has A,C,B,A,C,D,,,A,S, like that,and i want to convert it to (1,2,3,1,2,4,,,1,7)  Mask the nan values by using ~isnull(): Another way is to use the pandas.factorize function, which takes care of the nans automatically (assigns them -1):On numerous occasions I've been getting this error when trying to fit a gbm or rpart model. Finally I was able to reproduce it consistently using publicly available data. I have noticed that this error happens when using CV (or repeated cv). When I don't use any fit control I don't get this error. Can some shed some light one why I keep getting error consistently. There is a typo, it should be trControl instead of tcControl. And when the argument is provided as tcControl, caret passes this to rpart and this throws an error because this option was never available. I guess this answers your question of why you get this error when you try to have a cross-validation in training. Below is how it should work:I do not have a formal background in Natural Language Processing was wondering if someone from the NLP side can shed some light on this. I am playing around with the NLTK library and I was specifically looking into the stopwords function provided by this package: In [80]:
  nltk.corpus.stopwords.words('english') Out[80]:  ['i',  'me',  'my', 
  'myself',  'we',  'our',  'ours', 
  'ourselves',  'you',  'your', 
  'yours',  'yourself',  'yourselves', 
  'he',  'him',  'his',  'himself', 
  'she',  'her',  'hers',  'herself', 
  'it',  'its',  'itself',  'they', 
  'them',  'their',  'theirs', 
  'themselves',  'what',  'which', 
  'who',  'whom',  'this',  'that', 
  'these',  'those',  'am',  'is', 
  'are',  'was',  'were',  'be', 
  'been',  'being',  'have',  'has', 
  'had',  'having',  'do',  'does', 
  'did',  'doing',  'a',  'an',  'the', 
  'and',  'but',  'if',  'or', 
  'because',  'as',  'until',  'while', 
  'of',  'at',  'by',  'for',  'with', 
  'about',  'against',  'between', 
  'into',  'through',  'during', 
  'before',  'after',  'above', 
  'below',  'to',  'from',  'up', 
  'down',  'in',  'out',  'on',  'off', 
  'over',  'under',  'again', 
  'further',  'then',  'once',  'here', 
  'there',  'when',  'where',  'why', 
  'how',  'all',  'any',  'both', 
  'each',  'few',  'more',  'most', 
  'other',  'some',  'such',  'no', 
  'nor',  'not',  'only',  'own', 
  'same',  'so',  'than',  'too', 
  'very',  's',  't',  'can',  'will', 
  'just',  'don',  'should',  'now'] What I don't understand is, why is the word "not" present? Isn't that necessary to determine the sentiment inside a sentence? For instance, a sentence like this: I am not sure what the problem is. is totally different once the stopword not is removed changing the meaning of the sentence to its opposite (I am sure what the problem is). If that is the case, is there a set of rules that I am missing on when not to use these stopwords? The concept of stop word list does not have a universal meaning and depends on what you want to do. If you have a task where you need to understand the polarity, sentiment or a similar characteristic of a phrase and if your method depends on detecting negation (like in your example), obviously you shouldn't remove "not" as a stop word (note that you may still want to remove other very common unrelated words which would constitute your new stop word list).  However, to answer your question, most of the sentiment analysis methods are very superficial. They look for emotion/sentiment-laden words, and -- most of the time -- they do not attempt a deep analysis of the sentence.  As an another example where you would like to keep the stop words: if you are trying to classify the documents according to their authors (authorship attribution) or carrying out stylometrics, you should definitely keep these functional words as they characterize a big part of the style and the discourse. However, for many other kinds of analyses (e.g. word space models, document similarity, search, etc.) removing very common, functional words makes sense both computationally (you process fewer words) and in some cases practically (you may even get better results with the stop words removed). If I'm trying to understand the context in which a specific word is used very often, I'd like to see the content words, not the functional words.I'm trying to run LDA (Latent Dirichlet Allocation) on a non-English text dataset. From sklearn's tutorial, there's this part where you count term frequency of the words to feed into the LDA: Which has built-in stop words feature which is only available for English I think. How could I use my own stop words list for this? You may just assign a list of your own words to the stop_words, e.g.:I saw a sample of code (too big to paste here) where the author used model.train_on_batch(in, out) instead of model.fit(in, out). The official documentation of Keras says:  Single gradient update over one batch of samples. But I don't get it. Is it the same as fit(), but instead of doing many feed-forward and backprop steps, it does it once? Or am I wrong? Yes, train_on_batch trains using a single batch only and once.  While fit trains many batches for many epochs. (Each batch causes an update in weights). The idea of using train_on_batch is probably to do more things yourself between each batch. It is used when we want to understand and do some custom changes after each batch training. A more precide use case is with the GANs.
You have to update discriminator but during update the GAN network you have to keep the discriminator untrainable. so you first train the discriminator and then train the gan keeping discriminator untrainable.
see this for more understanding:
https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3  The method fit of the model train the model for one pass through the data you gave it, however because of the limitations in memory (especially GPU memory), we can't train on a big number of samples at once, so we need to divide this data into small piece called mini-batches (or just batchs). The methode fit of keras models will do this data dividing for you and pass through all the data you gave it. However, sometimes we need more complicated training procedure we want for example to randomly select new samples to put in the batch buffer each epoch (e.g. GAN training and Siamese CNNs training ...), in this cases we don't use the fancy an simple fit method but instead we use the train_on_batch method. To use this methode we generate a batch of inputs and a batch of outputs(labels) in each iteration and pass it to this method and it will train the model on the whole samples in the batch at once and gives us the loss and other metrics calculated with respect to the batch samples.I need to find optimal discount for each product (in e.g. A, B, C) so that I can maximize total sales. I have existing Random Forest models for each product that map discount and season to sales.  How do I combine these models and feed them to an optimiser to find the optimum discount per product? Reason for model selection: Input data: sample data used to build model at product level. Glance of the data as below:
 Idea/Steps followed by me: sudo/sample code # as I am unable to find a way to pass the product_models into optimizer. Dear SO experts, Request your guidance(struggling to find any guidance since couple of weeks) on how to use the PSO optimizer(or any other optimizer if I am not following right one) with RF. Adding functions used for model: EDIT: updated dataset to simplified version. you can find a complete solution below ! The fundamental differences with your approach are the following : Additional modifications to your original code include : The code is therefore : It gives :I've got a series of modelled class labels from the knn function. I've got a data frame with basic numeric training data, and another data frame for test data. How would I go about drawing a decision boundary for the returned values from the knn function? I'll have to replicate my findings on a locked-down machine, so please limit the use of 3rd party libraries if possible. I only have two class labels, "orange" and "blue". They're plotted on a simple 2D plot with the training data. Again, I just want to draw a boundary around the results from the knn function. Code: classes is just a vector of class labels determined from an earlier bit of code. If you need it, below is the complete code for my work: Get the class probability predictions on a grid, and draw a contour line at P=0.5 (or whatever you want the cutoff point to be). This is also the method used in the classic MASS textbook by Venables and Ripley, and in Elements of Statistical Learning by Hastie, Tibshirani and Friedman.  See also basically the same question on CrossValidated.I keep getting this error when importing top2vec. Python version: 3.9.7 (64-bit) Have installed MSBuild No errors when pip installing this package Does anyone know a solution to this problem or experienced a similar problem? There is new release (ver. 0.8.29) of hdbscan from 31 Oct. 2022 that fix the issue. See my original answer for more details. Original Answer: It looks like you are using latest (as of 23 Sept 2022) versions of hdbscan and joblib packages available on PyPI. cachedir was removed from joblib.Memory in commit on 2 Feb 2022 as depreciated. The latest version on PyPi is ver. 1.2.0 released on Sep 16, 2022, i.e. it incorporate this change The relevant part of hdbscan source code on GitHub was updated on 16 Sept 2022. Unfortunately the latest (as of 23 Sept 2022) hdbscan release on PyPi is ver. 0.8.28 released on Feb 8, 2022 and still not updated. It still use memory=Memory(cachedir=None, verbose=0) One possible solution is to force using joblib version before cachedir was removed - ver. 1.1.0 as of Oct 7, 2021. However note my edits below. There are open issues on hdbscan repo (#563) and (#565). Note there is vulnerability CVE-2022-21797 when using joblib < 1.2.0 There is new release (ver. 0.8.29) of hdbscan from 31 Oct. 2022. Thank you! It worked for me. I downgraded the joblib package by using pip install --upgrade joblib==1.1.0; however, please be advised that this version of joblib has a known vulnerability of Arbitrary Code Execution via the pre_dispatch flag in Parallel() class due to the eval() statement. So, please use it with caution and not in production. Happy coding. :)Is it possible to minimise a loss function by changing only some elements of a variable? In other words, if I have a variable X of length 2, how can I minimise my loss function by changing X[0] and keeping X[1] constant? Hopefully this code I have attempted will describe my problem: which outputs: where I would like to to find the optimal value of X0 = 2 and thus X = [2, 2] edit Motivation for doing this: I would like to import a trained graph/model and then tweak various elements of some of the variables depending on some new data I have.   You can use this trick to restrict the gradient calculation to one index: part_X becomes the value you want to change in a one-hot vector of the same shape as X. part_X + tf.stop_gradient(-part_X + X) is the same as X in the forward pass, since part_X - part_X is 0. However in the backward pass the tf.stop_gradient prevents all unnecessary gradient calculations. I'm not sure if it is possible with the SciPy optimizer interface, but using one of the regular tf.train.Optimizer subclasses you can do something like that by calling compute_gradients first, then masking the gradients and then calling apply_gradients,
 instead of calling minimize (which, as the docs say, basically calls the previous ones). Output: This should be pretty easy to do by using the var_list parameter of the minimize function. You should note that by convention all trainable variables are added to the tensorflow default collection GraphKeys.TRAINABLE_VARIABLES, so you can get a list of all trainable variables using: This is just a list of variables which you can manipulate as you see fit and use as the var_list parameter. As a tangent to your question, if you ever want to take customizing the optimization process a step further you can also compute the gradients manually using grads = tf.gradients(loss, var_list) manipulate the gradients as you see fit, then call tf.train.GradientDescentOptimizer(...).apply_gradients(grads_and_vars_as_list_of_tuples). Under the hood minimize is just doing these two steps for you. Also note that you are perfectly free to create different optimizers for different collections of variables. You could create an SGD optimizer with learning rate 1e-4 for some variables, and another Adam optimizer with learning rate 1e-2 for another set of variables. Not that there's any specific use case for this, I'm just pointing out the flexibility you now have. The answer by Oren in the second link below calls a function (defined in the first link) that takes a Boolean hot matrix of the parameters to optimize and the tensor of parameters. It uses stop_gradient and works like a charm for a neural network I developed. Update only part of the word embedding matrix in Tensorflow https://github.com/tensorflow/tensorflow/issues/9162I met problem with processing of spark wide dataframe (about 9000 columns and sometimes more).
Task: So I made extensive frame and try to create vector with VectorAssembler, cached it and trained on it KMeans. 
It took about 11 minutes for assembling and 2 minutes for KMeans for 7 different count of clusters on my pc in standalone mode for frame ~500x9000. Another side this processing in pandas (pivot df, and iterate 7 clusters) takes less one minute. 
Obviously I understand overhead and performance decreasing for standalone mode and caching and so on but it's really discourages me. 
Could somebody explain how I can avoid this overhead? 
How peoples work with wide DF instead of using vectorassembler and getting performance decreasing? 
More formal question (for sof rules) sound like - How can I speed up this code? Config: VectorAssembler's transform function processes all the columns and stores metadata on each column in addition to the original data. This takes time, and also takes up RAM. To put an exact figure on how much things have increased, you can dump your data frame before and after the transformation as parquet files and compare. In my experience, a feature vector built by hand or other feature extraction methods compared to one built by VectorAssembler can cause a size increase of 10x and this was for a logistic regression with only 10 parameters. Things will get a lot worse with a data set with as many columns as you have. A few suggestions: Actually solution was found in map for rdd. Advantages: Example of code: scala implementation.I am trying to load two datasets and use them both for training.  Package versions: python 3.7; 
pytorch 1.3.1 It is possible to create data_loaders seperately and train on them sequentially:  Note: MyDataset is a custom dataset class which has def __len__(self): def __getitem__(self, index): implemented. As the above configuration works it seems that this is implementation is OK.  But I would ideally like to combine them into a single dataloader object. I attempted this as per the pytorch documentation:  However, on random batches I get the following 'expected a tensor as element X in argument 0, but got a tuple instead' type of error. Any help would be much appreciated! If I got your question right, you have train and dev sets (and their corresponding loaders) as follows: And you want to concatenate them in order to use train+dev as the training data, right? If so, you just simply call: The train_dev_loader is the loader containing data from both sets. Now, be sure your data has the same shapes and the same types, that is, the same number of features, or the same categories/numbers, etc. I'd guess the two datasets are sometimes returning different types.  When the data are Tensors, torch stacks them, and they better be the same shape.  If they're something like strings, torch will make a tuple out of them.  So this sounds like one of your datasets is sometimes returning something that's not a tensor.  I'd put some asserts on the output of your dataset to check that it's doing what you want, or dive in with pdb. Adding to @Leopd's answer, you can use the collate_fn function provided by PyTorch. The idea is that in the collate_fn, you will define how the examples should be stacked to make a batch. Since you are on torch 1.3.1, make sure you are looking at the correct version of the documentation.  Let me know if this helps or if you have any followup questions :)Is it practically possible to have decreasing loss and decreasing accuracy at each epoch when training a CNN model?
I am getting the below result while training.  Can someone explain the possible reasons why this is happening? There are at least 5 reasons which might cause such behavior: Outliers: imagine that you have 10 exactly the same images and 9 out of them belong to class A and one belongs to class B. In this case, a model will start to assign a high probability of class A to this example because of the majority of examples. But then - a signal from outlier might destabilize model and make accuracy decreasing.  In theory, a model should stabilize at assigning score 90% to class A but it might last many epochs. Solutions: In order to deal with such examples I advise you to use gradient clipping (you may add such option in your optimizer). If you want to check if this phenomenon occurs - you may check your losses distributions (losses of individual examples from training set) and look for outliers. Bias: Now imagine that you have 10 exactly the same images but 5 of them have assigned class A and 5 - class B. In this case, a model will try to assign approximately 50%-50% distribution on both of these classes. Now - your model can achieve at most 50% of accuracy here - choosing one class out of two valid. Solution: Try to increase the model capacity - very often you have a set of really similar images - adding expressive power might help to discriminate similar examples. Beware of overfitting though. Another solution is to try this strategy in your training. If you want to check if such phenomenon occurs - check the distribution of losses of individual examples. If a distribution would be skewed toward higher values - you are probably suffering from bias. Class inbalance: Now imagine that 90% of your images belong to class A. In an early stage of your training, your model is mainly concentrating on assigning this class to almost all of examples. This might make individual losses to achieve really high values and destabilize your model by making a predicted distribution more unstable.  Solution: once again - gradient clipping. Second thing - patience, try simply leaving your model for more epochs. A model should learn more subtle in a further phase of training. And of course - try class balancing - by either assigning sample_weights or class_weights. If you want to check if this phenomenon occurs - check your class distribution. Too strong regularization: if you set your regularization to be too strict - a training process is mainly concentrated on making your weights to have smaller norm than actually learning interesting insights. Solution: add a categorical_crossentropy as a metric and observe if it's also decreasing. If not - then it means that your regularization is too strict - try to assign less weight penalty then. Bad model design - such behavior might be caused by a wrong model design. There are several good practices which one might apply in order to improve your model: Batch Normalization - thanks to this technique you are preventing your model from radical changes of inner network activations. This makes training much more stable and efficient. With a small batch size, this might be also a genuine way of regularizing your model. Gradient clipping - this makes your model training much more stable and efficient. Reduce bottleneck effect - read this fantastic paper and check if your model might suffer from bottleneck problem. Add auxiliary classifiers - if you are training your network from scratch - this should make your features much more meaningful and your training - faster and more efficient. Yes, this is possible. To provide an intuitive example of why this might happen, suppose that your classifier outputs roughly the same probability for classes A and B, and class A has the highest density overall. Within this setting, changing the model’s parameters minimally might turn B into the most probable class. This effect would make the cross-entropy loss vary minimally, since it depends directly on the probability distribution, but the change would be clearly noticed for the accuracy, because it depends on the argmax of the output probability distribution. As a conclusion, minimizing the cross-entropy loss does not always imply improving the accuracy, mainly because cross-entropy is a smooth function, while the accuracy is non-smooth. It is possible to get decreasing loss with decreasing accuracy but it is far from being called as a good model. This problem can be resolve up to some extinct using Batch normalization at every conv layer of model. this could be possible because loss function also accounts for the confidence of prediction but accuracy only accounts for correctness. Following excel sheet shows an example, on the left side loss and accuracy are low on right side accuracy increases at the same time loss is also increased check spreadsheet to try it yourself    This is same with multi-class classification using softmax function softmax-cross-entropy as loss function
 Loss will be low if probability for positive class is high Hope this makes it clear why this is possible. This is my intuition, if someone this this is not correct you feedback is welcomedI'm using the caret package to analyse Random Forest models built using ranger. I can't figure out how to call the train function using the tuneGrid argument to tune the model parameters. I think I'm calling the tuneGrid argument wrong, but can't figure out why it's wrong. Any help would be appreciated. Here is the syntax for ranger in caret: add . prior to tuning parameters: Only these three are supported by caret and not the number of trees. In train you can specify num.trees and importance: to get variable importance: To check if this works set number of trees to 1000+ - the fit will be much slower. After changing importance = "impurity": If it does not work I recommend installing latest ranger from CRAN and caret from git hub: To train the number of trees you can use lapply with fixed folds created by createMultiFolds or createFolds. EDIT: while the above example works with caret package version 6.0-84, using the names of hyper parameters without dots works as well.I have a personal implementation of a RL algorithm that generates performance metrics every x time steps. That metric is simply a scalar, so I have an array of scalars that I want to display as a simple graph such as:  I want to display it in real time in tensorboard like my above example. Thanks in advance If you really want to use tensorboard you can start looking at tensorflow site and this datacamp tutorial on tensorboard. With tensorflow you can use summary.scalar to plot your custom data (as the example), no need for particular format, as the summary is taking care of that, the only condition is that data has to be a real numeric scalar value, convertible to a float32 Tensor.  That said, if you are not planning on using tensorflow with your implementation, I would suggest you just use matplotlib as this library also enables you to plot data in real time https://youtu.be/Ercd-Ip5PfQ?t=444.I've been searching the net for ~3 hours but I couldn't find a solution yet. I want to give a precomputed kernel to libsvm and classify a dataset, but: How can I generate a precomputed kernel? (for example, what is the basic precomputed kernel for Iris data?) In the libsvm documentation, it is stated that: For precomputed kernels, the first element of each instance must be
the ID. For example, What is a ID? There's no further details on that. Can I assign ID's sequentially? Any libsvm help and an example of precomputed kernels really appreciated. First of all, some background to kernels and SVMs... If you want to pre-compute a kernel for n vectors (of any dimension), what need to do is calculate the kernel function between each pair of examples. The kernel function takes two vectors and gives a scalar, so you can think of a precomputed kernel as a nxn matrix of scalars. It's usually called the kernel matrix, or sometimes the Gram matrix. There are many different kernels, the simplest is the linear kernel (also known as the dot product): sum(x_i * y_i) for i in [1..N]  where (x_1,...,x_N) (y_1,..,y_N) are vectors Secondly, trying to answer your problem... The documentation about precomputed kernels in libsvm is actually pretty good... Each vector here in the second example is a row in the kernel matrix. The value at index zero is the ID value and it just seems to be a sequential count. The value at index 1 of the first vector is the value of the kernel function of the first vector from the first example with itself (i.e. (1x1)+(1x1)+(1x1)+(1x1) = 4), the second is the value of the kernel function of the first vector with the second (i.e. (1x3)+(1x3)=6). It follows on like that for the rest of the example. You can see in that the kernel matrix is symmetric, as it should be, because K(x,y) = K(y,x). It's worth pointing out that the first set of vectors are represented in a sparse format (i.e. missing values are zero), but the kernel matrix isn't and shouldn't be sparse. I don't know why that is, it just seems to be a libsvm thing. scikit-learn hides most of the details of libsvm when handling custom kernels. You can either just pass an arbitrary function as your kernel and it will compute the gram matrix for you or pass the precomputed Gram matrix of the kernel. For the first one, the syntax is: where my_kernel is your kernel function, and then you can call clf.fit(X, y) and it will compute the kernel matrix for you. In the second case the syntax is: And when you call clf.fit(X, y), X must be the matrix k(X, X), where k is your kernel. See also this example for more details: http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html Here is a simple two category 3 vector custom kernel input file that works correctly. I will explain the parts (though you should also see StompChicken's answer): 
1  0:1 1:10 2:12 3:21
2 0:2 1:12 2:19 3:30
1 0:3 1:21 2:30 3:130
 The first number on each line is which category it belongs to.
The next entry on each line is of the form 0:n and it must be sequential, i.e. 
0:1 on first entry 
0:2 on second entry 
0:3 on thrid entry  A possible reason for this is that libsvm returns values alpha_i that go with your vectors in the output file, but for precomputed kernels the vectors are not displayed (which could be truly huge) rather the index 0:n that went with that vector is shown to make your output easier to match up with your input. Especially since the output is not in the same order you put them in it is grouped by category. It is thus very useful for you when reading the input file to be able to match up libsvm's outputs with your own inputs to have those 0:n values. Here you can see the output svm_type c_svc 
kernel_type precomputed 
nr_class 2
total_sv 3
rho -1.53951
label 1 2
nr_sv 2 1
SV
0.4126650675419768 0:1 
0.03174528241667363 0:3 
-0.4444103499586504 0:2  It is important to note that with precomputed kernels you cannot omit the zero entries like you can with all other kernels. They must be explicitly included. I believe that the scikit-learn's python binding of libSVM should address the problem. See the documentation at http://scikit-learn.sourceforge.net/modules/svm.html#kernel-functions , for more information.I've a sample tiny CNN implemented in both Keras and PyTorch. When I print summary of both the networks, the total number of trainable parameters are same but total number of parameters and number of parameters for Batch Normalization don't match.  Here is the CNN implementation in Keras: And the summary printed for above model is: Here's the implementation of the same model architecture in PyTorch: And following is the output of summary of the above model: As you can see in above results, Batch Normalization in Keras has more number of parameters than PyTorch (2x to be exact). So what's the difference in above CNN architectures? If they are equivalent, then what am I missing here? Keras treats as parameters (weights) many things that will be "saved/loaded" in the layer.  While both implementations naturally have the accumulated "mean" and "variance" of the batches, these values are not trainable with backpropagation.  Nevertheless, these values are updated every batch, and Keras treats them as non-trainable weights, while PyTorch simply hides them. The term "non-trainable" here means "not trainable by backpropagation", but doesn't mean the values are frozen.  In total they are 4 groups of "weights" for a BatchNormalization layer. Considering the selected axis (default = -1, size=32 for your layer) The advantage of having it like this in Keras is that when you save the layer, you also save the mean and variance values the same way you save all other weights in the layer automatically. And when you load the layer, these weights are loaded together.Below the code below the Error message Note:` I think, the main problem is Tensorflow version. I used somes command and that's are bellow, and I also used the below command But it don't works , Please help for solve the error. you need to update the version of your TensorFlow. For me, 2.2.0 solved the problem. I also checked with the higher versions and worked ok. or You need update TensorFlow. You can try with or, if you use gpu version If doesn't solve your issue, you can also try with 2.2.0 version. For more details, in this issue follow this answerI need to perform clustering without knowing in advance the number of clusters. The number of cluster may be from 1 to 5, since I may find cases where all the samples belong to the same instance, or to a limited number of group.
I thought affinity propagation could be my choice, since I could control the number of clusters by setting the preference parameter.
However, if I have a single cluster artificially generated and I set preference to the minimal euclidean distance among nodes (to minimize number of clusters), I get terrible over clustering.  Is there any flaw in my approach of using Affinity Propagation? Conversely, is Affinity Propagation unsuited for this task, so should I use something else? No, there is no flaw. AP does not use distances, but requires you to specify a similarity. I don't know the scikit implementation so well, but according to what I read, it uses negative squared Euclidean distances by default to compute the similarity matrix. If you set the input preference to the minimal Euclidean distance, you get a positive value, while all similarities are negative. So this will typically result in as many clusters as you have samples (note: the higher the input preference, the more clusters). I'd rather suggest to set the input preference to the minimal negative squared distance, i.e. -1 times the square of the largest distance in the data set. This will give you a much smaller number of clusters, but not necessarily one single cluster. I don't know whether the preferenceRange() function exists also in the scikit implementation. There is Matlab code on the AP homepage and it is also implemented in the R package 'apcluster' that I am maintaining. This function allows for determining meaningful bounds for the input preference parameter. I hope that helps. You can control it by specifying the minimum preferences, but it's not sure that you will found a single cluster. And also, I would suggest you to don't wanna make a single cluster because it would generate errors, as some data must not be the same or have similarity with examplers but as you provide the minimum preferences so the AP will commit the error. You can also merge clusters together by essentially running the algorithm a second time using the center samples or manually merging the most similar ones. So you could iteratively merge the closest clusters till you get your number, making the choice of preference easier since you can just choose anything that will result in a decent number of clusters (This worked decently well when I tried).I have some machine learning results that I don't quite understand. I am using python sciki-learn, with 2+ million data of about 14 features. The classification of 'ab' looks pretty bad on the precision-recall curve, but the ROC for Ab looks just as good as most other groups' classification. What can explain that?   Class imbalance.  Unlike the ROC curve, PR curves are very sensitive to imbalance. If you optimize your classifier for good AUC on an unbalanced data you are likely to obtain poor precision-recall results.I am new to R (day 2) and have been tasked with building a forest of random forests. Each individual random forest will be built using a different training set and we will combine all the forests at the end to make predictions. I am implementing this in R and am having some difficulty combining two forests not built using the same set. My attempt is as follows:  This of course produces an error: I have been browsing the web for some time looking at a clue for this but haven't had any success yet. Any help here would be most appreciated.  Ah. This is either an oversight in combine or what you're trying to do is nonsensical, depending on your point of view. The votes matrix records the number of votes in the forest for each case in the training data for each response category. Naturally, it will have the same number of rows as the number of rows in your training data.  combine is assuming that you ran your random forests twice on the same set of data, so the dimensions of those matrices will be the same. It's doing this because it wants to provide you with some "overall" error estimates for the combined forest. But if the two data sets are different combining the votes matrices becomes simply nonsensical. You could get combine to run by simply removing one row from your larger training data set, but the resulting votes matrix in the combined forest would be gibberish, since each row would be a combination of votes for two different training cases. So maybe this is simply something that should be an option that can be turned off in combine. Because it should still make sense to combine the actual trees and predict on the resulting object. But some of "combined" error estimates in the output from combine will be meaningless. Long story short, make each training data set the same size, and it will run. But if you do,  I wouldn't use the resulting object for anything other than making new predictions. Anything that is combined that was summarizing the performance of the forests will be nonsense. However, I think the intended way to use combine is to fit multiple random forests on the full data set, but with a reduced number of trees and then to combine those forests. Edit I went ahead and modified combine to "handle" unequal training set sizes. All that means really is that I removed a large chunk of code that was trying to stitch things together that weren't going to match up. But I kept the portion that combines the forests, so you can still use predict: And then you can test it like this: Obviously, this comes with absolutely no warranty! :)I need to write a file with the result of the data test of a Convolutional Neural Network that I trained. The data include speech data collection. The file format needs to be "file name, prediction", but I am having a hard time to extract the file name. I load the data like this: and I am trying to write to the file as follows: The problem with os.listdir(TESTH_DATA_PATH + "/all")[i] is that it is not synchronized with the loaded files order of test_loader. What can I do? Well, it depends on how your Dataset is implemented. For instance, in the torchvision.datasets.MNIST(...) case, you cannot retrieve the filename simply because there is no such thing as the filename of a single sample (MNIST samples are loaded in a different way). As you did not show your Dataset implementation, I'll tell you how this could be done with the torchvision.datasets.ImageFolder(...) (or any torchvision.datasets.DatasetFolder(...)): You can see that the path of the file is retrieved during the __getitem__(self, index), especifically here. If you implemented your own Dataset (and perhaps would like to support shuffle and batch_size > 1), then I would return the sample_fname on the __getitem__(...) call and do something like this: This way you wouldn't need to care about shuffle. And if the batch_size is greater than 1, you would need to change the content of the loop for something more generic, e.g.: In general case DataLoader is there to provide you the batches from the Dataset(s) it has inside. AS @Barriel mentioned in case of single/multi-label classification problems, the DataLoader doesn't have image file name, just the tensors representing the images , and the classes / labels. However, DataLoader constructor when loading objects can take small things (together with the Dataset you may pack the targets/labels and the file names if you like) , even a dataframe This way, the DataLoader may somehow grab that what you need. If you using PyCharm or any IDE that has debug tool, let use it to take a look inside your data_loader, hope you can see a list of filenames, like my case. In my case,
My data_loader was created by mmsegmentation.I working on an application for processing document images (mainly invoices) and basically, I'd like to convert certain regions of interest into an XML-structure and then classify the document based on that data. Currently I am using ImageJ for analyzing the document image and Asprise/tesseract for OCR.  Now I am looking for something to make developing easier. Specifically, I am looking for something to automatically deskew a document image and analyze the document structure (e.g. converting an image into a quadtree structure for easier processing). Although I prefer Java and ImageJ I am interested in any libraries/code/papers regardless of the programming language it's written in.  While the system I am working on should as far as possible process data automatically, the user should oversee the results and, if necessary, correct the classification suggested by the system. Therefore I am interested in using machine learning techniques to achieve more reliable results. When similar documents are processed, e.g. invoices of a specific company, its structure is usually the same. When the user has previously corrected data of documents from a company, these corrections should be considered in the future. I have only limited knowledge of machine learning techniques and would like to know how I could realize my idea.  The following prototype in Mathematica finds the coordinates of blocks of text and performs OCR within each block.  You may need to adapt the parameters values to fit the dimensions of your actual images. I do not address the machine learning part of the question; perhaps you would not even need it for this application. Import the picture, create a binary mask for the printed parts, and enlarge these parts using an horizontal closing (dilation and erosion).  Query for each blob's orientation, cluster the orientations, and determine the overall  rotation by averaging the orientations of the largest cluster.  Use the previous angle to straighten the image. At this time OCR is possible, but you would lose the spatial information for the blocks of text, which will make the post-processing much more difficult than it needs to be. Instead, find blobs of text by horizontal closing.   For each connected component, query for the bounding box position and the centroid position. Use the bounding box positions to extract the corresponding image patch and perform OCR on the patch.  At this point, you have a list of strings and their spatial positions. That's not XML yet, but it sounds like a good starting point to be tailored straightforwardly to your needs. This is the code. Again, the parameters (structuring elements) of the morphological functions may need to change, based on the scale of your actual images; also, if the invoice is too tilted, you may need to "rotate" roughly the structuring elements in order to still achieve good "un-skewing." The paper we use for skew angle detection is: Skew detection and text line position determination in digitized documents by Gatos et. al. The only limitation with this paper is that it can detect skew upto -5 and +5 degrees. After that, we need something to slap the user with a message! :) In your case, where there are primarily invoice scans, you may beautifully use: Multiresolution Analysis in Extraction of Reference Lines from Documents with Gray Level Background by Tag et. al. We wrote the code in MATLAB, if you need help let me know! I worked on a similar project once, and for being a long time user of OpenCV I ended up using it once again. OpenCV is a popular-cross-platform-computer-vision-library that offers programming interfaces for C and C++.  I found an interesting blog that had a post on how to detect the skew angle of a text using OpenCV, and then another on how to deskew. To retrieve the text of the document and be able to pass a smaller image to tesseract, I suggest taking a look at the bounding box technique. I don't know if the image acquisition procedure is your responsibility, but if it is you might want to take a look at how to do camera calibration with OpenCV to fix the distortion in the image caused by some camera lenses.Accuracy, precision, recall and f-score are measures of a system quality in machine-learning systems. It depends on a confusion matrix of True/False Positives/Negatives. Given a binary classification task, I have tried the following to get a function that returns accuracy, precision, recall and f-score: But it seems like I have redundantly looped through the dataset 4 times to get the True/False Positives/Negatives. Also the multiple try-excepts to catch the ZeroDivisionError is a little redundant. So what is the pythonic way to get the counts of the True/False Positives/Negatives without multiple loops through the dataset? How do I pythonically catch the ZeroDivisionError without the multiple try-excepts? I could also do the following to count the True/False Positives/Negatives in one loop but is there an alternative way without the multiple if?: what is the pythonic way to get the counts of the True/False
  Positives/Negatives without multiple loops through the dataset? I would use a collections.Counter, roughly what you're doing with all of the ifs (you should be using elifs, as your conditions are mutually exclusive) at the end: Then e.g. true_pos = counts[1, 1]. How do I pythonically catch the ZeroDivisionError without the multiple
  try-excepts? For a start, you should (almost) never use a bare except:. If you're catching ZeroDivisionErrors, then write except ZeroDivisionError. You could also consider a "look before you leap" approach, checking whether the denominator is 0 before trying the division, e.g. This is a pretty natural use case for the bitarray package.  There's some type conversion overhead, but after that, the bitwise operations are much faster.  For 100 instances, timeit on my PC gives 0.036  for your method and 0.017 using bitarray at 1000 passes. For 1000 instances, it goes to 0.291 and 0.093. For 10000, 3.177 and 0.863. You get the idea. It scales pretty well, using no loops, and doesn't have to store a large intermediate representation building a temporary list of tuples in zip.  Depending on your needs, there are several libraries that will calculate precision, recall, F-score, etc. One that I have used is scikit-learn. Assuming that you have aligned lists of actual and predicted values, then it is as simple as... One of the advantages of using this library is that different flavors of metrics (such as micro-averaging, macro-averaging, weighted, binary, etc.) come free out of the box.