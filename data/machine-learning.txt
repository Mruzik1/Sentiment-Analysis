How is Accuracy defined when the loss function is mean square error? Is it mean absolute percentage error? The model I use has output activation linear and is compiled with loss= mean_squared_error and the output looks like this: So what does e.g. val_acc: 0.3250 mean? Mean_squared_error should be a scalar not a percentage - shouldnt it? So is val_acc - mean squared error, or mean percentage error or another function? From definition of MSE on wikipedia:https://en.wikipedia.org/wiki/Mean_squared_error The MSE is a measure of the quality of an estimator—it is always
  non-negative, and values closer to zero are better. Does that mean a value of val_acc: 0.0 is better than val_acc: 0.325? edit: more examples of the output of accuracy metric when I train - where the accuracy is increase as I train more. While the loss function - mse should decrease. Is Accuracy well defined for mse - and how is it defined in Keras? There are at least two separate issues with your question. The first one should be clear by now from the comments by Dr. Snoopy and the other answer: accuracy is meaningless in a regression problem, such as yours; see also the comment by patyork in this Keras thread. For good or bad, the fact is that Keras will not "protect" you or any other user from putting not-meaningful requests in your code, i.e. you will not get any error, or even a warning, that you are attempting something that does not make sense, such as requesting the accuracy in a regression setting. Having clarified that, the other issue is: Since Keras does indeed return an "accuracy", even in a regression setting, what exactly is it and how is it calculated? To shed some light here, let's revert to a public dataset (since you do not provide any details about your data), namely the Boston house price dataset (saved locally as housing.csv), and run a simple experiment as follows: As in your case, the model fitting history (not shown here) shows a decreasing loss, and an accuracy roughly increasing. Let's evaluate now the model performance in the same training set, using the appropriate Keras built-in function: The exact contents of the score array depend on what exactly we have requested during model compilation; in our case here, the first element is the loss (MSE), and the second one is the "accuracy". At this point, let us have a look at the definition of Keras binary_accuracy in the metrics.py file: So, after Keras has generated the predictions y_pred, it first rounds them, and then checks to see how many of them are equal to the true labels y_true, before getting the mean. Let's replicate this operation using plain Python & Numpy code in our case, where the true labels are Y: Well, bingo! This is actually the same value returned by score[1] above... To make a long story short: since you (erroneously) request metrics=['accuracy'] in your model compilation, Keras will do its best to satisfy you, and will return some "accuracy" indeed, calculated as shown above, despite this being completely meaningless in your setting. There are quite a few settings where Keras, under the hood, performs rather meaningless operations without giving any hint or warning to the user; two of them I have happened to encounter are: Giving meaningless results when, in a multi-class setting, one happens to request loss='binary_crossentropy' (instead of categorical_crossentropy) with metrics=['accuracy'] - see my answers in Keras binary_crossentropy vs categorical_crossentropy performance? and Why is binary_crossentropy more accurate than categorical_crossentropy for multiclass classification in Keras? Disabling completely Dropout, in the extreme case when one requests a dropout rate of 1.0 - see my answer in Dropout behavior in Keras with rate=1 (dropping all input units) not as expected The loss function (Mean Square Error in this case) is used to indicate how far your predictions deviate from the target values. In the training phase, the weights are updated based on this quantity. If you are dealing with a classification problem, it is quite common to define an additional metric called accuracy. It monitors in how many cases the correct class was predicted. This is expressed as a percentage value. Consequently, a value of 0.0 means no correct decision and 1.0 only correct decisons.
While your network is training, the loss is decreasing and usually the accuracy increases. Note, that in contrast to loss, the accuracy is usally not used to update the parameters of your network. It helps to monitor the learning progress and the current performane of the network. @desertnaut has said it very clearly. Consider the following two pieces of code compile code binary_accuracy code Your labels should be integer，Because keras does not round y_true, and you get high accuracy.......I have a machine learning classification problem with 80% categorical variables. Must I use one hot encoding if I want to use some classifier for the classification? Can i pass the data to a classifier without the encoding?  I am trying to do the following for feature selection: I read the train file: I change the type of the categorical features to 'category': I use one hot encoding:  The problem is that the 3'rd part often get stuck, although I am using a strong machine. Thus, without the one hot encoding I can't do any feature selection, for determining the importance of the features. What do you recommend? Approach 1: You can use pandas' pd.get_dummies. Example 1: Example 2: The following will transform a given column into one hot. Use prefix to have multiple dummies. Approach 2: Use Scikit-learn Using a OneHotEncoder has the advantage of being able to fit on some training data and then transform on some other data using the same instance. We also have handle_unknown to further control what the encoder does with unseen data. Given a dataset with three features and four samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. Here is the link for this example: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html Much easier to use Pandas for basic one-hot encoding. If you're looking for more options you can use scikit-learn. For basic one-hot encoding with Pandas you pass your data frame into the get_dummies function. For example, if I have a dataframe called imdb_movies:  ...and I want to one-hot encode the Rated column, I do this:  This returns a new dataframe with a column for every "level" of rating that exists, along with either a 1 or 0 specifying the presence of that rating for a given observation. Usually, we want this to be part of the original dataframe. In this case, we attach our new dummy coded frame onto the original frame using "column-binding. We can column-bind by using Pandas concat function:  We can now run an analysis on our full dataframe. SIMPLE UTILITY FUNCTION I would recommend making yourself a utility function to do this quickly: Usage: Result:  Also, as per @pmalbu comment, if you would like the function to remove the original feature_to_encode then use this version: You can encode multiple features at the same time as follows: You can do it with numpy.eye and a using the array element selection mechanism: The the return value of indices_to_one_hot(nb_classes, data) is now The .reshape(-1) is there to make sure you have the right labels format (you might also have [[2], [3], [4], [0]]). One hot encoding with pandas is very easy: EDIT: Another way to one_hot using sklearn's LabelBinarizer : Firstly, easiest way to one hot encode: use Sklearn. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html Secondly, I don't think using pandas to one hot encode is that simple (unconfirmed though) Creating dummy variables in pandas for python Lastly, is it necessary for you to one hot encode? One hot encoding exponentially increases the number of features, drastically increasing the run time of any classifier or anything else you are going to run. Especially when each categorical feature has many levels. Instead you can do dummy coding. Using dummy encoding usually works well, for much less run time and complexity. A wise prof once told me, 'Less is More'.  Here's the code for my custom encoding function if you want. EDIT: Comparison to be clearer: One-hot encoding: convert n levels to n-1 columns. You can see how this will explode your memory if you have many different types (or levels) in your categorical feature. Keep in mind, this is just ONE column. Dummy Coding: Convert to numerical representations instead. Greatly saves feature space, at the cost of a bit of accuracy. You can use numpy.eye function. Result pandas as has inbuilt function "get_dummies" to get one hot encoding of that particular column/s. one line code for one-hot-encoding: Here is a solution using DictVectorizer and the Pandas DataFrame.to_dict('records') method. One-hot encoding requires bit more than converting the values to indicator variables. Typically ML process requires you to apply this coding several times to validation or test data sets and applying the model you construct to real-time observed data. You should store the mapping (transform) that was used to construct the model. A good solution would use the DictVectorizer or LabelEncoder (followed by get_dummies. Here is a function that you can use: This works on a pandas dataframe and for each column of the dataframe it creates and returns a mapping back. So you would call it like this: Then on the test data, the call is made by passing the dictionary returned back from training: An equivalent method is to use DictVectorizer. A related post on the same is on my blog. I mention it here since it provides some reasoning behind this approach over simply using get_dummies post  (disclosure: this is my own blog). You can pass the data to catboost classifier without encoding. Catboost handles categorical variables itself by performing one-hot and target expanding mean encoding. You can do the following as well. Note for the below you don't have to use pd.concat.  You can also change explicit columns to categorical. For example, here I am changing the Color and Group I know I'm late to this party, but the simplest way to hot encode a dataframe in an automated way is to use this function: This works for me: Output: I used this in my acoustic model:
probably this helps in ur model. Here is a function to do one-hot-encoding without using numpy, pandas, or other packages. It takes a list of integers, booleans, or strings (and perhaps other types too). Example: I know there are already a lot of answers to this question, but I noticed two things. First, most of the answers use packages like numpy and/or pandas. And this is a good thing. If you are writing production code, you should probably be using robust, fast algorithms like those provided in the numpy/pandas packages. But, for the sake of education, I think someone should provide an answer which has a transparent algorithm and not just an implementation of someone else's algorithm. Second, I noticed that many of the answers do not provide a robust implementation of one-hot encoding because they do not meet one of the requirements below. Below are some of the requirements (as I see them) for a useful, accurate, and robust one-hot encoding function: A one-hot encoding function must: I tested many of the answers to this question and most of them fail on one of the requirements above. Try this: df_encoded.head() The resulting dataframe df_train_encoded is the same as the original, but the categorical features are now replaced with their one-hot-encoded versions. More information on category_encoders here. To add to other questions, let me provide how I did it with a Python 2.0 function using Numpy:  The line n_values = np.max(y_) + 1 could be hard-coded for you to use the good number of neurons in case you use mini-batches for example.  Demo project/tutorial where this function has been used: 
https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition It can and it should be easy as : Usage : Expanding @Martin Thoma's answer Lets assume out of 10 variables, you have 3 categorical variables in your data frame named as cname1, cname2 and cname3.
Then following code will automatically create one hot encoded variable in the new dataframe. A simple example using vectorize in numpy and apply example in pandas: Here i tried with this approach :I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using train_test_split from sklearn.cross_validation, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data.  I know that a workaround would be to use train_test_split two times and somehow adjust the indices. But is there a more standard / built-in way to split the data into 3 sets instead of 2? Numpy solution. We will shuffle the whole dataset first (df.sample(frac=1, random_state=42)) and then split our data set into the following parts: [int(.6*len(df)), int(.8*len(df))] - is an indices_or_sections  array for numpy.split(). Here is a small demo for np.split() usage - let's split 20-elements array into the following parts: 80%, 10%, 10%: However, one approach to dividing the dataset into train, test, cv with 0.6, 0.2, 0.2 would be to use the train_test_split method twice. Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets.     Here is a Python function that splits a Pandas dataframe into train, validation, and test dataframes with stratified sampling. It performs this split by calling scikit-learn's function train_test_split() twice. Below is a complete working example. Consider a dataset that has a label upon which you want to perform the stratification. This label has its own distribution in the original dataset, say 75% foo, 15% bar and 10% baz. Now let's split the dataset into train, validation, and test into subsets using a 60/20/20 ratio, where each split retains the same distribution of the labels. See the illustration below:  Here is the example dataset: Now, let's call the split_stratified_into_train_val_test() function from above to get train, validation, and test dataframes following a 60/20/20 ratio. The three dataframes df_train, df_val, and df_test contain all the original rows but their sizes will follow the above ratio. Further, each of the three splits will have the same distribution of the label, namely 75% foo, 15% bar and 10% baz. In the case of supervised learning, you may want to split both X and y (where X is your input and y the ground truth output).
You just have to pay attention to shuffle X and y the same way before splitting. Here, either X and y are in the same dataframe, so we shuffle them,  separate them and apply the split for each (just like in chosen answer), or X and y are in two different dataframes, so we shuffle X, reorder y the same way as the shuffled X and apply the split to each. It is very convenient to use train_test_split without performing reindexing after dividing to several sets and not writing some additional code. Best answer above does not mention that by separating two times using train_test_split not changing partition sizes won`t give initially intended partition: Then the portion of validation and test sets in the x_remain change and could be counted as In this occasion all initial partitions are saved. Here we split data 2 times with sklearn's train_test_split Considering that df id your original dataframe: 1 - First you split data between Train and Test (10%): 2 - Then you split the train set between train and validation (20%): 3 - Then, you slice the original dataframe according to the indices generated in the steps above: The result is going to be like this:  Note: This soluctions uses the workaround mentioned in the question. Split the dataset in training and testing set as in the other answers, using Then, if you fit your model, you can add validation_split as a parameter. Then you do not need to create the validation set in advance. For example: The validation set is meant to serve as a representative on-the-run-testing-set during training of the training set, taken entirely from the training set, be it by k-fold cross-validation (recommended) or by validation_split; then you do not need to create a validation set separately and still you split a dataset into the three sets you are asking for. ANSWER FOR ANY AMOUNT OF SUB-SETS: This work for any size of percentage. In your case, you should do percentage = [train_percentage, val_percentage, test_percentage]. The easiest way that I could think of is mapping split fractions to array index as follows: where data = random.shuffle(data)This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I'm aware of the gradient descent and the back-propagation algorithm. What I don't get is: when is using a bias important and how do you use it? For example, when mapping the AND function, when I use two inputs and one output, it does not give the correct weights. However, when I use three inputs (one of which is a bias), it gives the correct weights. I think that biases are almost always helpful.  In effect, a bias value allows you to shift the activation function to the left or right, which may be critical for successful learning. It might help to look at a simple example.  Consider this 1-input, 1-output network that has no bias:  The output of the network is computed by multiplying the input (x) by the weight (w0) and passing the result through some kind of activation function (e.g. a sigmoid function.) Here is the function that this network computes, for various values of w0:  Changing the weight w0 essentially changes the "steepness" of the sigmoid.  That's useful, but what if you wanted the network to output 0 when x is 2?  Just changing the steepness of the sigmoid won't really work -- you want to be able to shift the entire curve to the right. That's exactly what the bias allows you to do.  If we add a bias to that network, like so:  ...then the output of the network becomes sig(w0*x + w1*1.0).  Here is what the output of the network looks like for various values of w1:  Having a weight of -5 for w1 shifts the curve to the right, which allows us to have a network that outputs 0 when x is 2. A simpler way to understand what the bias is: it is somehow similar to the constant b of a linear function y = ax + b It allows you to move the line up and down to fit the prediction with the data better. Without b, the line always goes through the origin (0, 0) and you may get a poorer fit. Here are some further illustrations showing the result of a simple 2-layer feed forward neural network with and without bias units on a two-variable regression problem. Weights are initialized randomly and standard ReLU activation is used. As the answers before me concluded, without the bias the ReLU-network is not able to deviate from zero at (0,0). 

 Two different kinds of parameters can
  be adjusted during the training of an
  ANN, the weights and the value in the
  activation functions. This is
  impractical and it would be easier if
  only one of the parameters should be
  adjusted. To cope with this problem a
  bias neuron is invented. The bias
  neuron lies in one layer, is connected
  to all the neurons in the next layer,
  but none in the previous layer and it
  always emits 1. Since the bias neuron
  emits 1 the weights, connected to the
  bias neuron, are added directly to the
  combined sum of the other weights
  (equation 2.1), just like the t value
  in the activation functions.1 The reason it's impractical is because you're simultaneously adjusting the weight and the value, so any change to the weight can neutralize the change to the value that was useful for a previous data instance... adding a bias neuron without a changing value allows you to control the behavior of the layer. Furthermore the bias allows you to use a single neural net to represent similar cases.  Consider the AND boolean function represented by the following neural network:   
(source: aihorizon.com)  A single perceptron can be used to
  represent many boolean functions.  For example, if we assume boolean values
  of 1 (true) and -1 (false), then one
  way to use a two-input perceptron to
  implement the AND function is to set
  the weights w0 = -3, and w1 = w2 = .5.
  This perceptron can be made to
  represent the OR function instead by
  altering the threshold to w0 = -.3. In
  fact, AND and OR can be viewed as
  special cases of m-of-n functions:
  that is, functions where at least m of
  the n inputs to the perceptron must be
  true. The OR function corresponds to
  m = 1 and the AND function to m = n.
  Any m-of-n function is easily
  represented using a perceptron by
  setting all input weights to the same
  value (e.g., 0.5) and then setting the
  threshold w0 accordingly.  Perceptrons can represent all of the
  primitive boolean functions AND, OR,
  NAND ( 1 AND), and NOR ( 1 OR). Machine Learning- Tom Mitchell) The threshold is the bias and w0 is the weight associated with the bias/threshold neuron. The bias is not an NN term. It's a generic algebra term to consider. Y = M*X + C (straight line equation) Now if C(Bias) = 0 then, the line will always pass through the origin, i.e. (0,0), and depends on only one parameter, i.e. M, which is the slope so we have less things to play with. C, which is the bias takes any number and has the activity to shift the graph, and hence able to represent more complex situations. In a logistic regression, the expected value of the target is transformed by a link function to restrict its value to the unit interval. In this way, model predictions can be viewed as primary outcome probabilities as shown: Sigmoid function on Wikipedia This is the final activation layer in the NN map that turns on and off the neuron. Here also bias has a role to play and it shifts the curve flexibly to help us map the model. A layer in a neural network without a bias is nothing more than the multiplication of an input vector with a matrix. (The output vector might be passed through a sigmoid function for normalisation and for use in multi-layered ANN afterwards, but that’s not important.) This means that you’re using a linear function and thus an input of all zeros will always be mapped to an output of all zeros. This might be a reasonable solution for some systems but in general it is too restrictive. Using a bias, you’re effectively adding another dimension to your input space, which always takes the value one, so you’re avoiding an input vector of all zeros. You don’t lose any generality by this because your trained weight matrix needs not be surjective, so it still can map to all values previously possible. 2D ANN: For a ANN mapping two dimensions to one dimension, as in reproducing the AND or the OR (or XOR) functions, you can think of a neuronal network as doing the following: On the 2D plane mark all positions of input vectors. So, for boolean values, you’d want to mark (-1,-1), (1,1), (-1,1), (1,-1). What your ANN now does is drawing a straight line on the 2d plane, separating the positive output from the negative output values. Without bias, this straight line has to go through zero, whereas with bias, you’re free to put it anywhere.
So, you’ll see that without bias you’re facing a problem with the AND function, since you can’t put both (1,-1) and (-1,1) to the negative side. (They are not allowed to be on the line.) The problem is equal for the OR function. With a bias, however, it’s easy to draw the line. Note that the XOR function in that situation can’t be solved even with bias. When you use ANNs, you rarely know about the internals of the systems you want to learn. Some things cannot be learned without a bias. E.g., have a look at the following data: (0, 1), (1, 1), (2, 1), basically a function that maps any x to 1.  If you have a one layered network (or a linear mapping), you cannot find a solution. However, if you have a bias it's trivial! In an ideal setting, a bias could also map all points to the mean of the target points and let the hidden neurons model the differences from that point. Modification of neuron WEIGHTS alone only serves to manipulate the shape/curvature of your transfer function, and not its equilibrium/zero crossing point. The introduction of bias neurons allows you to shift the transfer function curve horizontally (left/right) along the input axis while leaving the shape/curvature unaltered.
This will allow the network to produce arbitrary outputs different from the defaults and hence you can customize/shift the input-to-output mapping to suit your particular needs. See here for graphical explanation:
http://www.heatonresearch.com/wiki/Bias In a couple of experiments in my masters thesis (e.g. page 59), I found that the bias might be important for the first layer(s), but especially at the fully connected layers at the end it seems not to play a big role. This might be highly dependent on the network architecture / dataset. If you're working with images, you might actually prefer to not use a bias at all. In theory, that way your network will be more independent of data magnitude, as in whether the picture is dark, or bright and vivid. And the net is going to learn to do it's job through studying relativity inside your data. Lots of modern neural networks utilize this. For other data having biases might be critical. It depends on what type of data you're dealing with. If your information is magnitude-invariant --- if inputting [1,0,0.1] should lead to the same result as if inputting [100,0,10], you might be better off without a bias. Bias determines how much angle your weight will rotate. In a two-dimensional chart, weight and bias can help us to find the decision boundary of outputs. Say we need to build a AND function, the input(p)-output(t) pair should be {p=[0,0], t=0},{p=[1,0], t=0},{p=[0,1], t=0},{p=[1,1], t=1}  Now we need to find a decision boundary, and the ideal boundary should be:  See? W is perpendicular to our boundary. Thus, we say W decided the direction of boundary. However, it is hard to find correct W at first time. Mostly, we choose original W value randomly. Thus, the first boundary may be this:
 Now the boundary is parallel to the y axis. We want to rotate the boundary. How? By changing the W. So, we use the learning rule function: W'=W+P:  W'=W+P  is equivalent to W' = W + bP, while b=1. Therefore, by changing the value of b(bias), you can decide the angle between W' and W. That is "the learning rule of ANN". You could also read Neural Network Design by Martin T. Hagan / Howard B. Demuth / Mark H. Beale, chapter 4 "Perceptron Learning Rule" In simpler terms, biases allow for more and more variations of weights to be learnt/stored... (side-note: sometimes given some threshold). Anyway, more variations mean that biases add richer representation of the input space to the model's learnt/stored weights. (Where better weights can enhance the neural net’s guessing power) For example, in learning models, the hypothesis/guess is desirably bounded by y=0 or y=1 given some input, in maybe some classification task... i.e some y=0 for some x=(1,1) and some y=1 for some x=(0,1). (The condition on the hypothesis/outcome is the threshold I talked about above. Note that my examples setup inputs X to be each x=a double or 2 valued-vector, instead of Nate's single valued x inputs of some collection X). If we ignore the bias, many inputs may end up being represented by a lot of the same weights (i.e. the learnt weights mostly occur close to the origin (0,0).
The model would then be limited to poorer quantities of good weights, instead of the many many more good weights it could better learn with bias. (Where poorly learnt weights lead to poorer guesses or a decrease in the neural net’s guessing power) So, it is optimal that the model learns both close to the origin, but also, in as many places as possible inside the threshold/decision boundary. With the bias we can enable degrees of freedom close to the origin, but not limited to origin's immediate region. In neural networks: In absence of bias, the neuron may not be activated by considering only the weighted sum from the input layer. If the neuron is not activated, the information from this neuron is not passed through rest of the neural network. The value of bias is learnable.  Effectively, bias = — threshold. You can think of bias as how easy it is to get the neuron to output a 1 — with a really big bias, it’s very easy for the neuron to output a 1, but if the bias is very negative, then it’s difficult. In summary: bias helps in controlling the value at which the activation function will trigger. Follow this video for more details. Few more useful links: geeksforgeeks towardsdatascience Expanding on zfy's explanation: The equation for one input, one neuron, one output should look: where x is the value from the input node and 1 is the value of the bias node;
y can be directly your output or be passed into a function, often a sigmoid function. Also note that the bias could be any constant, but to make everything simpler we always pick 1 (and probably that's so common that zfy did it without showing & explaining it). Your network is trying to learn coefficients a and b to adapt to your data.
So you can see why adding the element b * 1 allows it to fit better to more data: now you can change both slope and intercept. If you have more than one input your equation will look like: Note that the equation still describes a one neuron, one output network; if you have more neurons you just add one dimension to the coefficient matrix, to multiplex the inputs to all nodes and sum back each node contribution. That you can write in vectorized format as i.e. putting coefficients in one array and (inputs + bias) in another you have your desired solution as the dot product of the two vectors (you need to transpose X for the shape to be correct, I wrote XT a 'X transposed') So in the end you can also see your bias as is just one more input to represent the part of the output that is actually independent of your input. To think in a simple way, if you have y=w1*x where y is your output and w1 is the weight, imagine a condition where x=0 then y=w1*x equals to 0. If you want to update your weight you have to compute how much change by delw=target-y where target is your target output. In this case 'delw' will not change since y is computed as 0. So, suppose if you can add some extra value it will help y = w1x + w01, where bias=1 and weight can be adjusted to get a correct bias. Consider the example below. In terms of line slope, intercept is a specific form of linear equations. y = mx + b Check the image image Here b is (0,2) If you want to increase it to (0,3) how will you do it by changing the value of b the bias. For all the ML books I studied, the W is always defined as the connectivity index between two neurons, which means the higher connectivity between two neurons. The stronger the signals will be transmitted from the firing neuron to the target neuron or Y = w * X as a result to maintain the biological character of neurons, we need to keep the 1 >=W >= -1, but in the real regression, the W will end up with |W| >=1 which contradicts how the neurons are working. As a result, I propose W = cos(theta), while 1 >= |cos(theta)|, and Y= a * X = W * X + b while a = b + W = b + cos(theta), b is an integer. Bias acts as our anchor. It's a way for us to have some kind of baseline where we don't go below that. In terms of a graph, think of like y=mx+b it's like a y-intercept of this function. output = input times the weight value and added a bias value and then apply an activation function. The term bias is used to adjust the final output matrix as the y-intercept does. For instance, in the classic equation, y = mx + c, if c = 0, then the line will always pass through 0. Adding the bias term provides more flexibility and better generalisation to our neural network model. The bias helps to get a better equation. Imagine the input and output like a function y = ax + b and you need to put the right line between the input(x) and output(y) to minimise the global error between each point and the line, if you keep the equation like this y = ax, you will have one parameter for adaptation only, even if you find the best a minimising the global error it will be kind of far from the wanted value. You can say the bias makes the equation more flexible to adapt to the best valuesGiven a 1D array of indices: I want to one-hot encode this as a 2D array: Create a zeroed array b with enough columns, i.e. a.max() + 1.
Then, for each row i, set the a[i]th column to 1. In case you are using keras, there is a built in utility for that: And it does pretty much the same as @YXD's answer (see source-code). Here is what I find useful: Here num_classes stands for number of classes you have. So if you have a vector with shape of (10000,) this function transforms it to (10000,C). Note that a is zero-indexed, i.e. one_hot(np.array([0, 1]), 2) will give [[1, 0], [0, 1]]. Exactly what you wanted to have I believe. PS: the source is Sequence models - deeplearning.ai You can also use eye function of numpy: numpy.eye(number of classes)[vector containing the labels] You can use  sklearn.preprocessing.LabelBinarizer: Example: output: Amongst other things, you may initialize sklearn.preprocessing.LabelBinarizer() so that the output of transform is sparse. For 1-hot-encoding For Example ENJOY CODING You can use the following code for converting into a one-hot vector: let x is the normal class vector having a single column with classes 0 to some number: if 0 is not a class; then remove +1. Here is a function that converts a 1-D vector to a 2-D one-hot array. Below is some example usage: I think the short answer is no. For a more generic case in n dimensions, I came up with this: I am wondering if there is a better solution -- I don't like that I have to create those lists in the last two lines. Anyway, I did some measurements with timeit and it seems that the numpy-based (indices/arange) and the iterative versions perform about the same.  Just to elaborate on the excellent answer from K3---rnc, here is a more generic version: Also, here is a quick-and-dirty benchmark of this method and a method from the currently accepted answer by YXD (slightly changed, so that they offer the same API except that the latter works only with 1D ndarrays): The latter method is ~35% faster (MacBook Pro 13 2015), but the former is more general: If using tensorflow, there is one_hot(): I recently ran into a problem of same kind and found said solution which turned out to be only satisfying if you have numbers that go within a certain formation. For example if you want to one-hot encode following list: go ahead, the posted solutions are already mentioned above. But what if considering this data: If you do it with methods mentioned above, you will likely end up with 90 one-hot columns. This is because all answers include something like n = np.max(a)+1. I found a more generic solution that worked out for me and wanted to share with you: I hope someone encountered same restrictions on above solutions and this might come in handy Such type of encoding are usually part of numpy array. If you are using a numpy array like this : then there is very simple way to convert that to 1-hot encoding That's it. clean and easy solution: I find the easiest solution combines np.take and np.eye works for x of any shape. Here is an example function that I wrote to do this based upon the answers above and my own use case: I am adding for completion a simple function, using only numpy operators:  It takes as input a probability matrix: e.g.:  [[0.03038822 0.65810204 0.16549407 0.3797123 ] 
   ...
  [0.02771272 0.2760752  0.3280924  0.33458805]]   And it will return [[0 1 0 0]  ...  [0 0 0 1]] Here's a dimensionality-independent standalone solution. This will convert any N-dimensional array arr of nonnegative integers to a one-hot N+1-dimensional array one_hot, where one_hot[i_1,...,i_N,c] = 1 means arr[i_1,...,i_N] = c. You can recover the input via np.argmax(one_hot, -1) Use the following code. It works best. Found it here P.S You don't need to go into the link. Link to documentation: neuraxle.steps.numpy.OneHotEncoderSo I've been following Google's official tensorflow guide and trying to build a simple neural network using Keras. But when it comes to training the model, it does not use the entire dataset (with 60000 entries) and instead uses only 1875 entries for training. Any possible fix? Output: Here's the original google colab notebook where I've been working on this: https://colab.research.google.com/drive/1NdtzXHEpiNnelcMaJeEm6zmp34JMcN38 The number 1875 shown during fitting the model is not the training samples; it is the number of batches. model.fit includes an optional argument batch_size, which, according to the documentation: If unspecified, batch_size will default to 32. So, what happens here is - you fit with the default batch size of 32 (since you have not specified anything different), so the total number of batches for your data is It does not train on 1875 samples. 1875 here is the number of steps, not samples. In fit method, there is an argument, batch_size. The default value for it is 32. So 1875*32=60000. The implementation is correct. If you train it with batch_size=16, you will see the number of steps will be 3750 instead of 1875, since 60000/16=3750. Just use batch_size = 1, if you want the entire 60000 data samples to be visible.In the tensorflow API docs they use a keyword called logits. What is it? A lot of methods are written like: If logits is just a generic Tensor input, why is it named logits? Secondly, what is the difference between the following two methods? I know what tf.nn.softmax does, but not the other. An example would be really helpful. The softmax+logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear.  It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities (you might have an input of 5). Internally, it first applies softmax to the unscaled output, and then and then computes the cross entropy of those values vs. what they "should" be as defined by the labels. tf.nn.softmax produces the result of applying the softmax function to an input tensor.  The softmax "squishes" the inputs so that sum(input) = 1, and it does the mapping by interpreting the inputs as log-probabilities (logits) and then converting them back into raw probabilities between 0 and 1.  The shape of output of a softmax is the same as the input: See this answer for more about why softmax is used extensively in DNNs. tf.nn.softmax_cross_entropy_with_logits combines the softmax step with the calculation of the cross-entropy loss after applying the softmax function, but it does it all together in a more mathematically careful way.  It's similar to the result of: The cross entropy is a summary metric: it sums across the elements.  The output of tf.nn.softmax_cross_entropy_with_logits on a shape [2,5] tensor is of shape [2,1] (the first dimension is treated as the batch). If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.  Otherwise, you'll end up hacking it by adding little epsilons here and there. Edited 2016-02-07:
If you have single-class labels, where an object can only belong to one class, you might now  consider using tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.  This function was added after release 0.6.0. Short version: Suppose you have two tensors, where y_hat contains computed scores for each class (for example, from y = W*x +b) and y_true contains one-hot encoded true labels.  If you interpret the scores in y_hat as unnormalized log probabilities, then they are logits. Additionally, the total cross-entropy loss computed in this manner: is essentially equivalent to the total cross-entropy loss computed with the function softmax_cross_entropy_with_logits(): Long version: In the output layer of your neural network, you will probably compute an array that contains the class scores for each of your training instances, such as from a computation y_hat = W*x + b. To serve as an example, below I've created a y_hat as a 2 x 3 array, where the rows correspond to the training instances and the columns correspond to classes. So here there are 2 training instances and 3 classes. Note that the values are not normalized (i.e. the rows don't add up to 1). In order to normalize them, we can apply the softmax function, which interprets the input as unnormalized log probabilities (aka logits) and outputs normalized linear probabilities.  It's important to fully understand what the softmax output is saying. Below I've shown a table that more clearly represents the output above. It can be seen that, for example, the probability of training instance 1 being "Class 2" is 0.619. The class probabilities for each training instance are normalized, so the sum of each row is 1.0. So now we have class probabilities for each training instance, where we can take the argmax() of each row to generate a final classification. From above, we may generate that training instance 1 belongs to "Class 2" and training instance 2 belongs to "Class 1".  Are these classifications correct? We need to measure against the true labels from the training set. You will need a one-hot encoded y_true array, where again the rows are training instances and columns are classes. Below I've created an example y_true one-hot array where the true label for training instance 1 is "Class 2" and the true label for training instance 2 is "Class 3". Is the probability distribution in y_hat_softmax close to the probability distribution in y_true? We can use cross-entropy loss to measure the error.  We can compute the cross-entropy loss on a row-wise basis and see the results. Below we can see that training instance 1 has a loss of 0.479, while training instance 2 has a higher loss of 1.200. This result makes sense because in our example above, y_hat_softmax showed that training instance 1's highest probability was for "Class 2", which matches training instance 1 in y_true; however, the prediction for training instance 2 showed a highest probability for "Class 1", which does not match the true class "Class 3". What we really want is the total loss over all the training instances. So we can compute: Using softmax_cross_entropy_with_logits() We can instead compute the total cross entropy loss using the tf.nn.softmax_cross_entropy_with_logits() function, as shown below.  Note that total_loss_1 and total_loss_2 produce essentially equivalent results with some small differences in the very final digits. However, you might as well use the second approach: it takes one less line of code and accumulates less numerical error because the softmax is done for you inside of softmax_cross_entropy_with_logits(). tf.nn.softmax computes the forward propagation through a softmax layer. You use it during evaluation of the model when you compute the probabilities that the model outputs. tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer. It is only used during training.  The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them). When we wish to constrain an output between 0 and 1, but our model architecture outputs unconstrained values, we can add a normalisation layer to enforce this. A common choice is a sigmoid function.1 In binary classification this is typically the logistic function, and in multi-class tasks the multinomial logistic function (a.k.a softmax).2 If we want to interpret the outputs of our new final layer as 'probabilities', then (by implication) the unconstrained inputs to our sigmoid must be inverse-sigmoid(probabilities). In the logistic case this is equivalent to the log-odds of our probability (i.e. the log of the odds) a.k.a. logit: That is why the arguments to softmax is called logits in Tensorflow - because under the assumption that softmax is the final layer in the model, and the output p is interpreted as a probability, the input x to this layer is interpretable as a logit: In Machine Learning there is a propensity to generalise terminology borrowed from maths/stats/computer science, hence in Tensorflow logit (by analogy) is used as a synonym for the input to many normalisation functions. Above answers have enough description for the asked question. Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions. Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy() You can find prominent difference between them in a resource intensive model. Tensorflow 2.0 Compatible Answer: The explanations of dga and stackoverflowuser2010 are very detailed about Logits and the related Functions.  All those functions, when used in Tensorflow 1.x will work fine, but if you migrate your code from 1.x (1.14, 1.15, etc) to 2.x (2.0, 2.1, etc..), using those functions result in error. Hence, specifying the 2.0 Compatible Calls for all the functions, we discussed above, if we migrate from 1.x to 2.x, for the benefit of the community. Functions in 1.x: Respective Functions when Migrated from 1.x  to 2.x: For more information about migration from 1.x to 2.x, please refer this Migration Guide. One more thing that I would definitely like to highlight as logit is just a raw output, generally the output of last layer. This can be a negative value as well. If we use it as it's for "cross entropy" evaluation as mentioned below: then it wont work. As log of -ve is not defined.
So using o softmax activation, will overcome this problem. This is my understanding, please correct me if Im wrong. Logits are the unnormalized outputs of a neural network. Softmax is a normalization function that squashes the outputs of a neural network so that they are all between 0 and 1 and sum to 1. Softmax_cross_entropy_with_logits is a loss function that takes in the outputs of a neural network (after they have been squashed by softmax) and the true labels for those outputs, and returns a loss value.I'm trying to train a CNN to categorize text by topic. When I use binary cross-entropy I get ~80% accuracy, with categorical cross-entropy I get ~50% accuracy. I don't understand why this is. It's a multiclass problem, doesn't that mean that I have to use categorical cross-entropy and that the results with binary cross-entropy are meaningless? Then I compile it either it like this using categorical_crossentropy as the loss function: or  Intuitively it makes sense why I'd want to use categorical cross-entropy, I don't understand why I get good results with binary, and poor results with categorical. The reason for this apparent performance discrepancy between categorical & binary cross entropy is what user xtof54 has already reported in his answer below, i.e.: the accuracy computed with the Keras method evaluate is just plain
wrong when using binary_crossentropy with more than 2 labels I would like to elaborate more on this, demonstrate the actual underlying issue, explain it, and offer a remedy. This behavior is not a bug; the underlying reason is a rather subtle & undocumented issue at how Keras actually guesses which accuracy to use, depending on the loss function you have selected, when you include simply metrics=['accuracy'] in your model compilation. In other words, while your first compilation option is valid, your second one: will not produce what you expect, but the reason is not the use of binary cross entropy (which, at least in principle, is an absolutely valid loss function). Why is that? If you check the metrics source code, Keras does not define a single accuracy metric, but several different ones, among them binary_accuracy and categorical_accuracy. What happens under the hood is that, since you have selected binary cross entropy as your loss function and have not specified a particular accuracy metric, Keras (wrongly...) infers that you are interested in the binary_accuracy, and this is what it returns - while in fact you are interested in the categorical_accuracy. Let's verify that this is the case, using the MNIST CNN example in Keras, with the following modification: To remedy this, i.e. to use indeed binary cross entropy as your loss function (as I said, nothing wrong with this, at least in principle) while still getting the categorical accuracy required by the problem at hand, you should ask explicitly for categorical_accuracy in the model compilation as follows: In the MNIST example, after training, scoring, and predicting the test set as I show above, the two metrics now are the same, as they should be: System setup: UPDATE: After my post, I discovered that this issue had already been identified in this answer. It all depends on the type of classification problem you are dealing with. There are three main categories In the first case, binary cross-entropy should be used and targets should be encoded as one-hot vectors. In the second case, categorical cross-entropy should be used and targets should be encoded as one-hot vectors. In the last case, binary cross-entropy should be used and targets should be encoded as one-hot vectors. Each output neuron (or unit) is considered as a separate random binary variable, and the loss for the entire vector of outputs is the product of the loss of single binary variables. Therefore it is the product of binary cross-entropy for each single output unit. The binary cross-entropy is defined as  and categorical cross-entropy is defined as  where c is the index running over the number of classes C. I came across an "inverted" issue — I was getting good results with categorical_crossentropy (with 2 classes) and poor with binary_crossentropy. It seems that problem was with wrong activation function. The correct settings were: It's really interesting case. Actually in your setup the following statement is true: This means that up to a constant multiplication factor your losses are equivalent. The weird behaviour that you are observing during a training phase might be an example of a following phenomenon: That's why this constant factor might help in case of binary_crossentropy. After many epochs - the learning rate value is greater than in categorical_crossentropy case. I usually restart training (and learning phase) a few times when I notice such behaviour or/and adjusting a class weights using the following pattern: This makes loss from a less frequent classes balancing the influence of a dominant class loss at the beginning of a training and in a further part of an optimization process. EDIT: Actually - I checked that even though in case of maths: should hold - in case of keras it's not true, because keras is automatically normalizing all outputs to sum up to 1. This is the actual reason behind this weird behaviour as in case of multiclassification such normalization harms a training. After commenting @Marcin answer, I have more carefully checked one of my students code where I found the same weird behavior, even after only 2 epochs ! (So @Marcin's explanation was not very likely in my case). And I found that the answer is actually very simple: the accuracy computed with the Keras method evaluate is just plain wrong when using binary_crossentropy with more than 2 labels. You can check that by recomputing the accuracy yourself (first call the Keras method "predict" and then compute the number of correct answers returned by predict): you get the true accuracy, which is much lower than the Keras "evaluate" one. a simple example under a multi-class setting to illustrate suppose you have 4 classes (onehot encoded) and below is just one prediction true_label = [0,1,0,0]
predicted_label = [0,0,1,0] when using categorical_crossentropy, the accuracy is just 0 , it only cares about if you get the concerned class right. however when using binary_crossentropy, the accuracy is calculated for all classes, it would be 50% for this prediction. and the final result will be the mean of the individual accuracies for both cases. it is recommended to use categorical_crossentropy for multi-class(classes are mutually exclusive) problem but binary_crossentropy for multi-label problem. As it is a multi-class problem, you have to use the categorical_crossentropy, the binary cross entropy will produce bogus results, most likely will only evaluate the first two classes only. 50% for a multi-class problem can be quite good, depending on the number of classes. If you have n classes, then 100/n is the minimum performance you can get by outputting a random class. You are passing a target array of shape (x-dim, y-dim) while using as loss categorical_crossentropy. categorical_crossentropy expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via: Alternatively, you can use the loss function sparse_categorical_crossentropy instead, which does expect integer targets. when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). Take a look at the equation you can find that binary cross entropy not only punish those label = 1, predicted =0, but also label = 0, predicted = 1. However categorical cross entropy only punish those label = 1 but predicted = 1.That's why we make assumption that there is only ONE label positive. The main point is answered satisfactorily with the brilliant piece of sleuthing by desernaut. However there are occasions when BCE (binary cross entropy) could throw different results than CCE (categorical cross entropy) and may be the preferred choice. While the thumb rules shared above (which loss to select) work fine for 99% of the cases, I would like to add a few new dimensions to this discussion. The OP had a softmax activation and this throws a probability distribution as the predicted value. It is a multi-class problem. The preferred loss is categorical CE. Essentially this boils down to -ln(p) where 'p' is the predicted probability of the lone positive class in the sample. This means that the negative predictions dont have a role to play in calculating CE. This is by intention. On a rare occasion, it may be needed to make the -ve voices count. This can be done by treating the above sample as a series of binary predictions. So if expected is [1 0 0 0 0] and predicted is [0.1 0.5 0.1 0.1 0.2], this is further broken down into: Now we proceed to compute 5 different cross entropies - one for each of the above 5 expected/predicted combo and sum them up. Then: The CE has a different scale but continues to be a measure of the difference between the expected and predicted values. The only difference is that in this scheme, the -ve values are also penalized/rewarded along with the +ve values. In case your problem is such that you are going to use the output probabilities (both +ve and -ves) instead of using the max() to predict just the 1 +ve label, then you may want to consider this version of CE. How about a multi-label situation where expected = [1 0 0 0 1]? Conventional approach is to use one sigmoid per output neuron instead of an overall softmax. This ensures that the output probabilities are independent of each other. So we get something like: By definition, CE measures the difference between 2 probability distributions. But the above two lists are not probability distributions. Probability distributions should always add up to 1. So conventional solution is to use same loss approach as before - break the expected and predicted values into 5 individual probability distributions, proceed to calculate 5 cross entropies and sum them up. Then: The challenge happens when the number of classes may be very high - say a 1000 and there may be only couple of them present in each sample. So the expected is something like: [1,0,0,0,0,0,1,0,0,0.....990 zeroes]. The predicted could be something like: [.8, .1, .1, .1, .1, .1, .8, .1, .1, .1.....990 0.1's] In this case the CE = You can see how the -ve classes are beginning to create a nuisance value when calculating the loss. The voice of the +ve samples (which may be all that we care about) is getting drowned out. What do we do? We can't use categorical CE (the version where only +ve samples are considered in calculation). This is because, we are forced to break up the probability distributions into multiple binary probability distributions because otherwise it would not be a probability distribution in the first place. Once we break it into multiple binary probability distributions, we have no choice but to use binary CE and this of course gives weightage to -ve classes. One option is to drown the voice of the -ve classes by a multiplier. So we multiply all -ve losses by a value gamma where gamma < 1. Say in above case, gamma can be .0001. Now the loss comes to: The nuisance value has come down. 2 years back Facebook did that and much more in a paper they came up with where they also multiplied the -ve losses by p to the power of x. 'p' is the probability of the output being a +ve and x is a constant>1. This penalized -ve losses even further especially the ones where the model is pretty confident (where 1-p is close to 1). This combined effect of punishing negative class losses combined with harsher punishment for the easily classified cases (which accounted for majority of the -ve cases) worked beautifully for Facebook and they called it focal loss. So in response to OP's question of whether binary CE makes any sense at all in his case, the answer is - it depends. In 99% of the cases the conventional thumb rules work but there could be occasions when these rules could be bent or even broken to suit the problem at hand. For a more in-depth treatment, you can refer to: https://towardsdatascience.com/cross-entropy-classification-losses-no-math-few-stories-lots-of-intuition-d56f8c7f06b0 The binary_crossentropy(y_target, y_predict) doesn't need to apply to binary classification problem. In the source code of binary_crossentropy(), the nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) of tensorflow was actually used. And, in the documentation, it says that: Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.Can I extract the underlying decision-rules (or 'decision paths') from a trained tree in a decision tree as a textual list? Something like: I believe that this answer is more correct than the other answers here: This prints out a valid Python function. Here's an example output for a tree that is trying to return its input, a number between 0 and 10. Here are some stumbling blocks that I see in other answers: I created my own function to extract the rules from the decision trees created by sklearn: This function first starts with the nodes (identified by -1 in the child arrays) and then recursively finds the parents. I call this a node's 'lineage'.  Along the way, I grab the values I need to create if/then/else SAS logic: The sets of tuples below contain everything I need to create SAS if/then/else statements. I do not like using do blocks in SAS which is why I create logic describing a node's entire path. The single integer after the tuples is the ID of the terminal node in a path. All of the preceding tuples combine to create that node.  Scikit learn introduced a delicious new method called export_text in version 0.21 (May 2019) to extract the rules from a tree. Documentation here. It's no longer necessary to create a custom function. Once you've fit your model, you just need two lines of code. First, import export_text: Second, create an object that will contain your rules. To make the rules look more readable, use the feature_names argument and pass a list of your feature names. For example, if your model is called model and your features are named in a dataframe called X_train, you could create an object called tree_rules: Then just print or save tree_rules. Your output will look like this: I modified the code submitted by Zelazny7 to print some pseudocode: if you call get_code(dt, df.columns) on the same example you will obtain: There is a new DecisionTreeClassifier method, decision_path, in the 0.18.0 release.  The developers provide an extensive (well-documented) walkthrough. The first section of code in the walkthrough that prints the tree structure seems to be OK.  However, I modified the code in the second section to interrogate one sample.  My changes denoted with # <-- Edit The changes marked by # <-- in the code below have since been updated in walkthrough link after the errors were pointed out in pull requests #8653 and #10951. It's much easier to follow along now.  Change the sample_id to see the decision paths for other samples.  I haven't asked the developers about these changes, just seemed more intuitive when working through the example. You can see a digraph Tree. Then, clf.tree_.feature and clf.tree_.value are array of nodes splitting feature and array of nodes values respectively. You can refer to more details from this github source.  I needed a more human-friendly format of rules from the Decision Tree. I'm building open-source AutoML Python package and many times MLJAR users want to see the exact rules from the tree. That's why I implemented a function based on paulkernfeld answer. The rules are sorted by the number of training samples assigned to each rule. For each rule, there is information about the predicted class name and probability of prediction for classification tasks. For the regression task, only information about the predicted value is printed. The printed rules: I've summarized the ways to extract rules from the Decision Tree in my article: Extract Rules from Decision Tree in 3 Ways with Scikit-Learn and Python. This is the code you need I have modified the top liked code to indent in a jupyter notebook python 3 correctly Just because everyone was so helpful I'll just add a modification to Zelazny7 and Daniele's beautiful solutions. This one is for python 2.7, with tabs to make it more readable: I've been going through this, but i needed the rules to be written in this format  So I adapted the answer of @paulkernfeld (thanks) that you can customize to your need Now you can use export_text. A complete example from [sklearn][1] Codes below is my approach under anaconda python 2.7 plus a package name "pydot-ng" to making a PDF file with decision rules. I hope it is helpful. a tree graphy show here Here is a way to translate the whole tree into a single (not necessarily too human-readable) python expression using the SKompiler library: This builds on @paulkernfeld 's answer. If you have a dataframe X with your features and a target dataframe y with your resonses and you you want to get an idea which y value ended in which node (and also ant to plot it accordingly) you can do the following: not the most elegant version but it does the job...   Here is a function, printing rules of a scikit-learn decision tree under python 3 and with offsets for conditional blocks to make the structure more readable: You can also make it more informative by distinguishing it to which class it belongs or even by mentioning its output value.  Here is my approach to extract the decision rules in a form that can be used in directly in sql, so the data can be grouped by node. (Based on the approaches of previous posters.)  The result will be subsequent CASE clauses that can be copied to an sql statement, ex. SELECT COALESCE(*CASE WHEN <conditions> THEN > <NodeA>*, > *CASE WHEN
 <conditions> THEN <NodeB>*, > ....)NodeName,* > FROM <table or view> Modified Zelazny7's code to fetch SQL from the decision tree. Apparently a long time ago somebody already decided to try to add the following function to the official scikit's tree export functions (which basically only supports export_graphviz) Here is his full commit: https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py Not exactly sure what happened to this comment. But you could also try to use that function. I think this warrants a serious documentation request to the good people of scikit-learn to properly document the sklearn.tree.Tree API which is the underlying tree structure that DecisionTreeClassifier exposes as its attribute tree_. Just use the function from sklearn.tree like this And then look in your project folder for the file tree.dot, copy the ALL the content and paste it here http://www.webgraphviz.com/ and generate your graph :) Thank for the wonderful solution of @paulkerfeld. On top of his solution, for all those who want to have a serialized version of trees, just use tree.threshold, tree.children_left, tree.children_right, tree.feature and tree.value. Since the leaves don't have splits and hence no feature names and children, their placeholder in tree.feature and tree.children_*** are _tree.TREE_UNDEFINED and _tree.TREE_LEAF. Every split is assigned a unique index by depth first search. 
 Notice that the tree.value is of shape [n, 1, 1] Here is a function that generates Python code from a decision tree by converting the output of export_text: Sample usage: Sample output: The above example is generated with names = ['f'+str(j+1) for j in range(NUM_FEATURES)]. One handy feature is that it can generate smaller file size with reduced spacing. Just set spacing=2. From this answer, you get a readable and efficient representation: https://stackoverflow.com/a/65939892/3746632 Output looks like this. X is 1d vector to represent a single instance's features.I am relatively new to machine learning/python/ubuntu. I have a set of images in .jpg format where half contain a feature I want caffe to learn and half don't. I'm having trouble in finding a way to convert them to the required lmdb format. I have the necessary text input files.  My question is can anyone provide a step by step guide on how to use convert_imageset.cpp in the ubuntu terminal? Thanks First thing you must do is build caffe and caffe's tools (convert_imageset is one of these tools).
After installing caffe and makeing it make sure you ran make tools as well.
Verify that a binary file convert_imageset is created in $CAFFE_ROOT/build/tools. Images: put all images in a folder (I'll call it here /path/to/jpegs/).
Labels: create a text file (e.g., /path/to/labels/train.txt) with a line per input image  . For example:   img_0000.jpeg 1
  img_0001.jpeg 0
  img_0002.jpeg 0   In this example the first image is labeled 1 while the other two are labeled 0.  Run the binary in shell Command line explained:   Other flags that might be useful: You can check out $CAFFE_ROOT/examples/imagenet/convert_imagenet.sh
for an example how to use convert_imageset.This is my test code: The output is: But What Happend? The documentation says:  Note: if the input to the layer has a rank greater than 2, then it is
  flattened prior to the initial dot product with kernel. While the output is reshaped? Currently, contrary to what has been stated in documentation, the Dense layer is applied on the last axis of input tensor: Contrary to the documentation, we don't actually flatten it. It's
  applied on the last axis independently. In other words, if a Dense layer with m units is applied on an input tensor of shape (n_dim1, n_dim2, ..., n_dimk) it would have an output shape of (n_dim1, n_dim2, ..., m). As a side note: this makes TimeDistributed(Dense(...)) and Dense(...) equivalent to each other. Another side note: be aware that this has the effect of shared weights. For example, consider this toy network: The model summary: As you can see the Dense layer has only 60 parameters. How? Each unit in the Dense layer is connected to the 5 elements of each row in the input with the same weights, therefore 10 * 5 + 10 (bias params per unit) = 60. Update. Here is a visual illustration of the example above:This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I've been developing an internal website for a portfolio management tool.  There is a lot of text data, company names etc.  I've been really impressed with some search engines ability to very quickly respond to queries with "Did you mean: xxxx". I need to be able to intelligently take a user query and respond with not only raw search results but also with a "Did you mean?" response when there is a highly likely alternative answer etc [I'm developing in ASP.NET (VB - don't hold it against me! )] UPDATE:
OK, how can I mimic this without the millions of 'unpaid users'? Here's the explanation directly from the source ( almost )  at min 22:03 Worth watching! Basically and according  to Douglas Merrill former CTO of Google it is like this: 1) You write a  ( misspelled )  word  in google  2) You don't find what you wanted ( don't click on any results ) 3) You realize you misspelled the word  so you rewrite the word in the search box. 4) You find what you want ( you click in the first links )  This pattern multiplied millions of times, shows what are the most common misspells and what are the most "common" corrections.  This way Google can almost instantaneously, offer spell correction in every language. Also this means if overnight everyone start to spell night as "nigth" google would suggest that word instead.   EDIT @ThomasRutter: Douglas describe it as "statistical machine learning".  They know who correct the query, because they know which query comes from which user ( using cookies )  If the users perform a query, and only 10% of the users click on a result and 90% goes back and type another query ( with the corrected word ) and this time that 90% clicks on a result, then they know they have found a correction.  They can also know if those are "related" queries of two different, because they have information of all the links they show.  Furthermore, they are now including the context into the spell check, so they can even suggest different word depending on the context.  See this demo of google wave ( @ 44m 06s )  that shows how the context is taken into account to automatically correct the spelling. Here it is explained how that natural language processing works. And finally here is an awesome demo of what can be done adding automatic machine translation ( @ 1h 12m 47s )  to the mix.  
  I've added anchors of minute and seconds to the videos to skip directly to the content, if they don't work, try reloading the page or scrolling by hand to the mark. 
 I found this article some time ago: How to Write a Spelling Corrector, written by Peter Norvig (Director of Research at Google Inc.). It's an interesting read about the "spelling correction" topic. The examples are in Python but it's clear and simple to understand, and I think that the algorithm can be easily 
translated to other languages. Below follows a short description of the algorithm.
The algorithm consists of two steps, preparation and word checking. Step 1: Preparation - setting up the word database Best is if you can use actual search words and their occurence.
If you don't have that a large set of text can be used instead.
Count the occurrence (popularity) of each word. Step 2. Word checking - finding words that are similar to the one checked Similar means that the edit distance is low (typically 0-1 or 0-2). The edit distance is the minimum number of inserts/deletes/changes/swaps needed to transform one word to another. Choose the most popular word from the previous step and suggest it as a correction (if other than the word itself). For the theory of "did you mean" algorithm you can refer to Chapter 3 of Introduction to Information Retrieval. It is available online for free. Section 3.3 (page 52) exactly answers your question. And to specifically answer your update you only need a dictionary of words and nothing else (including millions of users). Hmm... I thought that google used their vast corpus of data (the internet) to do some serious NLP (Natural Language Processing).  For example, they have so much data from the entire internet that they can count the number of times a three-word sequence occurs (known as a trigram). So if they see a sentence like: "pink frugr concert", they could see it has few hits, then find the most likely "pink * concert" in their corpus. They apparently just do a variation of what Davide Gualano was saying, though, so definitely read that link. Google does of course use all web-pages it knows as a corpus, so that makes its algorithm particularly effective. My guess is that they use a combination of a Levenshtein distance algorithm and the masses of data they collect regarding the searches that are run. They could pull a set of searches that have the shortest Levenshtein distance from the entered search string, then pick the one with the most results. Normally a production spelling corrector utilizes several methodologies to provide a spelling suggestion. Some are: Decide on a way to determine whether spelling correction is required. These may include insufficient results, results which are not specific or accurate enough (according to some measure), etc. Then: Use a large body of text or a dictionary, where all, or most are known to be correctly spelled. These are easily found online, in places such as LingPipe. Then to determine  the best suggestion you look for a word which is the closest match based on several measures. The most intuitive one is similar characters. What has been shown through research and experimentation is that two or three character sequence matches work better. (bigrams and trigrams). To further improve results, weigh a higher score upon a match at the beginning, or end of the word. For performance reasons, index all these words as trigrams or bigrams, so that when you are performing a lookup, you convert to n-gram, and lookup via hashtable or trie. Use heuristics related to potential keyboard mistakes based on character location. So that "hwllo" should be "hello" because 'w' is close to 'e'. Use a phonetic key (Soundex, Metaphone) to index the words and lookup possible corrections. In practice this normally returns worse results than using n-gram indexing, as described above. In each case you must select the best correction from a list. This may be a distance metric such as levenshtein, the keyboard metric, etc. For a multi-word phrase, only one word may be misspelled, in which case you can use the remaining words as context in determining a best match. Use Levenshtein distance, then create a Metric Tree (or Slim tree) to index words.
Then run a 1-Nearest Neighbour query, and you got the result. Google apparently suggests queries with best results, not with those which are spelled correctly. But in this case, probably a spell-corrector would be more feasible, Of course you could store some value for every query, based on some metric of how good results it returns. So, You need a dictionary (english or based on your data) Generate a word trellis and calculate probabilities for the transitions using your dictionary. Add a decoder to calculate minimum error distance using your trellis. Of course you should take care of insertions and deletions when calculating distances. Fun thing is that QWERTY keyboard maximizes the distance if you hit keys close to each other.(cae would turn car, cay would turn cat) Return the word which has the minimum distance.  Then you could compare that to your query database and check if there is better results for other close matches. Here is the best answer I found, Spelling corrector implemented and described by Google's Director of Research Peter Norvig. If you want to read more about the theory behind this, you can read his book chapter. The idea of this algorithm is based on statistical machine learning.  I saw something on this a few years back, so may have changed since, but apparently they started it by analysing their logs for the same users submitting very similar queries in a short space of time, and used machine learning based on how users had corrected themselves. As a guess... it could  Could be something from AI like Hopfield network or back propagation network, or something else "identifying fingerprints", restoring broken data, or spelling corrections as Davide mentioned already ... Simple. They have tons of data. They have statistics for every possible term, based on how often it is queried, and what variations of it usually yield results the users click... so, when they see you typed a frequent misspelling for a search term, they go ahead and propose the more usual answer. Actually, if the misspelling is in effect the most frequent searched term, the algorythm will take it for the right one. regarding your question how to mimic the behavior without having tons of data - why not use tons of data collected by google? Download the google sarch results for the misspelled word and search for "Did you mean:" in the HTML. I guess that's called mashup nowadays :-) Apart from the above answers, in case you want to implement something by yourself quickly, here is a suggestion -  You can find the implementation and detailed documentation of this algorithm on  GitHub. You mean to say spell checker? If it is a spell checker rather than a whole phrase then I've got a link about the spell checking where the algorithm is developed in python. Check this link  Meanwhile, I am also working on project that includes searching databases using text. I guess this would solve your problem This is an old question, and I'm surprised that nobody suggested the OP using Apache Solr. Apache Solr is a full text search engine that besides many other functionality also provides spellchecking or query suggestions. From the documentation: By default, the Lucene Spell checkers sort suggestions first by the
  score from the string distance calculation and second by the frequency
  (if available) of the suggestion in the index. There is a specific data structure - ternary search tree - that naturally supports partial matches and near-neighbor matches. Easiest way to figure it out is to Google dynamic programming. It's an algorithm that's been borrowed from Information Retrieval and is used heavily in modern day bioinformatics to see how similiar two gene sequences are. Optimal solution uses dynamic programming and recursion. This is a very solved problem with lots of solutions.  Just google around until you find some open source code.I am learning neural networks and I built a simple one in Keras for the iris dataset classification from the UCI machine learning repository. I used a one hidden layer network with a 8 hidden nodes. Adam optimizer is used with a learning rate of 0.0005 and is run for 200 Epochs. Softmax is used at the output with loss as catogorical-crossentropy. I am getting the following learning curves.   As you can see, the learning curve for the accuracy has a lot of flat regions and I don't understand why. The error seems to be decreasing constantly but the accuracy doesn't seem to be increasing in the same manner. What does the flat regions in the accuracy learning curve imply? Why is the accuracy not increasing at those regions even though error seems to be decreasing?  Is this normal in training or it is more likely that I am doing something wrong here? A little understanding of the actual meanings (and mechanics) of both loss and accuracy will be of much help here (refer also to this answer of mine, although I will reuse some parts)... For the sake of simplicity, I will limit the discussion to the case of binary classification, but the idea is generally applicable; here is the equation of the (logistic) loss:  Now, let's suppose that we have a true label y[k] = 1, for which, at an early point during training, we make a rather poor prediction of p[k] = 0.1; then, plugging the numbers to the loss equation above: Suppose now that, an the next training step, we are getting better indeed, and we get p[k] = 0.22; now we have: Hopefully you start getting the idea, but let's see one more later snapshot, where we get, say, p[k] = 0.49; then: As you can see, our classifier indeed got better in this particular sample, i.e. it went from a loss of 2.3 to 1.5 to 0.71, but this improvement has still not shown up in the accuracy, which cares only for correct classifications: from an accuracy viewpoint, it doesn't matter that we get better estimates for our p[k], as long as these estimates remain below the threshold of 0.5. The moment our p[k] exceeds the threshold of 0.5, the loss continues to decrease smoothly as it has been so far, but now we have a jump in the accuracy contribution of this sample from 0 to 1/n, where n is the total number of samples. Similarly, you can confirm by yourself that, once our p[k] has exceeded 0.5, hence giving a correct classification (and now contributing positively to the accuracy), further improvements of it (i.e getting closer to 1.0) still continue to decrease the loss, but have no further impact to the accuracy. Similar arguments hold for cases where the true label y[m] = 0 and the corresponding estimates for p[m] start somewhere above the 0.5 threshold; and even if p[m] initial estimates are below 0.5 (hence providing correct classifications and already contributing positively to the accuracy), their convergence towards 0.0 will decrease the loss without improving the accuracy any further. Putting the pieces together, hopefully you can now convince yourself that a smoothly decreasing loss and a more "stepwise" increasing accuracy not only are not incompatible, but they make perfect sense indeed. On a more general level: from the strict perspective of mathematical optimization, there is no such thing called "accuracy" - there is only the loss; accuracy gets into the discussion only from a business perspective (and a different business logic might even call for a threshold different than the default 0.5). Quoting from my own linked answer: Loss and accuracy are different things; roughly speaking, the accuracy is what we are actually interested in from a business perspective, while the loss is the objective function that the learning algorithms (optimizers) are trying to minimize from a mathematical perspective. Even more roughly speaking, you can think of the loss as the "translation" of the business objective (accuracy) to the mathematical domain, a translation which is necessary in classification problems (in regression ones, usually the loss and the business objective are the same, or at least can be the same in principle, e.g. the RMSE)...Hi I have been trying to make a custom loss function in keras for dice_error_coefficient. It has its implementations in tensorboard and I tried using the same function in keras with tensorflow but it keeps returning a NoneType when I used model.train_on_batch or model.fit where as it gives proper values when used in metrics in the model. Can please someone help me out with what should i do? I have tried following libraries like Keras-FCN by ahundt where he has used custom loss functions but none of it seems to work. The target and output in the code are y_true and y_pred respectively as used in the losses.py file in keras. There are two steps in implementing a parameterized custom loss function in Keras. First, writing a method for the coefficient/metric. Second, writing a wrapper function to format things the way Keras needs them to be. It's actually quite a bit cleaner to use the Keras backend instead of tensorflow directly for simple custom loss functions like DICE. Here's an example of the coefficient implemented that way: Now for the tricky part. Keras loss functions must only take (y_true, y_pred) as parameters. So we need a separate function that returns another function.  Finally, you can use it as follows in Keras compile. According to the documentation, you can use a custom loss function like this: Any callable with the signature loss_fn(y_true, y_pred) that returns an array of losses (one of sample in the input batch) can be passed to compile() as a loss. Note that sample weighting is automatically supported for any such loss. As a simple example: Complete example: In addition, you can extend an existing loss function by inheriting from it. For example masking the BinaryCrossEntropy: A good starting point is the custom log guide: https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_lossesHaving this: And running: I get: This is incorrect. The tags for quick brown lazy in the sentence should be: Testing this through their online tool gives the same result; quick, brown and fox should be adjectives not nouns. In short: NLTK is not perfect. In fact, no model is perfect. Note: As of NLTK version 3.1, default pos_tag function is no longer the old MaxEnt English pickle.  It is now the perceptron tagger from @Honnibal's implementation, see nltk.tag.pos_tag Still it's better but not perfect: At some point, if someone wants TL;DR solutions, see https://github.com/alvations/nltk_cli In long: Try using other tagger (see https://github.com/nltk/nltk/tree/develop/nltk/tag) , e.g.: Using default MaxEnt POS tagger from NLTK, i.e. nltk.pos_tag: Using Stanford POS tagger: Using HunPOS (NOTE: the default encoding is ISO-8859-1 not UTF8): Using Senna (Make sure you've the latest version of NLTK, there were some changes made to the API): Or try building a better POS tagger: Complains about pos_tag accuracy on stackoverflow include: Issues about NLTK HunPos include: Issues with NLTK and Stanford POS tagger include: Solutions such as changing to the Stanford or Senna or HunPOS tagger will definitely yield results, but here is a much simpler way to experiment with different taggers that are also included within NLTK. The default POS tagger in NTLK right now is the averaged perceptron tagger.  Here's a function that will opt to use the Maxent Treebank Tagger instead: I have found that the averaged perceptron pre-trained tagger in NLTK is biased to treating some adjectives as nouns, as in your example.  The treebank tagger has gotten more adjectives correct for me.I'm evaluating tools for production ML based applications and one of our options is Spark MLlib , but I have some questions about how to serve a model once its trained?  For example in Azure ML, once trained, the model is exposed as a web service which can be consumed from any application, and it's a similar case with Amazon ML. How do you serve/deploy ML models in Apache Spark ? From one hand, a machine learning model built with spark can't be served the way you serve in Azure ML or Amazon ML in a traditional manner.  Databricks claims to be able to deploy models using it's notebook but I haven't actually tried that yet.  On other hand, you can use a model in three ways : Those are the three possible ways.  Of course, you can think of an architecture in which you have RESTful service behind which you can build using spark-jobserver per example to train and deploy but needs some development. It's not a out-of-the-box solution.  You might also use projects like Oryx 2 to create your full lambda architecture to train, deploy and serve a model. Unfortunately, describing each of the mentioned above solution is quite broad and doesn't fit in the scope of SO.  One option is to use MLeap to serve a Spark PipelineModel online with no dependencies on Spark/SparkContext. Not having to use the SparkContext is important as it will drop scoring time for a single record from ~100ms to single-digit microseconds. In order to use it, you have to: MLeap is well integrated with all the Pipeline Stages available in Spark MLlib (with the exception of LDA at the time of this writing). However, things might get a bit more complicated if you are using custom Estimators/Transformers. Take a look at the MLeap FAQ for more info about custom transformers/estimators, performances, and integration. You are comparing two rather different things. Apache Spark is a computation engine, while mentioned by you Amazon and Microsoft solutions are offering services. These services might as well have Spark with MLlib behind the scene. They save you from the trouble building a web service yourself, but you pay extra. Number of companies, like Domino Data Lab, Cloudera or IBM offer products that you can deploy on your own Spark cluster and easily build service around your models (with various degrees of flexibility). Naturally you build a service yourself with various open source tools. Which specifically? It all depends on what you are after. How user should interact with the model? Should there be some sort of UI or jest a REST API? Do you need to change some parameters on the model or the model itself? Are the jobs more of a batch or real-time nature? You can naturally build all-in-one solution, but that's going to be a huge effort. My personal recommendation would be to take advantage, if you can, of one of the available services from Amazon, Google, Microsoft or whatever. Need on-premises deployment? Check Domino Data Lab, their product is mature and allows easy working with models (from building till deployment). Cloudera is more focused on cluster computing (including Spark), but it will take a while before they have something mature. [EDIT] I'd recommend to have a look at Apache PredictionIO, open source machine learning server  - amazing project with lot's of potential.  I have been able to just get this to work. Caveats: Python 3.6 + using Spark ML API (not MLLIB, but sure it should work the same way) Basically, follow this example provided on MSFT's AzureML github. Word of warning: the code as-is will provision but there is an error in the example run() method at the end: Should be: Also, completely agree with MLeap assessment answer, this can make the process run way faster but thought I would answer the question specificallyI am developing a Bi-LSTM model and want to add a attention layer to it. But I am not getting how to add it. My current code for the model is And the model summary is This can be a possible custom solution with a custom layer that computes attention on the positional/temporal dimension it's build to receive 3D tensors and output 3D tensors (return_sequences=True) or 2D tensors (return_sequences=False). below a dummy example with return_sequences=True with return_sequences=False You can integrate it into your networks easily here the running notebook In case, someone is using only Tensorflow and not keras externally, this is the way to do it.I'm new to neural networks/machine learning/genetic algorithms, and for my first implementation I am writing a network that learns to play snake (An example in case you haven't played it before) I have a few questions that I don't fully understand: Before my questions I just want to make sure I understand the general idea correctly. There is a population of snakes, each with randomly generated DNA. The DNA is the weights used in the neural network. Each time the snake moves, it uses the neural net to decide where to go (using a bias). When the population dies, select some parents (maybe highest fitness), and crossover their DNA with a slight mutation chance.  1) If given the whole board as an input (about 400 spots) enough hidden layers (no idea how many, maybe 256-64-32-2?), and enough time, would it learn to not box itself in? 2) What would be good inputs? Here are some of my ideas: 3) Given the input method, what would be a good starting place for hidden layer sizes (of course plan on tweaking this, just don't know what a good starting place) 4) Finally, the fitness of the snake. Besides time to get the apple, it's length, and it's lifetime, should anything else be factored in? In order to get the snake to learn to not block itself in, is there anything else I could add to the fitness to help that? Thank you! In this post, I will advise you of: General opinion of your idea: I can see what you're trying to do, and I believe that your game idea (of using randomly generated identities of adversaries that control their behavior in a way that randomly alters the way they're using artificial intelligence to behave intelligently) has a lot of potential.  Mapping navigational instructions to action sequences with a neural network For processing your game board, because it involves dense (as opposed to sparse) data, you could find a Convolutional Neural Network (CNN) to be useful. However, because you need to translate the map to an action sequence, sequence-optimized neural networks (such as Recurrent Neural Networks) will likely be the most useful for you. I did find some studies that use neural networks to map navigational instructions to action sequences, construct the game map, and move a character through a game with many types of inputs: General opinion of what will help you It sounds like you're missing some basic understanding of how neural networks work, so my primary recommendation to you is to study more of the underlying mechanics behind neural networks in general. It's important to keep in mind that a neural network is a type of machine learning model. So, it doesn't really make sense to just construct a neural network with random parameters. A neural network is a machine learning model that is trained from sample data, and once it is trained, it can be evaluated on test data (e.g. to perform predictions).  The root of machine learning is largely influenced by Bayesian statistics, so you might benefit from getting a textbook on Bayesian statistics to gain a deeper understanding of how machine-based classification works in general.  It will also be valuable for you to learn the differences between different types of neural networks, such as Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs).  If you want to tinker with how neural networks can be used for classification tasks, try this:  To learn the math:
My professional opinion is that learning the underlying math of neural networks is very important. If it's intimidating, I give you my testimony that I was able to learn all of it on my own. But if you prefer learning in a classroom environment, then I recommend that you try that. A great resource and textbook for learning the mechanics and mathematics of neural networks is:  Tutorials for neural network libraries I recommend that you try working through the tutorials for a neural network library, such as:  I saw similar application. Inputs usually were snake coordinates, apple coordinates and some sensory data(is wall next to snake head or no in your case). Using genetic algorithm is a good idea in this case. You doing only parametric learning(finding set of weights), but structure will be based on your estimation. GA can be also used for structure learning(finding topology of ANN). But using GA for both will be very computational hard. Professor Floreano did something similar. He use GA for finding weights for neural network controller of robot. Robot was in labyrinth and perform some task. Neural network hidden layer was one neuron with recurrent joints on inputs and one lateral connection on himself. There was two outputs. Outputs were connected on input layer and hidden layer(mentioned one neuron). But Floreano did something more interesting. He say, We don't born with determined synapses, our synapses change in our lifetime. So he use GA for finding rules for change of synapses. These rules was based on Hebbian learning. He perform node encoding(for all weights connected to neuron will apply same rule). On beginning, he initialized weights on small random values. Finding rules instead of numerical value of synapse leads to better results.
One from Floreno's articles. And on the and my own experience. In last semester I and my schoolmate get a task finding the rules for synapse with GA but for Spiking neural network. Our SNN was controller for kinematic model of mobile robot and task was lead robot in to the chosen point. We obtained some results but not expected. You can see results here. So I recommend you use "ordinary" ANN instead off SNN because SNN brings new phenomens.I'm using linear_model.LinearRegression from scikit-learn as a predictive model. It works and it's perfect. I have a problem to evaluate the predicted results using the accuracy_score metric. This is my true Data : My predicted Data: My code: Error message: Despite the plethora of wrong answers here that attempt to circumvent the error by numerically manipulating the predictions, the root cause of your error is a theoretical and not computational issue: you are trying to use a classification metric (accuracy) in a regression (i.e. numeric prediction) model (LinearRegression), which is meaningless. Just like the majority of performance metrics, accuracy compares apples to apples (i.e true labels of 0/1 with predictions again of 0/1); so, when you ask the function to compare binary true labels (apples) with continuous predictions (oranges), you get an expected error, where the message tells you exactly what the problem is from a computational point of view: Despite that the message doesn't tell you directly that you are trying to compute a metric that is invalid for your problem (and we shouldn't actually expect it to go that far), it is certainly a good thing that scikit-learn at least gives you a direct and explicit warning that you are attempting something wrong; this is not necessarily the case with other frameworks - see for example the behavior of Keras in a very similar situation, where you get no warning at all, and one just ends up complaining for low "accuracy" in a regression setting... I am super-surprised with all the other answers here (including the accepted & highly upvoted one) effectively suggesting to manipulate the predictions in order to simply get rid of the error; it's true that, once we end up with a set of numbers, we can certainly start mingling with them in various ways (rounding, thresholding etc) in order to make our code behave, but this of course does not mean that our numeric manipulations are meaningful in the specific context of the ML problem we are trying to solve. So, to wrap up: the problem is that you are applying a metric (accuracy) that is inappropriate for your model (LinearRegression): if you are in a classification setting, you should change your model (e.g. use LogisticRegression instead); if you are in a regression (i.e. numeric prediction) setting, you should change the metric. Check the list of metrics available in scikit-learn, where you can confirm that accuracy is used only in classification. Compare also the situation with a recent SO question, where the OP is trying to get the accuracy of a list of models: where the first 6 models work OK, while all the rest (commented-out) ones give the same error. By now, you should be able to convince yourself that all the commented-out models are regression (and not classification) ones, hence the justified error. A last important note: it may sound legitimate for someone to claim: OK, but I want to use linear regression and then just
round/threshold the outputs, effectively treating the predictions as
"probabilities" and thus converting the model into a classifier Actually, this has already been suggested in several other answers here, implicitly or not; again, this is an invalid approach (and the fact that you have negative predictions should have already alerted you that they cannot be interpreted as probabilities). Andrew Ng, in his popular Machine Learning course at Coursera, explains why this is a bad idea - see his Lecture 6.1 - Logistic Regression | Classification at Youtube (explanation starts at ~ 3:00), as well as section 4.2 Why Not Linear Regression [for classification]? of the (highly recommended and freely available) textbook An Introduction to Statistical Learning by Hastie, Tibshirani and coworkers... accuracy_score is a classification metric, you cannot use it for a regression problem. You can see the available regression metrics in the docs. The problem is that the true y is binary (zeros and ones), while your predictions are not. You probably generated probabilities and not predictions, hence the result :)
Try instead to generate class membership, and it should work! The sklearn.metrics.accuracy_score(y_true, y_pred) method defines y_pred as:  y_pred : 1d array-like, or label indicator array / sparse matrix. 
Predicted labels, as returned by a classifier. Which means y_pred has to be an array of 1's or 0's (predicated labels). They should not be probabilities. The predicated labels (1's and 0's) and/or predicted probabilites can be generated using the LinearRegression() model's methods predict() and predict_proba() respectively. 1. Generate predicted labels: output:  y_preds can now be used for the accuracy_score() method: accuracy_score(y_true, y_pred) 2. Generate probabilities for labels: Some metrics such as 'precision_recall_curve(y_true, probas_pred)' require probabilities, which can be generated as follows: output: This resolve same problem for me,
use .round() for preditions, I was facing the same issue.The dtypes of y_test and y_pred were different. Make sure that the dtypes are same for both.
 The error is because difference in datatypes of y_pred and y_true. y_true might be dataframe and y_pred is arraylist. If you convert both to arrays, then issue will get resolved. Just use accuracy_score is a classification metric, you cannot use it for a regression problem. Use this way:How do I save a trained Naive Bayes classifier to disk and use it to predict data? I have the following sample program from the scikit-learn website: Classifiers are just objects that can be pickled and dumped like any other. To continue your example: Edit: if you are using a sklearn Pipeline in which you have custom transformers that cannot be serialized by pickle (nor by joblib), then using Neuraxle's custom ML Pipeline saving is a solution where you can define your own custom step savers on a per-step basis. The savers are called for each step if defined upon saving, and otherwise joblib is used as default for steps without a saver. You can also use joblib.dump and joblib.load which is much more efficient at handling numerical arrays than the default python pickler. Joblib is included in scikit-learn: Edit: in Python 3.8+ it's now possible to use pickle for efficient pickling of object with large numerical arrays as attributes if you use pickle protocol 5 (which is not the default). What you are looking for is called Model persistence in sklearn words and it is documented in introduction and in model persistence sections. So you have initialized your classifier and trained it for a long time with After this you have two options: 1) Using Pickle 2) Using Joblib One more time it is helpful to read the above-mentioned links  In many cases, particularly with text classification it is not enough just to store the classifier but you'll need to store the vectorizer as well so that you can vectorize your input in future. future use case: Before dumping the vectorizer, one can delete the stop_words_ property of vectorizer by: to make dumping more efficient.
Also if your classifier parameters is sparse (as in most text classification examples) you can convert the parameters from dense to sparse which will make a huge difference in terms of memory consumption, loading and dumping. Sparsify the model by: Which will automatically work for SGDClassifier but in case you know your model is sparse (lots of zeros in clf.coef_) then you can manually convert clf.coef_ into a csr scipy sparse matrix by: and then you can store it more efficiently. sklearn estimators implement methods to make it easy for you to save relevant trained properties of an estimator. Some estimators implement __getstate__ methods themselves, but others, like the GMM just use the base implementation which simply saves the objects inner dictionary: The recommended method to save your model to disc is to use the pickle module: However, you should save additional data so you can retrain your model in the future, or suffer dire consequences (such as being locked into an old version of sklearn). From the documentation: In order to rebuild a similar model with future versions of
  scikit-learn, additional metadata should be saved along the pickled
  model:  The training data, e.g. a reference to a immutable snapshot  The python source code used to generate the model  The versions of scikit-learn and its dependencies  The cross validation score obtained on the training data This is especially true for Ensemble estimators that rely on the tree.pyx module written in Cython(such as IsolationForest), since it creates a coupling to the implementation, which is not guaranteed to be stable between versions of sklearn. It has seen backwards incompatible changes in the past. If your models become very large and loading becomes a nuisance, you can also use the more efficient joblib. From the documentation: In the specific case of the scikit, it may be more interesting to use
  joblib’s replacement of pickle (joblib.dump & joblib.load), which is
  more efficient on objects that carry large numpy arrays internally as
  is often the case for fitted scikit-learn estimators, but can only
  pickle to the disk and not to a string: sklearn.externals.joblib has been deprecated since 0.21 and will be removed in v0.23:  /usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/init.py:15:
  FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will
  be removed in 0.23. Please import this functionality directly from
  joblib, which can be installed with: pip install joblib. If this
  warning is raised when loading pickled models, you may need to
  re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning) Therefore, you need to install joblib:  and finally write the model to disk:  Now in order to read the dumped file all you need to run is:Caffe has a layer type "Python".   For instance, this layer type can be used as a loss layer.
On other occasions it is used as an input layer. What is this layer type?
How can this layer be used? Prune's and Bharat's answers gives the overall purpose of a "Python" layer: a general purpose layer which is implemented in python rather than c++. I intend this answer to serve as a tutorial for using "Python" layer. Please see the excellent answers of Prune and Bharat. In order to use 'Python" layer you need to compile caffe with flag set in 'Makefile.config'. A "Python" layer should be implemented as a python class derived from caffe.Layer base class. This class must have the following four methods: What are these methods? def setup(self, bottom, top): This method is called once when caffe builds the net. This function should check that number of inputs (len(bottom)) and number of outputs (len(top)) is as expected.
You should also allocate internal parameters of the net here (i.e., self.add_blobs()), see this thread for more information.
This method has access to self.param_str - a string passed from the prototxt to the layer. See this thread for more information. def reshape(self, bottom, top): This method is called whenever caffe reshapes the net. This function should allocate the outputs (each of the top blobs). The outputs' shape is usually related to the bottoms' shape. def forward(self, bottom, top): Implementing the forward pass from bottom to top. def backward(self, top, propagate_down, bottom): This method implements the backpropagation, it propagates the gradients from top to bottom. propagate_down is a Boolean vector of len(bottom) indicating to which of the bottoms the gradient should be propagated. Some more information about bottom and top inputs you can find in this post.   Examples
You can see some examples of simplified python layers here, here and here.
Example of "moving average" output layer can be found here. Trainable parameters
"Python" layer can have trainable parameters (like "Conv", "InnerProduct", etc.).
You can find more information on adding trainable parameters in this thread and this one. There's also a very simplified example in caffe git. See Bharat's answer for details.
You need to add the following to your prototxt: It's very simple: Invoking python code from caffe is nothing you need to worry about. Caffe uses boost API to call python code from compiled c++.
What do you do need to do?
Make sure the python module implementing your layer is in $PYTHONPATH so that when caffe imports it - it can be found.
For instance, if your module my_python_layer.py is in /path/to/my_python_layer.py then should work just fine. You should always test your layer before putting it to use.
Testing the forward function is entirely up to you, as each layer has a different functionality.
Testing the backward method is easy, as this method only implements a gradient of forward it can be numerically tested automatically!
Check out test_gradient_for_python_layer testing utility: It is worth while noting that python code runs on CPU only. Thus, if you plan to have a Python layer in the middle of your net you will see a significant degradation in performance if you plan on using GPU. This happens because caffe needs to copy blobs from GPU to CPU before calling python layer and then copy back to GPU to proceed with the forward/backward pass.
This degradation is far less significant if the python layer is either an input layer or the topmost loss layer.
Update: On Sep 19th, 2017 PR #5904 was merged into master. This PR exposes GPU pointers of blobs via the python interface.
You may access blob._gpu_data_ptr and blob._gpu_diff_ptr directly from python at your own risk. Very simply, it's a layer in which you provide the implementation code, rather than using one of the pre-defined types -- which are all backed by efficient functions. If you want to define a custom loss function, go ahead: write it yourself, and create the layer with type Python.  If you have non-standard input needs, perhaps some data-specific pre-processing, no problem: write it yourself, and create the layer with type Python. Python layers are different from C++ layers which need to be compiled, their parameters need to be added to the proto file and finally you need to register the layer in layer_factory. If you write a python layer, you don't need to worry about any of these things. Layer parameters can be defined as a string, which are accessible as a string in python. For example: if you have a parameter in a layer, you can access it using 'self.param_str', if param_str was defined in your prototxt file. Like other layers, you need to define a class with the following functions: Prototxt example: Here, name of the layer is rpn-data, bottom and top are input and output details of the layer respectively. python_param defines what are the parameters of the Python layer. 'module' specifies what is the file name of your layer. If the file called 'anchor_target_layer.py' is located inside a folder called 'rpn', the parameter would be 'rpn.anchor_target_layer'. The 'layer' parameter is the name of your class, in this case it is 'AnchorTargetLayer'. 'param_str' is a parameter for the layer, which contains a value 16 for the key 'feat_stride'. Unlike C++/CUDA layers, Python layers do not work in a multi-GPU setting in caffe as of now, so that is a disadvantage of using them.From the Udacity's deep learning class, the softmax of y_i is simply the exponential divided by the sum of exponential of the whole Y vector:  Where S(y_i) is the softmax function of y_i and e is the exponential and j is the no. of columns in the input vector Y. I've tried the following: which returns: But the suggested solution was: which produces the same output as the first implementation, even though the first implementation explicitly takes the difference of each column and the max and then divides by the sum. Can someone show mathematically why? Is one correct and the other one wrong? Are the implementation similar in terms of code and time complexity? Which is more efficient? They're both correct, but yours is preferred from the point of view of numerical stability. You start with By using the fact that a^(b - c) = (a^b)/(a^c) we have Which is what the other answer says. You could replace max(x) with any variable and it would cancel out. (Well... much confusion here, both in the question and in the answers...) To start with, the two solutions (i.e. yours and the suggested one) are not equivalent; they happen to be equivalent only for the special case of 1-D score arrays. You would have discovered it if you had tried also the 2-D score array in the Udacity quiz provided example. Results-wise, the only actual difference between the two solutions is the axis=0 argument. To see that this is the case, let's try your solution (your_softmax) and one where the only difference is the axis argument: As I said, for a 1-D score array, the results are indeed identical: Nevertheless, here are the results for the 2-D score array given in the Udacity quiz as a test example: The results are different - the second one is indeed identical with the one expected in the Udacity quiz, where all columns indeed sum to 1, which is not the case with the first (wrong) result. So, all the fuss was actually for an implementation detail - the axis argument. According to the numpy.sum documentation: The default, axis=None, will sum all of the elements of the input array while here we want to sum row-wise, hence axis=0. For a 1-D array, the sum of the (only) row and the sum of all the elements happen to be identical, hence your identical results in that case... The axis issue aside, your implementation (i.e. your choice to subtract the max first) is actually better than the suggested solution! In fact, it is the recommended way of implementing the softmax function - see here for the justification (numeric stability, also pointed out by some other answers here). So, this is really a comment to desertnaut's answer but I can't comment on it yet due to my reputation. As he pointed out, your version is only correct if your input consists of a single sample. If your input consists of several samples, it is wrong. However, desertnaut's solution is also wrong. The problem is that once he takes a 1-dimensional input and then he takes a 2-dimensional input. Let me show this to you. Lets take desertnauts example: This is the output: You can see that desernauts version would fail in this situation. (It would not if the input was just one dimensional like np.array([1, 2, 3, 6]). Lets now use 3 samples since thats the reason why we use a 2 dimensional input. The following x2 is not the same as the one from desernauts example.  This input consists of a batch with 3 samples. But sample one and three are essentially the same. We now expect 3 rows of softmax activations where the first should be the same as the third and also the same as our activation of x1! I hope you can see that this is only the case with my solution. Additionally, here is the results of TensorFlows softmax implementation: And the result: I would say that while both are correct mathematically, implementation-wise, first one is better. When computing softmax, the intermediate values may become very large. Dividing two large numbers can be numerically unstable. These notes (from Stanford) mention a normalization trick which is essentially what you are doing.  sklearn also offers implementation of softmax From mathematical point of view both sides are equal.  And you can easily prove this. Let's m=max(x). Now your function softmax returns a vector, whose i-th coordinate is equal to  notice that this works for any m, because for all (even complex) numbers e^m != 0 from computational complexity point of view they are also equivalent and both run in O(n) time, where n is the size of a vector.  from numerical stability point of view, the first solution is preferred, because e^x grows very fast and even for pretty small values of x it will overflow. Subtracting the maximum value allows to get rid of this overflow. To practically experience the stuff I was talking about try to feed x = np.array([1000, 5]) into both of your functions. One will return correct probability, the second will overflow with nan your solution works only for vectors (Udacity quiz wants you to calculate it for matrices as well). In order to fix it you need to use sum(axis=0) EDIT. As of version 1.2.0, scipy includes softmax as a special function:  https://scipy.github.io/devdocs/generated/scipy.special.softmax.html I wrote a function applying the softmax over any axis: Subtracting the max, as other users described, is good practice. I wrote a detailed post about it here. Here you can find out why they used - max.  From there: "When you’re writing code for computing the Softmax function in practice, the intermediate terms may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick." I was curious to see the performance difference between these Using Increasing the values inside x (+100 +200 +500...) I get consistently better results with the original numpy version (here is just one test) Until.... the values inside x reach ~800, then I get As some said, your version is more numerically stable 'for large numbers'. For small numbers could be the other way around. A more concise version is: To offer an alternative solution, consider the cases where your arguments are extremely large in magnitude such that exp(x) would underflow (in the negative case) or overflow (in the positive case). Here you want to remain in log space as long as possible, exponentiating only at the end where you can trust the result will be well-behaved. I needed something compatible with the output of a dense layer from Tensorflow.  The solution from @desertnaut does not work in this case because I have batches of data. Therefore, I came with another solution that should work in both cases: Results:
 Ref: Tensorflow softmax I would suggest this: It will work for stochastic as well as the batch.
For more detail see :
https://medium.com/@ravish1729/analysis-of-softmax-function-ad058d6a564d In order to maintain for numerical stability, max(x) should be subtracted. The following is the code for softmax function; def softmax(x): Already answered in much detail in above answers. max is subtracted to avoid overflow. I am adding here one more implementation in python3. Everybody seems to post their solution so I'll post mine: I get the exact same results as the imported from sklearn: Based on all the responses and CS231n notes, allow me to summarise: Usage: Output: The softmax function is an activation function that turns numbers into probabilities which sum to one. The softmax function outputs a vector that represents the probability distributions of a list of outcomes. It is also a core element used in deep learning classification tasks. Softmax function is used when we have multiple classes. It is useful for finding out the class which has the max. Probability. The Softmax function is ideally used in the output layer, where we are actually trying to attain the probabilities to define the class of each input. It ranges from 0 to 1. Softmax function turns logits [2.0, 1.0, 0.1] into probabilities [0.7, 0.2, 0.1], and the probabilities sum to 1. Logits are the raw scores output by the last layer of a neural network. Before activation takes place. To understand the softmax function, we must look at the output of the (n-1)th layer. The softmax function is, in fact, an arg max function. That means that it does not return the largest value from the input, but the position of the largest values. For example: Before softmax After softmax Code: This also works with np.reshape. I would like to supplement a little bit more understanding of the problem. Here it is correct of subtracting max of the array. But if you run the code in the other post, you would find it is not giving you right answer when the array is 2D  or higher dimensions. Here I give you some suggestions: Follow the result you will get the correct answer by doing vectorization. Since it is related to the college homework, I cannot post the exact code here, but I would like to give more suggestions if you don't understand. Goal was to achieve similar results using Numpy and Tensorflow. The only change from original answer is axis parameter for np.sum api. Initial approach : axis=0 - This however does not provide intended results when dimensions are N. Modified approach: axis=len(e_x.shape)-1 - Always sum on the last dimension. This provides similar results as tensorflow's softmax function. Here is generalized solution using numpy and comparision for correctness with tensorflow ans scipy: Data preparation: Output: Softmax using tensorflow: Output: Softmax using scipy: Output: Softmax using numpy (https://nolanbconaway.github.io/blog/2017/softmax-numpy) : Output: The purpose of the softmax function is to preserve the ratio of the vectors as opposed to squashing the end-points with a sigmoid as the values saturate (i.e. tend to +/- 1 (tanh) or from 0 to 1 (logistical)). This is because it preserves more information about the rate of change at the end-points and thus is more applicable to neural nets with 1-of-N Output Encoding (i.e. if we squashed the end-points it would be harder to differentiate the 1-of-N output class because we can't tell which one is the "biggest" or "smallest" because they got squished.); also it makes the total output sum to 1, and the clear winner will be closer to 1 while other numbers that are close to each other will sum to 1/p, where p is the number of output neurons with similar values. The purpose of subtracting the max value from the vector is that when you do e^y exponents you may get very high value that clips the float at the max value leading to a tie, which is not the case in this example. This becomes a BIG problem if you subtract the max value to make a negative number, then you have a negative exponent that rapidly shrinks the values altering the ratio, which is what occurred in poster's question and yielded the incorrect answer. The answer supplied by Udacity is HORRIBLY inefficient. The first thing we need to do is calculate e^y_j for all vector components, KEEP THOSE VALUES, then sum them up, and divide. Where Udacity messed up is they calculate e^y_j TWICE!!! Here is the correct answer: This generalizes and assumes you are normalizing the trailing dimension. I used these three simple lines:Is it possible to specify your own distance function using scikit-learn K-Means Clustering? Here's a small kmeans that uses any of the 20-odd distances in
scipy.spatial.distance, or a user function.
Comments would be welcome (this has had only one user so far, not enough);
in particular, what are your N, dim, k, metric ? Some notes added 26mar 2012: 1) for cosine distance, first normalize all the data vectors to |X| = 1; then is fast. For bit vectors, keep the norms separately from the vectors
instead of expanding out to floats
(although some programs may expand for you).
For sparse vectors, say 1 % of N, X . Y should take time O( 2 % N ),
space O(N); but I don't know which programs do that. 2)
Scikit-learn clustering
gives an excellent overview of k-means, mini-batch-k-means ...
with code that works on scipy.sparse matrices. 3) Always check cluster sizes after k-means.
If you're expecting roughly equal-sized clusters, but they come out
[44 37  9  5  5] % ... (sound of head-scratching). Unfortunately no: scikit-learn current implementation of k-means only uses Euclidean distances. It is not trivial to extend k-means to other distances and denis' answer above is not the correct way to implement k-means for other metrics. Just use nltk instead where you can do this, e.g. Yes you can use a difference metric function; however, by definition, the k-means clustering algorithm relies on the eucldiean distance from the mean of each cluster.  You could use a different metric, so even though you are still calculating the mean you could use something like the mahalnobis distance.  There is pyclustering which is python/C++ (so its fast!) and lets you specify a custom metric function Actually, i haven't tested this code but cobbled it together from a ticket and example code. k-means of Spectral Python allows the use of L1 (Manhattan) distance. Sklearn Kmeans uses the Euclidean distance. It has no metric parameter. This said, if you're clustering time series, you can use the tslearn python package, when you can specify a metric (dtw, softdtw, euclidean). The Affinity propagation algorithm from the sklearn library allows you to pass the similarity matrix instead of the samples. So, you can use your metric to compute the similarity matrix (not the dissimilarity matrix) and pass it to the function by setting the "affinity" term to "precomputed".https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation.fit
In terms of the K-Mean, I think it is also possible but I have not tried it.
However, as the other answers stated, finding the mean using a different metric will be the issue. Instead, you can use PAM (K-Medoids) algorthim as it calculates the change in Total Deviation (TD), thus it does not rely on the distance metric. https://python-kmedoids.readthedocs.io/en/latest/#fasterpam Yes, in the current stable version of sklearn (scikit-learn 1.1.3), you can easily use your own distance metric. All you have to do is create a class that inherits from sklearn.cluster.KMeans and overwrites its _transform method. The below example is for the IOU distance from the Yolov2 paper.I have asked a question a few days back on how to find the nearest neighbors for a given vector. My vector is now 21 dimensions and before I proceed further, because I am not from the domain of Machine Learning nor Math, I am beginning to ask myself some fundamental questions: Can someone please clarify the some (or all) of the above questions? I currently study such problems -- classification, nearest neighbor searching -- for music information retrieval. You may be interested in Approximate Nearest Neighbor (ANN) algorithms. The idea is that you allow the algorithm to return sufficiently near neighbors (perhaps not the nearest neighbor); in doing so, you reduce complexity. You mentioned the kd-tree; that is one example. But as you said, kd-tree works poorly in high dimensions. In fact, all current indexing techniques (based on space partitioning) degrade to linear search for sufficiently high dimensions [1][2][3]. Among ANN algorithms proposed recently, perhaps the most popular is Locality-Sensitive Hashing (LSH), which maps a set of points in a high-dimensional space into a set of bins, i.e., a hash table [1][3]. But unlike traditional hashes, a locality-sensitive hash places nearby points into the same bin. LSH has some huge advantages. First, it is simple. You just compute the hash for all points in your database, then make a hash table from them. To query, just compute the hash of the query point, then retrieve all points in the same bin from the hash table. Second, there is a rigorous theory that supports its performance. It can be shown that the query time is sublinear in the size of the database, i.e., faster than linear search. How much faster depends upon how much approximation we can tolerate. Finally, LSH is compatible with any Lp norm for 0 < p <= 2. Therefore, to answer your first question, you can use LSH with the Euclidean distance metric, or you can use it with the Manhattan (L1) distance metric. There are also variants for Hamming distance and cosine similarity. A decent overview was written by Malcolm Slaney and Michael Casey for IEEE Signal Processing Magazine in 2008 [4]. LSH has been applied seemingly everywhere. You may want to give it a try. [1] Datar, Indyk, Immorlica, Mirrokni, "Locality-Sensitive Hashing Scheme Based on p-Stable Distributions," 2004. [2] Weber, Schek, Blott, "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces," 1998. [3] Gionis, Indyk, Motwani, "Similarity search in high dimensions via hashing," 1999. [4] Slaney, Casey, "Locality-sensitive hashing for finding nearest neighbors", 2008. I. The Distance Metric First, the number of features (columns) in a data set is not a factor in selecting a distance metric for use in kNN. There are quite a few published studies directed to precisely this question, and the usual bases for comparison are:  the underlying statistical
distribution of your data; the relationship among the features
that comprise your data (are they
independent--i.e., what does the
covariance matrix look like); and the coordinate space from which your
data was obtained. If you have no prior knowledge of the distribution(s) from which your data was sampled, at least one (well documented and thorough) study concludes that Euclidean distance is the best choice. YEuclidean metric used in mega-scale Web Recommendation Engines as well as in current academic research. Distances calculated by Euclidean have intuitive meaning and the computation scales--i.e., Euclidean distance is calculated the same way, whether the two points are in two dimension or in twenty-two dimension space. It has only failed for me a few times, each of those cases Euclidean distance failed because the underlying (cartesian) coordinate system was a poor choice. And you'll usually recognize this because for instance path lengths (distances) are no longer additive--e.g., when the metric space is a chessboard, Manhattan distance is better than Euclidean, likewise when the metric space is Earth and your distances are trans-continental flights, a distance metric suitable for a polar coordinate system is a good idea (e.g., London to Vienna is is 2.5 hours, Vienna to St. Petersburg is another 3 hrs, more or less in the same direction, yet London to St. Petersburg isn't 5.5 hours, instead, is a little over 3 hrs.) But apart from those cases in which your data belongs in a non-cartesian coordinate system, the choice of distance metric is usually not material. (See this blog post from a CS student, comparing several distance metrics by examining their effect on kNN classifier--chi square give the best results, but the differences are not large; A more comprehensive study is in the academic paper, Comparative Study of Distance Functions for Nearest Neighbors--Mahalanobis (essentially Euclidean normalized by to account for dimension covariance) was the best in this study. One important proviso: for distance metric calculations to be meaningful, you must re-scale your data--rarely is it possible to build a kNN model to generate accurate predictions without doing this. For instance, if you are building a kNN model to predict athletic performance, and your expectation variables are height (cm), weight (kg), bodyfat (%), and resting pulse (beats per minute), then a typical data point might look something like this: [ 180.4, 66.1, 11.3, 71 ]. Clearly the distance calculation will be dominated by height, while the contribution by bodyfat % will be almost negligible. Put another way, if instead, the data were reported differently, so that bodyweight was in grams rather than kilograms, then the original value of 86.1, would be 86,100, which would have a large effect on your results, which is exactly what you don't want. Probably the most common scaling technique is subtracting the mean and dividing by the standard deviation (mean and sd refer calculated separately for each column, or feature in that data set; X refers to an individual entry/cell within a data row): 
II. The Data Structure If you are concerned about performance of the kd-tree structure, A Voronoi Tessellation is a conceptually simple container but that will drastically improve performance and scales better than kd-Trees. This is not the most common way to persist kNN training data, though the application of VT for this purpose, as well as the consequent performance advantages, are well-documented (see e.g. this Microsoft Research report). The practical significance of this is that, provided you are using a 'mainstream' language (e.g., in the TIOBE Index) then you ought to find a library to perform VT. I know in Python and R, there are multiple options for each language (e.g., the voronoi package for R available on CRAN) Using a VT for kNN works like this:: From your data, randomly select w points--these are your Voronoi centers. A Voronoi cell encapsulates all neighboring points that are nearest to each center. Imagine if you assign a different color to each of Voronoi centers, so that each point assigned to a given center is painted that color. As long as you have a sufficient density, doing this will nicely show the boundaries of each Voronoi center (as the boundary that separates two colors. How to select the Voronoi Centers? I use two orthogonal guidelines. After random selecting the w points, calculate the VT for your training data. Next check the number of data points assigned to each Voronoi center--these values should be about the same (given uniform point density across your data space). In two dimensions, this would cause a VT with tiles of the same size.That's the first rule, here's the second. Select w by iteration--run your kNN algorithm with w as a variable parameter, and measure performance (time required to return a prediction by querying the VT). So imagine you have one million data points..... If the points were persisted in an ordinary 2D data structure, or in a kd-tree, you would perform on average a couple million distance calculations for each new data points whose response variable you wish to predict. Of course, those calculations are performed on a single data set. With a V/T, the nearest-neighbor search is performed in two steps one after the other, against two different populations of data--first against the Voronoi centers, then once the nearest center is found, the points inside the cell corresponding to that center are searched to find the actual nearest neighbor (by successive distance calculations) Combined, these two look-ups are much faster than a single brute-force look-up. That's easy to see: for 1M data points, suppose you select 250 Voronoi centers to tesselate your data space. On average, each Voronoi cell will have 4,000 data points. So instead of performing on average 500,000 distance calculations (brute force), you perform far lesss, on average just 125 + 2,000. III. Calculating the Result (the predicted response variable) There are two steps to calculating the predicted value from a set of kNN training data. The first is identifying n, or the number of nearest neighbors to use for this calculation. The second is how to weight their contribution to the predicted value. W/r/t the first component, you can determine the best value of n by solving an optimization problem (very similar to least squares optimization). That's the theory; in practice, most people just use n=3. In any event, it's simple to run your kNN algorithm over a set of test instances (to calculate predicted values) for n=1, n=2, n=3, etc. and plot the error as a function of n. If you just want a plausible value for n to get started, again, just use n = 3. The second component is how to weight the contribution of each of the neighbors (assuming n > 1). The simplest weighting technique is just multiplying each neighbor by a weighting coefficient, which is just the 1/(dist * K), or the inverse of the distance from that neighbor to the test instance often multiplied by some empirically derived constant, K. I am not a fan of this technique because it often over-weights the closest neighbors (and concomitantly under-weights the more distant ones); the significance of this is that a given prediction can be almost entirely dependent on a single neighbor, which in turn increases the algorithm's sensitivity to noise. A must better weighting function, which substantially avoids this limitation is the gaussian function, which in python, looks like this: To calculate a predicted value using your kNN code, you would identify the n nearest neighbors to the data point whose response variable you wish to predict ('test instance'), then call the weight_gauss function, once for each of the n neighbors, passing in the distance between each neighbor the the test point.This function will return the weight for each neighbor, which is then used as that neighbor's coefficient in the weighted average calculation. What you are facing is known as the curse of dimensionality. It is sometimes useful to run an algorithm like PCA or ICA to make sure that you really need all 21 dimensions and possibly find a linear transformation which would allow you to use less than 21 with approximately the same result quality. Update:
I encountered them in a book called Biomedical Signal Processing by Rangayyan (I hope I remember it correctly). ICA is not a trivial technique, but it was developed by researchers in Finland and I think Matlab code for it is publicly available for download. PCA is a more widely used technique and I believe you should be able to find its R or other software implementation. PCA is performed by solving linear equations iteratively. I've done it too long ago to remember how. = ) The idea is that you break up your signals into independent eigenvectors (discrete eigenfunctions, really) and their eigenvalues, 21 in your case. Each eigenvalue shows the amount of contribution each eigenfunction provides to each of your measurements. If an eigenvalue is tiny, you can very closely represent the signals without using its corresponding eigenfunction at all, and that's how you get rid of a dimension. Top answers are good but old, so I'd like to add up a 2016 answer. As said, in a high dimensional space, the curse of dimensionality lurks around the corner, making the traditional approaches, such as the popular k-d tree, to be as slow as a brute force approach. As a result, we turn our interest in Approximate Nearest Neighbor Search (ANNS), which in favor of some accuracy, speedups the process. You get a good approximation of the exact NN, with a good propability. Hot topics that might be worthy: You can also check my relevant answers: To answer your questions one by one: Here is a nice paper to get you started in the right direction. "When in Nearest Neighbour meaningful?" by Beyer et all. I work with text data of dimensions 20K and above. If you want some text related advice, I might be able to help you out. Cosine similarity is a common way to compare high-dimension vectors. Note that since it's a similarity not a distance, you'd want to maximize it not minimize it.  You can also use a domain-specific way to compare the data, for example if your data was DNA sequences, you could use a sequence similarity that takes into account probabilities of mutations, etc. The number of nearest neighbors to use varies depending on the type of data, how much noise there is, etc.  There are no general rules, you just have to find what works best for your specific data and problem by trying all values within a range.  People have an intuitive understanding that the more data there is, the fewer neighbors you need.  In a hypothetical situation where you have all possible data, you only need to look for the single nearest neighbor to classify. The k Nearest Neighbor method is known to be computationally expensive.  It's one of the main reasons people turn to other algorithms like support vector machines. kd-trees indeed won't work very well on high-dimensional data. Because the pruning step no longer helps a lot, as the closest edge - a 1 dimensional deviation - will almost always be smaller than the full-dimensional deviation to the known nearest neighbors. But furthermore, kd-trees only work well with Lp norms for all I know, and there is the distance concentration effect that makes distance based algorithms degrade with increasing dimensionality. For further information, you may want to read up on the curse of dimensionality, and the various variants of it (there is more than one side to it!) I'm not convinced there is a lot use to just blindly approximating Euclidean nearest neighbors e.g. using LSH or random projections. It may be necessary to use a much more fine tuned distance function in the first place! A lot depends on why you want to know the nearest neighbors. You might look into the mean shift  algorithm http://en.wikipedia.org/wiki/Mean-shift if what you really want is to find the modes of your data set.  I think cosine on tf-idf of boolean features would work well for most problems. That's because its time-proven heuristic used in many search engines like Lucene. Euclidean distance in my experience shows bad results for any text-like data. Selecting different weights and k-examples can be done with training data and brute-force parameter selection. iDistance is probably the best for exact knn retrieval in high-dimensional data.  You can view it as an approximate Voronoi tessalation. I've experienced the same problem and can say the following.  Euclidean distance is a good distance metric, however it's computationally more expensive than the Manhattan distance, and sometimes yields slightly poorer results, thus, I'd choose the later. The value of k can be found empirically. You can try different values and check the resulting ROC curves or some other precision/recall measure in order to find an acceptable value. Both Euclidean and Manhattan distances respect the Triangle inequality, thus you can use them in metric trees. Indeed, KD-trees have their performance severely degraded when the data have more than 10 dimensions (I've experienced that problem myself). I found VP-trees to be a better option. KD Trees work fine for 21 dimensions, if you quit early,
after looking at say 5 % of all the points.
FLANN does this (and other speedups)
to match 128-dim SIFT vectors. (Unfortunately FLANN does only the Euclidean metric,
and the fast and solid 
scipy.spatial.cKDTree
does only Lp metrics;
these may or may not be adequate for your data.)
There is of course a speed-accuracy tradeoff here. (If you could describe your Ndata, Nquery, data distribution,
that might help people to try similar data.) Added 26 April, run times for cKDTree with cutoff on my old mac ppc, to give a very rough idea of feasibility: You could try a z order curve. It's easy for 3 dimension. I had a similar question a while back. For fast Approximate Nearest Neighbor Search you can use the annoy library from spotify: https://github.com/spotify/annoy This is some example code for the Python API, which is optimized in C++. They provide different distance measurements. Which distance measurement you want to apply depends highly on your individual problem. Also consider prescaling (meaning weighting) certain dimensions for importance first. Those dimension or feature importance weights might be calculated by something like entropy loss or if you have a supervised learning problem gini impurity gain or mean average loss, where you check how much worse your machine learning model performs, if you scramble this dimensions values. Often the direction of the vector is more important than it's absolute value. For example in the semantic analysis of text documents, where we want document vectors to be close when their semantics are similar, not their lengths. Thus we can either normalize those vectors to unit length or use angular distance (i.e. cosine similarity) as a distance measurement. Hope this is helpful. Is Euclidean distance a good metric for finding the nearest neighbors in the first place? If not, what are my options? I would suggest soft subspace clustering, a pretty common approach nowadays, where feature weights are calculated to find the most relevant dimensions. You can use these weights when using euclidean distance, for example. See curse of dimensionality for common problems and also this article can enlighten you somehow: A k-means type clustering algorithm for subspace clustering of mixed numeric and
categorical datasetsThe classifiers in machine learning packages like liblinear and nltk offer a method show_most_informative_features(), which is really helpful for debugging features: My question is if something similar is implemented for the classifiers in scikit-learn. I searched the documentation, but couldn't find anything the like. If there is no such function yet, does somebody know a workaround how to get to those values? The classifiers themselves do not record feature names, they just see numeric arrays. However, if you extracted your features using a Vectorizer/CountVectorizer/TfidfVectorizer/DictVectorizer, and you are using a linear model (e.g. LinearSVC or Naive Bayes) then you can apply the same trick that the document classification example uses. Example (untested, may contain a bug or two): This is for multiclass classification; for the binary case, I think you should use clf.coef_[0] only. You may have to sort the class_labels. With the help of larsmans code I came up with this code for the binary case: To add an update, RandomForestClassifier now supports the .feature_importances_ attribute. This attribute tells you how much of the observed variance is explained by that feature. Obviously, the sum of all these values must be <= 1.  I find this attribute very useful when performing feature engineering.  Thanks to the scikit-learn team and contributors for implementing this! edit: This works for both RandomForest and GradientBoosting. So RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier and GradientBoostingRegressor all support this.  We've recently released a library (https://github.com/TeamHG-Memex/eli5) which allows to do that: it handles variuos classifiers from scikit-learn, binary / multiclass cases, allows to highlight text according to feature values, integrates with IPython, etc. I actually had to find out Feature Importance on my NaiveBayes classifier and although I used the above functions, I was not able to get feature importance based on classes. I went through the scikit-learn's documentation and tweaked the above functions a bit to find it working for my problem. Hope it helps you too! Note that your classifier(in my case it's NaiveBayes) must have attribute feature_count_ for this to work. You can also do something like this to create a graph of importance features by order: RandomForestClassifier does not yet have a coef_ attrubute, but it will in the 0.17 release, I think. However, see the RandomForestClassifierWithCoef class in Recursive feature elimination on Random Forest using scikit-learn. This may give you some ideas to work around the limitation above. Not exactly what you are looking for, but a quick way to get the largest magnitude coefficients (assuming a pandas dataframe columns are your feature names):  You trained the model like:  Get the 10 largest negative coefficient values (or change to reverse=True for largest positive) like:  First make a list, I give this list the name label.  After that extracting all features name and column name I add in label list. Here I use naive bayes model. In naive bayes model, feature_log_prob_ give probability of features.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Why do we have to normalize the input for a neural network? I understand that sometimes, when for example the input values are non-numerical a certain transformation must be performed, but when we have a numerical input? Why the numbers must be in a certain interval? What will happen if the data is not normalized? It's explained well here. If the input variables are combined linearly, as in an MLP [multilayer perceptron], then it is
  rarely strictly necessary to standardize the inputs, at least in theory. The
  reason is that any rescaling of an input vector can be effectively undone by
  changing the corresponding weights and biases, leaving you with the exact
  same outputs as you had before. However, there are a variety of practical
  reasons why standardizing the inputs can make training faster and reduce the
  chances of getting stuck in local optima. Also, weight decay and Bayesian
  estimation can be done more conveniently with standardized inputs.  In neural networks, it is good idea not just to normalize data but also to scale them. This is intended for faster approaching to global minima at error surface. See the following pictures:
  Pictures are taken from the coursera course about neural networks. Author of the course is Geoffrey Hinton.  Some inputs to NN might not have a 'naturally defined' range of values. For example, the average value might be slowly, but continuously increasing over time (for example a number of records in the database).  In such case feeding this raw value into your network will not work very well. You will teach your network on values from lower part of range, while the actual inputs will be from the higher part of this range (and quite possibly above range, that the network has learned to work with).  You should normalize this value. You could for example tell the network by how much the value has changed since the previous input. This increment usually can be defined with high probability in a specific range, which makes it a good input for network. There are 2 Reasons why we have to Normalize Input Features before Feeding them to Neural Network: Reason 1: If a Feature in the Dataset is big in scale compared to others then this big scaled feature becomes dominating and as a result of that, Predictions of the Neural Network will not be Accurate. Example: In case of Employee Data, if we consider Age and Salary, Age will be a Two Digit Number while Salary can be 7 or 8 Digit (1 Million, etc..). In that Case, Salary will Dominate the Prediction of the Neural Network. But if we Normalize those Features, Values of both the Features will lie in the Range from (0 to 1). Reason 2: Front Propagation of Neural Networks involves the Dot Product of Weights with Input Features. So, if the Values are very high (for Image and Non-Image Data), Calculation of Output takes a lot of Computation Time as well as Memory. Same is the case during Back Propagation. Consequently, Model Converges slowly, if the Inputs are not Normalized. Example: If we perform Image Classification, Size of Image will be very huge, as the Value of each Pixel ranges from 0 to 255. Normalization in this case is very important. Mentioned below are the instances where Normalization is very important: When you use unnormalized input features, the loss function is likely to have very elongated valleys. When optimizing with gradient descent, this becomes an issue because the gradient will be steep with respect some of the parameters. That leads to large oscillations in the search space, as you are bouncing between steep slopes. To compensate, you have to stabilize optimization with small learning rates. Consider features x1 and x2, where range from 0 to 1 and 0 to 1 million, respectively. It turns out the ratios for the corresponding parameters (say, w1 and w2) will also be large. Normalizing tends to make the loss function more symmetrical/spherical. These are easier to optimize because the gradients tend to point towards the global minimum and you can take larger steps. Looking at the neural network from the outside, it is just a function that takes some  arguments and produces a result. As with all functions, it has a domain (i.e. a set of legal arguments). You have to normalize the values that you want to pass to the neural net in order to make sure it is in the domain. As with all functions, if the arguments are not in the domain, the result is not guaranteed to be appropriate.  The exact behavior of the neural net on arguments outside of the domain depends on the implementation of the neural net. But overall, the result is useless if the arguments are not within the domain. I believe the answer is dependent on the scenario. Consider NN (neural network) as an operator F, so that F(input) = output. In the case where this relation is linear so that F(A * input) = A * output, then you might choose to either leave the input/output unnormalised in their raw forms, or normalise both to eliminate A. Obviously this linearity assumption is violated in classification tasks, or nearly any task that outputs a probability, where F(A * input) = 1 * output In practice, normalisation allows non-fittable networks to be fittable, which is crucial to experimenters/programmers. Nevertheless, the precise impact of normalisation will depend not only on the network architecture/algorithm, but also on the statistical prior for the input and output.  What's more, NN is often implemented to solve very difficult problems in a black-box fashion, which means the underlying problem may have a very poor statistical formulation, making it hard to evaluate the impact of normalisation, causing the technical advantage (becoming fittable) to dominate over its impact on the statistics. In statistical sense, normalisation removes variation that is believed to be non-causal in predicting the output, so as to prevent NN from learning this variation as a predictor (NN does not see this variation, hence cannot use it).  The reason normalization is needed is because if you look at how an adaptive step proceeds in one place in the domain of the function, and you just simply transport the problem to the equivalent of the same step translated by some large value in some direction in the domain, then you get different results. It boils down to the question of adapting a linear piece to a data point.  How much should the piece move without turning and how much should it turn in response to that one training point?  It makes no sense to have a changed adaptation procedure in different parts of the domain! So normalization is required to reduce the difference in the training result.  I haven't got this written up, but you can just look at the math for a simple linear function and how it is trained by one training point in two different places.  This problem may have been corrected in some places, but I am not familiar with them.  In ALNs, the problem has been corrected and I can send you a paper if you write to wwarmstrong AT shaw.ca On a high level, if you observe as to where normalization/standardization is mostly used, you will notice that, anytime there is a use of magnitude difference in model building process, it becomes necessary to standardize the inputs so as to ensure that important inputs with small magnitude don't loose their significance midway the model building process. √(3-1)^2+(1000-900)^2 ≈ √(1000-900)^2 

Here, (3-1) contributes hardly a thing to the result and hence the input corresponding to these values is considered futile by the model. Consider the following: Both distance measure(Clustering) and cost function(NNs) use magnitude difference in some way and hence standardization ensures that magnitude difference doesn't command over important input parameters and the algorithm works as expected. Hidden layers are used in accordance with the complexity of our data. If we have input data which is linearly separable then we need not to use hidden layer e.g. OR gate but if we have a non linearly seperable data then we need to use hidden layer for example ExOR logical gate.
Number of nodes taken at any layer depends upon the degree of cross validation of our output.I've noticed that a frequent occurrence during training is NANs being introduced. Often times it seems to be introduced by weights in inner-product/fully-connected or convolution layers blowing up. Is this occurring because the gradient computation is blowing up? Or is it because of weight initialization (if so, why does weight initialization have this effect)? Or is it likely caused by the nature of the input data? The overarching question here is simply: What is the most common reason for NANs to occurring during training? And secondly, what are some methods for combatting this (and why do they work)? I came across this phenomenon several times. Here are my observations: Reason: large gradients throw the learning process off-track. What you should expect: Looking at the runtime log, you should look at the loss values per-iteration. You'll notice that the loss starts to grow significantly from iteration to iteration, eventually the loss will be too large to be represented by a floating point variable and it will become nan. What can you do: Decrease the base_lr (in the solver.prototxt) by an order of magnitude (at least). If you have several loss layers, you should inspect the log to see which layer is responsible for the gradient blow up and decrease the loss_weight (in train_val.prototxt) for that specific layer, instead of the general base_lr. Reason: caffe fails to compute a valid learning rate and gets 'inf' or 'nan' instead, this invalid rate multiplies all updates and thus invalidating all parameters. What you should expect: Looking at the runtime log, you should see that the learning rate itself becomes 'nan', for example: What can you do: fix all parameters affecting the learning rate in your 'solver.prototxt' file.
For instance, if you use lr_policy: "poly" and you forget to define max_iter parameter, you'll end up with  lr = nan...
For more information about learning rate in caffe, see this thread. Reason: Sometimes the computations of the loss in the loss layers causes nans to appear. For example, Feeding InfogainLoss layer with non-normalized values, using custom loss layer with bugs, etc. What you should expect: Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a nan appears. What can you do: See if you can reproduce the error, add printout to the loss layer and debug the error. For example: Once I used a loss that normalized the penalty by the frequency of label occurrence in a batch. It just so happened that if one of the training labels did not appear in the batch at all - the loss computed produced nans. In that case, working with large enough batches (with respect to the number of labels in the set) was enough to avoid this error. Reason: you have an input with nan in it! What you should expect: once the learning process "hits" this faulty input - output becomes nan. Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a nan appears. What can you do: re-build your input datasets (lmdb/leveldn/hdf5...) make sure you do not have bad image files in your training/validation set. For debug you can build a simple net that read the input layer, has a dummy loss on top of it and runs through all the inputs: if one of them is faulty, this dummy net should also produce nan. For some reason, choosing stride > kernel_size for pooling may results with nans. For example: results with nans in y. It was reported that under some settings "BatchNorm" layer may output nans due to numerical instabilities.
This issue was raised in bvlc/caffe and PR #5136 is attempting to fix it. Recently, I became aware of debug_info flag: setting debug_info: true in 'solver.prototxt' will make caffe print to log more debug information (including gradient magnitudes and activation values) during training: This information can help in spotting gradient blowups and other problems in the training process. In my case, not setting the bias in the convolution/deconvolution layers was the cause. Solution: add the following to the convolution layer parameters. This answer is not about a cause for nans, but rather proposes a way to help debug it. 
You can have this python layer: Adding this layer into your train_val.prototxt at certain points you suspect may cause trouble: learning_rate is high and should be decreased
The accuracy in the RNN code was nan, with select the low value for learning rate it fixes I was trying to build a sparse autoencoder and had several layers in it to induce sparsity. While running my net, I encountered the NaN's. On removing some of the layers (in my case, I actually had to remove 1), I found that the NaN's disappeared. So, I guess too much sparsity may lead to NaN's as well (some 0/0 computations may have been invoked!?) One more solution for anyone stuck like I just was- I was receiving nan or inf losses on a network I setup with float16 dtype across the layers and input data.  After all else failed, it occurred to me to switch back to float32, and the nan losses were solved! So bottom line, if you switched dtype to float16, change it back to float32.I have a set of dataframes where one of the columns contains a categorical variable. I'd like to convert it to several dummy variables, in which case I'd normally use get_dummies. What happens is that get_dummies looks at the data available in each dataframe to find out how many categories there are, and thus create the appropriate number of dummy variables. However, in the problem I'm working right now, I actually know in advance what the possible categories are. But when looking at each dataframe individually, not all categories necessarily appear. My question is: is there a way to pass to get_dummies (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s? Something that would make this: Become this: TL;DR: is there a way to pass to get_dummies (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s? Yes, there is! Pandas has a special type of Series just for categorical data. One of the attributes of this series is the possible categories, which get_dummies takes into account. Here's an example: Then, get_dummies will do exactly what you want! There are a bunch of other ways to create a categorical Series or DataFrame, this is just the one I find most convenient. You can read about all of them in the pandas documentation. EDIT: I haven't followed the exact versioning, but there was a bug in how pandas treats sparse matrices, at least until version 0.17.0. It was corrected by version 0.18.1 (released May 2016). For version 0.17.0, if you try to do this with the sparse=True option with a DataFrame, the column of zeros for the missing dummy variable will be a column of NaN, and it will be converted to dense. It looks like pandas 0.21.0 added a CategoricalDType, and creating categoricals which explicitly include the categories as in the original answer was deprecated, I'm not quite sure when. Using transpose and reindex Try this: I did ask this on the pandas github. Turns out it is really easy to get around it when you define the column as a Categorical where you define all the possible categories. get_dummies() will do the rest then as expected. I don't think get_dummies provides this out of the box, it only allows for creating an extra column that highlights NaN values.  To add the missing columns yourself, you could use pd.concat along axis=0 to vertically 'stack' the DataFrames (the dummy columns plus a DataFrame id)  and automatically create any missing columns, use fillna(0) to replace missing values, and then use .groupby('id') to separate the various DataFrame again. Adding the missing category in the test set: Notice that this code also remove column resulting from category in the test dataset but not present in the training dataset As suggested by others - Converting your Categorical features to 'category' data type should resolve the unseen label issue using 'get_dummies'. The shorter the better: Result: Notes: Here's a shorter-shorter version (changed the Index values): Result: Bonus track!  I imagine you have the categories because you did a previous dummy/one hot using training data. You can save the original encoding (.columns), and then apply during production time: Result: If you know your categories you can first apply pd.get_dummies() as you suggested and add the missing category columns afterwards. This will create your example with the missing cat_c: Now simply add the missing category columns with a union operation (as suggested here). I was recently looking to solve this same issue, but working with a multi-column dataframe and with two datasets (a train set and test set for a machine learning task). The test dataframe had the same categorical columns as the train dataframe, but some of these columns had missing categories that were present in the train dataframe. I did not want to manually define all the possible categories for every column. Instead, I combined the train and test dataframes into one, called get_dummies, and then split that back into two.I am working on classifying simple data using KNN with Euclidean distance. I have seen an example on what I would like to do that is done with the MATLAB knnsearch function as shown below: The above code takes a new point i.e. [5 1.45] and finds the 10 closest values to the new point. Can anyone please show me a MATLAB algorithm with a detailed explanation of what the knnsearch function does? Is there any other way to do this? The basis of the K-Nearest Neighbour (KNN) algorithm is that you have a data matrix that consists of N rows and M columns where N is the number of data points that we have, while M is the dimensionality of each data point.   For example, if we placed Cartesian co-ordinates inside a data matrix, this is usually a N x 2 or a N x 3 matrix.  With this data matrix, you provide a query point and you search for the closest k points within this data matrix that are the closest to this query point. We usually use the Euclidean distance between the query and the rest of your points in your data matrix to calculate our distances.  However, other distances like the L1 or the City-Block / Manhattan distance are also used.  After this operation, you will have N Euclidean or Manhattan distances which symbolize the distances between the query with each corresponding point in the data set.  Once you find these, you simply search for the k nearest points to the query by sorting the distances in ascending order and retrieving those k points that have the smallest distance between your data set and the query. Supposing your data matrix was stored in x, and newpoint is a sample point where it has M columns (i.e. 1 x M), this is the general procedure you would follow in point form: Let's do each step slowly. One way that someone may do this is perhaps in a for loop like so: If you wanted to implement the Manhattan distance, this would simply be: dists would be a N element vector that contains the distances between each data point in x and newpoint.  We do an element-by-element subtraction between newpoint and a data point in x, square the differences, then sum them all together.  This sum is then square rooted, which completes the Euclidean distance.  For the Manhattan distance, you would perform an element by element subtraction, take the absolute values, then sum all of the components together.  This is probably the most simplest of the implementations to understand, but it could possibly be the most inefficient... especially for larger sized data sets and larger dimensionality of your data. Another possible solution would be to replicate newpoint and make this matrix the same size as x, then doing an element-by-element subtraction of this matrix, then summing over all of the columns for each row and doing the square root.  Therefore, we can do something like this: For the Manhattan distance, you would do: repmat takes a matrix or vector and repeats them a certain amount of times in a given direction.  In our case, we want to take our newpoint vector, and stack this N times on top of each other to create a N x M matrix, where each row is M elements long.  We subtract these two matrices together, then square each component.  Once we do this, we sum over all of the columns for each row and finally take the square root of all result.  For the Manhattan distance, we do the subtraction, take the absolute value and then sum. However, the most efficient way to do this in my opinion would be to use bsxfun. This essentially does the replication that we talked about under the hood with a single function call.  Therefore, the code would simply be this: To me this looks much cleaner and to the point.  For the Manhattan distance, you would do: Now that we have our distances, we simply sort them.  We can use sort to sort our distances: d would contain the distances sorted in ascending order, while ind tells you for each value in the unsorted array where it appears in the sorted result.  We need to use ind, extract the first k elements of this vector, then use ind to index into our x data matrix to return those points that were the closest to newpoint. The final step is to now return those k data points that are closest to newpoint.  We can do this very simply by: ind_closest should contain the indices in the original data matrix x that are the closest to newpoint.  Specifically, ind_closest contains which rows you need to sample from in x to obtain the closest points to newpoint.  x_closest will contain those actual data points. For your copying and pasting pleasure, this is what the code looks like: Running through your example, let's see our code in action: By inspecting ind_closest and x_closest, this is what we get: If you ran knnsearch, you will see that your variable n matches up with ind_closest.  However, the  variable d returns the distances from newpoint to each point x, not the actual data points themselves.  If you want the actual distances, simply do the following after the code I wrote: Note that the above answer uses only one query point in a batch of N examples.  Very frequently KNN is used on multiple examples simultaneously.  Supposing that we have Q query points that we want to test in the KNN.  This would result in a k x M x Q matrix where for each example or each slice, we return the k closest points with a dimensionality of M.  Alternatively, we can return the IDs of the k closest points thus resulting in a Q x k matrix.  Let's compute both. A naive way to do this would be to apply the above code in a loop and loop over every example. Something like this would work where we allocate a Q x k matrix and apply the bsxfun based approach to set each row of the output matrix to the k closest points in the dataset, where we will use the Fisher Iris dataset just like what we had before.  We'll also keep the same dimensionality as we did in the previous example and I'll use four examples, so Q = 4 and M = 2: Though this is very nice, we can do even better.  There is a way to efficiently compute the squared Euclidean distance between two sets of vectors.  I'll leave it as an exercise if you want to do this with the Manhattan.  Consulting this blog, given that A is a Q1 x M matrix where each row is a point of dimensionality M with Q1 points and B is a Q2 x M matrix where each row is also a point of dimensionality M with Q2 points, we can efficiently compute a distance matrix D(i, j) where the element at row i and column j denotes the distance between row i of A and row j of B using the following matrix formulation: Therefore, if we let A be a matrix of query points and B be the dataset consisting of your original data, we can determine the k closest points by sorting each row individually and determining the k locations of each row that were the smallest.  We can also additionally use this to retrieve the actual points themselves. Therefore: We see that we used the logic for computing the distance matrix is the same but some variables have changed to suit the example.  We also sort each row independently using the two input version of sort and so ind will contain the IDs per row and d will contain the corresponding distances.  We then figure out which indices are the closest to each query point by simply truncating this matrix to k columns.  We then use permute and reshape to determine what the associated closest points are.  We first use all of the closest indices and create a point matrix that stacks all of the IDs on top of each other so we get a Q * k x M matrix.  Using reshape and permute allows us to create our 3D matrix so that it becomes a k x M x Q matrix like we have specified.  If you wanted to get the actual distances themselves, we can index into d and grab what we need.  To do this, you will need to use sub2ind to obtain the linear indices so we can index into d in one shot.  The values of ind_closest already give us which columns we need to access.  The rows we need to access are simply 1, k times, 2, k times, etc. up to Q.  k is for the number of points we wanted to return: When we run the above code for the above query points, these are the indices, points and distances we get: To compare this with knnsearch, you would instead specify a matrix of points for the second parameter where each row is a query point and you will see that the indices and sorted distances match between this implementation and knnsearch. Hope this helps you.  Good luck!I'd like to choose the best algorithm for future. I found some solutions, but I didn't understand which R-Squared value is correct. For this, I divided my data into two as test and training, and I printed two different R squared values ​​below. First R-Squared result is -4.28.
Second R-Squared result is 0.84 But I didn't understand which value is correct. Arguably, the real challenge in such cases is to be sure that you compare apples to apples. And in your case, it seems that you don't. Our best friend is always the relevant documentation, combined with simple experiments. So... Although scikit-learn's LinearRegression() (i.e. your 1st R-squared) is fitted by default  with fit_intercept=True (docs), this is not the case with statsmodels' OLS (your 2nd R-squared); quoting from the docs: An intercept is not included by default and should be added by the user. See statsmodels.tools.add_constant. Keeping this important detail in mind, let's run some simple experiments with dummy data: For all practical purposes, these two values of R-squared produced by scikit-learn and statsmodels are identical. Let's go a step further, and try a scikit-learn model without intercept, but where we use the artificially "intercepted" data X_ we have already built for use with statsmodels: Again, the R-squared is identical with the previous values. So, what happens when we "accidentally" forget to account for the fact that statsmodels OLS is fitted without an intercept? Let's see: Well, an R-squared of 0.80 is indeed very far from the one of 0.16 returned by a model with an intercept, and arguably this is exactly what has happened in your case. So far so good, and I could easily finish the answer here; but there is indeed a point where this harmonious world breaks down: let's see what happens when we fit both models without intercept and with the initial data X where we have not artificially added any interception. We have already fitted the OLS model above, and got an R-squared of 0.80; what about a similar model from scikit-learn? Ooops...! What the heck?? It seems that scikit-earn, when computes the r2_score, always assumes an intercept, either explicitly in the model (fit_intercept=True) or implicitly in the data (the way we have produced X_ from X above, using statsmodels' add_constant); digging a little online reveals a Github thread (closed without a remedy) where it is confirmed that the situation is indeed like that. [UPDATE Dec 2021: for a more detailed & in-depth investigation and explanation of why the two scores are different in this particular case (i.e. both models fitted without an intercept), see this great answer by Flavia] Let me clarify that the discrepancy I have described above has nothing to do with your issue: in your case, the real issue is that you are actually comparing apples (a model with intercept) with oranges (a model without intercept). So, why scikit-learn not only fails in such an (admittedly edge) case, but even when the fact emerges in a Github issue it is actually treated with indifference? (Notice also that the scikit-learn core developer who replies in the above thread casually admits that "I'm not super familiar with stats"...). The answer goes a little beyond coding issues, such as the ones SO is mainly about, but it may be worth elaborating a little here. Arguably, the reason is that the whole R-squared concept comes in fact directly from the world of statistics, where the emphasis is on interpretative models, and it has little use in machine learning contexts, where the emphasis is clearly on predictive models; at least AFAIK, and beyond some very introductory courses, I have never (I mean never...) seen a predictive modeling problem where the R-squared is used for any kind of performance assessment; neither it's an accident that popular machine learning introductions, such as Andrew Ng's Machine Learning at Coursera, do not even bother to mention it. And, as noted in the Github thread above (emphasis added): In particular when using a test set, it's a bit unclear to me what the R^2 means. with which I certainly concur. As for the edge case discussed above (to include or not an intercept term?), I suspect it would sound really irrelevant to modern deep learning practitioners, where the equivalent of an intercept (bias parameters) is always included by default in neural network models... See the accepted (and highly upvoted) answer in the Cross Validated question Difference between statsmodel OLS and scikit linear regression for a more detailed discussion along these last lines. The discussion (and links) in Is R-squared Useless?, triggered by some relevant (negative) remarks by the great statistician Cosma Shalizi, is also enlightening and highly recommended. You seem to be using sklearn.metrics_r2_score. The documentation states that Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse) The Wikipedia article which the documentation leads to points out that values of R2 outside the range 0 to 1 can occur when the model fits the data worse than a horizontal hyperplane. This would occur when the wrong model was chosen, or nonsensical constraints were applied by mistake. For this reason, the fact that you had such a negative r2_score is probably far more significant than that you had a relatively good (but not great) R^2 statistic computed in the other way. If the first score indicates that your model choice is poor then the second statistic is likely to be just an artifact of overfitting. As you note, and as the Wikipedia article notes, there are multiple definitions of "r squared" or "R squared." However, the common ones all have the property that they range from 0 to 1. They are usually positive, as is clear from the "squared" part of the name. (For exceptions to this general rule, see the Wikipedia article.) Your "First R-Squared result" is -4.28, which is not between 0 and 1 and is not even positive. Thus it is not really an "R squared" at all. So use the "Second R-Squared result" which is in the correct range.I am trying to build a model to predict house prices. I have some features X (no. of bathrooms , etc.) and target Y (ranging around $300,000 to $800,000) I have used sklearn's Standard Scaler to standardize Y before fitting it to the model. Here is my Keras model: I am having trouble trying to interpret the results -- what does a MSE of 0.617454319755 mean? Do I have to inverse transform this number, and square root the results, getting an error rate of 741.55 in dollars? I apologise for sounding silly as I am starting out! I apologise for sounding silly as I am starting out! Do not; this is a subtle issue of great importance, which is usually (and regrettably) omitted in tutorials and introductory expositions. Unfortunately, it is not as simple as taking the square root of the inverse-transformed MSE, but it is not that complicated either; essentially what you have to do is: in order to get a performance indicator of your model that will be meaningful in the business context of your problem (e.g. US dollars here). Let's see a quick example with toy data, omitting the model itself (which is irrelevant here, and in fact can be any regression model - not only a Keras one): Now, let's say that we fit our Keras model (not shown here) using the scaled sets X_train and Y_train, and get predictions on the training set: The MSE reported by Keras is actually the scaled MSE, i.e.: while the 3 steps I have described above are simply: So, in our case, if our initial Y were US dollars, the actual error in the same units (dollars) would be 0.32 (dollars). Notice how the naive approach of inverse-transforming the scaled MSE would give a very different (and incorrect) result: MSE is mean square error, here is the formula.
 Basically it is a mean of square of different of expected output and prediction. Making square root of this will not give you the difference between error and output. This is useful for training.  Currently  you have build a model. 
If you want to train the model use these function. If you want to do prediction of the output you should use following code. You can find more details here. https://keras.io/models/about-keras-models/ https://keras.io/models/sequential/I try to understand LSTMs and how to build them with Keras. I found out, that there are principally the 4 modes to run a RNN (the 4 right ones in the picture) 
Image source: Andrej Karpathy Now I wonder how a minimalistic code snippet for each of them would look like in Keras.
So something like for each of the 4 tasks, maybe with a little bit of explanation. So: One-to-one: you could use a Dense layer as you are not processing sequences: One-to-many: this option is not supported well as chaining models is not very easy in Keras, so the following version is the easiest one: Many-to-one: actually, your code snippet is (almost) an example of this approach: Many-to-many: This is the easiest snippet when the length of the input and output matches the number of recurrent steps: Many-to-many when number of steps differ from input/output length: this is freaky hard in Keras. There are no easy code snippets to code that. EDIT: Ad 5 In one of my recent applications, we implemented something which might be similar to many-to-many from the 4th image. In case you want to have a network with the following architecture (when an input is longer than the output): You could achieve this in the following manner: Where N is the number of last steps you want to cover (on image N = 3). From this point getting to: is as simple as artificial padding sequence of length N using e.g. with 0 vectors, in order to adjust it to an appropriate size. Great Answer by @Marcin Możejko I would add the following to NR.5 (many to many with different in/out length): A) as Vanilla LSTM B) as Encoder-Decoder LSTMPerhaps too general a question, but can anyone explain what would cause a Convolutional Neural Network to diverge? Specifics: I am using Tensorflow's iris_training model with some of my own data and keep getting ERROR:tensorflow:Model diverged with loss = NaN. Traceback... tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError: NaN loss during training. Traceback originated with line: I've tried adjusting the optimizer, using a zero for learning rate, and using no optimizer. Any insights into network layers, data size, etc is appreciated. There are lots of things I have seen make a model diverge. Too high of a learning rate.  You can often tell if this is the case if the loss begins to increase and then diverges to infinity. I am not to familiar with the DNNClassifier but I am guessing it uses the categorical cross entropy cost function.  This involves taking the log of the prediction which diverges as the prediction approaches zero.  That is why people usually add a small epsilon value to the prediction to prevent this divergence. I am guessing the DNNClassifier probably does this or uses the tensorflow opp for it.  Probably not the issue. Other numerical stability issues can exist such as division by zero where adding the epsilon can help.  Another less obvious one if the square root whose derivative can diverge if not properly simplified when dealing with finite precision numbers. Yet again I doubt this is the issue in the case of the DNNClassifier. You may have an issue with the input data.  Try calling assert not np.any(np.isnan(x)) on the input data to make sure you are not introducing the nan.  Also make sure all of the target values are valid.  Finally, make sure the data is properly normalized. You probably want to have the pixels in the range [-1, 1] and not [0, 255]. The labels must be in the domain of the loss function, so if using a logarithmic-based loss function all labels must be non-negative (as noted by evan pu and the comments below). If you're training for cross entropy, you want to add a small number like 1e-8 to your output probability. Because log(0) is negative infinity, when your model trained enough the output distribution will be very skewed, for instance say I'm doing a 4 class output, in the beginning my probability looks like but toward the end the probability will probably look like And you take a cross entropy of this distribution everything will explode. The fix is to artifitially add a small number to all the terms to prevent this. In my case I got NAN when setting distant integer LABELs. ie: So, not use a very distant Label. EDIT
You can see the effect in the following simple code: The result shows the NANs after adding the label 8000: If using integers as targets, makes sure they aren't symmetrical at 0.  I.e., don't use classes -1, 0, 1. Use instead 0, 1, 2. If you'd like to gather more information on the error and if the error occurs in the first few iterations, I suggest you run the experiment in CPU-only mode (no GPUs).  The error message will be much more specific.   Source: https://github.com/tensorflow/tensor2tensor/issues/574 Although most of the points are already discussed. But I would like to highlight again one more reason for NaN which is missing. By default activation function is "Relu". It could be possible that intermediate layer's generating a negative value and "Relu" convert it into the 0. Which gradually stops training. I observed the "LeakyRelu" able to solve such problems. Regularization can help. For a classifier, there is a good case for activity regularization, whether it is binary or a multi-class classifier. For a regressor, kernel regularization might be more appropriate.  The reason for nan, inf or -inf often comes from the fact that division by 0.0 in TensorFlow doesn't result in a division by zero exception. It could result in a nan, inf or -inf "value". In your training data you might have 0.0 and thus in your loss function it could happen that you perform a division by 0.0. Output is the following tensor: Adding a small eplison (e.g., 1e-5) often does the trick. Additionally, since TensorFlow 2 the opteration tf.math.division_no_nan is defined. I'd like to plug in some (shallow) reasons I have experienced as follows:      Hope that helps.   I found some interesting thing when battling whit this problem,in addition to the above answers when your data labels are arranged like below  applying shuffle to data may help: I had the same problem. My labels were enjoyment ratings [1, 3, 5]. I read all the answers and they didn't make much sense to the problem I was facing. I changed the labels to [0 1 2] and it worked. Don't know how this happened. TensorFlow uses the labels as positions in a tensor in some contexts so they have to be 0, 1, ..., L-1.  Negative numbers, non-integers, etc. can instead cause the loss to be NaN. The reason could also be using very small values (like 1e9).
Try replacing them with: Or (If you manually changed tf.keras.backend.floatx):Considering the example code. I would like to know How to apply gradient clipping on this network on the RNN where there is a possibility of exploding gradients. This is an example that could be used but where do I introduce this ?
In the def of RNN  But this doesn't make sense as the tensor _X is the input and not the grad what is to be clipped?  Do I have to define my own Optimizer for this or is there a simpler option? Gradient clipping needs to happen after computing the gradients, but before applying them to update the model's parameters. In your example, both of those things are handled by the AdamOptimizer.minimize() method. In order to clip your gradients you'll need to explicitly compute, clip, and apply them as described in this section in TensorFlow's API documentation. Specifically you'll need to substitute the call to the minimize() method with something like the following: Despite what seems to be popular, you probably want to clip the whole gradient by its global norm: Clipping each gradient matrix individually changes their relative scale but is also possible: In TensorFlow 2, a tape computes the gradients, the optimizers come from Keras, and we don't need to store the update op because it runs automatically without passing it to a session: It's easy for tf.keras! This optimizer will clip all gradients to values between [-1.0, 1.0]. See the docs. This is actually properly explained in the documentation.: Calling minimize() takes care of both computing the gradients and
  applying them to the variables. If you want to process the gradients
  before applying them you can instead use the optimizer in three steps: And in the example they provide they use these 3 steps: Here MyCapper is any function that caps your gradient. The list of useful functions (other than tf.clip_by_value()) is here. For those who would like to understand the idea of gradient clipping (by norm): Whenever the gradient norm is greater than a particular threshold, we clip the gradient norm so that it stays within the threshold. This threshold is sometimes set to 5. Let the gradient be g and the max_norm_threshold be j.  Now, if ||g|| > j , we do: g = (  j * g ) /  ||g|| This is the implementation done in tf.clip_by_norm IMO the best solution is wrapping your optimizer with TF's estimator decorator tf.contrib.estimator.clip_gradients_by_norm: This way you only have to define this once, and not run it after every gradients calculation. Documentation:
https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/clip_gradients_by_norm Gradient Clipping basically helps in case of exploding or vanishing gradients.Say your loss is too high which will result in exponential gradients to flow through the network which may result in Nan values . To overcome this we clip gradients within a specific range (-1 to 1 or any range as per condition) . clipped_value=tf.clip_by_value(grad, -range, +range), var) for grad, var in grads_and_vars
 where grads _and_vars are the pairs of gradients (which you calculate via tf.compute_gradients) and their variables they will be applied to. After clipping we simply apply its value using an optimizer.
optimizer.apply_gradients(clipped_value) if  you are training your model using your custom training loop then the one update step will look like Or you could also simply just replace the first line in above code as below second method will also work if you are using model.compile -> model.fit pipeline.How do I initialize weights and biases of a network (via e.g. He or Xavier initialization)? To initialize the weights of a single layer, use a function from torch.nn.init. For instance: Alternatively, you can modify the parameters by writing to conv1.weight.data (which is a torch.Tensor). Example: The same applies for biases: Pass an initialization function to torch.nn.Module.apply. It will initialize the weights in the entire nn.Module recursively. apply(fn): Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init). Example: If you follow the principle of Occam's razor, you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case. With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.  A uniform distribution has the equal probability of picking any number from a set of numbers.  Let's see how well the neural network trains using a uniform weight initialization, where low=0.0 and high=1.0. Below, we'll see another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, we can:  The general rule for setting the weights in a neural network is to set them to be close to zero without being too small.  Good practice is to start your weights in the range of [-y, y] where y=1/sqrt(n)
  (n is the number of inputs to a given neuron). below we compare performance of NN, weights initialized with uniform distribution [-0.5,0.5) versus the one whose weight is initialized using general rule  The normal distribution should have a mean of 0 and a standard deviation of y=1/sqrt(n), where n is the number of inputs to NN below we show the performance of two NN one initialized using uniform-distribution and the other using normal-distribution   To initialize layers, you typically don't need to do anything. PyTorch will do it for you. If you think about it, this makes a lot of sense. Why should we initialize layers, when PyTorch can do that following the latest trends? For instance, the Linear layer's __init__ method will do Kaiming He initialization: Similarly, this holds for other layers types. For e.g., Conv2d, check here. NOTE: The advantage of proper initialization is faster training speed. If your problem requires special initialization, you can still do it afterwards. If you want some extra flexibility, you can also set the weights manually.  Say you have input of all ones: And you want to make a dense layer with no bias (so we can visualize): Set all the weights to 0.5 (or anything else): The weights: All your weights are now 0.5. Pass the data through: Remember that each neuron receives 8 inputs, all of which have weight 0.5 and value of 1 (and no bias), so it sums up to 4 for each.  Sorry for being so late, I hope my answer will help. To initialise weights with a normal distribution use: Or to use a constant distribution write: Or to use an uniform distribution: You can check other methods to initialise tensors here If you cannot use apply for instance if the model does not implement Sequential directly: You can try with torch.nn.init.constant_(x, len(x.shape)) to check that they are appropriately initialized: Here is the better way, just pass your whole model Cuz I haven't had the enough reputation so far, I can't add a comment under  the answer posted by prosti in Jun 26 '19 at 13:16. But I wanna point out that actually we know some assumptions in the paper of Kaiming He, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, are not appropriate, though it looks like the deliberately designed initialization method makes a hit in practice. E.g., within the subsection of Backward Propagation Case, they assume that $w_l$ and $\delta y_l$ are independent of each other. But as we all known, take the score map $\delta y^L_i$ as an instance, it often is $y_i-softmax(y^L_i)=y_i-softmax(w^L_ix^L_i)$ if we use a typical cross entropy loss function objective. So I think the true underlying reason why He's Initialization works well remains to unravel. Cuz everyone has witnessed its power on boosting deep learning training. If you see a deprecation warning (@Fábio Perez)...We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 2 years ago. I noticed that LSH seems a good way to find similar items with high-dimension properties. After reading the paper http://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf, I'm still confused with those formulas. Does anyone know a blog or article that explains that the easy way? The best tutorial I have seen for LSH is in the book: Mining of Massive Datasets.
Check Chapter 3 - Finding Similar Items
http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf Also I recommend the below slide:
http://www.cs.jhu.edu/%7Evandurme/papers/VanDurmeLallACL10-slides.pdf .
The example in the slide helps me a lot in understanding the hashing for cosine similarity. I borrow two slides from Benjamin Van Durme & Ashwin Lall, ACL2010 and try to explain the intuitions of LSH Families for Cosine Distance a bit.
  I have some sample code (just 50 lines) in python here which is using cosine similarity. 
https://gist.github.com/94a3d425009be0f94751 Tweets in vector space can be a great example of high dimensional data. Check out my blog post on applying Locality Sensitive Hashing to tweets to find similar ones.  http://micvog.com/2013/09/08/storm-first-story-detection/ And because one picture is a thousand words check the picture below: 
http://micvog.files.wordpress.com/2013/08/lsh1.png Hope it helps.
@mvogiatzis Here's a presentation from Stanford that explains it. It made a big difference for me. Part two is more about LSH, but part one covers it as well. A picture of the overview (There are much more in the slides):  Near Neighbor Search in High Dimensional Data - Part1:
http://www.stanford.edu/class/cs345a/slides/04-highdim.pdf Near Neighbor Search in High Dimensional Data - Part2:
http://www.stanford.edu/class/cs345a/slides/05-LSH.pdf  It is important to underline that different similarity measures have different implementations of LSH.   In my blog, I tried to thoroughly explain LSH for the cases of minHashing( jaccard similarity measure) and simHashing (cosine distance measure). I hope you find it useful:
https://aerodatablog.wordpress.com/2017/11/29/locality-sensitive-hashing-lsh/ I am a visual person. Here is what works for me as an intuition. Say each of the things you want to search for approximately are physical objects such as an apple, a cube, a chair. My intuition for an LSH is that it is similar to take the shadows of these objects. Like if you take the shadow of a 3D cube you get a 2D square-like on a piece of paper, or a 3D sphere will get you a circle-like shadow on a piece of paper. Eventually, there are many more than three dimensions in a search problem (where each word in a text could be one dimension) but the shadow analogy is still very useful to me. Now we can efficiently compare strings of bits in software. A fixed length bit string is kinda, more or less, like a line in a single dimension.  So with an LSH, I project the shadows of objects eventually as points (0 or 1) on a single fixed length line/bit string. The whole trick is to take the shadows such that they still make sense in the lower dimension  e.g. they resemble the original object in a good enough way that can be recognized.  A 2D drawing of a cube in perspective tells me this is a cube. But I cannot distinguish easily a 2D square from a 3D cube shadow without perspective: they both looks like a square to me.  How I present my object to the light will determine if I get a good recognizable shadow or not. So I think of a "good" LSH as the one that will turn my objects in front of a light such that their shadow is best recognizable as representing my object. So to recap: I think of things to index with an LSH as physical objects like a cube, a table, or chair, and I project their shadows in 2D and eventually along a line (a bit string). And a "good" LSH "function" is how I present my objects in front of a light to get an approximately distinguishable shape in the 2D flatland and later my bit string. Finally when I want to search if an object I have is similar to some objects that I indexed, I take the shadows of this "query" object using the same way to present my object in front of the light (eventually ending up with a bit string too). And now I can compare how similar is that bit string with all my other indexed bit strings which is a proxy for searching for my whole objects if I found a good and recognizable way to present my objects to my light. As a very short, tldr answer:  An example of locality sensitive hashing could be to first set planes randomly (with a rotation and offset) in your space of inputs to hash, and then to drop your points to hash in the space, and for each plane you measure if the point is above or below it (e.g.: 0 or 1), and the answer is the hash. So points similar in space will have a similar hash if measured with the cosine distance before or after.  You could read this example using scikit-learn: https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-LayerMy theta from the above code is 100.2 100.2, but it should be 100.2 61.09 in matlab which is correct. I think your code is a bit too complicated and it needs more structure, because otherwise you'll be lost in all equations and operations. In the end this regression boils down to four operations: In your case, I guess you have confused m with n. Here m denotes the number of examples in your training set, not the number of features. Let's have a look at my variation of your code: At first I create a small random dataset which should look like this:  As you can see I also added the generated regression line and formula that was calculated by excel. You need to take care about the intuition of the regression using gradient descent. As you do a complete batch pass over your data X, you need to reduce the m-losses of every example to a single weight update. In this case, this is the average of the sum over the gradients, thus the division by m.  The next thing you need to take care about is to track the convergence and adjust the learning rate. For that matter you should always track your cost every iteration, maybe even plot it. If you run my example, the theta returned will look like this: Which is actually quite close to the equation that was calculated by excel (y = x + 30). Note that as we passed the bias into the first column, the first theta value denotes the bias weight. Below you can find my implementation of gradient descent for linear regression problem.  At first, you calculate gradient like X.T * (X * w - y) / N and update your current theta with this gradient simultaneously.  Here is the python code: 

 Most of these answers are missing out some explanation on linear regression, as well as having code that is a little convoluted IMO. The thing is, if you have a dataset of "m" samples, each sample called "x^i" (n-dimensional vector), and a vector of outcomes y (m-dimensional vector), you can construct the following matrices:  Now, the goal is to find "w" (n+1 dimensional vector), which describes the line for your linear regression, "w_0" is the constant term, "w_1" and so on are your coefficients of each dimension (feature) in an input sample. So in essence, you want to find "w" such that "X*w" is as close to "y" as possible, i.e. your line predictions will be as close to the original outcomes as possible. Note also that we added an extra component/dimension at the start of each "x^i", which is just "1", to account for the constant term. In addition, "X" is just the matrix you get by "stacking" each outcome as a row, so it's an (m by n+1) matrix. Once you construct that, the Python & Numpy code for gradient descent is actually very straight forward: And voila! That returns the vector "w", or description of your prediction line. But how does it work?
In the code above, I am finding the gradient vector of the cost function (squared differences, in this case), then we are going "against the flow", to find the minimum cost given by the best "w". The actual formula used is in the line For the full maths explanation, and code including the creation of the matrices, see this post on how to implement gradient descent in Python. Edit: For illustration, the above code estimates a line which you can use to make predictions. The image below shows an example of the "learned" gradient descent line (in red), and the original data samples (in blue scatter) from the "fish market" dataset from Kaggle.  I know this question already have been answer but I have made some update to the GD function : This function reduce the alpha over the iteration making the function too converge faster see Estimating linear regression with Gradient Descent (Steepest Descent) for an example in R. I apply the same logic but in Python. Following @thomas-jungblut implementation in python, i did the same for Octave. If you find something wrong please let me know and i will fix+update. Data comes from a txt file with the following rows: think about it as a very rough sample for features [number of bedrooms] [mts2] and last column [rent price] which is what we want to predict. Here is the Octave implementation:When creating a Sequential model in Keras, I understand you provide the input shape in the first layer. Does this input shape then make an implicit input layer? For example, the model below explicitly specifies 2 Dense layers, but is this actually a model with 3 layers consisting of one input layer implied by the input shape, one hidden dense layer with 32 neurons, and then one output layer with 10 possible outputs? Well, it actually is an implicit input layer indeed, i.e. your model is an example of a "good old" neural net with three layers - input, hidden, and output. This is more explicitly visible in the Keras Functional API (check the example in the docs), in which your model would be written as: Actually, this implicit input layer is the reason why you have to include an input_shape argument only in the first (explicit) layer of the model in the Sequential API - in subsequent layers, the input shape is inferred from the output of the previous ones (see the comments in the source code of core.py). You may also find the documentation on tf.contrib.keras.layers.Input enlightening. It depends on your perspective :-) Rewriting your code in line with more recent Keras tutorial examples, you would probably use: ...which makes it much more explicit that you only have 2 Keras layers. And this is exactly what you do have (in Keras, at least) because the "input layer" is not really a (Keras) layer at all: it's only a place to store a tensor, so it may as well be a tensor itself. Each Keras layer is a transformation that outputs a tensor, possibly of a different size/shape to the input. So while there are 3 identifiable tensors here (input, outputs of the two layers), there are only 2 transformations involved corresponding to the 2 Keras layers. On the other hand, graphically, you might represent this network with 3 (graphical) layers of nodes, and two sets of lines connecting the layers of nodes. Graphically, it's a 3-layer network. But "layers" in this graphical notation are bunches of circles that sit on a page doing nothing, whereas a layers in Keras transform tensors and do actual work for you. Personally, I would get used to the Keras perspective :-) Note finally that for fun and/or simplicity, I substituted input_dim=784 for input_shape=(784,) to avoid the syntax that Python uses to both confuse newcomers and create a 1-D tuple: (<value>,).If there's a binary classification problem, the labels are 0 and 1.
I know the prediction is a floating-point number because p is the probability of belonging to that class.  The following is the cross-entropy loss function.  However, p is not necessarily 0 or 1, so how does Keras calculate the accuracy? Will Keras automatically round our predictions to 0 or 1? For example, in the following code, the accuracy is 0.749, but the targets are 0 and 1 and the predictions are floating-point numbers that are not necessarily 0.0 or 1.0. You are a little confused here; you speak about accuracy, while showing the formula for the loss. The equation you show is indeed the cross-entropy loss formula for binary classification (or simply logistic loss). y[i] are the labels, which are indeed either 0 or 1. p[i] are the predictions, usually interpreted as probabilities, which are real numbers in [0,1] (without any rounding). Now for each i, only one term in the sum will survive - the first term vanishes when y[i] = 0, and similarly the second term vanishes when y[i] = 1. Let's see some examples: Suppose that y[0] = 1, while we have predicted p[0] = 0.99 (i.e. a rather good prediction). The second term of the sum vanishes (since 1 - y[0] = 0), while the first one becomes log(0.99) = -0.01; so, the contribution of this sample prediction (i=0) to the overall loss is 0.01 (due to the - sign in front of the sum). Suppose now that the true label of the next sample is again 1, i.e. y[1] = 1, but here we have made a rather poor prediction of p[1] = 0.1; again, the second term vanishes, and the contribution of this prediction to the overall loss is now -log(0.1) = 2.3, which is indeed greater than our first, good prediction, as we should expect intuitively. As a final example, let's suppose that y[2] = 0, and we have made a perfectly good prediction here of p[2] = 0; hence, the first term vanishes, and the second term becomes  i.e. we have no loss contributed, again as we intuitively expected, since we have made a perfectly good prediction here for i=2. The logistic loss formula simply computes all these errors of the individual predictions, sums them, and divides by their number n. Nevertheless, this is the loss (i.e. scores[0] in your snippet), and not the accuracy. Loss and accuracy are different things; roughly speaking, the accuracy is what we are actually interested in from a business perspective, while the loss is the objective function that the learning algorithms (optimizers) are trying to minimize from a mathematical perspective. Even more roughly speaking, you can think of the loss as the "translation" of the business objective (accuracy) to the mathematical domain, a translation which is necessary in classification problems (in regression ones, usually the loss and the business objective are the same, or at least can be the same in principle, e.g. the RMSE)... Will Keras automatically round our predictions to 0 or 1? Actually yes: to compute the accuracy, we implicitly set a threshold in the predicted probabilities (usually 0.5 in binary classification, but this may differ in the case of highly imbalanced data); so, in model.evaluate, Keras actually converts our predictions to 1 if p[i] > 0.5 and to 0 otherwise. Then, the accuracy is computed by simply counting the cases where y_true==y_pred (correct predictions) and dividing by the total number of samples, to give a number in [0,1]. So, to summarize:In the following TensorFlow function, we must feed the activation of artificial neurons in the final layer. That I understand. But I don't understand why it is called logits? Isn't that a mathematical function?  Logits is an overloaded term which can mean many different things: In Math, Logit is a function that maps probabilities ([0, 1]) to R ((-inf, inf))  Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to > 0.5. In ML, it can be  the vector of raw (non-normalized) predictions that a classification
  model generates, which is ordinarily then passed to a normalization
  function. If the model is solving a multi-class classification
  problem, logits typically become an input to the softmax function. The
  softmax function then generates a vector of (normalized) probabilities
  with one value for each possible class. Logits also sometimes refer to the element-wise inverse of the sigmoid function. Just adding this clarification so that anyone who scrolls down this much can at least gets it right, since there are so many wrong answers upvoted. Diansheng's answer and JakeJ's answer get it right.
A new answer posted by Shital Shah is an even better and more complete answer. Yes, logit  as a mathematical function in statistics, but the logit used in context of neural networks is different. Statistical logit doesn't even make any sense here. I couldn't find a formal definition anywhere, but logit basically means: The raw predictions which come out of the last layer of the neural network.
  1. This is the very tensor on which you apply the argmax function to get the predicted class.
  2. This is the very tensor which you feed into the softmax function to get the probabilities for the predicted classes. Also, from a tutorial on official tensorflow website: The final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0–9), with linear activation (the default): If you are still confused, the situation is like this: where, predicted_class_index_by_raw and predicted_class_index_by_prob will be equal. Another name for raw_predictions in the above code is logit. As for the why logit... I have no idea. Sorry.
[Edit: See this answer for  the historical motivations behind the term.] Although, if you want to, you can apply statistical logit to probabilities that come out of the softmax function.  If the probability of a certain class is p,
Then the log-odds of that class is L = logit(p). Also, the probability of that class can be recovered as p = sigmoid(L), using the sigmoid function. Not very useful to calculate log-odds though. Summary In context of deep learning the logits layer means the layer that feeds in to softmax (or other such normalization). The output of the softmax are the probabilities for the classification task and its input is logits layer. The logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1. Historical Context Where does this term comes from? In 1930s and 40s, several people were trying to adapt linear regression to the problem of predicting probabilities. However linear regression produces output from -infinity to +infinity while for probabilities our desired output is 0 to 1. One way to do this is by somehow mapping the probabilities 0 to 1 to -infinity to +infinity and then use linear regression as usual. One such mapping is cumulative normal distribution that was used by Chester Ittner Bliss in 1934 and he called this "probit" model, short for "probability unit". However this function is computationally expensive while lacking some of the desirable properties for multi-class classification. In 1944 Joseph Berkson used the function log(p/(1-p)) to do this mapping and called it logit, short for "logistic unit". The term logistic regression derived from this as well. The Confusion Unfortunately the term logits is abused in deep learning. From pure mathematical perspective logit is a function that performs above mapping. In deep learning people started calling the layer "logits layer" that feeds in to logit function. Then people started calling the output values of this layer "logit" creating the confusion with logit the function. TensorFlow Code Unfortunately TensorFlow code further adds in to confusion by names like tf.nn.softmax_cross_entropy_with_logits. What does logits mean here? It just means the input of the function is supposed to be the output of last neuron layer as described above. The _with_logits suffix is redundant, confusing and pointless. Functions should be named without regards to such very specific contexts because they are simply mathematical operations that can be performed on values derived from many other domains. In fact TensorFlow has another similar function sparse_softmax_cross_entropy where they fortunately forgot to add _with_logits suffix creating inconsistency and adding in to confusion. PyTorch on the other hand simply names its function without these kind of suffixes. Reference The Logit/Probit lecture slides is one of the best resource to understand logit. I have also updated Wikipedia article with some of above information. Logit is a function that maps probabilities [0, 1] to [-inf, +inf].  Softmax is a function that maps [-inf, +inf] to [0, 1] similar as Sigmoid. But Softmax also normalizes the sum of the values(output vector) to be 1. Tensorflow "with logit": It means that you are applying a softmax function to logit numbers to normalize it. The input_vector/logit is not normalized and can scale from [-inf, inf].  This normalization is used for multiclass classification problems. And for multilabel classification problems sigmoid normalization is used i.e. tf.nn.sigmoid_cross_entropy_with_logits Personal understanding, in TensorFlow domain, logits are the values to be used as input to softmax. I came to this understanding based on this tensorflow tutorial. https://www.tensorflow.org/tutorials/layers Although it is true that logit is a function in maths(especially in statistics), I don't think that's the same 'logit' you are looking at. In the book Deep Learning by Ian Goodfellow, he mentioned, The function σ−1(x) is called the logit in statistics, but this term
  is more rarely used in machine learning. σ−1(x) stands for the
  inverse function of logistic sigmoid function. In TensorFlow, it is frequently seen as the name of last layer. In Chapter 10 of the book Hands-on Machine Learning with Scikit-learn and TensorFLow by Aurélien Géron, I came across this paragraph, which stated logits layer clearly. note that logits is the output of the neural network before going
  through the softmax activation function: for optimization reasons, we
  will handle the softmax computation later. That is to say, although we use softmax as the activation function in the last layer in our design, for ease of computation, we take out logits separately. This is because it is more efficient to calculate softmax and cross-entropy loss together. Remember that cross-entropy is a cost function, not used in forward propagation.  (FOMOsapiens). If you check math Logit function, it converts real space from [0,1] interval to infinity [-inf, inf]. Sigmoid and softmax will do exactly the opposite thing. They will convert the [-inf, inf] real space to [0, 1] real space. This is why, in machine learning we may use logit before sigmoid and softmax function (since they match). And this is why "we may call" anything in machine learning that goes in front of sigmoid or softmax function the logit. Here is G. Hinton video using this term. Here is a concise answer for future readers. Tensorflow's logit is defined as the output of a neuron without applying activation function: x: input, w: weight, b: bias. That's it. The following is irrelevant to this question. For historical lectures, read other answers. Hats off to Tensorflow's "creatively" confusing naming convention. In PyTorch, there is only one CrossEntropyLoss and it accepts un-activated outputs. Convolutions, matrix multiplications and activations are same level operations. The design is much more modular and less confusing. This is one of the reasons why I switched from Tensorflow to PyTorch.  The vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, logits typically become an input to the softmax function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class. In addition, logits sometimes refer to the element-wise inverse of the sigmoid function. For more information, see tf.nn.sigmoid_cross_entropy_with_logits. official tensorflow documentation They are basically the fullest learned model you can get from the network, before it's been squashed down to apply to only the number of classes we are interested in.  Check out how some researchers use them to train a shallow neural net based on what a deep network has learned:  https://arxiv.org/pdf/1312.6184.pdf It's kind of like how when learning a subject in detail, you will learn a great many minor points, but then when teaching a student, you will try to compress it to the simplest case.  If the student now tried to teach, it'd be quite difficult, but would be able to describe it just well enough to use the language. The logit (/ˈloʊdʒɪt/ LOH-jit) function is the inverse of the sigmoidal "logistic" function or logistic transform used in mathematics, especially in statistics. When the function's variable represents a probability p, the logit function gives the log-odds, or the logarithm of the odds p/(1 − p). See here: https://en.wikipedia.org/wiki/LogitI have an example of a neural network with two layers. The first layer takes two arguments and has one output. The second should take one argument as result of the first layer and one additional argument. It should looks like this: So, I'd created a model with two layers and tried to merge them but it returns an error: The first layer in a Sequential model must get an "input_shape" or "batch_input_shape" argument. on the line result.add(merged). Model: You're getting the error because result defined as Sequential() is just a container for the model and you have not defined an input for it. Given what you're trying to build set result to take the third input x3. However, my preferred way of building a model that has this type of input structure would be to use the functional api. Here is an implementation of your requirements to get you started: To answer the question in the comments: Concatenation works like this: i.e rows are just joined. Adding to the above-accepted answer so that it helps those who are using tensorflow 2.0  Result: You can experiment with model.summary() (notice the concatenate_XX (Concatenate) layer size) You can view notebook here for detail:
https://nbviewer.jupyter.org/github/anhhh11/DeepLearning/blob/master/Concanate_two_layer_keras.ipynbI apply the
decision tree classifier and the random forest classifier to my data with the following code: Why the result are so much better for the random forest classifier (for 100 runs, with randomly sampling 2/3 of data for the training and 1/3 for the test)? The random forest estimators with one estimator isn't just a decision tree?
Have i done something wrong or misunderstood the concept? The random forest estimators with one estimator isn't just a decision tree? Well, this is a good question, and the answer turns out to be no; the Random Forest algorithm is more than a simple bag of individually-grown decision trees. Apart from the randomness induced from ensembling many trees, the Random Forest (RF) algorithm also incorporates randomness when building individual trees in two distinct ways, none of which is present in the simple Decision Tree (DT) algorithm. The first is the number of features to consider when looking for the best split at each tree node: while DT considers all the features, RF considers a random subset of them, of size equal to the parameter max_features (see the docs). The second is that, while DT considers the whole training set, a single RF tree considers only a bootstrapped sub-sample of it; from the docs again: The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default). The RF algorihm is essentially the combination of two independent ideas: bagging, and random selection of features (see the Wikipedia entry for a nice overview). Bagging is essentially my second point above, but applied to an ensemble; random selection of features is my first point above, and it seems that it had been independently proposed by Tin Kam Ho before Breiman's RF (again, see the Wikipedia entry). Ho had already suggested that random feature selection alone improves performance. This is not exactly what you have done here (you still use the bootstrap sampling idea from bagging, too), but you could easily replicate Ho's idea by setting bootstrap=False in your RandomForestClassifier() arguments. The fact is that, given this research, the difference in performance is not unexpected... To replicate exactly the behaviour of a single tree in RandomForestClassifier(), you should use both bootstrap=False and max_features=None arguments, i.e. in which case neither bootstrap sampling nor random feature selection will take place, and the performance should be roughly equal to that of a single decision tree.When facing difficulties during training (nans, loss does not converge, etc.) it is sometimes useful to look at more verbose training log by setting debug_info: true in the 'solver.prototxt' file. The training log then looks something like: What does it mean? At first glance you can see this log section divided into two: [Forward] and [Backward]. Recall that neural network training is done via forward-backward propagation:
A training example (batch) is fed to the net and a forward pass outputs the current prediction.
Based on this prediction a loss is computed.
The loss is then derived, and a gradient is estimated and propagated backward using the chain rule. Caffe Blob data structure
Just a quick re-cap. Caffe uses Blob data structure to store data/weights/parameters etc. For this discussion it is important to note that Blob has two "parts": data and diff. The values of the Blob are stored in the data part. The diff part is used to store element-wise gradients for the backpropagation step. Forward pass You will see all the layers from bottom to top listed in this part of the log. For each layer you'll see: Layer "conv1" is a convolution layer that has 2 param blobs: the filters and the bias. Consequently, the log has three lines. The filter blob (param blob 0) has data That is the current L2 norm of the convolution filter weights is 0.00899.
The current bias (param blob 1): meaning that currently the bias is set to 0. Last but not least, "conv1" layer has an output, "top" named "conv1" (how original...). The L2 norm of the output is Note that all L2 values for the [Forward] pass are reported on the data part of the  Blobs in question. Loss and gradient
At the end of the [Forward] pass comes the loss layer: In this example the batch loss is 2031.85, the gradient of the loss w.r.t. fc1 is computed and passed to diff part of fc1 Blob. The L2 magnitude of the gradient is 0.1245. Backward pass
All the rest of the layers are listed in this part top to bottom. You can see that the L2 magnitudes reported now are of the diff part of the Blobs (params and layers' inputs). Finally
The last log line of this iteration: reports the total L1 and L2 magnitudes of both data and gradients. What should I look for? If you have nans in your loss, see at what point your data or diff turns into nan: at which layer? at which iteration? Look at the gradient magnitude, they should be reasonable. IF you are starting to see values with e+8 your data/gradients are starting to blow up. Decrease your learning rate! See that the diffs are not zero. Zero diffs mean no gradients = no updates = no learning. If you started from random weights, consider generating random weights with higher variance. Look for activations (rather than gradients) going to zero. If you are using "ReLU" this means your inputs/weights lead you to regions where the ReLU gates are "not active" leading to "dead neurons". Consider normalizing your inputs to have zero mean, add "BatchNorm" layers, setting negative_slope in ReLU.I'm using Python and have some confusion matrixes. I'd like to calculate precisions and recalls and f-measure by confusion matrixes in multiclass classification. My result logs don't contain y_true and y_pred, just contain confusion matrix. Could you tell me how to get these scores from confusion matrix in multiclass classification? Let's consider the case of MNIST data classification (10 classes), where for a test set of 10,000 samples we get the following confusion matrix cm (Numpy array): In order to get the precision & recall (per class), we need to compute the TP, FP, and FN per class. We don't need TN, but we will compute it, too, as it will help us for our sanity check. The True Positives are simply the diagonal elements: The False Positives are the sum of the respective column, minus the diagonal element (i.e. the TP element): Similarly, the False Negatives are the sum of the respective row, minus the diagonal (i.e. TP) element: Now, the True Negatives are a little trickier; let's first think what exactly a True Negative means, with respect to, say class 0: it means all the samples that have been correctly identified as not being 0. So, essentially what we should do is remove the corresponding row & column from the confusion matrix, and then sum up all the remaining elements: Let's make a sanity check: for each class, the sum of TP, FP, FN, and TN must be equal to the size of our test set (here 10,000): let's confirm that this is indeed the case: The result is Having calculated these quantities, it is now straightforward to get the precision & recall per class: which for this example are Similarly we can compute related quantities, like specificity (recall that sensitivity is the same thing with recall): Results for our example: You should now be able to compute these quantities virtually for any size of your confusion matrix. If you have confusion matrix in the form of:  Following simple function can be made:  Testing: Output: Above function can also be extended to produce other scores, the formulae for which are mentioned on https://en.wikipedia.org/wiki/Confusion_matrix There is a package called  'disarray'. So, if I have four classes : I can use disarray to calculate 13 matrices : which gives :This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Is there a rule-of-thumb for how to best divide data into training and validation sets? Is an even 50/50 split advisable? Or are there clear advantages of having more training data relative to validation data (or vice versa)? Or is this choice pretty much application dependent? I have been mostly using an 80% / 20% of training and validation data, respectively, but I chose this division without any principled reason. Can someone who is more experienced in machine learning advise me? There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage. If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive). Assuming you have enough data to do proper held-out test data (rather than cross-validation), the following is an instructive way to get a handle on variances: You'd be surprised to find out that 80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle. It's usually a safe bet if you use that ratio. However, depending on the training/validation methodology you employ, the ratio may change. For example: if you use 10-fold cross validation, then you would end up with a validation set of 10% at each fold. There has been some research into what is the proper ratio between the training set and the validation set: The fraction of patterns reserved for the validation set should be
  inversely proportional to the square root of the number of free
  adjustable parameters. In their conclusion they specify a formula: Validation set (v) to training set (t) size ratio, v/t, scales like
  ln(N/h-max), where N is the number of families of recognizers and
  h-max is the largest complexity of those families. What they mean by complexity is:  Each family of recognizer is characterized by its complexity, which
  may or may not be related to the VC-dimension, the description
  length, the number of adjustable parameters, or other measures of
  complexity. Taking the first rule of thumb (i.e.validation set should be inversely proportional to the square root of the number of free adjustable parameters), you can conclude that if you have 32 adjustable parameters, the square root of 32 is ~5.65, the fraction should be 1/5.65 or 0.177 (v/t). Roughly 17.7% should be reserved for validation and 82.3% for training. Last year, I took Prof: Andrew Ng’s online machine learning course. His recommendation was: Training: 60% Cross-validation: 20% Testing: 20% Well, you should think about one more thing. If you have a really big dataset, like 1,000,000 examples, split 80/10/10 may be unnecessary, because 10% = 100,000 examples may be just too much for just saying that model works fine. Maybe 99/0.5/0.5 is enough because 5,000 examples can represent most of the variance in your data and you can easily tell that model works good based on these 5,000 examples in test and dev. Don't use 80/20 just because you've heard it's ok. Think about the purpose of the test set. Perhaps a 63.2% / 36.8% is a reasonable choice. The reason would be that if you had a total sample size n and wanted to randomly sample with replacement (a.k.a. re-sample, as in the statistical bootstrap) n cases out of the initial n, the probability of an individual case being selected in the re-sample would be approximately 0.632, provided that n is not too small, as explained here: https://stats.stackexchange.com/a/88993/16263 For a sample of n=250, the probability of an individual case being selected for a re-sample to 4 digits is 0.6329.
For a sample of n=20000, the probability is 0.6321. It all depends on the data at hand. If you have considerable amount of data then 80/20 is a good choice as mentioned above. But if you do not Cross-Validation with a 50/50 split might help you a lot more and prevent you from creating a model over-fitting your training data. Suppose you have less data, I suggest to try 70%, 80% and 90% and test which is giving better result. In case of 90% there are chances that for 10% test you get poor accuracy.I was following a tutorial which was available at Part 1 & Part 2. Unfortunately the author didn't have the time for the final section which involved using cosine similarity to actually find the distance between two documents. I followed the examples in the article with the help of the following link from stackoverflow, included is the code mentioned in the above link (just so as to make life easier) as a result of the above code I have the following matrix I am not sure how to use this output in order to calculate cosine similarity, I know how to implement cosine similarity with respect to two vectors of similar length but here I am not sure how to identify the two vectors. First off, if you want to extract count features and apply TF-IDF normalization and row-wise euclidean normalization you can do it in one operation with TfidfVectorizer: Now to find the cosine distances of one document (e.g. the first in the dataset) and all of the others you just need to compute the dot products of the first vector with all of the others as the tfidf vectors are already row-normalized.   As explained by Chris Clark in comments and here Cosine Similarity does not take into account the magnitude of the vectors. Row-normalised have a magnitude of 1 and so the Linear Kernel is sufficient to calculate the similarity values. The scipy sparse matrix API is a bit weird (not as flexible as dense N-dimensional numpy arrays). To get the first vector you need to slice the matrix row-wise to get a submatrix with a single row: scikit-learn already provides pairwise metrics (a.k.a. kernels in machine learning parlance) that work for both dense and sparse representations of vector collections. In this case we need a dot product that is also known as the linear kernel: Hence to find the top 5 related documents, we can use argsort and some negative array slicing (most related documents have highest cosine similarity values, hence at the end of the sorted indices array): The first result is a sanity check: we find the query document as the most similar document with a cosine similarity score of 1 which has the following text: The second most similar document is a reply that quotes the original message hence has many common words: WIth the Help of @excray's comment, I manage to figure it out the answer, What we need to do is actually write a simple for loop to iterate over the two arrays that represent the train data and test data.  First implement a simple lambda function to hold formula for the cosine calculation: And then just write a simple for loop to iterate over the to vector, logic is for every "For each vector in trainVectorizerArray, you have to find the cosine similarity with the vector in testVectorizerArray." Here is the output: I know its an old post. but I tried the http://scikit-learn.sourceforge.net/stable/ package. here is my code to find the cosine similarity. The question was how will you calculate the cosine similarity with this package and here is my code for that Here suppose the query is the first element of train_set and doc1,doc2 and doc3 are the documents which I want to rank with the help of cosine similarity. then I can use this code.  Also the tutorials provided in the question was very useful. Here are all the parts for it 
part-I,part-II,part-III the output will be as follows : here 1 represents that query is matched with itself and the other three are the scores for matching the query with the respective documents. Let me give you another tutorial written by me. It answers your question, but also makes an explanation why we are doing some of the things. I also tried to make it concise.  So you have a list_of_documents which is just an array of strings and another document which is just a string. You need to find such document from the list_of_documents that is the most similar to document. Let's combine them together: documents = list_of_documents + [document] Let's start with dependencies. It will become clear why we use each of them. One of the approaches that can be uses is a bag-of-words approach, where we treat each word in the document independent of others and just throw all of them together in the big bag. From one point of view, it looses a lot of information (like how the words are connected), but from another point of view it makes the model simple. In English and in any other human language there are a lot of "useless" words like 'a', 'the', 'in' which are so common that they do not possess a lot of meaning. They are called stop words and it is a good idea to remove them. Another thing that one can notice is that words like 'analyze', 'analyzer', 'analysis' are really similar. They have a common root and all can be converted to just one word. This process is called stemming and there exist different stemmers which differ in speed, aggressiveness and so on. So we transform each of the documents to list of stems of words without stop words. Also we discard all the punctuation. So how will this bag of words help us? Imagine we have 3 bags: [a, b, c], [a, c, a] and [b, c, d]. We can convert them to vectors in the basis [a, b, c, d]. So we end up with vectors: [1, 1, 1, 0], [2, 0, 1, 0] and [0, 1, 1, 1]. The similar thing is with our documents (only the vectors will be way to longer). Now we see that we removed a lot of words and stemmed other also to decrease the dimensions of the vectors. Here there is just interesting observation. Longer documents will have way more positive elements than shorter, that's why it is nice to normalize the vector. This is called term frequency TF, people also used additional information about how often the word is used in other documents - inverse document frequency IDF. Together we have a metric TF-IDF which have a couple of flavors. This can be achieved with one line in sklearn :-)   Actually vectorizer allows to do a lot of things like removing stop words and lowercasing. I have done them in a separate step only because sklearn does not have non-english stopwords, but nltk has. So we have all the vectors calculated. The last step is to find which one is the most similar to the last one. There are various ways to achieve that, one of them is Euclidean distance which is not so great for the reason discussed here. Another approach is cosine similarity. We iterate all the documents and calculating cosine similarity between the document and the last one: Now minimum will have information about the best document and its score. This should help you.   and output will be: Here is a function that compares your test data against the training data, with the Tf-Idf transformer fitted with the training data. Advantage is that you can quickly pivot or group by to find the n closest elements, and that the calculations are down matrix-wise.I am currently using H2O for a classification problem dataset. I am testing it out with H2ORandomForestEstimator in a python 3.6 environment. I noticed the results of the predict method was giving values between 0 to 1(I am assuming this is the probability).  In my data set, the target attribute is numeric i.e. True values are 1 and False values are 0. I made sure I converted the type to category for the target attribute, I was still getting the same result.  Then I modified to the code to convert the target column to factor using asfactor() method on the H2OFrame still, there wasn't any change on the result.  But when I changed the values in the target attribute to True and False for 1 and 0 respectively, I was getting the expected result(i.e) the output was the classification rather than the probability. In principle & in theory, hard & soft classification (i.e. returning classes & probabilities respectively) are different approaches, each one with its own merits & downsides. Consider for example the following, from the paper Hard or Soft Classification? Large-margin Unified Machines: Margin-based classifiers have been popular in both machine learning and statistics for classification problems. Among numerous classifiers, some are hard classifiers while some are soft ones. Soft classifiers explicitly estimate the class conditional probabilities and then perform classification based on estimated probabilities. In contrast, hard classifiers directly target on the classification decision boundary without producing the probability estimation. These two types of classifiers are based on different philosophies and each has its own merits. That said, in practice, most of the classifiers used today, including Random Forest (the only exception I can think of is the SVM family) are in fact soft classifiers: what they actually produce underneath is a probability-like measure, which subsequently, combined with an implicit threshold (usually 0.5 by default in the binary case), gives a hard class membership like 0/1 or True/False. What is the right way to get the classified prediction result? For starters, it is always possible to go from probabilities to hard classes, but the opposite is not true. Generally speaking, and given the fact that your classifier is in fact a soft one, getting just the end hard classifications (True/False) gives a "black box" flavor to the process, which in principle should be undesirable; handling directly the produced probabilities, and (important!) controlling explicitly the decision threshold should be the preferable way here. According to my experience, these are subtleties that are often lost to new practitioners; consider for example the following, from the Cross Validated thread Reduce Classification probability threshold: the statistical component of your exercise ends when you output a probability for each class of your new sample. Choosing a threshold beyond which you classify a new observation as 1 vs. 0 is not part of the statistics any more. It is part of the decision component. Apart from "soft" arguments (pun unintended) like the above, there are cases where you need to handle directly the underlying probabilities and thresholds, i.e. cases where the default threshold of 0.5 in binary classification will lead you astray, most notably when your classes are imbalanced; see my answer in High AUC but bad predictions with imbalanced data (and the links therein) for a concrete example of such a case. To be honest, I am rather surprised by the behavior of H2O you report (I haven't use it personally), i.e. that the kind of the output is affected by the representation of the input; this should not be the case, and if it is indeed, we may have an issue of bad design. Compare for example the Random Forest classifier in scikit-learn, which includes two different methods, predict and predict_proba, to get the hard classifications and the underlying probabilities respectively (and checking the docs, it is apparent that the output of predict is based on the probability estimates, which have been computed already before). If probabilities are the outcomes for numerical target values, then how do I handle it in case of a multiclass classification? There is nothing new here in principle, apart from the fact that a simple threshold is no longer meaningful; again, from the Random Forest predict docs in scikit-learn: the predicted class is the one with highest mean probability estimate That is, for 3 classes (0, 1, 2), you get an estimate of [p0, p1, p2] (with elements summing up to one, as per the rules of probability), and the predicted class is the one with the highest probability, e.g. class #1 for the case of [0.12, 0.60, 0.28]. Here is a reproducible example with the 3-class iris dataset (it's for the GBM algorithm and in R, but the rationale is the same). Adding to @desertnaut's answer, and since you tagged this question as Python, here is how you handle the last part of your question: If probabilities are the outcomes for numerical target values, then how do I handle it in case of a multiclass classification? This will convert a (num_examples, n_classes) array of probability values to a (num_examples, ) array of predicted classes.When should I use .eval()? I understand it is supposed to allow me to "evaluate my model". How do I turn it back off for training? Example training code using .eval(). model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation: BUT, don't forget to turn back to training mode after eval step: You can turn off evaluation mode by running model.train(). You should use it when running your model as an inference engine - i.e. when testing, validating, and predicting (though practically it will make no difference if your model does not include any of the differently behaving layers). model.eval is a method of torch.nn.Module: Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False). The opposite method is model.train explained nicely by Umang Gupta. An extra addition to the above answers: I recently started working with Pytorch-lightning, which wraps much of the boilerplate in the training-validation-testing pipelines. Among other things, it makes model.eval() and model.train() near redundant by allowing the train_step and validation_step callbacks which wrap the eval and train so you never forget to.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. Can anyone please clearly explain the difference between 1D, 2D, and 3D convolutions in convolutional neural networks (in deep learning) with the use of examples? I want to explain with picture from C3D. In a nutshell, convolutional direction & output shape is important!  ↑↑↑↑↑ 1D Convolutions - Basic ↑↑↑↑↑  ↑↑↑↑↑ 2D Convolutions - Basic ↑↑↑↑↑  ↑↑↑↑↑ 3D Convolutions - Basic ↑↑↑↑↑  ↑↑↑↑↑ 2D Convolutions with 3D input  - LeNet, VGG, ..., ↑↑↑↑↑ 
↑↑↑↑↑ Bonus 1x1 conv in CNN  - GoogLeNet, ..., ↑↑↑↑↑  
↑↑↑↑↑ 1D Convolutions with 1D input   ↑↑↑↑↑ 
↑↑↑↑↑ 1D Convolutions with 2D input   ↑↑↑↑↑    Following the answer from @runhani I am adding a few more details to make the explanation a bit more clear and will try to explain this a bit more (and of course with exmaples from TF1 and TF2). One of the main additional bits I'm including are,  Here's how you might do 1D convolution using TF 1 and TF 2. And to be specific my data has following shapes, It's way less work with TF2 as TF2 does not need Session and variable_initializer for example. So let's understand what this is doing using a signal smoothing example. On the left you got the original and on the right you got output of a Convolution 1D which has 3 output channels.  Multiple channels are basically multiple feature representations of an input. In this example you have three representations obtained by three different filters. The first channel is the equally-weighted smoothing filter. The second is a filter that weights the middle of the filter more than the boundaries. The final filter does the opposite of the second. So you can see how these different filters bring about different effects. 1D convolution has been successful used for the sentence classification task.  Off to 2D convolution. If you are a deep learning person, chances that you haven't come across 2D convolution is … well about zero. It is used in CNNs for image classification, object detection, etc. as well as in NLP problems that involve images (e.g. image caption generation). Let's try an example, I got a convolution kernel with the following filters here, And to be specific my data has following shapes, Here you can see the output produced by above code. The first image is the original and going clock-wise you have outputs of the 1st filter, 2nd filter and 3 filter.
 In the context if 2D convolution, it is much easier to understand what these multiple channels mean. Say you are doing face recognition. You can think of (this is a very unrealistic simplification but gets the point across) each filter represents an eye, mouth, nose, etc. So that each feature map would be a binary representation of whether that feature is there in the image you provided. I don't think I need to stress that for a face recognition model those are very valuable features. More information in this article. This is an illustration of what I'm trying to articulate.  2D convolution is very prevalent in the realm of deep learning.  CNNs (Convolution Neural Networks) use 2D convolution operation for almost all computer vision tasks (e.g. Image classification, object detection, video classification).  Now it becomes increasingly difficult to illustrate what's going as the number of dimensions increase. But with good understanding of how 1D and 2D convolution works, it's very straight-forward to generalize that understanding to 3D convolution. So here goes. And to be specific my data has following shapes, 3D convolution has been used when developing machine learning applications involving LIDAR (Light Detection and Ranging) data which is 3 dimensional in nature. Alright you're nearly there. So hold on. Let's see what is stride and padding is. They are quite intuitive if you think about them. If you stride across a corridor, you get there faster in fewer steps. But it also means that you observed lesser surrounding than if you walked across the room. Let's now reinforce our understanding with a pretty picture too! Let's understand these via 2D convolution.  When you use tf.nn.conv2d for example, you need to set it as a vector of 4 elements. There's no reason to get intimidated by this. It just contain the strides in the following order. 2D Convolution - [batch stride, height stride, width stride, channel stride]. Here, batch stride and channel stride you just set to one (I've been implementing deep learning models for 5 years and never had to set them to anything except one). So that leaves you only with 2 strides to set. 3D Convolution - [batch stride, height stride, width stride, depth stride, channel stride]. Here you worry about height/width/depth strides only. Now, you notice that no matter how small your stride is (i.e. 1) there is an unavoidable dimension reduction happening during convolution (e.g. width is 3 after convolving a 4 unit wide image). This is undesirable especially when building deep convolution neural networks. This is where padding comes to the rescue. There are two most commonly used padding types.  Below you can see the difference.  Final word: If you are very curious, you might be wondering. We just dropped a bomb on whole automatic dimension reduction and now talking about having different strides. But the best thing about stride is that you control when where and how the dimensions get reduced. In summary, In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data. In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional. Mostly used on Image data. In 3D CNN, kernel moves in 3 directions. Input and output data of 3D CNN is 4 dimensional. Mostly used on 3D Image data (MRI, CT Scans). You can find more details here: https://medium.com/@xzz201920/conv1d-conv2d-and-conv3d-8a59182c4d6 CNN 1D,2D, or 3D refers to convolution direction, rather than input or filter dimension. For 1 channel input, CNN2D equals to CNN1D is kernel length = input length. (1 conv direction)This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I am trying to build a neural network from scratch.
Across all AI literature there is a consensus that weights should be initialized to random numbers in order for the network to converge faster. But why are neural networks initial weights initialized as random numbers?  I had read somewhere that this is done to "break the symmetry" and this makes the neural network learn faster. How does breaking the symmetry make it learn faster? Wouldn't initializing the weights to 0 be a better idea? That way the weights would be able to find their values (whether positive or negative) faster? Is there some other underlying philosophy behind randomizing the weights apart from hoping that they would be near their optimum values when initialized? Breaking symmetry is essential here, and not for the reason of performance. Imagine first 2 layers of multilayer perceptron (input and hidden layers):   During forward propagation each unit in hidden layer gets signal:  That is, each hidden unit gets sum of inputs multiplied by the corresponding weight.  Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, each hidden unit will get exactly the same signal. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs sigmoid(sum(inputs))). If all weights are zeros, which is even worse, every hidden unit will get zero signal. No matter what was the input - if all weights are the same, all units in hidden layer will be the same too.  This is the main issue with symmetry and reason why you should initialize weights randomly (or, at least, with different values). Note, that this issue affects all architectures that use each-to-each connections.  Analogy: Imagine that someone has dropped you from a helicopter to an unknown mountain top, and you're trapped there. Fog everywhere. You only know that you should get down to the sea level somehow. Which direction should you take to get down to the lowest possible point? If you couldn't reach sea level, the helicopter would take you again and drop you at the same mountain top. You would have to take the same directions again because you're "initializing" yourself to the same starting positions. However, each time the helicopter drops you somewhere randomly on the mountain, you would take different directions and steps. So, you would have a better chance of reaching the lowest possible point. That is what is meant by breaking the symmetry. The initialization is asymmetric (which is different), so you can find different solutions to the same problem. In this analogy, where you land is the weight. So, with different weights, there's a better chance of reaching the lowest (or lower) point. Also, it increases the entropy in the system so the system can create more information to help you find the lower points (local or global minimums).  The answer is pretty simple. The basic training algorithms are greedy in nature - they do not find the global optimum, but rather - "nearest" local solution. As the result, starting from any fixed initialization biases your solution towards some one particular set of weights. If you do it randomly (and possibly many times) then there is much less probable that you will get stuck in some weird part of the error surface. The same argument applies to other algorithms, which are not able to find a global optimum (k-means, EM, etc.) and does not apply to the global optimization techniques (like SMO algorithm for SVM). As you mentioned, the key point is breaking the symmetry. Because if you initialize all  weights to zero then all of the hidden neurons(units) in your neural network will be doing the exact same calculations. This is not something we desire because we want different hidden units to compute different functions. However, this is not possible if you initialize all to the same value. Wouldn't initializing the weights to 0 be a better idea? That way the weights would be able to find their values (whether positive or negative) faster? How does breaking the symmetry make it learn faster? If you initialize all the weights to be zero, then all the the neurons of all the layers performs the same calculation, giving the same output and there by making the whole deep net useless.  If the weights are zero, complexity of the whole deep net would be the same as that of a single neuron and the predictions would be nothing better than random. Nodes that are side-by-side in a hidden layer connected to the same inputs must have different weights for the learning algorithm to update the weights. By making weights as non zero ( but close to 0 like 0.1 etc), the algorithm will learn the weights in next iterations and won't be stuck. In this way, breaking the symmetry happens. Stochastic optimization algorithms such as stochastic gradient descent use randomness in selecting a starting point for the search and in the progression of the search. The progression of the search or learning of a neural network is known as convergence. Discovering a sub-optimal solution or local optima result into premature convergence. Instead of relying on one local optima, if you run your algorithm multiple times with different random weights, there is a best possibility of finding global optima without getting stuck at local optima. Post 2015, due to advancements in machine learning research, He-et-al Initialization is introduced to replace random initialization The weights are still random but differ in range depending on the size of the previous layer of neurons. In summary, non-zero random weights help us Let be more mathematical. In fact, the reason I answer is that I found this bit lacking in the other answers.
Assume you have 2 layers. If we look at the back-propagation algorithm, the computation of  dZ2 = A2 - Y dW2 = (1/m) * dZ2 * A2.T Let's ignore db2. (Sorry not sorry ;) ) dZ1 = W2.T * dZ2 .* g1'(Z1) ... The problem you see is in bold. Computing dZ1 (which is required to compute dW1) has W2 in it which is 0. We never got a chance to change the weights to anything beyond 0 and we never will. So essentially, the neural network does not learn anything. I think it is worse than logistic regression (single unit). In the case of logistic regression, you learn with more iterations since you get different input thanks to X. In this case, the other layers are always giving the same output so you don't learn at all. In addition to initialization with random values, initial weights should not start with large values. This is because we often use the tanh and sigmoid functions in hidden layers and output layers. If you look at the graphs of the two functions, after forward propagation at the first iteration results in higher values, and these values correspond to the places in the sigmoid and tanh functions that converge the derivative to zero. This leads to a cold start of the learning process and an increase in learning time. As a result, if you start weights at random, you can avoid these problems by multiplying these values by values such as "0.01" or "0.001". I learned one thing: if you initialize the weight to zeros, it's obvious that the activation units in the same layer will be the same, that means they'll have the same values. When you backbrop, you will find that all the rows of the gradient dW are the same also, hence all the rows of the weight matrix W are the same after gradient descent updates. In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with n[l]=1n[l]=1 for every layer, and the network is no more powerful than a linear classifier such as logistic regression. 
Andrew Ng course: First of all, some algorithms converge even with zero initial weightings. A simple example is a Linear Perceptron Network. Of course, many learning networks require random initial weighting (although this is not a guarantee of getting the fastest and best answer). Neural networks use Back-propagation to learn and to update weights, and the problem is that in this method, weights converge to the local optimal (local minimum cost/loss), not the global optimal. Random weighting helps the network to take chances for each direction in the available space and gradually improve them to arrive at a better answer and not be limited to one direction or answer. [The image below shows a one-dimensional example of how convergence. Given the initial location, local optimization is achieved but not a global optimization. At higher dimensions, random weighting can increase the chances of being in the right place or starting better, resulting in converging weights to better values.][1] [1]: https://i.stack.imgur.com/2dioT.png [Kalhor, A. (2020). Classification and Regression NNs. Lecture.] In the simplest case, the new weight is as follows: Here the cost function gradient is added to the previous weight to get a new weight. If all the previous weights are the same, then in the next step all the weights may be equal. As a result, in this case, from a geometric point of view, the neural network is inclined in one direction and all weights are the same. But if the weights are different, it is possible to update the weights by different amounts. (depending on the impact factor that each weight has on the result, it affects the cost and the updates of the weights. So even a small error in the initial random weighting can be solved). This was a very simple example, but it shows the effect of random weighting initialization on learning. This enables the neural network to go to different spaces instead of going to one side. As a result, in the process of learning, go to the best of these spacesI have Keras installed with the Tensorflow backend and CUDA.  I'd like to sometimes on demand force Keras to use CPU.  Can this be done without say installing a separate CPU-only Tensorflow in a virtual environment?  If so how?  If the backend were Theano, the flags could be set, but I have not heard of Tensorflow flags accessible via Keras.   If you want to force Keras to use CPU before Keras / Tensorflow is imported. Run your script as See also  This worked for me (win10), place before you import keras: A rather separable way of doing this is to use  Here, with booleans GPU and CPU, we indicate whether we would like to run our code with the GPU or CPU by rigidly defining the number of GPUs and CPUs the Tensorflow session is allowed to access. The variables num_GPU and num_CPU define this value. num_cores then sets the number of CPU cores available for usage via intra_op_parallelism_threads and inter_op_parallelism_threads. The intra_op_parallelism_threads variable dictates the number of threads a parallel operation in a single node in the computation graph is allowed to use (intra). While the inter_ops_parallelism_threads variable defines the number of threads accessible for parallel operations across the nodes of the computation graph (inter). allow_soft_placement allows for operations to be run on the CPU if any of the following criterion are met: there is no GPU implementation for the operation there are no GPU devices known or registered there is a need to co-locate with other inputs from the CPU All of this is executed in the constructor of my class before any other operations, and is completely separable from any model or other code I use.  Note: This requires tensorflow-gpu and cuda/cudnn to be installed because the option is given to use a GPU. Refs: What do the options in ConfigProto like allow_soft_placement and log_device_placement mean? Meaning of inter_op_parallelism_threads and intra_op_parallelism_threads  Just import tensortflow and use keras, it's that easy. As per keras tutorial, you can simply use the same tf.device scope as in regular tensorflow: I just spent some time figure it out.
Thoma's answer is not complete.
Say your program is test.py, you want to use gpu0 to run this program, and keep other gpus free. You should write CUDA_VISIBLE_DEVICES=0 python test.py Notice it's DEVICES not DEVICE For people working on PyCharm, and for forcing CPU, you can add the following line in the Run/Debug configuration, under Environment variables: To disable running on the GPU (tensor flow 2.9) use tf.config.set_visible_devices([], 'GPU'). The empty list argument is to say that there will be no GPUs visible for this run. Do this early in your code, e.g. before Keras initializes the tf configuration. See docs https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/config/set_visible_devicesI've gone through the official doc. I'm having a hard time understanding what this function is used for and how it works. Can someone explain this in layman's terms? unfold imagines a tensor as a longer tensor with repeated columns/rows of values 'folded' on top of each other, which is then "unfolded": E.g. for a 2x5 tensor, unfolding it with step=1, and patch size=2 across dim=1:  fold is roughly the opposite of this operation, but "overlapping" values are summed in the output. The unfold and fold are used to facilitate "sliding window" operations (like convolutions). Suppose you want to apply a function foo to every 5x5 window in a feature map/image: Now windows has size of batch-(55x.size(1))-num_windows, you can apply foo on windows: Now you need to "fold" processed back to the original size of x: You need to take care of padding, and kernel_size that may affect your ability to "fold" back processed to the size of x. Moreover, fold sums over overlapping elements, so you might want to divide the output of fold by patch size. Please note that torch.unfold performs a different operation than nn.Unfold. See this thread for details. Out:  Since there are no answers with 4-D tensors and nn.functional.unfold() only accepts 4-D tensor, I will would to explain this. Assuming the input tensor is of shape (batch_size, channels, height, width), and I have taken an example where batch_size = 1, channels = 2, height = 3, width = 3.  kernel_size = 2 which is nothing but a 2x2 kernelThis might be a beginner question but I have seen a lot of people using LabelEncoder() to replace categorical variables with ordinality. A lot of people using this feature by passing multiple columns at a time, however I have some doubt about having wrong ordinality in some of my features and how it will be effecting my model. Here is an example: Input Output As you can see, the ordinal values are not mapped correctly since my LabelEncoder only cares about the order in the column/array (it should be High=1, Med=2, Low=3 or vice versa). How drastically wrong mapping can effect the models and is there an easy way other than OrdinalEncoder() to map these values properly? TL;DR: Using a LabelEncoder to encode ordinal any kind of features is a bad idea! This is in fact clearly stated in the docs, where it is mentioned that as its name suggests this encoding method is aimed at encoding the label: This transformer should be used to encode target values, i.e. y, and not the input X. As you rightly point out in the question, mapping the inherent ordinality of an ordinal feature to a wrong scale will have a very negative impact on the performance of the model (that is, proportional to the relevance of the feature). And the same applies to a categorical feature, just that the original feature has no ordinality. An intuitive way to think about it, is in the way a decision tree sets its boundaries. During training, a decision tree will learn the optimal features to set at each node, as well as an optimal threshold whereby unseen samples will follow a branch or another depending on these values. If we encode an ordinal feature using a simple LabelEncoder, that could lead to a feature having say 1 represent warm, 2 which maybe would translate to hot, and a 0 representing boiling. In such case, the result will end up being a tree with an unnecessarily high amount of splits, and hence a much higher complexity for what should be simpler to model. Instead, the right approach would be to use an OrdinalEncoder, and define the appropriate mapping schemes for the ordinal features. Or in the case of having a categorical feature, we should be looking at OneHotEncoder or the various encoders available in Category Encoders. Though actually seeing why this is a bad idea will be more intuitive than just words. Let's use a simple example to illustrate the above, consisting on two ordinal features containing a range with the amount of hours spend by a student preparing for an exam and the average grade of all previous assignments, and a target variable indicating whether the exam was past or not. I've defined the dataframe's columns as pd.Categorical: The advantage of defining a categorical column as a pandas' categorical, is that we get to establish an order among its categories, as mentioned earlier. This allows for much faster sorting based on the established order rather than lexical sorting. And it can also be used as a simple way to get codes for the different categories according to their order. So the dataframe we'll be using looks as follows: The corresponding category codes can be obtained with: Now let's fit a DecisionTreeClassifier, and see what is how the tree has defined the splits: We can visualise the tree structure using plot_tree:  Is that all?? Well… yes! I've actually set the features in such a way that there is this simple and obvious relation between the Hours of dedication feature, and whether the exam is passed or not, making it clear that the problem should be very easy to model. Now let's try to do the same by directly encoding all features with an encoding scheme we could have obtained for instance through a LabelEncoder, so disregarding the actual ordinality of the features, and just assigning a value at random:  As expected the tree structure is way more complex than necessary for the simple problem we're trying to model. In order for the tree to correctly predict all training samples it has expanded until a depth of 4, when a single node should suffice. This will imply that the classifier is likely to overfit, since we’re drastically increasing the complexity. And by pruning the tree and tuning the necessary parameters to prevent overfitting we are not solving the problem either, since we’ve added too much noise by wrongly encoding the features. So to summarize, preserving the ordinality of the features once encoding them is crucial, otherwise as made clear with this example we'll lose all their predictable power and just add noise to our model.I have an image of size as RGB uint8(576,720,3) where I want to classify each pixel to a set of colors. I have transformed using rgb2lab from RGB to LAB space, and then removed the L layer so it is now a double(576,720,2) consisting of AB. Now, I want to classify this to some colors that I have trained on another image, and calculated their respective AB-representations as: Now, in order to classify/label each pixel to a cluster 1-7, I currently do the following (pseudo-code): However, this is terribly slow (52 seconds) because of the image resolution and that I manually loop through each x and y. Are there some built-in functions I can use that performs the same job? There must be.  To summarize: I need a classification method that classifies pixel images to an already defined set of clusters. For a N x 2 sized points/pixels array, you can avoid permute as suggested in the other solution by Luis, which could slow down things a bit, to have a kind of "permute-unrolled" version of it and also let's bsxfun work towards a 2D array instead of a 3D array, which must be better with performance. Thus, assuming clusters to be ordered as a N x 2 sized array, you may try this other bsxfun based approach - You can try out another approach that leverages  fast matrix multiplication in MATLAB and is based on this smart solution - Let us consider two matrices A and B between whom we want to calculate the distance matrix. For the sake of an easier explanation that follows next, let us consider A as 3 x 2 and B as 4 x 2 sized arrays, thus indicating that we are working with X-Y points. If we had A as N x 3 and B as M x 3 sized arrays, then those would be X-Y-Z points. Now, if we have to manually calculate the first element of the square of distance matrix, it would look like this – which would be – Now, according to our proposed matrix multiplication, if you check the output of A_ext and B_ext after the loop in the earlier code ends, they would look like the following –   So, if you perform matrix multiplication between A_ext and transpose of B_ext, the first element of the product would be the sum of elementwise multiplication between the first rows of A_ext and B_ext, i.e. sum of these –  The result would be identical to the result obtained from Equation (1) earlier. This would continue for all the elements of A against all the elements of B that are in the same column as in A.  Thus, we would end up with the complete squared distance matrix. That’s all there is!! Vectorized variations of the matrix multiplication based distance matrix calculations are possible, though there weren't any big performance improvements seen with them. Two such variations are listed next. Variation #1 Variation #2 So, these could be considered as experimental versions too. Use pdist2 (Statistics Toolbox) to compute the distances in a vectorized manner:  If you don't have the Statistics Toolbox, you can replace the third line by This gives squared distance instead of distance, but for the purposes of minimizing it doesn't matter.In most of the models, there is a steps parameter indicating the number of steps to run over data. But yet I see in most practical usage, we also execute the fit function N epochs.  What is the difference between running 1000 steps with 1 epoch and running 100 steps with 10 epoch? Which one is better in practice? Any logic changes between consecutive epochs? Data shuffling? A training step is one gradient update. In one step batch_size examples are processed. An epoch consists of one full cycle through the training data. This is usually many steps. As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of: If you choose your training image randomly (and independently) in each step, you normally do not call it epoch. [This is where my answer differs from the previous one. Also see my comment.] An epoch usually means one iteration over all of the training data.  For instance if you have 20,000 images and a batch size of 100 then the epoch should contain 20,000 / 100 = 200 steps.  However I usually just set a fixed number of steps like 1000 per epoch even though I have a much larger data set.  At the end of the epoch I check the average cost and if it improved I save a checkpoint.  There is no difference between steps from one epoch to another.  I just treat them as checkpoints. People often shuffle around the data set between epochs.  I prefer to use the random.sample function to choose the data to process in my epochs. So say I want to do 1000 steps with a batch size of 32.  I will just randomly pick 32,000 samples from the pool of training data. As I am currently experimenting with the tf.estimator API I would like to add my dewy findings here, too. I don't know yet if the usage of steps and epochs parameters is consistent throughout TensorFlow and therefore I am just relating to tf.estimator (specifically tf.estimator.LinearRegressor) for now.  Training steps defined by num_epochs: steps not explicitly defined Comment: I have set num_epochs=1 for the training input and the doc entry for numpy_input_fn tells me "num_epochs: Integer, number of epochs to iterate over data. If None will run forever.". With num_epochs=1 in the above example the training runs exactly x_train.size/batch_size times/steps (in my case this was 175000 steps as x_train had a size of 700000 and batch_size was 4). Training steps defined by num_epochs: steps explicitly defined higher than number of steps implicitly defined by num_epochs=1 Comment: num_epochs=1 in my case would mean 175000 steps (x_train.size/batch_size with x_train.size=700,000 and batch_size=4) and this is exactly the number of steps estimator.train albeit the steps parameter was set to 200,000 estimator.train(input_fn=train_input, steps=200000).  Training steps defined by steps Comment: Although I have set num_epochs=1 when calling numpy_input_fnthe training stops after 1000 steps. This is because steps=1000 in estimator.train(input_fn=train_input, steps=1000) overwrites the num_epochs=1 in tf.estimator.inputs.numpy_input_fn({'x':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True). Conclusion:
Whatever the parameters num_epochs for tf.estimator.inputs.numpy_input_fn and steps for estimator.train define, the lower bound determines the number of steps which will be run through. In easy words 
Epoch: Epoch is considered as number of one pass from entire dataset
Steps: In tensorflow one steps is considered as number of epochs multiplied by examples divided by batch size
 Epoch: A training epoch represents a complete use of all training data for gradients calculation and optimizations(train the model). Step: A training step means using one batch size of training data to train the model. Number of training steps per epoch: total_number_of_training_examples / batch_size. Total number of training steps: number_of_epochs x Number of training steps per epoch. According to Google's Machine Learning Glossary, an epoch is defined as "A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch_size training iterations, where N is the total number of examples." If  you are training model for 10 epochs with batch size 6, given total 12 samples that means: the model will be able to see whole dataset in 2 iterations ( 12 / 6  = 2) i.e. single epoch. overall, the model will have 2 X 10 = 20 iterations (iterations-per-epoch X no-of-epochs) re-evaluation of loss and model parameters will be performed after each iteration! Since there’re no accepted answer yet : 
By default an epoch run over all your training data. In this case you have n steps, with n = Training_lenght / batch_size. If your training data is too big you can decide to limit the number of steps during an epoch.[https://www.tensorflow.org/tutorials/structured_data/time_series?_sm_byp=iVVF1rD6n2Q68VSN] When the number of steps reaches the limit that you’ve set the process will start over, beginning the next epoch.
When working in TF, your data is usually transformed first into a list of batches that will be fed to the model for training. At each step you process one batch. As to whether it’s better to set 1000 steps for 1 epoch or 100 steps with 10 epochs I don’t know if there’s a straight answer. 
But here are results on training a CNN with both approaches using TensorFlow timeseries data tutorials : In this case, both approaches lead to very similar prediction, only the training profiles differ. steps = 20 / epochs = 100
  steps = 200 / epochs = 10   Divide the length of x_train by the batch size with We split the training set into many batches. When we run the algorithm, it requires one epoch to analyze the full training set. An epoch is composed of many iterations (or batches). Iterations: the number of batches needed to complete one Epoch. Batch Size: The number of training samples used in one iteration. Epoch: one full cycle through the training dataset. A cycle is composed of many iterations. Number of Steps per Epoch = (Total Number of Training Samples) / (Batch Size) Example
Training Set = 2,000 images
Batch Size = 10 Number of Steps per Epoch = 2,000 / 10 = 200 steps Hope this helps for better understanding.I'm following this tutorial to make this ML prediction: I'm using Python 3.6 and I get error "Expected 2D array, got 1D array instead:"
I think the script is for older versions, but I don't know how to convert it to the 3.6 version. Already try with the: You are just supposed to provide the predict method with the same 2D array, but with one value that you want to process (or more). In short, you can just replace With And it should work. EDIT: This answer became popular so I thought I'd add a little more explanation about ML. The short version: we can only use predict on data that is of the same dimensionality as the training data (X) was.  In the example in question, we give the computer a bunch of rows in X (with 2 values each) and we show it the correct responses in y. When we want to predict using new values, our program expects the same - a bunch of rows. Even if we want to do it to just one row (with two values), that row has to be part of another array.  The problem is occurring when you run prediction on the array [0.58,0.76]. Fix the problem by reshaping it before you call predict(): I use the below approach. I faced the same issue except that the data type of the instance I wanted to predict was a panda.Series object. Well I just needed to predict one input instance. I took it from a slice of my data. In this case, you'll need to convert it into a 1-D array  and then reshape it. From the docs, values will convert Series into a numpy array. I faced the same problem. You just have to make it an array and moreover you have to put double squared brackets to make it a single element of the 2D array as first bracket initializes the array and the second makes it an element of that array.  So simply replace the last statement by: Just insert the argument between a double square bracket: that worked for me I was facing the same issue earlier but I have somehow found the solution,
You can try reg.predict([[3300]]).   The API used to allow scalar value but now you need to give a 2D array. With one feature my Dataframe list converts to a Series. I had to convert it back to a Dataframe list and it worked. Just enclose your numpy object with two square brackets or vice versa. For example: If initially your x = [8,9,12,7,5] change it to x = [ [8,9,12,7,5] ]. That should fix the dimension issue You can do it like this: np.array(x)[:, None] The X and Y matrix of Independent Variable and Dependent Variable respectively to DataFrame from int64 Type so that it gets converted from 1D array to 2D array.. 
i.e X=pd.DataFrame(X) and Y=pd.dataFrame(Y) where pd is of pandas class in python. and thus feature scaling in-turn doesn't lead to any error!I have a matrix A and I want 2 matrices U and L such that U contains the upper triangular elements of A (all elements above and not including diagonal) and similarly for L(all elements below and not including diagonal). Is there a numpy method to do this? e.g To extract the upper triangle values to a flat vector,
you can do something like the following: Similarly, for the lower triangle, use np.tril. If you want to extract the values that are above the diagonal (or below) then use the k argument. This is usually used when the matrix is symmetric. To put back the extracted vector into a 2D symmetric array, one can follow my answer here: https://stackoverflow.com/a/58806626/5025009 Try numpy.triu (triangle-upper) and numpy.tril (triangle-lower). Code example: Use the Array Creation Routines of numpy.triu and numpy.tril to return a copy of a matrix with the elements above or below the k-th diagonal zeroed.After Training, I saved Both Keras whole Model and Only Weights using  Models and Weights were saved successfully and there was no error.
I can successfully load the weights simply using model.load_weights and they are good to go, but when i try to load the save model via load_model, i am getting an error. I never received this error and i used to load any models successfully. I am using Keras 2.2.4 with tensorflow backend. Python 3.6.
My Code for training is : For me the solution was downgrading the h5py package (in my case to 2.10.0), apparently putting back only Keras and Tensorflow to the correct versions was not enough. I downgraded my h5py package with the following command, Restarted my ipython kernel and it worked. For me it was the version of h5py that was superior to my previous build.
Fixed it by setting to 2.10.0. Downgrade h5py package with the following command to resolve the issue, I had the same problem, solved putting compile=False in load_model: saved using TF format file and not h5py: save_format='tf'. In my case: This is probably due to a model saved from a different version of keras. I got the same problem when loading a model generated by tensorflow.keras (which is similar to keras 2.1.6 for tf 1.12 I think) from keras 2.2.6. You can load the weights with model.load_weights and resave the complete model from the keras version you want to use. The solution than works for me was: I still kept having this error after having tensorflow==2.4.1, h5py==2.1.0, and python 3.8 in my environment.
what fixed it was downgrading the python version to 3.6.9 Downgrading python, tensorflow, keras and h5py resolved the issue.I use KerasClassifier to train the classifier. The code is below: But How to save the final model for future prediction? I usually use below code to save model: But I don't know how to insert the saving model's code into KerasClassifier's code. Thank you. The model has a save method, which saves all the details necessary to reconstitute the model. An example from the keras documentation: you can save the model in json and weights in a hdf5 file format.  files "model_num.h5" and "model_num.json" are created which contain our model and weights  To use the same trained model for further testing you can simply load the hdf5 file and use it for the prediction of different data. 
here's how to load the model from saved files. To predict for different data you can use this You can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain: In your Python code probable the last line should be: This allows you to save the entirety of the state of a model in a single file.
Saved models can be reinstantiated via keras.models.load_model(). The model returned by load_model() is a compiled model ready to be used (unless the saved model was never compiled in the first place). model.save() arguments: you can save the model and load in this way. Generally, we save the model and weights in the same file by calling the save() function. For saving, For Loading the model, In this case, we can simply save and load the model without re-compiling our model again.
Note - This is the preferred way for saving and loading your Keras model. Saving a Keras model: Loading the model back: For more information, read Documentation You can save the best model using keras.callbacks.ModelCheckpoint() Example: This will save the best model in your working directory. Since the syntax of keras, how to save a model, changed over the years I will post a fresh answer. In principle the earliest answer of bogatron, posted Mar 13 '17 at 12:10 is still good, if you want to save your model including the weights into one file. model.save("my_model.h5") This will save the model in the older Keras H5 format. However, there is a new format, the TensorFlow SavedModel format, which will be used if you do not specify the extension .h5, .hdf5 or .keras after the filename. The syntax in this case is model.save("path/to/folder") If the given folder name does not yet exist, it will be created. Two files and two folders will be created within this folder: keras_metadata.pb, saved_model.pb, assets, variables So far you can still decide whether you want to store your model into one single file or into a folder containing files and folders. (See keras documentation at www.tensorflow.org.)I'm trying to train a network with an unbalanced data. I have A (198 samples), B (436 samples), C (710 samples), D (272 samples) and I have read about the "weighted_cross_entropy_with_logits" but all the examples I found are for binary classification so I'm not very confident in how to set those weights. Total samples: 1616 A_weight: 198/1616 = 0.12? The idea behind, if I understood, is to penalize the errors of the majority class and value more positively the hits in the minority one, right? My piece of code: I have read this one and others examples with binary classification but still not very clear. Note that weighted_cross_entropy_with_logits is the weighted variant of sigmoid_cross_entropy_with_logits. Sigmoid cross entropy is typically used for binary classification. Yes, it can handle multiple labels, but sigmoid cross entropy basically makes a (binary) decision on each of them -- for example, for a face recognition net, those (not mutually exclusive) labels could be "Does the subject wear glasses?", "Is the subject female?", etc. In binary classification(s), each output channel corresponds to a binary (soft) decision. Therefore, the weighting needs to happen within the computation of the loss. This is what weighted_cross_entropy_with_logits does, by weighting one term of the cross-entropy over the other. In mutually exclusive multilabel classification, we use softmax_cross_entropy_with_logits, which behaves differently: each output channel corresponds to the score of a class candidate. The decision comes after, by comparing the respective outputs of each channel. Weighting in before the final decision is therefore a simple matter of modifying the scores before comparing them, typically by multiplication with weights. For example, for a ternary classification task, You could also rely on tf.losses.softmax_cross_entropy to handle the last three steps. In your case, where you need to tackle data imbalance, the class weights could indeed be inversely proportional to their frequency in your train data. Normalizing them so that they sum up to one or to the number of classes also makes sense. Note that in the above, we penalized the loss based on the true label of the samples. We could also have penalized the loss based on the estimated labels by simply defining and the rest of the code need not change thanks to broadcasting magic. In the general case, you would want weights that depend on the kind of error you make. In other words, for each pair of labels X and Y, you could choose how to penalize choosing label X when the true label is Y. You end up with a whole prior weight matrix, which results in weights above being a full (num_samples, num_classes) tensor. This goes a bit beyond what you want, but it might be useful to know nonetheless that only your definition of the weight tensor need to change in the code above. See this answer for an alternate solution which works with sparse_softmax_cross_entropy: Tensorflow 2.0 Compatible Answer: Migrating the Code specified in P-Gn's Answer to 2.0, for the benefit of the community. For more information about migration of code from Tensorflow Version 1.x to 2.x, please refer this Migration Guide.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 7 years ago. Does anyone know of recent academic work which has been done on logo recognition in images?
Please answer only if you are familiar with this specific subject (I can search Google for "logo recognition" myself, thank you very much).
Anyone who is knowledgeable in computer vision and has done work on object recognition is welcome to comment as well.  Update:
Please refer to the algorithmic aspects (what approach you think is appropriate, papers in the field, whether it should work(and has been tested) for real world data, efficiency considerations) and not the technical sides (the programming language used or whether it was with OpenCV...)
Work on image indexing and content based image retrieval can also help. You could try to use local features like SIFT here:
http://en.wikipedia.org/wiki/Scale-invariant_feature_transform It should work because logo shape is usually constant, so extracted features shall match well. The workflow will be like this: Detect corners (e.g. Harris corner detector) - for Nike logo they are two sharp ends. Compute descriptors (like SIFT - 128D integer vector) On training stage remember them; on matching stage find nearest neighbours for every feature in the database obtained during training. Finally, you have a set of matches (some of them are probably wrong). Seed out wrong matches using RANSAC. Thus you'll get the matrix that describes transform from ideal logo image to one where you find the logo. Depending on the settings, you could allow different kinds of transforms (just translation; translation and rotation; affine transform). Szeliski's book has a chapter (4.1) on local features.
http://research.microsoft.com/en-us/um/people/szeliski/Book/ P.S.  I assumed you wanna find logos in photos, for example find all Pepsi billboards, so they could be distorted. If you need to find a TV channel logo on the screen (so that it is not rotated and scaled), you could do it easier (pattern matching or something). Conventional SIFT does not consider color information. Since logos usually have constant colors (though the exact color depends on lightning and camera) you might want to consider color information somehow. We worked on logo detection/recognition in real-world images. We also created a dataset FlickrLogos-32 and made it publicly available, including data, ground truth and evaluation scripts. In our work we treated logo recognition as retrieval problem to simplify multi-class recognition and to allow such systems to be easily scalable to many (e.g. thousands) logo classes. Recently, we developed a bundling technique called Bundle min-Hashing that aggregates spatial configurations of multiple local features into highly distinctive feature bundles. The bundle representation is usable for both retrieval and recognition. See the following example heatmaps for logo detections: 
 You will find more details on the internal operations, potential applications of the approach, experiments on its performance and of course also many references to related work in the papers [1][2]. Worked on that: Trademark matching and retrieval in sports video databases
get a PDF of the paper: http://scholar.google.it/scholar?cluster=9926471658203167449&hl=en&as_sdt=2000 We used SIFT as trademark and image descriptors, and a normalized threshold matching to compute the distance between models and images. In our latest work we have been able to greatly reduce computation using meta-models, created evaluating the relevance of the SIFT points that are present in different versions of the same trademark. I'd say that in general working with videos is harder than working on photos due to the very bad visual quality of the TV standards currently used. Marco I worked on a project where we had to do something very similar.  At first I tried using Haar Training techniques using this software OpenCV It worked, but was not an optimal solution for our needs. Our source images (where we were looking for the logo) were a fixed size and only contained the logo.  Because of this we were able to use cvMatchShapes with a known good match and compare the value returned to deem a good match.http://xgboost.readthedocs.org/en/latest/python/python_intro.html On the homepage of xgboost(above link), it says:
To install XGBoost, do the following steps: You need to run make in the root directory of the project In the python-package directory run python setup.py install However, when I did it, for step 1 the following error appear:
make : The term 'make' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the
 spelling of the name, or if a path was included, verify that the path is correct and try again. then I skip step1 and did step 2 directly, another error appear: Does anyone know how to install xgboost for python on Windows10 platform? Thanks for your help! In case anyone's looking for a simpler solution that doesn't require compiling it yourself: If you find it won't install because of a missing dependency, download and install the dependency first and retry.  If it complains about access permissions, try opening your command prompt as Administrator and retry. This gives you xgboost and the scikit-learn wrapper, and saves you from having to go through the pain of compiling it yourself. :) Note that as of the most recent release the Microsoft Visual Studio instructions no longer seem to apply as this link returns a 404 error: https://github.com/dmlc/xgboost/tree/master/windows You can read more about the removal of the MSVC build from Tianqi Chen's comment here. So here's what I did to finish a 64-bit build on Windows: These should be all the tools you need to build the xgboost project.  To get the source code run these lines: Note that I ran this part from a Cygwin shell.  If you are using the Windows command prompt you should be able to change cp to copy and arrive at the same result.  However, if the build fails on you for any reason I would recommend trying again using cygwin. If the build finishes successfully, you should have a file called xgboost.exe located in the project root.  To install the Python package, do the following:   Now you should be good to go.  Open up Python, and you can import the package with: To test the installation, I went ahead and ran the basic_walkthrough.py file that was included in the demo/guide-python folder of the project and didn't get any errors.   I installed XGBoost successfully in Windows 8 64bit, Python 2.7 with Visual Studio 2013 (don't need mingw64) Updated 15/02/2017 With newer version of XGBoost, here are my steps Step 1. Install cmake https://cmake.org/download/ Verify cmake have been installed successfully Step 2. Clone xgboost source Step 3. Create Visual Studio Project Step 4. Build Visual Studio 2013 project After build solution, two new files libxgboost.dll and xgboost.exe are created in folder xgboost_dir/lib Step 5. Build python package Verify xgboost have been installed successfully Old Answer Here are my steps: I just installed xgboost both for my python 2.7 and python 3.5, anaconda, 64bit machine and 64 bit python. both VERY simple, NO VS2013 or git required. I think it works for normal python, too. If you use python 3.5: 1: download the package here, the version depends on your python version, python3.5 or python 3.6, 32bit or 64bit.  2: use the command window, use cd to make the download folder as your pwd, then use OK, finished.
For more detailed steps, see this answer  if you use python 2.7, you do NOT need to download the VS2013 to build it yourself, because I have built it, you can download the file I built and install it directly 1: Download it here by google drive 2: Download it, decompress it, paste it here: "your python path\Lib\site-packages" Then you should have something look like this:  3: In python-package folder showed above, use cmd window, cd there and run use this code  in your python to check whether you have installed mingw-64 or not, No error information means you have installed the mingw-64 and you are finished. If there are error information  "WindowsError: [Error 126] " That means you have not installed mingw-64, and you have one more step to go. Download the mingw-64 here: http://sourceforge.net/projects/mingw-w64/ Choose x86_64 instead of the default "i686" when you installed the mingw-64,
then add "your install path\x86_64-6.2.0-posix-seh-rt_v5-rev1\mingw64\bin;" to your PATH, it should be something like this: "C:\Program Files\mingw-w64\x86_64-6.2.0-posix-seh-rt_v5-rev1\mingw64\bin;" (this is mine). Don't forget the ";" in the PATH. Then you are finished,you can use  in your python to check that, Yeah! PS: if you don't know how to add path, just google it to get solutions. Don't worry, it's very simple. If You are installing XGBoost for a particular Project and You are using Pycahrm then you need to follow the procedures given below: Download xgboost‑0.72‑cp36‑cp36m‑win_amd64.whl from Here (as I am using Python 3.6 if you use different version of Python like 2.7 then you need to install xgboost‑0.72‑cp27‑cp27m‑win_amd64.whl). Copy the to your Project Interpreter directory. You can find the directory of Project Interpreter by clicking File -> Settings -> Project Interpreter from Pycharm. Open Command Prompt. Go to directory to you Project Interpreter from cmd. Write the following command: pip install xgboost-0.72-cp36-cp36m-win_amd64.whl On windows 10 , with python 3.6, below command worked. From Anaconda Prompt, below command can be used directly. The screenshot is attached as proof.
pip install xgboost
 After build the c++ version, copy the release dll and lib files in ../windows/x64/Release/..(if you build x64 version) to ../wrapper/ then run python setup.py install  I followed the steps listed in https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13043/run-xgboost-from-windows-and-python. I will summarize what I did below. 1) Download Visual Basic Studio. You can download the community edition at visual studio website. There is a "free visual studio button on the upper right corner" 2) Copy all content from the git hub repository of xgboost/tree/master/windows and Open Visual studio existing project on Visual studio 3) There are a couple of drop down menus you need to select ( "Release" and "X64" and then select build --> build all from the upper menu. It should look something like the attached screenshot. 4) if you see the message ========== Build: 3 succeeded, 0 failed, 0 up-to-date, 0 skipped ==========, it is all good 5) Browse to python-packages folder where the setup file for XGB resides and run the install command 'python setup.py install'. You can find a similar thread at Install xgboost under python with 32-bit msys failing Hope this helps. To add to the solution by Disco4ever for those attempting to build on 32bit Windows machines. After doing step 6 and creating a config.mk file you need to go into this file and edit the following lines to remove the -m64 flag Adding "git checkout 9a48a40" to Disco4Ever's solution above worked for me: This was originally posted by Cortajarena here:
https://github.com/dmlc/xgboost/issues/1267 Also, for what it's worth, I originally had 32 bit Python running on my 64 bit machine and I had to upload 64 bit Python for XGBoost to work. Thanks to disco4ever answer. I was trying to build xgboost for Python Anaconda environment in my windows 10 64 bit machine. Used Git, mingw64 and basic windows cmd. Everthing worked for me till the copy step: cp make/mingw64.mk config.mk, as I was using windows cmd I modified it to copy c:\xgboost\make\mingw64.mk c:\xgboost\config.mk when I proceeded to the next step : make -j4, I got error that build failed. At this stage after so much frustration just tired something different by clicking on build.sh (shell script). It started executing and auto finished.  Then I executed the same step make -j4, to my awe build was successful. I have seen the most awaited xgboost.exe file in my xgboost folder.  I then proceeded with further steps and executed python setup.py install. finally everything installed perfectly. Then I went to my spyder and checked whether it is working or not. But I was one step away to my happiness because I was still seeing the import error.  Closed all command prompts (Anaconda, Git bash, Windows CMD, cygwin terminal) then again opened spyder and typed 'import xgboost'. SUCCESS, No ERROR.  Once again thank you for everyone. You can install xGBoost using either Visual Studio or minGW. Since, the official xgboost website says that MSVC build is not yet updated, I tried using mingw64.
I am running xgboost (python package) on my win7 x64. Steps I followed were: 1) Follow Disco4Ever's steps for ming64 installation (mentioned above in the answers). 2) Install Git for windows. windows download link. This will also install Git Bash. Add git-installation-directory\cmd to your system environment variable PATH list. 3) Now clone xGBoost in desired location. Type the following in cmd: 4) In xgboost's root directory there should be a shell script named "build". Open it. It'll open up a Git Bash and start building. After building, xgboost.exe file will be created. 5) Now install python package : You can test by importing xgboost in python.   It took a whole day, but I successfully installed xgboost on windows 7 64-bit box using TDM-GCC with OpenMP enabled, instead of MingW following this link - http://dnc1994.com/2016/03/installing-xgboost-on-windows/ Here's a very helpful link with important points to pay attention to during installation. It's very important to install "openmp". Otherwise you'll get error message. The link provides a step by step instruction for installing. Here's some quote: Building Xgboost To be fair, there is nothing wrong about the official guide for
  installing xgboost on Windows. But still, I’d love to stress several
  points here to save your time. Makefile_win is a modified version (thanks to Zhou Xiyou) of the
  original Makefile to suit the building process on Windows. You can
  wget it or download it here. Be sure to use a UNIX shell for thi
  because Windows CMD has issue with mkdir -p command. Git Bash is
  recommended. Be sure to use --recursive option with git clone. Be sure
  to use a proper MinGW. TDM-GCC is recommended. Note that by default it
  wouldn’t install OpenMP for you. You need to specifiy it otherwise the
  building would fail. Another helpful link is the official procedure: official guide Good luck! I would like to add a small workaround to Disco4ever 's solution. For me I was unable to perform cloning in cygwin. So the workaround is perform it in command prompt in windows and do the rest of the task in cygwin. Use cd c:\xgboost in the 3rd line to make it work in cygwin. So the updated last part is like this. And after installation is complete you can uninstall git and cygwin but xgboost and mingw64 must be kept as it is. Note that: before "make -j4" use gcc -v to check your gcc version. As to me, My environment is win10 + anaconda(python 2.7), when I run make -j4. It shows std::mutex error. After I use gcc- v It echo gcc4.7(anaconda's default gcc).After I choose my gcc to mingw64's 6.2 gcc ,then it works. Finally, I use "/d/Anaconda2/python.exe setup.py install" install xgboost python packet. You can install XGBoost using following 3 steps:  Gather information of your system (python version and system architecture - 32 bit or 64 bit) download related .whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/ e.g. if your python version is 3.7 and windows is 32 bit, then suitable file is:  xgboost‑0.72‑cp37‑cp37m‑win32.whl You can also find detailed steps here.  I use Jupyter notebook and I found a really simple way to install XGBoost within Anaconda: Done(An update to this question has been added.) I am a graduate student at the university of Ghent, Belgium; my research is about emotion recognition with deep convolutional neural networks. I'm using the Caffe framework to implement the CNNs. Recently I've run into a problem concerning class imbalance. I'm using 9216 training samples, approx. 5% are labeled positively (1), the remaining samples are labeled negatively (0). I'm using the SigmoidCrossEntropyLoss layer to calculate the loss. When training, the loss decreases and the accuracy is extremely high after even a few epochs. This is due to the imbalance: the network simply always predicts negative (0). (Precision and recall are both zero, backing this claim) To solve this problem, I would like to scale the contribution to the loss depending on the prediction-truth combination (punish false negatives severely). My mentor/coach has also advised me to use a scale factor when backpropagating through stochastic gradient descent (sgd): the factor would be correlated to the imbalance in the batch. A batch containing only negative samples would not update the weights at all. I have only added one custom-made layer to Caffe: to report other metrics such as precision and recall. My experience with Caffe code is limited but I have a lot of expertise writing C++ code. Could anyone help me or point me in the right direction on how to adjust the SigmoidCrossEntropyLoss and Sigmoid layers to accomodate the following changes: Thanks in advance! I have incorporated the InfogainLossLayer as suggested by Shai. I've also added another custom layer that builds the infogain matrix H based on the imbalance in the current batch. Currently, the matrix is configured as follows: I'm planning on experimenting with different configurations for the matrix in the future. I have tested this on a 10:1 imbalance. The results have shown that the network is learning useful things now: (results after 30 epochs) These numbers were reached at around 20 epochs and didn't change significantly after that. !! The results stated above are merely a proof of concept, they were obtained by training a simple network on a 10:1 imbalanced dataset. !! Why don't you use the InfogainLoss layer to compensate for the imbalance in your training set? The Infogain loss is defined using a weight matrix H (in your case 2-by-2) The meaning of its entries are So, you can set the entries of H to reflect the difference between errors in predicting 0 or 1. You can find how to define matrix H for caffe in this thread. Regarding sample weights, you may find this post interesting: it shows how to modify the SoftmaxWithLoss layer to take into account sample weights. Recently, a modification to cross-entropy loss was proposed by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár Focal Loss for Dense Object Detection, (ICCV 2017).
The idea behind focal-loss is to assign different weight for each example based on the relative difficulty of predicting this example (rather based on class size etc.). From the brief time I got to experiment with this loss, it feels superior to "InfogainLoss" with class-size weights. I have also come across this class imbalance problem in my classification task. Right now I am using CrossEntropyLoss with weight (documentation here) and it works fine. The idea is to give more loss to samples in classes with smaller number of images. weight for each class in inversely proportional to the image number in this class. Here is a snippet to calculate weight for all class using numpy,I have a set of fairly complicated models that I am training and I am looking for a way to save and load the model optimizer states. The "trainer models" consist of different combinations of several other "weight models", of which some have shared weights, some have frozen weights depending on the trainer, etc. It is a bit too complicated of an example to share, but in short, I am not able to use model.save('model_file.h5') and keras.models.load_model('model_file.h5') when stopping and starting my training.  Using model.load_weights('weight_file.h5') works fine for testing my model if the training has finished, but if I attempt to continue training the model using this method, the loss does not come even close to returning to its last location. I have read that this is because the optimizer state is not saved using this method which makes sense. However, I need a method for saving and loading the states of the optimizers of my trainer models. It seems as though keras once had a model.optimizer.get_sate() and model.optimizer.set_sate() that would accomplish what I am after, but that does not seem to be the case anymore (at least for the Adam optimizer). Are there any other solutions with the current Keras? You can extract the important lines from the load_model and save_model functions. For saving optimizer states, in save_model: For loading optimizer states, in load_model: Combining the lines above, here's an example: For those who are not using model.compile and instead performing automatic differentiation to apply the gradients manually with optimizer.apply_gradients, I think I have a solution. First, save the optimizer weights: np.save(path, optimizer.get_weights()) Then, when you are ready to reload the optimizer, show the newly instantiated optimizer the size of the weights it will update by calling optimizer.apply_gradients on a list of tensors of the size of the variables for which you calculate gradients. It is extremely important to then set the weights of the model AFTER you set the weights of the optimizer because momentum-based optimizers like Adam will update the weights of the model even if we give it gradients which are zero. Note that if we try to set the weights before calling apply_gradients for the first time, an error is thrown that the optimizer expects a weight list of length zero. Completing Alex Trevithick answer, it is possible to avoid re calling model.set_weights, simply by saving the state of the variables before applying the gradient and then reloading. This can useful when loading a model from an h5 file, and looks cleaner (imo). The saving/loading functions are the following (thanks Alex again): upgrading Keras to 2.2.4 and using pickle solved this issue for me. with keras release 2.2.3 Keras models can now be safely pickled. Anyone trying to use @Yu-Yang's solution in a distributed setting might run in the following error: or similar. To solve this problem, you simply need to run the model's optimizer weights setting on each replica using the following: For some reason, this isn't needed for setting the model weights, but make sure that you create (via the call here) and load the weights of the model within the strategy scope or you might get an error along the lines of ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x14ffdce82c50>), which is different from the scope used for the original variable. If you want the full-on example, I created a colab showcasing this solution. The code below works for me (Tensorflow 2.5).
I'm using the universal sentence encoder as model, together with an Adam optimizer. Basically what I do is: I make use of a dummy input which sets the optimizer correctly.
Afterwards I set the weights. Save the weights of the optimizer load the optimizer From version 2.11 optimizer.get_weights() is no longer accessible. You can eventually switch to tf.optimizers.legacy classes but it is not recommended. Instead, The class tf.train.Checkpoint is specially designed for saving both model and optimizer weights: Finally, then class tf.train.CheckpointManager manages multiple checkpoint versions and make it very easy:Let's suppose I have a sequence of integers: 0,1,2, .. and want to predict the next integer given the last 3 integers, e.g.: [0,1,2]->5, [3,4,5]->6, etc Suppose I setup my model like so: It is my understanding that model has the following structure (please excuse the crude drawing):  First Question: is my understanding correct? Note I have drawn the previous states C_{t-1}, h_{t-1} entering the picture as this is exposed when specifying stateful=True.  In this simple "next integer prediction" problem, the performance should improve by providing this extra information (as long as the previous state results from the previous 3 integers). This brings me to my main question:  It seems the standard practice (for example see this blog post and the TimeseriesGenerator keras preprocessing utility), is to feed a staggered set of inputs to the model during training. For example: This has me confused because it seems this is requires the output of the 1st Lstm Cell (corresponding to the 1st time step).  See this figure:  From the tensorflow docs: stateful: Boolean (default False). If True, the last state for each
  sample at index i in a batch will be used as initial state for the
  sample of index i in the following batch. it seems this "internal" state isn't available and all that is available is the final state.  See this figure:  So, if my understanding is correct (which it's clearly not), shouldn't we be feeding non-overlapped windows of samples to the model when using stateful=True?  E.g.: The answer is: depends on problem at hand. For your case of one-step prediction - yes, you can, but you don't have to. But whether you do or not will significantly impact learning.  Batch vs. sample mechanism ("see AI" = see "additional info" section) All models treat samples as independent examples; a batch of 32 samples is like feeding 1 sample at a time, 32 times (with differences - see AI). From model's perspective, data is split into the batch dimension, batch_shape[0], and the features dimensions, batch_shape[1:] - the two "don't talk." The only relation between the two is via the gradient (see AI). Overlap vs no-overlap batch Perhaps the best approach to understand it is information-based. I'll begin with timeseries binary classification, then tie it to prediction: suppose you have 10-minute EEG recordings, 240000 timesteps each. Task: seizure or non-seizure? Take 10 samples, shape (240000, 1). How to feed?  Which of the two above do you take? If (2), your neural net will never confuse a seizure for a non-seizure for those 10 samples. But it'll also be clueless about any other sample. I.e., it will massively overfit, because the information it sees per iteration barely differs (1/54000 = 0.0019%) - so you're basically feeding it the same batch several times in a row. Now suppose (3): A lot more reasonable; now our windows have a 50% overlap, rather than 99.998%. Prediction: overlap bad? If you are doing a one-step prediction, the information landscape is now changed: This dramatically changes your loss function, and what is 'good practice' for minimizing it: What should I do? First, make sure you understand this entire post, as nothing here's really "optional." Then, here's the key about overlap vs no-overlap, per batch: Your goal: balance the two; 1's main edge over 2 is: Should I ever use (2) in prediction?  LSTM stateful: may actually be entirely useless for your problem. Stateful is used when LSTM can't process the entire sequence at once, so it's "split up" - or when different gradients are desired from backpropagation. With former, the idea is - LSTM considers former sequence in its assessment of latter: In other words: do not overlap in stateful in separate batches. Same batch is OK, as again, independence - no "state" between the samples. When to use stateful: when LSTM benefits from considering previous batch in its assessment of the next. This can include one-step predictions, but only if you can't feed the entire seq at once: When and how does LSTM "pass states" in stateful? Per above, you cannot do this: This implies 21 causally follows 10 - and will wreck training. Instead do: Batch vs. sample: additional info A "batch" is a set of samples - 1 or greater (assume always latter for this answer)
. Three approaches to iterate over data: Batch Gradient Descent (entire dataset at once), Stochastic GD (one sample at a time), and Minibatch GD (in-between). (In practice, however, we call the last SGD also and only distinguish vs BGD - assume it so for this answer.) Differences: BONUS DIAGRAMS:Using a LogisticRegression class in scikit-learn on a version of the flight delay dataset. I use pandas to select some columns: I fill in NaN values with 0: Make sure the categorical columns are marked with the 'category' data type: Then call get_dummies() from pandas: Now I train and test my data set: Once I call the score method I get around 0.867. However, when I call the roc_auc_score method I get a much lower number of around 0.583 Is there any reason why the ROC AUC is much lower than what the score method provides? To start with, saying that an AUC of 0.583 is "lower" than a score* of 0.867 is exactly like comparing apples with oranges. [* I assume your score is mean accuracy, but this is not critical for this discussion - it could be anything else in principle] According to my experience at least, most ML practitioners think that the AUC score measures something different from what it actually does: the common (and unfortunate) use is just like any other the-higher-the-better metric, like accuracy, which may naturally lead to puzzles like the one you express yourself. The truth is that, roughly speaking, the AUC measures the performance of a binary classifier averaged across all possible decision thresholds. The (decision) threshold in binary classification is the value above which we decide to label a sample as 1 (recall that probabilistic classifiers actually return a value p in [0, 1], usually interpreted as a probability - in scikit-learn it is what predict_proba returns). Now, this threshold, in methods like scikit-learn predict which return labels (1/0), is set to 0.5 by default, but this is not the only possibility, and it may not even be desirable in come cases (imbalanced data, for example). The point to take home is that: Given these clarifications, your particular example provides a very interesting case in point: I get a good-enough accuracy ~ 87% with my model; should I care that, according to an AUC of 0.58, my classifier does only slightly better than mere random guessing? Provided that the class representation in your data is reasonably balanced, the answer by now should hopefully be obvious: no, you should not care; for all practical cases, what you care for is a classifier deployed with a specific threshold, and what this classifier does in a purely theoretical and abstract situation when averaged across all possible thresholds should pose very little interest for a practitioner (it does pose interest for a researcher coming up with a new algorithm, but I assume that this is not your case). (For imbalanced data, the argument changes; accuracy here is practically useless, and you should consider precision, recall, and the confusion matrix instead). For this reason, AUC has started receiving serious criticism in the literature (don't misread this - the analysis of the ROC curve itself is highly informative and useful); the Wikipedia entry and the references provided therein are highly recommended reading: Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. [...] One recent explanation of the problem with ROC AUC is that reducing the ROC Curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance points plotted and not the performance of an individual system Emphasis mine - see also On the dangers of AUC... I don't know what exactly AIR_DEL15 is, which you use as your label (it is not in the original data). My guess is that it is an imbalanced feature, i.e there are much more 0's than 1's; in such a case, accuracy as a metric is not meaningful, and you should use precision, recall, and the confusion matrix instead - see also this thread). Just as an extreme example, if 87% of your labels are 0's, you can have a 87% accuracy "classifier" simply (and naively) by classifying all samples as 0; in such a case, you would also have a low AUC (fairly close to 0.5, as in your case). For a more general (and much needed, in my opinion) discussion of what exactly AUC is, see my other answer.On Caffe, I am trying to implement a Fully Convolution Network for semantic segmentation. I was wondering is there a specific strategy to set up your 'solver.prototxt' values for the following hyper-parameters: Does it depend on the number of images you have for your training set? If so, how?  In order to set these values in a meaningful manner, you need to have a few more bits of information regarding your data: 1. Training set size the total number of training examples you have, let's call this quantity T.
2. Training batch size the number of training examples processed together in a single batch, this is usually set by the input data layer in the 'train_val.prototxt'. For example, in this file the train batch size is set to 256. Let's denote this quantity by tb.
3. Validation set size the total number of examples you set aside for validating your model, let's denote this by V.
4. Validation batch size value set in batch_size for the TEST phase. In this example it is set to 50. Let's call this vb. Now, during training, you would like to get an un-biased estimate of the performance of your net every once in a while. To do so you run your net on the validation set for test_iter iterations. To cover the entire validation set you need to have test_iter = V/vb.
How often would you like to get this estimation? It's really up to you. If you have a very large validation set and a slow net, validating too often will make the training process too long. On the other hand, not validating often enough may prevent you from noting if and when your training process failed to converge. test_interval determines how often you validate: usually for large nets you set test_interval in the order of 5K, for smaller and faster nets you may choose lower values. Again, all up to you.  In order to cover the entire training set (completing an "epoch") you need to run T/tb iterations. Usually one trains for several epochs, thus max_iter=#epochs*T/tb. Regarding iter_size: this allows to average gradients over several training mini batches, see this thread fro more information.I am trying to understand the process of model evaluation and validation in machine learning. Specifically, in which order and how the training, validation and test sets must be used.   Let's say I have a dataset and I want to use linear regression. I am hesitating among various polynomial degrees (hyper-parameters). In this wikipedia article, it seems to imply that the sequence should be:  However, this seems strange to me: how can you fit your model with the training set if you haven't chosen yet your hyper-parameters (polynomial degree in this case)?  I see three alternative approachs, I am not sure if they would be correct. So the question is: The Wikipedia article is not wrong; according to my own experience, this is a frequent point of confusion among newcomers to ML. There are two separate ways of approaching the problem: So, the standard point is that you always put aside a portion of your data as test set; this is used for no other reason than assessing the performance of your model in the end (i.e. not back-and-forth and multiple assessments, because in that case you are using your test set as a validation set, which is bad practice). After you have done that, you choose if you will cut another portion of your remaining data to use as a separate validation set, or if you will proceed with cross-validation (in which case, no separate and fixed validation set is required). So, essentially, both your first and third approaches are valid (and mutually exclusive, i.e. you should choose which one you will go with). The second one, as you describe it (CV only in the validation set?), is certainly not (as said, when you choose to go with CV you don't assign a separate validation set). Apart from a brief mention of cross-validation, what the Wikipedia article actually describes is your first approach. Questions of which approach is "better" cannot of course be answered at that level of generality; both approaches are indeed valid, and are used depending on the circumstances. Very loosely speaking, I would say that in most "traditional" (i.e. non deep learning) ML settings, most people choose to go with cross-validation; but there are cases where this is not practical (most deep learning settings, again loosely speaking), and people are going with a separate validation set instead. What Wikipedia means is actually your first approach. 1 Split data into training set, validation set and test set  2 Use the
  training set to fit the model (find the best parameters: coefficients
  of the polynomial). That just means that you use your training data to fit a model. 3 Afterwards, use the validation set to find the best hyper-parameters
  (in this case, polynomial degree) (wikipedia article says:
  "Successively, the fitted model is used to predict the responses for
  the observations in a second dataset called the validation dataset") That means that you use your validation dataset to predict its values with the previously (on the training set) trained model to get a score of how good your model performs on unseen data. You repeat step 2 and 3 for all hyperparameter combinations you want to look at (in your case the different polynomial degrees you want to try) to get a score (e.g. accuracy) for every hyperparmeter combination. Finally, use the test set to score the model fitted with the training
  set. Why you need the validation set is pretty well explained in this stackexchange question 
https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set  In the end you can use any of your three aproaches. approach: is the fastest because you only train one model for every hyperparameter.
also you don't need as much data as for the other two. approach: is slowest because you train for k folds k classifiers plus the final one with all your training data to validate it for every hyperparameter combination. You also need a lot of data because you split your data three times and that first  part again in k folds. But here you have the least variance in your results. Its pretty unlikely to get k good classifiers and a good validation result by coincidence. That could happen more likely in the first approach. Cross Validation is also way more unlikely to overfit. approach: is in its pros and cons in between of the other two. Here you also have less likely overfitting. In the end it will depend on how much data you have and if you get into more complex models like neural networks, how much time/calculationpower you have and are willing to spend. Edit As @desertnaut mentioned: Keep in mind that you should use training- and validationset as training data for your evaluation with the test set. Also you confused training with validation set in your second approach.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. What is the difference between epoch and iteration when training a multi-layer perceptron? In the neural network terminology: For example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch. FYI: Tradeoff batch size vs. number of iterations to train a neural network The term "batch" is ambiguous: some people use it to designate the entire training set, and some people use it to refer to the number of training examples in one forward/backward pass (as I did in this answer). To avoid that ambiguity and make clear that batch corresponds to the number of training examples in one forward/backward pass, one can use the term mini-batch. Epoch and iteration describe different things. An epoch describes the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, an epoch has been completed. An iteration describes the number of times a batch of data passed through the algorithm. In the case of neural networks, that means the forward pass and backward pass. So, every time you pass a batch of data through the NN, you completed an iteration. An example might make it clearer. Say you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs. Therefore, in each epoch, you have 5 batches (10/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.
Since you've specified 3 epochs, you have a total of 15 iterations (5*3 = 15) for training. Many neural network training algorithms involve making multiple presentations of the entire data set to the neural network.  Often, a single presentation of the entire data set is referred to as an "epoch".  In contrast, some algorithms present data to the neural network a single case at a time. "Iteration" is a much more general term, but since you asked about it together with "epoch", I assume that your source is referring to the presentation of a single case to a neural network. To understand the difference between these you must understand the Gradient Descent Algorithm and its Variants. Before I start with the actual answer, I would like to build some background. A batch is the complete dataset. Its size is the total number of training examples in the available dataset. mini-batch size is the number of examples the learning algorithm processes in a single pass (forward and backward). A Mini-batch is a small part of the dataset of given mini-batch size. Iterations is the number of batches of data the algorithm has seen (or simply the number of passes the algorithm has done on the dataset). Epochs is the number of times a learning algorithm sees the complete dataset. Now, this may not be equal to the number of iterations, as the dataset can also be processed in mini-batches, in essence, a single pass may process only a part of the dataset. In such cases, the number of iterations is not equal to the number of epochs. In the case of Batch gradient descent, the whole batch is processed on each training pass. Therefore, the gradient descent optimizer results in smoother convergence than Mini-batch gradient descent, but it takes more time. The batch gradient descent is guaranteed to find an optimum if it exists. Stochastic gradient descent is a special case of mini-batch gradient descent in which the mini-batch size is 1.   I guess in the context of neural network terminology: In order to define iteration (a.k.a steps), you first need to know about batch size: Batch Size: You probably wouldn't like to process the entire training instances all at one forward pass as it is inefficient and needs a huge deal of memory. So what is commonly done is splitting up training instances into subsets (i.e., batches), performing one pass over the selected subset (i.e., batch), and then optimizing the network through backpropagation. The number of training instances within a subset (i.e., batch) is called batch_size. Iteration: (a.k.a training steps) You know that your network has to go over all training instances in one pass in order to complete one epoch. But wait! when you are splitting up your training instances into batches, that means you can only process one batch (a subset of training instances) in one forward pass, so what about the other batches? This is where the term Iteration comes into play: Definition: The number of forwarding passes (The number of batches that you have created) that your network has to do in order to complete one epoch (i.e., going over all training instances) is called Iteration. For example, when you have 10,000 training instances and you want to do batching with the size of 10; you have to do 10,000/10 = 1,000 iterations to complete 1 epoch. Hope this could answer your question! You have training data which you shuffle and pick mini-batches from it. When you adjust your weights and biases using one mini-batch, you have completed one iteration. Once you run out of your mini-batches, you have completed an epoch. Then you shuffle your training data again, pick your mini-batches again, and iterate through all of them again. That would be your second epoch. Typically, you'll split your test set into small batches for the network to learn from, and make the training go step by step through your number of layers, applying gradient-descent all the way down. All these small steps can be called iterations. An epoch corresponds to the entire training set going through the entire network once. It can be useful to limit this, e.g. to fight to overfit. To my understanding, when you need to train a NN, you need a large dataset that involves many data items. when NN is being trained, data items go into NN one by one, that is called an iteration; When the whole dataset goes through, it is called an epoch. I believe iteration is equivalent to a single batch forward+backprop in batch SGD. Epoch is going through the entire dataset once (as someone else mentioned). An epoch contains a few iterations. That's actually what this epoch is. Let's define epoch as the number of iterations over the data set in order to train the neural network. Epoch is 1 complete cycle where the Neural network has seen all the data. One might have said 100,000 images to train the model, however, memory space might not be sufficient to process all the images at once, hence we split training the model on smaller chunks of data called batches. e.g. batch size is 100. We need to cover all the images using multiple batches. So we will need 1000 iterations to cover all the 100,000 images. (100 batch size * 1000 iterations) Once Neural Network looks at the entire data it is called 1 Epoch (Point 1). One might need multiple epochs to train the model. (let us say 10 epochs). An epoch is an iteration of a subset of the samples for training, for example, the gradient descent algorithm in a neural network. A good reference is: http://neuralnetworksanddeeplearning.com/chap1.html Note that the page has a code for the gradient descent algorithm which uses epoch Look at the code. For each epoch, we randomly generate a subset of the inputs for the gradient descent algorithm. Why epoch is effective is also explained on the page. Please take a look. According to Google's Machine Learning Glossary, an epoch is defined as "A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch_size training iterations, where N is the total number of examples." If  you are training model for 10 epochs with batch size 6, given total 12 samples that means: the model will be able to see the whole dataset in 2 iterations ( 12 / 6  = 2) i.e. single epoch. overall, the model will have 2 X 10 = 20 iterations (iterations-per-epoch X no-of-epochs) re-evaluation of loss and model parameters will be performed after each iteration! epoch A full training pass over the entire dataset such that each
example has been seen once. Thus, an epoch represents N/batch
size training iterations, where N is the total number of
examples. iteration A single update of a model's weights during training.
An iteration consists of computing the gradients of the parameters
with respect to the loss on a single batch of data. as bonus: batch The set of examples used in one iteration (that is, one gradient
update) of model training. See also batch size. source: https://developers.google.com/machine-learning/glossary/I have a large set of vectors in 3 dimensions. I need to cluster these based on Euclidean distance such that all the vectors in any particular cluster have a Euclidean distance between each other less than a threshold "T". I do not know how many clusters exist. At the end, there may be individual vectors existing that are not part of any cluster because its euclidean distance is not less than "T" with any of the vectors in the space. What existing algorithms / approach should be used here? You can use hierarchical clustering. It is a rather basic approach, so there are lots of implementations available. It is for example included in Python's scipy.  See for example the following script: Which produces a result similar to the following image. 
 The threshold given as a parameter is a distance value on which basis the decision is made whether points/clusters will be merged into another cluster. The distance metric being used can also be specified. Note that there are various methods for how to compute the intra-/inter-cluster similarity, e.g. distance between the closest points, distance between the furthest points, distance to the cluster centers and so on. Some of these methods are also supported by scipys hierarchical clustering module (single/complete/average... linkage). According to your post I think you would want to use complete linkage.  Note that this approach also allows small (single point) clusters if they don't meet the similarity criterion of the other clusters, i.e. the distance threshold. There are other algorithms that will perform better, which will become relevant in situations with lots of data points. As other answers/comments suggest you might also want to have a look at the DBSCAN algorithm: For a nice overview on these and other clustering algorithms, also have a look at this demo page (of Python's scikit-learn library): Image copied from that place:   As you can see, each algorithm makes some assumptions about the number and shape of the clusters that need to be taken into account. Be it implicit assumptions imposed by the algorithm or explicit assumptions specified by parameterization.  The answer by moooeeeep recommended using hierarchical clustering.  I wanted to elaborate on how to choose the treshold of the clustering. One way is to compute clusterings based on different thresholds t1, t2, t3,... and then compute a metric for the "quality" of the clustering.  The premise is that the quality of a clustering with the optimal number of clusters will have the maximum value of the quality metric. An example of a good quality metric I've used in the past is Calinski-Harabasz.  Briefly: you  compute the average inter-cluster distances and divide them by the within-cluster distances.  The optimal clustering assignment will have clusters that are separated from each other the most, and clusters that are "tightest". By the way, you don't have to use hierarchical clustering.  You can also use something like k-means, precompute it for each k, and then pick the k that has the highest Calinski-Harabasz score. Let me know if you need more references, and I'll scour my hard disk for some papers. Check out the DBSCAN algorithm. It clusters based on local density of vectors, i.e. they must not be more than some ε distance apart, and can determine the number of clusters automatically. It also considers outliers, i.e. points with an unsufficient number of ε-neighbors, to not be part of a cluster. The Wikipedia page links to a few implementations. Use OPTICS, which works well with large data sets. OPTICS: Ordering Points To Identify the Clustering Structure Closely related to DBSCAN, finds core sample of high density and expands clusters from them 1. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN Fine tune eps, min_samples as per your requirement. I want to add to moooeeeep's answer by using hierarchical clustering.
This solution work for me, though it quite "random" to pick threshold value.
By referrence to other source and test by myself, I got better method and threshold could be easily picked by dendrogram: You will see the plot like this
click here.
Then by drawing the horizontal line, let say at distance = 1, the number of conjunctions will be your desire number of clusters. So here I choose threshold = 1 for 4 clusters. Now each value in cluster_list will be an assigned cluster-id of the corresponding point in ori_array. You may have no solution: it is the case when the distance between any two distinct input data points is always greater than T. If you want to compute the number of clusters only from the input data, you may look at MCG, a hierarchical clustering method with an automatic stop criterion: see the free seminar paper at https://hal.archives-ouvertes.fr/hal-02124947/document (contains bibliographic references). I needed a way to "fuzzy sort" lines from OCR output, when the output is sometimes out of order, but within blocks, the lines are usually in order. In this case, the items to sort are dictionaries, which describe words at a location 'x','y' and with size 'w','h'. The general clustering algorithms seemed like overkill and I needed to maintain the order of the items during the sort. Here, I can set the tolerance tol to be about 1/4 the line spacing, and this is called with the field being 'y'. The trouble is that the 'y' coordinate of the OCR output are based on the outline around the word and a later word in the same line might have a 'y' coordinate that is lower than an earlier word. So a full sort by 'y' does not work. This is much like the clustering algorithm, but the intention is a bit different. I am not interested in the statistics of the data points, but I am interested in exactly which cluster each is placed and also it is important to maintain the original order. Maybe there is some way to fuzzy sort using the sorting built-ins, and it might be an alternative to the clustering options for 1-D problems.I'm trying to recover from a PCA done with scikit-learn, which features are selected as relevant. A classic example with IRIS dataset. This returns How can I recover which two features allow these two explained variance among the dataset ?
Said diferently, how can i get the index of this features in iris.feature_names ? This information is included in the pca attribute: components_. As described in the documentation, pca.components_ outputs an array of [n_components, n_features], so to get how components are linearly related with the different features you have to: Note: each coefficient represents the correlation between a particular pair of component and feature IMPORTANT: As a side comment, note the PCA sign does not affect its interpretation since the sign does not affect the variance contained in each component. Only the relative signs of features forming the PCA dimension are important. In fact, if you run the PCA code again, you might get the PCA dimensions with the signs inverted. For an intuition about this, think about a vector and its negative in 3-D space - both are essentially representing the same direction in space.
Check this post for further reference. Edit: as others have commented, you may get same values from .components_ attribute. Each principal component is a linear combination of the original variables:  where X_is are the original variables, and Beta_is are the corresponding weights or so called coefficients. To obtain the weights, you may simply pass identity matrix to the transform method: Each column of the coef matrix above shows the weights in the linear combination which obtains corresponding principal component: For example, above shows that the second principal component (PC-2) is mostly aligned with sepal width, which has the highest weight of 0.926 in absolute value; Since the data were normalized, you can confirm that the principal components have variance 1.0 which is equivalent to each coefficient vector having norm 1.0: One may also confirm that the principal components can be calculated as the dot product of the above coefficients and the original variables: Note that we need to use numpy.allclose instead of regular equality operator, because of floating point precision error. The way this question is phrased reminds me of a misunderstanding of Principle Component Analysis when I was first trying to figure it out. I’d like to go through it here in the hope that others won’t spend as much time on a road-to-nowhere as I did before the penny finally dropped. The notion of “recovering” feature names suggests that PCA identifies those features that are most important in a dataset. That’s not strictly true. PCA, as I understand it, identifies the features with the greatest variance in a dataset, and can then use this quality of the dataset to create a smaller dataset with a minimal loss of descriptive power. The advantages of a smaller dataset is that it requires less processing power and should have less noise in the data. But the features of greatest variance are not the "best" or "most important" features of a dataset, insofar as such concepts can be said to exist at all. To bring that theory into the practicalities of @Rafa’s sample code above:
 consider the following:
 In this case, post_pca_array has the same 150 rows of data as data_scaled, but data_scaled’s four columns have been reduced from four to two. The critical point here is that the two columns – or components, to be terminologically consistent – of post_pca_array are not the two “best” columns of data_scaled. They are two new columns, determined by the algorithm behind sklearn.decomposition’s PCA module. The second column, PC-2 in @Rafa’s example, is informed by sepal_width more than any other column, but the values in PC-2 and data_scaled['sepal_width'] are not the same. As such, while it’s interesting to find out how much each column in original data contributed to the components of a post-PCA dataset, the notion of “recovering” column names is a little misleading, and certainly misled me for a long time. The only situation where there would be a match between post-PCA and original columns would be if the number of principle components were set at the same number as columns in the original. However, there would be no point in using the same number of columns because the data would not have changed. You would only have gone there to come back again, as it were. This prints: So on the PC1 the feature named e is the most important and on PC2 the d. Given your fitted estimator pca, the components are to be found in pca.components_, which represent the directions of highest variance in the dataset.I have performed a PCA analysis over my original dataset and from the compressed dataset transformed by the PCA I have also selected the number of PC I want to keep (they explain almost the 94% of the variance). Now I am struggling with the identification of the original features that are important in the reduced dataset. 
How do I find out which feature is important and which is not among the remaining Principal Components after the dimension reduction?
Here is my code: Furthermore, I tried also to perform a clustering algorithm on the reduced dataset but surprisingly for me, the score is lower than on the original dataset. How is it possible?  First of all, I assume that you call features the variables and not the samples/observations. In this case, you could do something like the following by creating a biplot function that shows everything in one plot. In this example, I am using the iris data. Before the example, please note that the basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings). See my last paragraph after the plot for more details. Overview: PART1: I explain how to check the importance of the features and how to plot a biplot. PART2: I explain how to check the importance of the features and how to save them into a pandas dataframe using the feature names. Visualize what's going on using the biplot  Now, the importance of each feature is reflected by the magnitude of the corresponding values in the eigenvectors (higher magnitude - higher importance) Let's see first what amount of variance does each PC explain. PC1 explains 72% and PC2 23%. Together, if we keep PC1 and PC2 only, they explain 95%. Now, let's find the most important features. Here, pca.components_ has shape [n_components, n_features]. Thus, by looking at the PC1 (First Principal Component) which is the first row: [0.52237162 0.26335492 0.58125401 0.56561105]] we can conclude that feature 1, 3 and 4 (or Var 1, 3 and 4 in the biplot) are the most important. This is also clearly visible from the biplot (that's why we often use this plot to summarize the information in a visual way). To sum up, look at the absolute values of the Eigenvectors' components corresponding to the k largest Eigenvalues. In sklearn the components are sorted by explained_variance_. The larger they are these absolute values, the more a specific feature contributes to that principal component. The important features are the ones that influence more the components and thus, have a large absolute value/score on the component. To  get the most important features on the PCs with names and save them into a pandas dataframe use this: This prints: So on the PC1 the feature named e is the most important and on PC2 the d. Nice article as well here: https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=friends_link&sk=65bf5440e444c24aff192fedf9f8b64f the pca library contains this functionality. A demonstration to extract the feature importance is as following: Plot the explained variance  Make the biplot. It can be nicely seen that the first feature with most variance (f1), is almost horizontal in the plot, whereas the second most variance (f2) is almost vertical. This is expected because most of the variance is in f1, followed by f2 etc.  Biplot in 3d. Here we see the nice addition of the expected f3 in the plot in the z-direction.I've just started to experiment with AWS SageMaker and would like to load data from an S3 bucket into a pandas dataframe in my SageMaker python jupyter notebook for analysis. I could use boto to grab the data from S3, but I'm wondering whether there is a more elegant method as part of the SageMaker framework to do this in my python code? In the simplest case you don't need boto3, because you just read resources.
Then it's even simpler: But as Prateek stated make sure to configure your SageMaker notebook instance to have access to s3. This is done at configuration step in Permissions > IAM role If you have a look here it seems you can specify this in the InputDataConfig. Search for "S3DataSource" (ref) in the document. The first hit is even in Python, on page 25/26. You could also access your bucket as your file system using s3fs  Do make sure the Amazon SageMaker role has policy attached to it to have access to S3. It can be done in IAM. You can also use AWS Data Wrangler https://github.com/awslabs/aws-data-wrangler: A similar answer with the f-string. This code sample to import csv file from S3, tested at SageMaker notebook. Use pip or conda to install s3fs. !pip install s3fs There are multiple ways to read data into Sagemaker. To make the response more comprehensive i am adding details to read the data into Sagemaker Studio Notebook in memory as well as S3 mounting options. Though Notebooks are not recommend for data intensive modeling and are more used for prototyping based on my experience, there are multiple ways the data can be read into it. Both Boto3 and S3FS can also be used in conjunction with python libraries like Pandas to read the data in memory as well as can also be used to
copy the data to local instance EFS. These two options provide a mount like behaviour where the data appears to be in as if the local directory for higher IO operations. Both of these options have their pros and cons.When trying to get cross-entropy with sigmoid activation function, there is a difference between  But they are the same when with softmax activation function. Following is the sample code: You're confusing the cross-entropy for binary and multi-class problems. The formula that you use is correct and it directly corresponds to tf.nn.softmax_cross_entropy_with_logits: p and q are expected to be probability distributions over N classes. In particular, N can be 2, as in the following example: Note that q is computing tf.nn.softmax, i.e. outputs a probability distribution. So it's still multi-class cross-entropy formula, only for N = 2. This time the correct formula is Though mathematically it's a partial case of the multi-class case, the meaning of p and q is different. In the simplest case, each p and q is a number, corresponding to a probability of the class A.  Important: Don't get confused by the common p * -tf.log(q) part and the sum. Previous p was a one-hot vector, now it's a number, zero or one. Same for q - it was a probability distribution, now's it's a number (probability). If p is a vector, each individual component is considered an independent binary classification. See this answer that outlines the difference between softmax and sigmoid functions in tensorflow. So the definition p = [0, 0, 0, 1, 0] doesn't mean a one-hot vector, but 5 different features, 4 of which are off and 1 is on. The definition q = [0.2, 0.2, 0.2, 0.2, 0.2] means that each of 5 features is on with 20% probability. This explains the use of sigmoid function before the cross-entropy: its goal is to squash the logit to [0, 1] interval. The formula above still holds for multiple independent features, and that's exactly what tf.nn.sigmoid_cross_entropy_with_logits computes: You should see that the last three tensors are equal, while the prob1 is only a part of cross-entropy, so it contains correct value only when p is 1: Now it should be clear that taking a sum of -p * tf.log(q) along axis=1 doesn't make sense in this setting, though it'd be a valid formula in multi-class case. you can understand differences between softmax and sigmoid cross entropy in following way: so anyway the cross entropy is: for softmax cross entropy it looks exactly as above formula， but for sigmoid, it looks a little different for it has multi binary probability distribution
for each binary probability distribution, it is p and (1-p) you can treat as two class probability within each binary probability distributionIs there a way to plot both the training losses and validation losses on the same graph? It's easy to have two separate scalar summaries for each of them individually, but this puts them on separate graphs. If both are displayed in the same graph it's much easier to see the gap between them and whether or not they have begin to diverge due to overfitting. Is there a built in way to do this? If not, a work around way? Thank you much! The work-around I have been doing is to use two SummaryWriter with different log dir for training set and cross-validation set respectively. And you will see something like this:  Rather than displaying the two lines separately, you can instead plot the difference between validation and training losses as its own scalar summary to track the divergence. This doesn't give as much information on a single plot (compared with adding two summaries), but it helps with being able to compare multiple runs (and not adding multiple summaries per run). Just for anyone coming accross this via a search: The current best practice to achieve this goal is to just use the SummaryWriter.add_scalars method from torch.utils.tensorboard. From the docs: Expected result:  Many thanks to niko for the tip on Custom Scalars. I was confused by the official custom_scalar_demo.py because there's so much going on, and I had to study it for quite a while before I figured out how it worked. To show exactly what needs to be done to create a custom scalar graph for an existing model, I put together the following complete example: The above consists of an "original model" augmented by three blocks of code indicated by
 My "original model" has these scalars:  and this graph:  My modified model has the same scalars and graph, together with the following custom scalar:  This custom scalar chart is simply a layout which combines the original two scalar charts. Unfortunately the resulting graph is hard to read because both values have the same color.  (They are distinguished only by marker.)  This is however consistent with TensorBoard's convention of having one color per log.   The idea is as follows.  You have some group of variables which you want to plot inside a single chart.  As a prerequisite, TensorBoard should be plotting each variable individually under the "SCALARS" heading.  (This is accomplished by creating a scalar summary for each variable, and then writing those summaries to the log. Nothing new here.)   To plot multiple variables in the same chart, we tell TensorBoard which of these summaries to group together.  The specified summaries are then combined into a single chart under the "CUSTOM SCALARS" heading.  We accomplish this by writing a "Layout" once at the beginning of the log.  Once TensorBoard receives the layout, it automatically produces a combined chart under "CUSTOM SCALARS" as the ordinary "SCALARS" are updated.   Assuming that your "original model" is already sending your variables (as scalar summaries) to TensorBoard, the only modification necessary is to inject the layout before your main iteration loop starts.  Each custom scalar chart selects which summaries to plot by means of a regular expression.  Thus for each group of variables to be plotted together, it can be useful to place the variables' respective summaries into a separate name scope.  (That way your regex can simply select all summaries under that name scope.) Important Note: The op which generates the summary of a variable is distinct from the variable itself.  For example, if I have a variable ns1/my_var, I can create a summary ns2/summary_op_for_myvar.  The custom scalars chart layout cares only about the summary op, not the name or scope of the original variable.  Here is an example, creating two tf.summary.FileWriters which share the same root directory. Creating a tf.summary.scalar shared by the two tf.summary.FileWriters. At every time step, get the summary and update each tf.summary.FileWriter.  Here is the result:  The orange line shows the result of the evaluation stage, and correspondingly, the blue line illustrates the data of the training stage. Also, there is a very useful post by TF team to which you can refer. For completeness, since tensorboard 1.5.0 this is now possible. You can use the custom scalars plugin. For this, you need to first make tensorboard layout configuration and write it to the event file. From the tensorboard example: A Category is group of Charts. Each Chart corresponds to a single plot which displays several scalars together. The Chart can plot simple scalars (MultilineChartContent) or filled areas (MarginChartContent, e.g. when you want to plot the deviation of some value). The tag member of MultilineChartContent must be a list of regex-es which match the tags of the scalars that you want to group in the Chart. For more details check the proto definitions of the objects in https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/custom_scalar/layout.proto. Note that if you have several FileWriters writing to the same directory, you need to write the layout in only one of the files. Writing it to a separate file also works. To view the data in TensorBoard, you need to open the Custom Scalars tab. Here is an example image of what to expect https://user-images.githubusercontent.com/4221553/32865784-840edf52-ca19-11e7-88bc-1806b1243e0d.png The solution in PyTorch 1.5 with the approach of two writers: Keys in the train_losses dict have to match those in the val_losses to be grouped on the same graph. Tensorboard is really nice tool but by its declarative nature can make it difficult to get it to do exactly what you want.   I recommend you checkout Losswise (https://losswise.com) for plotting and keeping track of loss functions as an alternative to Tensorboard.  With Losswise you specify exactly what should be graphed together: And then you get something that looks like:   Notice how the data is fed to a particular graph explicitly via the loss_graph.append call, the data for which then appears in your project's dashboard. In addition, for the above example Losswise would automatically generate a table with columns for min(training_loss) and min(validation_loss) so you can easily compare summary statistics across your experiments.  Very useful for comparing results across a large number of experiments. Please let me contribute with some code sample in the answer given by @Lifu Huang. First download the loger.py from here and then: Finally you run tensorboard --logdir=summaries/ --port=6006and you get:What is the meaning of the (None, 100) in Output Shape?
Is this("None") the Sample number or the hidden dimension? None means this dimension is variable.  The first dimension in a keras model is always the batch size. You don't need fixed batch sizes, unless in very specific cases (for instance, when working with stateful=True LSTM layers).     That's why this dimension is often ignored when you define your model. For instance, when you define input_shape=(100,200), actually you're ignoring the batch size and defining the shape of "each sample". Internally the shape will be (None, 100, 200), allowing a variable batch size, each sample in the batch having the shape (100,200).      The batch size will be then automatically defined in the fit or predict methods. Other None dimensions: Not only the batch dimension can be None, but many others as well.    For instance, in a 2D convolutional network, where the expected input is (batchSize, height, width, channels), you can have shapes like (None, None, None, 3), allowing variable image sizes.  In recurrent networks and in 1D convolutions, you can also make the length/timesteps dimension variable, with shapes like (None, None, featuresOrChannels) Yes, None in summary means a dynamic dimension of a batch (mini batch). 
This is why you can set any batch size to your model. The summary() method is part of TF that incorporates Keras method print_summary().I am trying to create a simple deep-learning based model to predict y=x**2
But looks like deep learning is not able to learn the general function outside the scope of its training set. Intuitively I can think that neural network might not be able to fit y=x**2 as there is no multiplication involved between the inputs. Please note I am not asking how to create a model to fit x**2. I have already achieved that. I want to know the answers to following questions: Path to complete notebook:
https://github.com/krishansubudhi/MyPracticeProjects/blob/master/KerasBasic-nonlinear.ipynb training input:  training code Evaluation on random test set  Deep learning in this example is not good at predicting a simple non linear function. But good at predicting values in the sample space of training data. Given my remarks in the comments that your network is certainly not deep, let's accept that your analysis is indeed correct (after all, your model does seem to do a good job inside its training scope), in order to get to your 2nd question, which is the interesting one. Well, this is the kind of questions not exactly suitable for SO, since the exact meaning of "very limited" is arguably unclear... So, let's try to rephrase it: should we expect DL models to predict such numerical functions outside the numeric domain on which they have been trained? An example from a different domain may be enlightening here: suppose we have built a model able to detect & recognize animals in photos with very high accuracy (it is not hypothetical; such models do exist indeed); should we complain when the very same model cannot detect and recognize airplanes (or trees, refrigerators etc - you name it) in these same photos? Put like that, the answer is a clear & obvious no - we should not complain, and in fact we are certainly not even surprised by such a behavior in the first place. It is tempting for us humans to think that such models should be able to extrapolate, especially in the numeric domain, since this is something we do very "easily" ourselves; but ML models, while exceptionally good at interpolating, they fail miserably in extrapolation tasks, such as the one you present here. Trying to make it more intuitive, think that the whole "world" of such models is confined in the domain of their training sets: my example model above would be able to generalize and recognize animals in unseen photos as long as these animals are "between" (mind the quotes) the ones it has seen during training; in a similar manner, your model does a good job predicting the function value for arguments between the sample you have used for training. But in neither case these models are expected to go beyond their training domain (i.e. extrapolate). There is no "world" for my example model beyond animals, and similarly for your model beyond [-500, 500]... For corroboration, consider the very recent paper Neural Arithmetic Logic Units, by DeepMind; quoting from the abstract: Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. See also a relevant tweet of a prominent practitioner:  On to your third question: As it should be clear by now, this is a (hot) area of current research; see the above paper for starters... So, are DL models limited? Definitely - forget the scary tales about AGI for the foreseeable future. Are they very limited, as you put it? Well, I don't know... But, given their limitation in extrapolating, are they useful? This is arguably the real question of interest, and the answer is obviously - hell, yeah!This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. When I trained my neural network with Theano or Tensorflow, they will report a variable called "loss" per epoch. How should I interpret this variable? Higher loss is better or worse, or what does it mean for the final performance (accuracy) of my neural network? The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets. In the case of neural networks, the loss is usually negative log-likelihood and residual sum of squares for classification and regression respectively. Then naturally, the main objective in a learning model is to reduce (minimize) the loss function's value with respect to the model's parameters by changing the weight vector values through different optimization methods, such as backpropagation in neural networks. Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s). The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated. For example, if the number of test samples is 1000 and model classifies 952 of those correctly, then the model's accuracy is 95.2%.  There are also some subtleties while reducing the loss value. For instance, you may run into the problem of over-fitting in which the model "memorizes" the training examples and becomes kind of ineffective for the test set. Over-fitting also occurs in cases where you do not employ a regularization, you have a very complex model (the number of free parameters W is large) or the number of data points N is very low. They are two different metrics to evaluate your model's performance usually being used in different phases. Loss is often used in the training process to find the "best" parameter values for your model (e.g. weights in neural network). It is what you try to optimize in the training by updating weights. Accuracy is more from an applied perspective. Once you find the optimized parameters above, you use this metrics to evaluate how accurate your model's prediction is compared to the true data. Let us use a toy classification example. You want to predict gender from one's weight and height. You have 3 data, they are as follows:(0 stands for male, 1 stands for female) y1 = 0, x1_w = 50kg, x2_h = 160cm; y2 = 0, x2_w = 60kg, x2_h = 170cm; y3 = 1, x3_w = 55kg, x3_h = 175cm; You use a simple logistic regression model that is y = 1/(1+exp-(b1*x_w+b2*x_h)) How do you find b1 and b2? you define a loss first and use optimization method to minimize the loss in an iterative way by updating b1 and b2. In our example, a typical loss for this binary classification problem can be:
(a minus sign should be added in front of the summation sign)  We don't know what b1 and b2 should be. Let us make a random guess say b1 = 0.1 and b2 = -0.03. Then what is our loss now?    so the loss is
  Then you learning algorithm (e.g. gradient descent) will find a way to update b1 and b2 to decrease the loss. What if b1=0.1 and b2=-0.03 is the final b1 and b2 (output from gradient descent), what is the accuracy now? Let's assume if y_hat >= 0.5, we decide our prediction is female(1). otherwise it would be 0. Therefore, our algorithm predict y1 = 1, y2 = 1 and y3 = 1. What is our accuracy? We make wrong prediction on y1 and y2 and make correct one on y3. So now our accuracy is 1/3 = 33.33%  PS: In Amir's answer, back-propagation is said to be an optimization method in NN. I think it would be treated as a way to find gradient for weights in NN. Common optimization method in NN are GradientDescent and Adam. Just to clarify the Training/Validation/Test data sets:
The training set is used to perform the initial training of the model, initializing the weights of the neural network. The validation set is used after the neural network has been trained. It is used for tuning the network's hyperparameters, and comparing how changes to them affect the predictive accuracy of the model. Whereas the training set can be thought of as being used to build the neural network's gate weights, the validation set allows fine tuning of the parameters or architecture of the neural network model. It's useful as it allows repeatable comparison of these different parameters/architectures against the same data and networks weights, to observe how parameter/architecture changes affect the predictive power of the network. Then the test set is used only to test the predictive accuracy of the trained neural network on previously unseen data, after training and parameter/architecture selection with the training and validation data sets.In machine learning task. We should get a group of random w.r.t normal distribution with bound. We can get a normal distribution number with np.random.normal() but it does't offer any bound parameter. I want to know how to do that? The parametrization of truncnorm is complicated, so here is a function that translates the parametrization to something more intuitive: Instance the generator with the parameters: mean, standard deviation, and truncation range: Then, you can use X to generate a value: Or, a numpy array with N generated values: Here is the plot of three different truncated normal distributions:  If you're looking for the Truncated normal distribution, SciPy has a function for it called truncnorm The standard form of this distribution is a standard normal truncated
  to the range [a, b] — notice that a and b are defined over the domain
  of the standard normal. To convert clip values for a specific mean and
  standard deviation, use: a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std truncnorm takes a and b as shape parameters. The above example is bounded by -2 and 2 and returns 10 random variates (using the .rvs() method) Here's a histogram plot for -6, 6:  Besides @bakkal suggestion (+1) you might also want to take a look into Vincent Mazet recipe for achieving this, rewritten as py-rtnorm module by Christoph Lassner. You can subdivide your targeted range (by convention) to equal partitions and then calculate the integration of each and all area, then call uniform method on each partition according to the surface.
It's implemented in python: quad_vec(eval('scipy.stats.norm.pdf'), 1, 4,points=[0.5,2.5,3,4],full_output=True)I have constructed a CLDNN (Convolutional, LSTM, Deep Neural Network) structure for raw signal classification task. Each training epoch runs for about 90 seconds and the hyperparameters seems to be very difficult to optimize. I have been research various ways to optimize the hyperparameters (e.g. random or grid search) and found out about Bayesian Optimization. Although I am still not fully understanding the optimization algorithm, I feed like it will help me greatly. I would like to ask few questions regarding the optimization task. I would greatly appreciate any insights into this problem. Although I am still not fully understanding the optimization
  algorithm, I feed like it will help me greatly. First up, let me briefly explain this part.
Bayesian Optimization methods aim to deal with exploration-exploitation trade off in the multi-armed bandit problem. In this problem, there is an unknown function, which we can evaluate in any point, but each evaluation costs (direct penalty or opportunity cost), and the goal is to find its maximum using as few trials as possible. Basically, the trade off is this: you know the function in a finite set of points (of which some are good and some are bad), so you can try an area around the current local maximum, hoping to improve it (exploitation), or you can try a completely new area of space, that can potentially be much better or much worse (exploration), or somewhere in between. Bayesian Optimization methods (e.g. PI, EI, UCB), build a model of the target function using a Gaussian Process (GP) and at each step choose the most "promising" point based on their GP model (note that "promising" can be defined differently by different particular methods). Here's an example:  The true function is f(x) = x * sin(x) (black curve) on [-10, 10] interval. Red dots represent each trial, red curve is the GP mean, blue curve is the mean plus or minus one standard deviation. 
As you can see, the GP model doesn't match the true function everywhere, but the optimizer fairly quickly identified the "hot" area around -8 and started to exploit it. How do I set up the Bayesian Optimization with regards to a deep
  network? In this case, the space is defined by (possibly transformed) hyperparameters, usually a multidimensional unit hypercube.  For example, suppose you have three hyperparameters: a learning rate α in [0.001, 0.01], the regularizer λ in [0.1, 1] (both continuous) and the hidden layer size N in [50..100] (integer). The space for optimization is a 3-dimensional cube [0, 1]*[0, 1]*[0, 1]. Each point (p0, p1, p2) in this cube corresponds to a trinity (α, λ, N) by the following transformation: What is the function I am trying to optimize? Is it the cost of the
  validation set after N epochs? Correct, the target function is neural network validation accuracy. Clearly, each evaluation is expensive, because it requires at least several epochs for training. Also note that the target function is stochastic, i.e. two evaluations on the same point may slightly differ, but it's not a blocker for Bayesian Optimization, though it obviously increases the uncertainty. Is spearmint a good starting point for this task? Any other
  suggestions for this task? spearmint is a good library, you can definitely work with that. I can also recommend hyperopt. In my own research, I ended up writing my own tiny library, basically for two reasons: I wanted to code exact Bayesian method to use (in particular, I found a portfolio strategy of UCB and PI converged faster than anything else, in my case); plus there is another technique that can save up to 50% of training time called learning curve prediction (the idea is to skip full learning cycle when the optimizer is confident the model doesn't learn as fast as in other areas). I'm not aware of any library that implements this, so I coded it myself, and in the end it paid off. If you're interested, the code is on GitHub.I was working on a sign language detection project on jupyter notebook. While running the code for live detection I encountered an error as shown below: OpenCV(4.5.1) C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-1drr4hl0\opencv\modules\highgui\src\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage' The code that caused this error is: NB: I installed OpenCV using using pip install. Edit: This solution seems to work for a majority of users, but not
all. If you are in this case, see the proposed answer by
Sachin Mohan I had the exact same error using yolov5, on windows 10. Rebuilding the library by typing then worked for me. Few frustration hours later, saw this solution under the comment of the first answer by Karthik Thilakan This worked for me in the conda environment. Thanks Karthik! :) I installed another GPU and finally upgraded to Tensorflow 2 this week and suddenly, the same issue arose. I finally found my mistake and why uninstalling and reinstalling opencv works for some people. The issue is stated clearly in a text file in your opencv-python dist-packages named METADATA. It states; There are four different packages (see options 1, 2, 3 and 4 below) and you should SELECT ONLY ONE OF THEM. Do not install multiple different packages in the same environment. Further, the file says that; You should always use these packages if you do not use cv2.imshow et al. or you are using some other package (such as PyQt) than OpenCV to create your GUI. referring to Packages for server (headless) environments ... (with) no GUI library dependencies So, if you run; and the result is more than one opencv version, you've likely found your problem. While an uninstall & reinstall of opencv might solve your problem, a more masterful solution is to simply uninstall the headless version as that is the one that does not care about GUIs, as it should be used in server environments. I had the same problem when I wrote a similar program, but issue was with different versions of opencv packages. You can check them with the command: My output was: opencv-contrib-python         4.5.5.62 opencv-python                 4.5.5.62 opencv-python-headless        4.5.4.60 And it turned out that opencv-python-headless must be version 4.5.4 for the program to run properly. So the solution was to change the opencv-python version to be the same as opencv-python-headless. So in that case you can run: worked for me. I had this exact same issue a few weeks back and I'd like to perhaps complement some of the answers touching the headless elephant in the room. My complex project incorporates a few in-house subprojects by other colleagues. These tend to be developed and tested independently, so no cross-contamination occurs. However, since one of them used opencv-python and another went with opencv-python-headless, the final build installed both. THIS IS THE PROBLEM! Whenever I had both, a number of functions, particularly those pertaining visualisation, now failed. Worse: pip list revealed both opencv- versions installed! To make matters worse, whenever I uninstalled and installed again opencv-python (a simple --upgrade never worked, as it claimed the latest version was there and nothing needed upgrading), then it started working. We all hate witchcraft, so... I went down the compilation rabbit hole and obviously nothing good was there to be found. if you check into your .venv\Lib\site-packages, you'll find the following two folders: or whatever your version might be. These are the folders where pip gets its metadata from, but not where the actual code is. In fact, you don't do import opencv-..., but rather import cv2. You do import cv2 in both cases! In fact, -headless is a crippled drop-in for the real thing. So, if you look up in your list, you'll find a cv2 folder. Both libraries deposit their code in this folder. As we know, when it comes to saving files, the last on the scene wins. (Ok, I miss John Bercow.) Now, both libraries saving to the same folder, what is the order? Since they don't depend on one another, and in my case where poetry is being used to manage dependencies, alphabetical order is the default, and (drumroll) -headless comes last. At some point, I just decided to go nuts and remove -headless altogether. I am not the CV dev in the team, so I was just grasping for straws , but... it worked! That's when I looked int the whole drop-in thing. My colleagues were developing with a simple requirements.txt file, so when it came to gathering requirements in a nice proper pyproject.toml file, I just left the -headless option out. You can't have both. Whenever you have multi-part projects, I highly advise to run through the pip list after the environment is built and check for the couple. If you find both, always remove the -headless, as it is a subset of the main one. Achtung: check your .venv\pyvenv.cfg for a line with: This line means your project will be importing any libraries (other than the standard ones) from your global Python install and if you happen to have the -headless in the global environment, you're still in trouble. This error is mostly with Pycharm Ide , I resolved it by changing the project interpreter None of the given solution in the internet worked for me. This solved the issue for me: I was trying to move a set of files to my Windows10 from Ubuntu 18.04 LTD,  and running a cli for inference and the same error as mentioned in the opening post cropped up......I was checking on the versions of Open-CV and Open-CV Headless in both Ubuntu and Windows and they were exactly the same......While it was executing on Ubuntu, it threw the error in Windows......I removed Open-CV Headless and upgraded the Open-CV, and used the same set of commands and Windows started to execute the CLI for inferencing....... for streamlit cloud use opencv-python-headless
but in other platforms use opencv-python try: and then reinstall the OpenCV: you can also save image with single command and then open it from drive.
cv2.imwrite("TestImage.jpg",img) No need to waste time on cv2.imshow()Looking at an example 'solver.prototxt', posted on BVLC/caffe git, there is a training meta parameter What does this meta parameter mean? And what value should I assign to it? The weight_decay meta parameter govern the regularization term of the neural net. During training a regularization term is added to the network's loss to compute the backprop gradient. The weight_decay value determines how dominant this regularization term will be in the gradient computation.   As a rule of thumb, the more training examples you have, the weaker this term should be. The more parameters you have (i.e., deeper net, larger filters, larger InnerProduct layers etc.) the higher this term should be. Caffe also allows you to choose between L2 regularization (default) and L1 regularization, by setting However, since in most cases weights are small numbers (i.e., -1<w<1), the L2 norm of the weights is significantly smaller than their L1 norm. Thus, if you choose to use regularization_type: "L1" you might need to tune weight_decay to a significantly smaller value. While learning rate may (and usually does) change during training, the regularization weight is fixed throughout. Weight decay is a regularization term that penalizes big weights.
When the weight decay coefficient is big the penalty for big weights is also big, when it is small weights can freely grow. Look at this answer (not specific to caffe) for a better explanation:
Difference between neural net "weight decay" and "learning rate".I am trying to get Apple's sample Core ML Models that were demoed at the 2017 WWDC to function correctly. I am using the GoogLeNet to try and classify images (see the Apple Machine Learning Page). The model takes a CVPixelBuffer as an input. I have an image called imageSample.jpg that I'm using for this demo. My code is below: I am always getting the unexpected runtime error in the output rather than an image classification. My code to convert the image is below: I got this code from a previous StackOverflow post (last answer here). I recognize that the code may not be correct, but I have no idea of how to do this myself. I believe that this is the section that contains the error. The model calls for the following type of input: Image<RGB,224,224> You don't need to do a bunch of image mangling yourself to use a Core ML model with an image — the new Vision framework can do that for you. The WWDC17 session on Vision should have a bit more info — it's tomorrow afternoon. You can use a pure CoreML, but you should resize an image to (224,224) The expected image size for inputs you can find in the mimodel file:
 A demo project that uses both pure CoreML and Vision variants you can find here: https://github.com/handsomecode/iOS11-Demos/tree/coreml_vision/CoreML/CoreMLDemo If the input is UIImage, rather than an URL, and you want to use VNImageRequestHandler, you can use CIImage. From Classifying Images with Vision and Core MLI have an algorithm that is running on a set of objects. This algorithm produces a score value that dictates the differences between the elements in the set. The sorted output is something like this: [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230] If you lay these values down on a spreadsheet you see that they make up groups [1,1,5,6,1,5] [10,22,23,23] [50,51,51,52] [100,112,130] [500,512,600] [12000,12230] Is there a way to programatically get those groupings? Maybe some clustering algorithm using a machine learning library? Or am I overthinking this? I've looked at scikit but their examples are way too advanced for my problem... Clustering algorithms are designed for multivariate data. When you have 1-dimensional data, sort it, and look for the largest gaps. This is trivial and fast in 1d, and not possible in 2d. If you want something more advanced, use Kernel Density Estimation (KDE) and look for local minima to split the data set. There are a number of duplicates of this question: A good option if you don't know the number of clusters is MeanShift: Output for this algorithm: Modifying quantilevariable you can change the clustering number selection criteria You can use clustering to group these. The trick is to understand that there are two dimensions to your data: the dimension you can see, and the "spatial" dimension that looks like [1, 2, 3... 22]. You can create this matrix in numpy like so: Then you can perform clustering on the matrix, with: kclust's output will look like this: For you, the most interesting part is the first column of the matrix, which says what the centers are along that x dimension: You can then assign your points to a cluster based on which of the five centers they are closest to:I read this thread about the difference between SVC() and LinearSVC() in scikit-learn.  Now I have a data set of binary classification problem(For such a problem, the one-to-one/one-to-rest strategy difference between both functions could be ignore.) I want to try under what parameters would these 2 functions give me the same result. First of all, of course, we should set kernel='linear' for SVC()
However, I just could not get the same result from both functions. I could not find the answer from the documents, could anybody help me to find the equivalent parameter set I am looking for? Updated:
I modified the following code from an example of the scikit-learn website, and apparently they are not the same: Result:
Output Figure from previous code In mathematical sense you need to set: and Another element, which cannot be easily fixed is increasing intercept_scaling in LinearSVC, as in this implementation bias is regularized (which is not true in SVC nor should be true in SVM - thus this is not SVM) - consequently they will never be exactly equal (unless bias=0 for your problem), as they assume two different models Personally I consider LinearSVC one of the mistakes of sklearn developers - this class is simply not a linear SVM. After increasing intercept scaling (to 10.0)  However, if you scale it up too much - it will also fail, as now tolerance and number of iterations are crucial. To sum up: LinearSVC is not linear SVM, do not use it if do not have to.I have trained xor neural network in MATLAB and got these weights: Just from curiosity I have tried to write MATLAB code which computes the output of this network (two neurons in the hidden layer, and one in the output, TANSIG activation function). Code that I got: The problem is when input is lets say [1,1], it outputs -0.9989, when [0,1] 0.4902. While simulating network generated with MATLAB outputs adequately are 0.00055875 and 0.99943. What am I doing wrong? I wrote a simple example of an XOR network. I used newpr, which defaults  to tansig transfer function for both hidden and output layers. then we check the result by computing the output ourselves. The important thing to remember is that by default, inputs/outputs are scaled to the [-1,1] range: or more efficiently expressed as matrix product in one line: You usually don't use a sigmoid on your output layer--are you sure you should have the tansig on out3?  And are you sure you are looking at the weights of the appropriately trained network?  It looks like you've got a network trained to do XOR on [1,1] [1,-1] [-1,1] and [-1,-1], with +1 meaning "xor" and -1 meaning "same".I am building an image processing classifier and this code is an API to predict the image class of the image the whole code is running except this line (pred = model.predict_classes(test_image)) this API is made in Django framework and am using python 2.7 here is a point if I am running this code like normally ( without making an API) it's running perfectly Your test_image and input of tensorflow model is not match. The above is just assumption. If you want to debug, i guess you should print your image size and compare with first layout of your model definition. And check whe the size (width, height, depth) is matchhi I am building a image classifier for one-class classification in which i've used autoencoder while running this model I am getting this error by this line (autoencoder_model.fit) (ValueError: Error when checking target: expected model_2 to have shape (None, 252, 252, 1) but got array with shape (300, 128, 128, 3).) It's a simple incompatibility between the output shape of the decoder and the shape of your training data. (Target means output). I see you've got 2 MaxPoolings (dividing your image size by 4), and three upsamplings (multiplying the decoder's input by 8).  The final output of the autoencoder is too big and doesn't match your data. You must simply work in the model to make the output shape match your training data.  You're using wrong API Take a look at .fit method source code
from https://github.com/keras-team/keras/blob/master/keras/models.py So the x should be data, and the y should be label of the data.
Hope that helpWhen we train neural networks, we typically use gradient descent, which relies on a continuous, differentiable real-valued cost function. The final cost function might, for example, take the mean squared error. Or put another way, gradient descent implicitly assumes the end goal is regression - to minimize a real-valued error measure. Sometimes what we want a neural network to do is perform classification - given an input, classify it into two or more discrete categories. In this case, the end goal the user cares about is classification accuracy - the percentage of cases classified correctly. But when we are using a neural network for classification, though our goal is classification accuracy, that is not what the neural network is trying to optimize. The neural network is still trying to optimize the real-valued cost function. Sometimes these point in the same direction, but sometimes they don't. In particular, I've been running into cases where a neural network trained to correctly minimize the cost function, has a classification accuracy worse than a simple hand-coded threshold comparison. I've boiled this down to a minimal test case using TensorFlow. It sets up a perceptron (neural network with no hidden layers), trains it on an absolutely minimal dataset (one input variable, one binary output variable) assesses the classification accuracy of the result, then compares it to the classification accuracy of a simple hand-coded threshold comparison; the results are 60% and 80% respectively. Intuitively, this is because a single outlier with a large input value, generates a correspondingly large output value, so the way to minimize the cost function is to try extra hard to accommodate that one case, in the process misclassifying two more ordinary cases. The perceptron is correctly doing what it was told to do; it's just that this does not match what we actually want of a classifier. But the classification accuracy is not a continuous differentiable function, so we can't use it as the target for gradient descent. How can we train a neural network so that it ends up maximizing classification accuracy? How can we train a neural network so that it ends up maximizing classification accuracy? I'm asking for a way to get a continuous proxy function that's closer to the accuracy To start with, the loss function used today for classification tasks in (deep) neural nets was not invented with them, but it goes back several decades, and it actually comes from the early days of logistic regression. Here is the equation for the simple case of binary classification:  The idea behind it was exactly to come up with a continuous & differentiable function, so that we would be able to exploit the (vast, and still expanding) arsenal of convex optimization for classification problems. It is safe to say that the above loss function is the best we have so far, given the desired mathematical constraints mentioned above. Should we consider this problem (i.e. better approximating the accuracy) solved and finished? At least in principle, no. I am old enough to remember an era when the only activation functions practically available were tanh and sigmoid; then came ReLU and gave a real boost to the field. Similarly, someone may eventually come up with a better loss function, but arguably this is going to happen in a research paper, and not as an answer to a SO question... That said, the very fact that the current loss function comes from very elementary considerations of probability and information theory (fields that, in sharp contrast with the current field of deep learning, stand upon firm theoretical foundations) creates at least some doubt as to if a better proposal for the loss may be just around the corner. There is another subtle point on the relation between loss and accuracy, which makes the latter something qualitatively different than the former, and is frequently lost in such discussions. Let me elaborate a little... All the classifiers related to this discussion (i.e. neural nets, logistic regression etc) are probabilistic ones; that is, they do not return hard class memberships (0/1) but class probabilities (continuous real numbers in [0, 1]). Limiting the discussion for simplicity to the binary case, when converting a class probability to a (hard) class membership, we are implicitly involving a threshold, usually equal to 0.5, such as if p[i] > 0.5, then class[i] = "1". Now, we can find many cases whet this naive default choice of threshold will not work (heavily imbalanced datasets are the first to come to mind), and we'll have to choose a different one. But the important point for our discussion here is that this threshold selection, while being of central importance to the accuracy, is completely external to the mathematical optimization problem of minimizing the loss, and serves as a further "insulation layer" between them, compromising the simplistic view that loss is just a proxy for accuracy (it is not). As nicely put in the answer of this Cross Validated thread: the statistical component of your exercise ends when you output a probability for each class of your new sample. Choosing a threshold beyond which you classify a new observation as 1 vs. 0 is not part of the statistics any more. It is part of the decision component. Enlarging somewhat an already broad discussion: Can we possibly move completely away from the (very) limiting constraint of mathematical optimization of continuous & differentiable functions? In other words, can we do away with back-propagation and gradient descend? Well, we are actually doing so already, at least in the sub-field of reinforcement learning: 2017 was the year when new research from OpenAI on something called Evolution Strategies made headlines. And as an extra bonus, here is an ultra-fresh (Dec 2017) paper by Uber on the subject, again generating much enthusiasm in the community. I think you are forgetting to pass your output through a simgoid. Fixed below: The output:This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I have noticed that when One Hot encoding is used on a particular data set (a matrix) and used as training data for learning algorithms, it gives significantly better results with respect to prediction accuracy, compared to using the original matrix itself as training data. How does this performance increase happen?  Many learning algorithms either learn a single weight per feature, or they use distances between samples. The former is the case for linear models such as logistic regression, which are easy to explain. Suppose you have a dataset having only a single categorical feature "nationality", with values "UK", "French" and "US". Assume, without loss of generality, that these are encoded as 0, 1 and 2. You then have a weight w for this feature in a linear classifier, which will make some kind of decision based on the constraint w×x + b > 0, or equivalently w×x < b. The problem now is that the weight w cannot encode a three-way choice. The three possible values of w×x are 0, w and 2×w. Either these three all lead to the same decision (they're all < b or ≥b) or "UK" and "French" lead to the same decision, or "French" and "US" give the same decision. There's no possibility for the model to learn that "UK" and "US" should be given the same label, with "French" the odd one out. By one-hot encoding, you effectively blow up the feature space to three features, which will each get their own weights, so the decision function is now w[UK]x[UK] + w[FR]x[FR] + w[US]x[US] < b, where all the x's are booleans. In this space, such a linear function can express any sum/disjunction of the possibilities (e.g. "UK or US", which might be a predictor for someone speaking English). Similarly, any learner based on standard distance metrics (such as k-nearest neighbors) between samples will get confused without one-hot encoding. With the naive encoding and Euclidean distance, the distance between French and US is 1. The distance between US and UK is 2. But with the one-hot encoding, the pairwise distances between [1, 0, 0], [0, 1, 0] and [0, 0, 1] are all equal to √2. This is not true for all learning algorithms; decision trees and derived models such as random forests, if deep enough, can handle categorical variables without one-hot encoding. Regarding the increase of the features by doing one-hot-encoding one can use feature hashing. When you do hashing, you can specify the number of buckets to be much less than the number of the newly introduced features.   When you want to predict categories, you want to predict items of a set. Not using one-hot encoding is akin to letting the categories have neighbour categories (e.g.: if you did a regression with the integers of the categories instead) organized in a certain way and in a certain order.  Now, what happens if you assign category 0 to 0, category 1 to 1, and category 2 to 2 without one-hot encoding, and that your algorithm's prediction isn't sure if it should choose 0 or 2: should he predict 1 despite he thinks it's either 0 or 2? You see where it goes. The same goes for your data inputs: if they shouldn't be supposed to be neighbours, then don't show them to your algorithm as neighbours.Want to improve this question? Update the question so it can be answered with facts and citations by editing this post. Closed 2 years ago. If we have 10 eigenvectors then we can have 10 neural nodes in input layer.If we have 5 output classes then we can have 5 nodes in output layer.But what is the criteria for choosing number of hidden layer in a MLP and how many neural nodes in 1 hidden layer? how many hidden layers?  a model with zero hidden layers will resolve linearly separable data. So unless you already know your data isn't linearly separable, it doesn't hurt to verify this--why use a more complex model than the task requires? If it is linearly separable then a simpler technique will work, but a Perceptron will do the job as well. Assuming your data does require separation by a non-linear technique, then always start with one hidden layer. Almost certainly that's all you will need. If your data is separable using a MLP, then that MLP probably only needs a single hidden layer. There is theoretical justification for this, but my reason is purely empirical:  Many difficult classification/regression problems are solved using single-hidden-layer MLPs, yet I don't recall encountering any multiple-hidden-layer MLPs used to successfully model data--whether on ML bulletin boards, ML Textbooks, academic papers, etc. They exist, certainly, but the circumstances that justify their use is empirically quite rare. 
How many nodes in the hidden layer?  From the MLP academic literature. my own experience, etc., I have gathered and often rely upon several rules of thumb (RoT), and which I have also found to be reliable guides (ie., the guidance was accurate, and even when it wasn't, it was usually clear what to do next): RoT based on improving convergence: When you begin the model building, err on the side of more nodes
  in the hidden layer. Why? First, a few extra nodes in the hidden layer isn't likely do any any harm--your MLP will still converge. On the other hand, too few nodes in the hidden layer can prevent convergence. Think of it this way, additional nodes provides some excess capacity--additional weights to store/release signal to the network during iteration (training, or model building). Second, if you begin with additional nodes in your hidden layer, then it's easy to prune them later (during iteration progress). This is common and there are diagnostic techniques to assist you (e.g., Hinton Diagram, which is just a visual depiction of the weight matrices, a 'heat map' of the weight values,).  RoTs based on size of input layer and size of output layer: A rule of thumb is for the size of this [hidden] layer to be somewhere
  between the input layer size ... and the output layer size.... To calculate the number of hidden nodes we use a general rule of:
  (Number of inputs + outputs) x 2/3 RoT based on principal components: Typically, we specify as many hidden nodes as dimensions [principal
  components] needed to capture 70-90% of the variance of the input data
  set. And yet the NN FAQ author calls these Rules "nonsense" (literally) because they: ignore the number of training instances, the noise in the targets (values of the response variables), and the complexity of the feature space. In his view (and it always seemed to me that he knows what he's talking about), choose the number of neurons in the hidden layer based on whether your MLP includes some form of regularization, or early stopping.
 The only valid technique for optimizing the number of neurons in the Hidden Layer: During your model building, test obsessively; testing will reveal the signatures of "incorrect" network architecture. For instance, if you begin with an MLP having a hidden layer comprised of a small number of nodes (which you will gradually increase as needed, based on test results) your training and generalization error will both be high caused by bias and underfitting. Then increase the number of nodes in the hidden layer, one at a time, until the generalization error begins to increase, this time due to overfitting and high variance. In practice, I do it this way: input layer: the size of my data vactor (the number of features in my model) + 1 for the bias node and not including the response variable, of course output layer: soley determined by my model: regression (one node) versus classification (number of nodes equivalent to the number of classes, assuming softmax) hidden layer: to start, one hidden layer with a number of nodes equal to the size of the input layer. The "ideal" size is more likely to be smaller (i.e, some number of nodes between the number in the input layer and the number in the output layer) rather than larger--again, this is just an empirical observation, and the bulk of this observation is my own experience. If the project justified the additional time required, then I start with a single hidden layer comprised of a small number of nodes, then (as i explained just above) I add nodes to the Hidden Layer, one at a time, while calculating the generalization error, training error, bias, and variance. When generalization error has dipped and just before it begins to increase again, the number of nodes at that point is my choice. See figure below.  To automate the selection of the best number of layers and best number of neurons for each of the layers, you can use genetic optimization. The key pieces would be: You can also consider: It is very difficult to choose the number of neurons in a hidden layer, and to choose the number of hidden layers in your neural network. Usually, for most applications, one hidden layer is enough. Also, the number of neurons in that hidden layer should be between the number of inputs (10 in your example) and the number of outputs (5 in your example). But the best way to choose the number of neurons and hidden layers is experimentation. Train several neural networks with different numbers of hidden layers and hidden neurons, and measure the performance of those networks using cross-validation. You can stick with the number that yields the best performing network. Recently there is theoretical work on this https://arxiv.org/abs/1809.09953.  Assuming you use a RELU MLP, all hidden layers have the same number of nodes and your loss function and true function that you're approximating with a neural network obey some technical properties (in the paper), you can choose your depth to be of order $\log(n)$ and your width of hidden layers to be of order $n^{d/(2(\beta+d))}\log^2(n)$.  Here $n$ is your sample size, $d$ is the dimension of your input vector, and $\beta$ is a smoothness parameter for your true function.  Since $\beta$ is unknown, you will probably want to treat it as a hyperparameter. Doing this you can guarantee that with probability that converges to $1$ as function of sample size your approximation error converges to $0$ as a function of sample size.  They give the rate.  Note that this isn't guaranteed to be the 'best' architecture, but it can at least give you a good place to start with.  Further, my own experience suggests that things like dropout can still help in practice.It is not clear for  me the difference between loss function and metrics in Keras. The documentation was not helpful for me. The loss function is used to optimize your model. This is the function that will get minimized by the optimizer. A metric is used to judge the performance of your model. This is only for you to look at and has nothing to do with the optimization process. The loss function is that parameter one passes to Keras model.compile which is actually optimized while training the model . This loss function is generally minimized by the model. Unlike the loss function , the metric is another list of parameters passed to Keras model.compile which is actually used for judging the performance of the model. For example :   In classification problems, we want to minimize the cross-entropy loss, while also want to assess the model performance with the AUC. In this case, cross-entropy is the loss function and AUC is the metric. Metric is the model performance parameter that one can see while the model is judging itself on the validation set after each epoch of training. It is important to note that the metric is important for few Keras callbacks like EarlyStopping when one wants to stop training the model in case the metric isn't improving for a certaining no. of epochs. I have a contrived example in mind: Let's think about linear regression on a 2D-plane. In this case, loss function would be the mean squared error, the fitted line would minimize this error.  However, for some reason we are very very interested in the area under the curve from 0 to 1 of our fitted line, and thus this can be one of the metrics. And we monitor this metric while the model minimizes the mean squared error loss function.Here is my perceptron implementation in ANSI C: The training set I'm using: Data Set I have removed all irrelevant code. Basically what it does now it reads test1.txt file and loads values from it to three arrays: x, y, outputs. Then there is a perceptron learning algorithm which, for some reason, is not converging to 0 (globalError should converge to 0) and therefore I get an infinite do while loop. When I use a smaller training set (like 5 points), it works pretty well. Any ideas where could be the problem? I wrote this algorithm very similar to this C# Perceptron algorithm: EDIT: Here is an example with a smaller training set: In your current code, the perceptron successfully learns the direction of the decision boundary BUT is unable to translate it. (as someone pointed out, here is a more accurate version) The problem lies in the fact that your perceptron has no bias term, i.e. a third weight component connected to an input of value 1. The following is how I corrected the problem: ... with the following output: And here's a short animation of the code above using MATLAB, showing the decision boundary at each iteration:  It might help if you put the seeding of the random generator at the start of your main instead of reseeding on every call to randomFloat, i.e. Some small errors I spotted in your source code: Better change this to  so you doesn't have to rely on your x array to have the right size. You increase iterations inside the p loop, whereas the original C# code does this outside the p loop. Better move the printf and the iteration++ outside the p loop before the PAUSE statement - also I'd remove the PAUSE statement or change it to Even doing all those changes, your program still doesn't terminate using your data set, but the output is more consistent, giving an error oscillating somewhere between 56 and 60. The last thing you could try is to test the original C# program on this dataset, if it also doesn't terminate, there's something wrong with the algorithm (because your dataset looks correct, see my visualization comment). globalError will not become zero, it will converge to zero as you said, i.e. it will become very small. Change your loop like such: Give maxIterations and maxError values applicable to your problem.I've trained a sentiment classifier model using Keras library by following the below steps(broadly). Now for scoring using this model, I was able to save the model to a file and load from a file. However I've not found a way to save the Tokenizer object to file. Without this I'll have to process the corpus every time I need to score even a single sentence. Is there a way around this? The most common way is to use either pickle or joblib. Here you have an example on how to use pickle in order to save Tokenizer: Tokenizer class has a function to save date into JSON format: The data can be loaded using tokenizer_from_json function from keras_preprocessing.text: The accepted answer clearly demonstrates how to save the tokenizer. The following is a comment on the problem of (generally) scoring after fitting or saving. Suppose that a list texts is comprised of two lists Train_text and Test_text, where the set of tokens in Test_text is a subset of the set of tokens in Train_text (an optimistic assumption). Then fit_on_texts(Train_text) gives different results for texts_to_sequences(Test_text) as compared with first calling fit_on_texts(texts) and then text_to_sequences(Test_text). Concrete Example: Results: Of course, if the above optimistic assumption is not satisfied and the set of tokens in Test_text is disjoint from that of Train_test, then test 1 results in a list of empty brackets []. I've created the issue https://github.com/keras-team/keras/issues/9289 in  the keras Repo. Until the API is changed, the issue has a link to a gist that has code to demonstrate how to save and restore a tokenizer without having the original documents the tokenizer was fit on. I prefer to store all my model information in a JSON file (because reasons, but mainly mixed JS/Python environment), and this will allow for that, even with sort_keys=True  I found the following snippet provided at following link by @thusv89. Save objects: Load objects: Quite easy, because Tokenizer class has provided two funtions for save and load: save —— Tokenizer.to_json() load —— keras.preprocessing.text.tokenizer_from_json In to_json() method，it call "get_config" method which handle this:I am currently in the process of designing a recommender system for text articles (a binary case of 'interesting' or 'not interesting'). One of my specifications is that it should continuously update to changing trends.  From what I can tell, the best way to do this is to make use of machine learning algorithm that supports incremental/online learning.  Algorithms like the Perceptron and Winnow support online learning but I am not completely certain about Support Vector Machines. Does the scikit-learn python library support online learning and if so, is a support vector machine one of the algorithms that can make use of it? I am obviously not completely tied down to using support vector machines, but they are usually the go to algorithm for binary classification due to their all round performance. I would be willing to change to whatever fits best in the end. While online algorithms for SVMs do exist, it has become important to specify if you want kernel or linear SVMs, as many efficient algorithms have been developed for the special case of linear SVMs.  For the linear case, if you use the SGD classifier in scikit-learn with the hinge loss and L2 regularization you will get an SVM that can be updated online/incrementall. You can combine this with feature transforms that approximate a kernel to get similar to an online kernel SVM.  One of my specifications is that it should continuously update to changing trends. This is referred to as concept drift, and will not be handled well by a simple online SVM. Using the PassiveAggresive classifier will likely give you better results, as it's learning rate does not decrease over time.  Assuming you get feedback while training / running, you can attempt to detect decreases in accuracy over time and begin training a new model when the accuracy starts to decrease (and switch to the new one when you believe that it has become more accurate). JSAT has 2 drift detection methods (see jsat.driftdetectors) that can be used to track accuracy and alert you when it has changed.  It also has more online linear and kernel methods. (bias note: I'm the author of JSAT).  Maybe it's me being naive but I think it is worth mentioning how to actually update the sci-kit SGD classifier when you present your data incrementally: The short answer is no. Sklearn implementation (as well as most of the existing others) do not support online SVM training. It is possible to train SVM in  an incremental way, but it is not so trivial task. If you want to limit yourself to the linear case, than the answer is yes, as sklearn provides you with Stochastic Gradient Descent (SGD), which has option to minimize the SVM criterion. You can also try out pegasos library instead, which supports online SVM training. The problem of trend adaptation is currently very popular in ML community. As @Raff stated, it is called concept drift, and has numerous approaches, which are often kinds of meta models, which analyze "how the trend is behaving" and change the underlying ML model (by for example forcing it to retrain on the subset of the data). So you have two independent problems here: A way to scale SVM could be split your large dataset into batches that can be safely consumed by an SVM algorithm, then find support vectors for each batch separately, and then build a resulting SVM model on a dataset consisting of all the support vectors found in all the batches. Updating to trends could be achieved by maintaining a time window each time you run your training pipeline. For example, if you do your training once a day and there is enough information in a month's historical data, create your traning dataset from the historical data obtained in the recent 30 days. SGD for batch learning tasks normally has a decreasing learning rate and goes over training set multiple times. So, for purely online learning, make sure learning_rate is set to 'constant' in sklearn.linear_model.SGDClassifier() and eta0= 0.1 or any desired value. Therefore the process is as follows: If interested in online learning with concept drift then here is some previous work Learning under Concept Drift: an Overview
https://arxiv.org/pdf/1010.4784.pdf The problem of concept drift: definitions and related work
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.9085&rep=rep1&type=pdf A Survey on Concept Drift Adaptation
http://www.win.tue.nl/~mpechen/publications/pubs/Gama_ACMCS_AdaptationCD_accepted.pdf MOA Concept Drift Active Learning Strategies for Streaming Data
http://videolectures.net/wapa2011_bifet_moa/ A Stream of Algorithms for Concept Drift
http://people.cs.georgetown.edu/~maloof/pubs/maloof.heilbronn12.handout.pdf MINING DATA STREAMS WITH CONCEPT DRIFT
http://www.cs.put.poznan.pl/dbrzezinski/publications/ConceptDrift.pdf Analyzing time series data with stream processing and machine learning
http://www.ibmbigdatahub.com/blog/analyzing-time-series-data-stream-processing-and-machine-learningI have web application written in Flask. As suggested by everyone, I can't use Flask in production. So I thought of Gunicorn with Flask.   In Flask application I am loading some Machine Learning models. These are of size 8GB collectively. Concurrency of my web application can go upto 1000 requests. And the RAM of machine is 15GB.
So what is the best way to run this application? You can start your app with multiple workers or async workers with Gunicorn. Flask server.py Gunicorn with gevent async worker Gunicorn 1 worker 12 threads: Gunicorn with 4 workers (multiprocessing): More information on Flask concurrency in this post: How many concurrent requests does a single Flask process receive?. The best thing to do is to use pre-fork mode (preload_app=True). This will initialize your code in a "master" process and then simply fork off worker processes to handle requests. If you are running on linux and assuming your model is read-only, the OS is smart enough to reuse the physical memory amongst all the processes.What are the differences between all these cross-entropy losses? Keras is talking about While TensorFlow has What are the differences and relationships between them? What are the typical applications for them? What's the mathematical background? Are there other cross-entropy types that one should know? Are there any cross-entropy types without logits? There is just one cross (Shannon) entropy defined as: In machine learning usage, P is the actual (ground truth) distribution, and Q is the predicted distribution. All the functions you listed are just helper functions which accepts different ways to represent P and Q. There are basically 3 main things to consider: there are either 2 possibles outcomes (binary classification) or more. If there are just two outcomes, then Q(X=1) = 1 - Q(X=0) so a single float in (0,1) identifies the whole distribution, this is why neural network in binary classification has a single output (and so does logistic regresssion). If there are K>2 possible outcomes one has to define K outputs (one per each Q(X=...)) one either produces proper probabilities (meaning that Q(X=i)>=0 and SUM_i Q(X=i) =1 or one just produces a "score" and has some fixed method of transforming score to probability. For example a single real number can be "transformed to probability" by taking sigmoid, and a set of real numbers can be transformed by taking their softmax and so on. there is j such that P(X=j)=1 (there is one "true class", targets are "hard", like "this image represent a cat") or there are "soft targets" (like "we are 60% sure this is a cat, but for 40% it is actually a dog"). Depending on these three aspects, different helper function should be used: In the end one could just use "categorical cross entropy", as this is how it is mathematically defined, however since things like hard targets or binary classification are very popular - modern ML libraries do provide these additional helper functions to make things simpler. In particular "stacking" sigmoid and cross entropy might be numerically unstable, but if one knows these two operations are applied together - there is a numerically stable version of them combined (which is implemented in TF). It is important to notice that if you apply wrong helper function the code will usually still execute, but results will be wrong. For example if you apply softmax_* helper for binary classification with one output your network will be considered to always produce "True" at the output. As a final note - this answer considers classification, it is slightly different when you consider multi label case (when a single point can have multiple labels), as then Ps do not sum to 1, and one should use sigmoid_cross_entropy_with_logits despite having multiple output units. For this purpose, "logits" can be seen as the non-activated outputs of the model. Losses "with logits" will apply the activation internally.
Some functions allow you to choose logits=True or logits=False, which will tell the function whether to "apply" or "not apply" the activations.Sometimes I run into a problem: e.g. Where 1024 is my batch size and I don't know what's the rest. If I reduce the batch size or the number of neurons in the model, it runs fine. Is there a generic way to calculate optimal batch size based on model and GPU memory, so the program doesn't crash? In short: I want the largest batch size possible in terms of my model, which will fit into my GPU memory and won't crash the program. From the recent Deep Learning book by Goodfellow et al., chapter 8: Minibatch sizes are generally driven by the following factors: Which in practice usually means "in powers of 2 and the larger the better, provided that the batch fits into your (GPU) memory". You might want also to consult several good posts here in Stack Exchange: Just keep in mind that the paper by Keskar et al. 'On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima', quoted by several of the posts above, has received some objections by other respectable researchers of the deep learning community. Hope this helps... UPDATE (Dec 2017): There is a new paper by Yoshua Bengio & team, Three Factors Influencing Minima in SGD (Nov 2017); it is worth reading in the sense that it reports new theoretical & experimental results on the interplay between learning rate and batch size. UPDATE (Mar 2021): Of interest here is also another paper from 2018, Revisiting Small Batch Training for Deep Neural Networks (h/t to Nicolas Gervais), which runs contrary to the larger the better advice; quoting from the abstract: The best performance has been consistently obtained for mini-batch sizes between m=2 and m=32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands. You can estimate the largest batch size using: Max batch size= available GPU memory bytes / 4 / (size of tensors + trainable parameters) Use the summaries provided by pytorchsummary (pip install) or keras (builtin). E.g. Each instance you put in the batch will require a full forward/backward pass in memory, your model you only need once. People seem to prefer batch sizes of powers of two, probably because of automatic layout optimization on the gpu. Don't forget to linearly increase your learning rate when increasing the batch size. Let's assume we have a Tesla P100 at hand with 16 GB memory. Here is a function to find batch size for training the model: I ran into a similar GPU mem error which was solved by configuring the tensorflow session with the following: see: google colaboratory `ResourceExhaustedError` with GPUI trained my CNN (VGG) through google colab and generated .h5 file. Now problem is, I can predict my output successfully through google colab but when i download that .h5 trained model file and try to predict output on my laptop, I am getting error when loading the model. Here is the code: And the error: I ran into the same issue. After changing: from tensorflow import keras to: import keras life is once again worth living. I fixed the problem: Before: Works for me Wow I, just spent 6 Hours of my life trying to figure this out.. Dmitri posted a solution to this here: I trained a keras model on google colab. Now not able to load it locally on my system. I'm just basically reposting it here because it worked for me. This looks like some kind of a serialization bug in keras. 
If you wrap your load_model with the below CustomObjectScope thingy... all should work.. Changing to  solved my problem! To eliminate errors, import all things directly from Keras or TensorFlow. Mixing both of them in same project may result in problems. I had a same problem and was fixed this way. just don't save the optimizer with the model!
just change the save line like this: Second parameter tells Keras to overwrite the model if the file existed or not and the 3rd one tells it not to save the optimizer with the model. Edit:
I ran over the problem again on another system today and this did not helped me this time. so i saved the model conf as json and weights as h5 and used them to rebuild the model in another machine. you can do it like this.
save like this: rebuild the model like this: Something that helped me which wasn't in any of the answers: custom_objects={'GlorotUniform': glorot_uniform()} In either kaggle or colabs works well this worked for me when importing tensorflow keras if you are loading the architecture and weights separtly, while loading archtiecture of the model change : to :  and the problem is solved I had the same problem with a model built with tensorflow 1.11.0 (using tensorflow.python.keras.models.save_model) and loaded with tensoflow 1.11.0 (using tensorflow.python.keras.models.load_model). I solved it by upgrading everything to tensorflow 1.13.1, after building the model again with the new version, I could load it without this error. For the json file problem mentioned by @Derk in one of the comment, you can write the following: and in your import line, remember to write: instead of from keras.initializers import glorot_uniform. It worked out for me when I try to read a model saved in tf2.2 in the environment with only tf1.9.What is the difference between  and  in TensorFlow? What would be different in your computation graph when you construct your graph with + instead of tf.add()?  More generally, are  + or other operations overloaded for tensors? If at least one of x or y is a tf.Tensor object, the expressions tf.add(x, y) and x + y are equivalent. The main reason you might use tf.add() is to specify an explicit name keyword argument for the created op, which is not possible with the overloaded operator version. Note that if neither x nor y is a tf.Tensor—for example if they are NumPy arrays—then x + y will not create a TensorFlow op. tf.add() always creates a TensorFlow op and converts its arguments to tf.Tensor objects. Therefore, if you are writing a library function that might accept both tensors and NumPy arrays, you might prefer to use tf.add(). The following operators are overloaded in the TensorFlow Python API: Please note, __eq__ ( binary == ) is not overloaded. x == y will simply return a Python boolean whether x and y refer to the same tensor. You need to use tf.equal() explicitly to check for element-wise equality. Same goes for not equal, __ne__ ( binary != ). Mrry nicely explained that there is no real difference. I will just add when using tf.add is beneficial. tf.add has one important parameter which is name. It allows you to name the operation in a graph which will be visible in tensorboard. So my rule of thumb, if it will be beneficial to name an operation in tensorboard, I use tf. equivalent, otherwise I go for brevity and use overloaded version.I have a image with horizontal and vertical lines. In fact, this image is the BBC website converted to horizontal and vertical lines.
My problem is that I want to be able to find all the rectangles in the image. I want to write a computer program to find all the rectangles.
Does anyone know how to do this or suggest ideas on how to get started? This task is easy for me as a person to find the visual rectangles, but I am not sure how to describe it as a program. Image is the BBC website here http://www.bbc.co.uk/ Update to this, I wrote the code which converts the BBC website image to the horizontal and vertical line, the problem is these lines do not completely meet at the corners and sometimes they do not completely form a rectangle. Thanks! Opencv (image processing and computer vision library written in c) has implementation for hough transform (the simple hough transform find lines in an image, while the generalized one finds more complex objects) so that could be a good start. For the rectangles which do have closed corners there are also corner detectors such as cornerHarris which can help. I ran the houghlines demo provided with opencv and here's the result on the image you gave (detected lines marked in red):

(source: splintec.com)  I believe you are looking for the generalized Hough transform. In computer vision there is a algorithm called Generalized Hough Transform which maybe can solve your problem. There should be open source code having implemented this algorithm. Just search for it. Assuming it's a reasonably noise free image (not a video of a screen) then one of the simple floodfill algorithms should work. You might need to run a dilate/erode on the image to close up the gaps. The normal way to find lines is a Hough transform ( then find lines at right angles) 
Opencv is the easiest way. Take a look at this question OpenCV Object Detection - Center Point There are several different approaches to your problem. I'd use a morphological image processing tool like this one. You will have the flexibility to define "rectangle" even something that not "exactly closed" (where the fill algorithm will fail). Another possibility could be to use a machine learning approach, which basically is more data-driven than definition-driven like the previous one. You'll have to give your algorithm several "examples" of what a rectangle is, and it will eventually learn (with a bias and an error rate). iterate from left to right until you hit a color pixel then use modified flood fill algorithm. more info on the algo flood fill @ wiki another approach would be to find ANY colored pixel on the image then go with then do the same upwards.
now u have defined a single line. then use ends of the lines to approx match lines into rectangles. if they are not pixel perfect you could do some kind of tresholding. The flood fill would work, or you could use a modification of an edge tracking algorithm. what you do is:
create a 2d array (or any other d2 data struct)- each row represents a horizontal pixel line on screen, and each column a vertical line iterate through all the pixels, left to right, and whenever you find a coloured one add its coordinates to the array iterate through the array and findying lines and storing the begin and end pixel for each one (different data structure) knowing that the begin of each line is its left/top pixel, you can easily check to see if any 4 lines comprise a rectangle To get from the image you have with the nearly touching horizontal and vertical lines to just the rectangles: This will, with a bit of luck, first show the boxes with thick fat lines, leaving thick fat artifacts all over the image (after step 3) and then then after step 5 all thick fat artifacts will have been removed, while all boxes remain. You need to tweek the number of repeats in step 3 for best results. If you're interested in image morphology, this is the book of a really good introductory course I took.  Sample: (0=black, 1=white, pixels in the center of each 3x3 block are being considered, input left, output right)I want to create my own transformer for use with the sklearn Pipeline. I am creating a class that implements both fit and transform methods. The purpose of the transformer will be to remove rows from the matrix that have more than a specified number of NaNs. The issue I am facing is how can I change both the X and y matrices that are passed to the transformer? I believe this has to be done in the fit method since it has access to both X and y. Since python passes arguments by assignment once I reassign X to a new matrix with fewer rows the reference to the original X is lost (and of course the same is true for y). Is it possible to maintain this reference? I’m using a pandas DataFrame to easily drop the rows that have too many NaNs, this may not be the right way to do it for my use case. The current code looks like this: Modifying the sample axis, e.g. removing samples, does not (yet?) comply with the scikit-learn transformer API. So if you need to do this, you should do it outside any calls to scikit learn, as preprocessing. As it is now, the transformer API is used to transform the features of a given sample into something new. This can implicitly contain information from other samples, but samples are never deleted. Another option is to attempt to impute the missing values. But again, if you need to delete samples, treat it as preprocessing before using scikit learn. You have to modify the internal code of sklearn Pipeline. We define a transformer that removes samples where at least the value of a feature or the target is NaN during fitting (fit_transform). While it removes the samples where at least the value of a feature is NaN during inference (transform). Important to note that our transformer returns X and y in fit_transform so we need to handle this behaviour in the sklearn Pipeline. We only have to modify the original sklearn Pipeline in only two specific points in fit and in _fit method. The rest remains unchanged. This is required in order to unpack the values generated by Dropna().fit_transform(X, y) in the new X and y. Here is the full pipeline at work: Another trial with a further intermediate preprocessing step: More complex behaviors can be achieved with other simple modifications according to the needs. If you are interested also in Pipeline().fit_transform or Pipeline().fit_predict you need to operate the same changes. The package imblearn, which is built on top of sklearn, contains an estimator FunctionSampler that allows manipulating both the features array, X, and target array, y, in a pipeline step. Note that using it in a pipeline step requires using the Pipeline class in imblearn that inherits from the one in sklearn. Furthermore, by default, in the context of Pipeline, the method resample does nothing when it is not called immediately after fit (as in fit_resample). So, read the documentation ahead of time. Adding to @João Matias response: Here's an example of using imblearn to define a pipeline step that drops rows with missing values: Note, you have to use the imblearn pipeline. You can solve this easily by using the sklearn.preprocessing.FunctionTransformer method (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) You just need to put your alternations to X in a function then you get your transformer by calling  which you can use in the pipeline. The threshold can be set outside the drop_nans function. @eickenberg is the proper and clean answer. Nevertheless, I like to keep everything into one Pipeline, so if you are interested, I created a library (not yet deployed on pypi) that allow to apply transformation on Y: https://gitlab.com/thibaultB/transformers/ Usage is the following: Using this code, you can alter the number of rows if you put all the transformer that modify the numbers of rows before the "SplitXY" transformer. Transformer before the SplitXY transformer should keep columns name, it is why I also added a SklearnPandasWrapper that wrap sklearn transformer (that usually return numpy array) to keep columns name. You can use function transformer   Use "deep-copies" further on, down the pipeline and X, y remain protected .fit() can first assign on each call deep-copy to new class-variables and then reduce / transform these not to have more NaN-s than ordered by self.tresholdAfter spending a couple days trying to achieve this task, I would like to share my experience of how I went about answering the question: How do I use TS Object Detection to train using my own dataset? This assumes the module is already installed. Please refer to their documentation if not. Disclaimer This answer is not meant to be the right or only way of training the object detection module. This is simply I sharing my experience and what has worked for me. I'm open to suggestions and learning more about this as I am still new to ML in general. TL;DR Each section of this answer consists of a corresponding Edit (see below). After reading each section, please read its Edit as well for clarifications. Corrections and tips were added for each section. Tools used LabelImg: A tool for creating PASCAL VOC format annotations. 1. Create your own PASCAL VOC dataset PS: For simplicity, the folder naming convention of my answer follows that of Pascal VOC 2012 A peek into the May 2012 dataset, you'll notice the folder as having the following structure 
+VOCdevkit
  +VOC2012
    +Annotations
    +ImageSets
      +Action
      +Layout
      +Main
      +Segmentation
    +JPEGImages
    +SegmentationClass
    +SegmentationObject
 For the time being, amendments were made to the following folders: Annotations: This is were all the images' corresponding XML files will be placed in. Use the suggested tool above to create the annotations. Do not worry about <truncated> and <difficulty> tags as they will be ignored by the training and eval binaries. JPEGImages: Location of your actual images. Make sure they are of type JPEG because that's what is currently supported in order to create TFRecords using their provided script. ImageSets->Main: This simply consists of text files. For each class, there exists a corresponding train.txt, trainval.txt and val.txt. Below is a sample of the contents of the aeroplane_train.txt in the VOC 2012 folder The structure is basically image name followed by a boolean saying whether the corresponding object exists in that image or not. Take for example image 2008_000008 does not consist of an aeroplane hence marked with a -1 but image 2008_000033 does. I wrote a small Python script to generate these text files. Simply iterate through the image names and assign a 1 or -1 next to them for object existence. I added some randomness among my text files by shuffling the image names. The {classname}_val.txt files consist of the testing validation datasets. Think of this as the test data during training. You want to divide your dataset into training and validation. More info can be found here. The format of these files is similar to that of training. At this point, your folder structure should be 
+VOCdevkit
  +VOC2012
    +Annotations
     --(for each image, generated annotation)
    +ImageSets
      +Main
        --(for each class, generated *classname*_train.txt and *classname*_val.txt)
    +JPEGImages
     --(a bunch of JPEG images)
 1.1 Generating label map With the dataset prepared, we need to create the corresponding label maps.
Navigate to models/object_detection/data and open pascal_label_map.pbtxt. This file consists of a JSON that assigns an ID and name to each item. Make amendments to this file to reflect your desired objects. 2. Generate TFRecords If you look into their code especially this line, they explicitly grab the aeroplane_train.txt only. For curios minds, here's why. Change this file name to any of your class train text file. Make sure VOCdevkit is inside models/object_detection then you can go ahead and generate the TFRecords. Please go through their code first should you run into any problems. It is self explanatory and well documented. 3. Pipeline Configuration The instructions should be self explanatory to cover this segment. Sample configs can be found in object_detection/samples/configs. For those looking to train from scratch as I did, just make sure to remove the fine_tune_checkpoint and from_detection_checkpoint nodes. Here's what my config file looked like for reference. From here on you can continue with the tutorial and run the training process. 4. Visualize Be sure to run the eval in parallel to the training in order to be able to visualize the learning process. To quote Jonathan Huang the best way is to just run the eval.py binary. We typically run this
  binary in parallel to training, pointing it at the directory holding
  the checkpoint that is being trained. The eval.py binary will write
  logs to an eval_dir that you specify which you can then point to
  with Tensorboard. You want to see that the mAP has "lifted off" in the first few hours,
  and then you want to see when it converges. It's hard to tell without
  looking at these plots how many steps you need. EDIT I (28 July '17): I never expected my response to get this much attention so I decided to come back and review it. Tools For my fellow Apple users, you could actually use RectLabel for annotations. Pascal VOC After digging around, I finally realized that trainval.txt is actually the union of training and validation datasets. Please look at their official development kit to understand the format even better. Label Map Generation At the time of my writing, ID 0 represents none_of_the_above. It is recommended that your IDs start from 1. Visualize After running your evaluation and directed tensorboard to your Eval directory, it'll show you the mAP of each category along with each category's performance. This is good but I like seeing my training data as well in parallel with Eval. To do this, run tensorboard on a different port and point it to your train directory I wrote a blog post on Medium about my experience as well on how I trained an object detector (in particular, it's a Raccoon detector) with Tensorflow on my own dataset. This might also be useful for others and is complimentary to eshirima's answer.I'm using R to do machine learning. Following standard machine learning methodology, I would like to randomly split my data into training, validation, and test data sets. How do I do that in R? I know there are some related questions on how to split into 2 data sets (e.g. this post), but it is not obvious how to do it for 3 split data sets. By the way, the correct approach is to use 3 data sets (including a validation set to tune your hyperparameters). This linked approach for two groups (using floor) doesn't extend naturally to three. I'd do  To check the results: With set.seed(1) run just before, the result looks like Data.frames can be accessed like res$test or res[["test"]]. cut is the standard tool for partitioning based on shares. Following the approach shown in this post, here is working R code to divide a dataframe into three new dataframes for testing, validation, and test. The three subsets are non-overlapping. Some of these seem overly complex, here's a simple way using sample to split any dataset into 3, or even an arbitrary number of sets. If you'd rather reusable code: Here is one solution with a 60, 20 , 20 split that also ensures that there is no overlapping. However it is a trouble to adapt the split. If anyone could help me out, I appreciate it Caret also support data splitting with the function createDataPartition if your outcome y is Unbalanced factor ( yes >>> No and vice versa), ideally the random sampling occurs within each class and should preserve the overall class distribution of the data.
which is the case with createDataPartition Example: Note our outcome is unbalanced Splitting (80% train and 20% test): Verification: Note we preserve the overall class distribution I think my approach is the easiest one: First, it splits the data into 70% training data and the rest (idxNotTrain).
Then, the rest is again splitted into a validation data set (33%, 10% of the total data) and the rest (the testing data, 66%, 20% of the total data). Let me know if this would work. Just a simplified versionIn Keras (with Tensorflow backend), is the current input pattern available to my custom loss function? The current input pattern is defined as the input vector used to produce the prediction. For example, consider the following: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=False). Then the current input pattern is the current X_train vector associated with the y_train (which is termed y_true in the loss function). When designing a custom loss function, I intend to optimize/minimize a value that requires access to the current input pattern, not just the current prediction. I've taken a look through https://github.com/fchollet/keras/blob/master/keras/losses.py I've also looked through "Cost function that isn't just y_pred, y_true?" I am also familiar with previous examples to produce a customized loss function: Presumably (y_true,y_pred) are defined elsewhere. I've taken a look through the source code without success and I'm wondering whether I need to define the current input pattern myself or whether this is already accessible to my loss function. You can wrap the loss function as a inner function and pass your input tensor to it (as commonly done when passing additional arguments to the loss function). You can verify that input_tensor and the loss value (mostly, the K.mean(input_tensor) part) will change as different X is passed to the model. You can use add_loss to pass external layers to your loss, in your case the input tensor. Here an example: To use the model in inference mode (removing the target from inputs)Actually, there is a contradiction of 2 facts that are the possible answers to the question: The conventional answer is to do it after splitting as there can be information leakage, if done before, from the Test-Set. The contradicting answer is that, if only the Training Set chosen from the whole dataset is used for Feature Selection, then the feature selection or feature importance score orders is likely to be dynamically changed with change in random_state of the Train_Test_Split. And if the feature selection for any particular work changes, then no Generalization of Feature Importance can be done, which is not desirable. Secondly, if only Training Set is used for feature selection, then the test set may contain certain set of instances that defies/contradicts the feature selection done only on the Training Set as the overall historical data is not analyzed. Moreover, feature importance scores can only be evaluated when, given a set of instances rather than a single test/unknown instance. It is not actually difficult to demonstrate why using the whole dataset (i.e. before splitting to train/test) for selecting features can lead you astray. Here is one such demonstration using random dummy data with Python and scikit-learn: Since our data X are random ones (500 samples, 10,000 features) and our labels y are binary, we expect than we should never be able to exceed the baseline accuracy for such a setting, i.e. ~ 0.5, or around 50%. Let's see what happens when we apply the wrong procedure of using the whole dataset for feature selection, before splitting: Wow! We get 76% test accuracy on a binary problem where, according to the very basic laws of statistics, we should be getting something very close to 50%! Someone to call the Nobel Prize committee, and fast... ... the truth of course is that we were able to get such a test accuracy simply because we have committed a very basic mistake: we mistakenly think that our test data is unseen, but in fact the test data have already been seen by the model building process during feature selection, in particular here: How badly off can we be in reality? Well, again it is not difficult to see: suppose that, after we have finished with our model and we have deployed it (expecting something similar to 76% accuracy in practice with new unseen data), we get some really new data: where of course there is not any qualitative change, i.e. new trends or anything - these new data are generated by the very same underlying procedure. Suppose also we happen to know the true labels y, generated as above: How will our model perform here, when faced with these really unseen data? Not difficult to check: Well, it's true: we sent our model to the battle, thinking that it was capable of a ~ 76% accuracy, but in reality it performs just as a random guess... So, let's see now the correct procedure (i.e. split first, and select the features based on the training set only): Where the test accuracy 0f 0.528 is close enough to the theoretically predicted one of 0.5 in such a case (i.e. actually random guessing). Kudos to Jacob Schreiber  for providing the simple idea (check all the thread, it contains other useful examples), although in a slightly different context than the one you ask about here (cross-validation):  The conventional answer #1 is correct here; the arguments in the contradicting answer #2 do not actually hold. When having such doubts, it is useful to imagine that you simply do not have any access in any test set during the model fitting process (which includes feature importance); you should treat the test set as literally unseen data (and, since unseen, they could not have been used for feature importance scores). Hastie & Tibshirani have clearly argued long ago about the correct & wrong way to perform such processes; I have summarized the issue in a blog post, How NOT to perform feature selection! - and although the discussion is about cross-validation, it can be easily seen that the arguments hold for the case of train/test split, too. The only argument that actually holds in your contradicting answer #2 is that the overall historical data is not analyzed Nevertheless, this is the necessary price to pay in order to have an independent test set for performance assessment, otherwise, with the same logic, we should use the test set for training, too, shouldn't we? Wrap up: the test set is there solely for performance assessment of your model, and it should not be used in any stage of model building, including feature selection.  UPDATE (after comments): the trends in the Test Set may be different A standard (but often implicit) assumption here is that the training & test sets are qualitatively similar; it is exactly due to this assumption that we feel OK to just use simple random splits to get them. If we have reasons to believe that our data change in significant ways (not only between train & test, but during model deployment, too), the whole rationale breaks down, and completely different approaches are required. Also, on doing so, there can be a high probability of Over-fitting The only certain way of overfitting is to use the test set in any way during the pipeline (including for feature selection, as you suggest). Arguably, the linked blog post has enough arguments (including quotes & links) to be convincing. Classic example, the testimony in The Dangers of Overfitting or How to Drop 50 spots in 1 minute: as the competition went on, I began to use much more feature selection and preprocessing. However, I made the classic mistake in my cross-validation method by not including this in the cross-validation folds (for more on this mistake, see this short description or section 7.10.2 in The Elements of Statistical Learning). This lead to increasingly optimistic cross-validation estimates. As I have already said, although the discussion here is about cross-validation, it should not be difficult to convince yourself that it perfectly applies to the train/test case, too. feature selection should be done in such a way that Model Performance is enhanced Well, nobody can argue with this, of course! The catch is - which exact performance are we talking about? Because the Kaggler quoted above was indeed getting better "performance" as he was going along (applying a mistaken procedure), until his model was faced with real unseen data (the moment of truth!), and it unsurprisingly flopped. Admittedly, this is not trivial stuff, and it may take some time until you internalize them (it's no coincidence that, as Hastie & Tibshirani demonstrate, there are even research papers where the procedure is performed wrongly). Until then, my advice to keep you safe, is: during all stages of model building (including feature selection), pretend that you don't have access to the test set at all, and that it becomes available only when you need to assess the performance of your final model.I am getting a error when I try to use confusion matrix. I am doing my first deep learning project. I am new to it. I am using the mnist dataset provided by keras. I have trained and tested my model successfully.  However, when I try to use the scikit learn confusion matrix I get the error stated above. I have searched for an answer and while there are answers on this error, none of them worked for me. From what I found online it probably has something to do with the loss function (I use the categorical_crossentropy in my code). I tried changing it to sparse_categorical_crossentropy but that just gave me the  when I run the fit() function on the model.  This is the code. (I have left out the imports for the sake of brevity) How can i fix this?  Confusion matrix needs both labels & predictions as single-digits, not as one-hot encoded vectors; although you have done this with your predictions using model.predict_classes(), i.e. your test_labels are still one-hot encoded: So, you should convert them too to single-digit ones, as follows: After which, the confusion matrix should come up OK: The same problem is repeated here, and the solution is overall the same. That's why, that question is closed and unable to receive an answer. So I like to add an answer to this question here (hope that's not illegal). The below code is self-explanatory. @desertnaut gave exact reasons, so no need to explain more stuff. The author of the question tried to pass predicted features separately to the fit functions, which I believe can give a better understanding to the newcomer. Extract features from pre-trained weights (Transfer Learning). Reshape for further training process. The model with sequential API. Compile and Run. Evaluate.I am trying to approximate the sine() function using a neural network I wrote myself. I have tested my neural network on a simple OCR problem already and it worked, but I am having trouble applying it to approximate sine(). My problem is that during training my error converges on exactly 50%, so I'm guessing it's completely random. I am using one input neuron for the input (0 to PI), and one output neuron for the result. I have a single hidden layer in which I can vary the number of neurons but I'm currently trying around 6-10. I have a feeling the problem is because I am using the sigmoid transfer function (which is a requirement in my application) which only outputs between 0 and 1, while the output for sine() is between -1 and 1. To try to correct this I tried multiplying the output by 2 and then subtracting 1, but this didn't fix the problem. I'm thinking I have to do some kind of conversion somewhere to make this work. Any ideas? Use a linear output unit. Here is a simple example using R:  When you train the network, you should normalize the target (the sin function) to the range [0,1], then you can keep the sigmoid transfer function. Note that that we mapped the target before training. Once you train and simulate the network, you can map back the output of the net. The following is a MATLAB code to illustrate: 
 There is no reason your network shouldn't work, although 6 is definitely on the low side for approximating a sine wave. I'd try at least 10 maybe even 20. If that doesn't work then I think you need to give more detail about your system. i.e. the learning algorithm (back-propagation?), the learning rate etc. I get the same behavior if use vanilla gradient descent. Try using a different training algorithm. As far as the Java applet is concerned, I did notice something interesting: it does converge if I use a "bipolar sigmoid" and I start with some non-random weights (such as results from a previous training using a Quadratic function).I need a somehow descriptive example showing how to do a 10-fold SVM classification on a two class set of data. there is just one example in the MATLAB documentation but it is not with 10-fold. Can someone help me? Here's a complete example, using the following functions from the Bioinformatics Toolbox: SVMTRAIN, SVMCLASSIFY, CLASSPERF, CROSSVALIND. with the output: we obtained 99.33% accuracy with only one 'setosa' instance mis-classified as 'non-setosa' UPDATE: SVM functions have moved to Statistics toolbox in R2013aI have two numpy arrays light_points and time_points and would like to use some time series analysis methods on those data. I then tried this : This works but is not doing the correct thing.
Indeed, the measurements are not evenly time-spaced and if I just declare the time_points pandas DataFrame as the index of my frame, I get an error : I don't know how to correct this.
Also, it seems that pandas' TimeSeries are deprecated. I tried this : But it gives me a length mismatch : Nevertheless, I don't understand where it comes from, as rdf['light'] and 
tdf['time'] are of same length... Eventually, I tried by defining my rdf as a pandas Series : And I get this : Then, I tried instead replacing the index by  And it gives me an error on the seasonal_decompose method line : How can I work with unevenly spaced data ?
I was thinking about creating an approximately evenly spaced time array by adding many unknown values between the existing values and using interpolation to "evaluate" those points, but I think there could be a cleaner and easier solution. seasonal_decompose() requires a freq that is either provided as part of the DateTimeIndex meta information, can be inferred by pandas.Index.inferred_freq or else by the user as an integer that gives the number of periods per cycle. e.g., 12 for monthly (from docstring for seasonal_mean): To illustrate - using random sample data: So far, so good - now randomly dropping elements from the DateTimeIndex to create unevenly space data: Running the seasonal_decomp on this data 'works': The question is - how useful is the result. Even without gaps in the data that complicate inference of seasonal patterns (see example use of .interpolate() in the release notes, statsmodels qualifies this procedure as follows:Could you give an example of classification of 4 classes using Support Vector Machines (SVM) in matlab something like: SVMs were originally designed for binary classification. They have then been extended to handle multi-class problems. The idea is to decompose the problem into many binary-class problems and then combine them to obtain the prediction. One approach called one-against-all, builds as many binary classifiers as there are classes, each trained to separate one class from the rest. To predict a new instance, we choose the classifier with the largest decision function value. Another approach called one-against-one (which I believe is used in LibSVM), builds k(k-1)/2 binary classifiers, trained to separate each pair of classes against each other, and uses a majority voting scheme (max-win strategy) to determine the output prediction. There are also other approaches such as using Error Correcting Output Code (ECOC) to build many somewhat-redundant binary-classifiers, and use this redundancy to obtain more robust classifications (uses the same idea as Hamming codes). Example (one-against-one): Here is a sample output: MATLAB does not support multiclass SVM at the moment. You could use svmtrain (2-classes) to achieve this, but it would be much easier to use a standard SVM package. I have used LIBSVM and can confirm that it's very easy to use.I'm working with libsvm and I must implement the classification for multiclasses with one versus all.  How can I do it?
Does libsvm version 2011 use this? I think that my question is not very clear.
if libsvm don't use automatically one versus all,I will use one svm for every class, else how can i defined this parameters in the svmtrain function.
I had read README of libsvm.  According to the official libsvm documentation (Section 7): LIBSVM implements the "one-against-one" approach for multi-class
  classification. If k is the number of classes, then k(k-1)/2
  classifiers are constructed and each one trains data from two
  classes. In classification we use a voting strategy: each binary
  classification is considered to be a voting where votes can be cast
  for all data points x - in the end a point is designated to be in a
  class with the maximum number of votes. In the one-against-all approach, we build as many binary classifiers as there are classes, each trained to separate one class from the rest. To predict a new instance, we choose the classifier with the largest decision function value. As I mentioned before, the idea is to train k SVM models each one separating one class from the rest. Once we have those binary classifiers, we use the probability outputs (the -b 1 option) to predict new instances by picking the class with the highest probability. Consider the following example: Here is my implementation for the one-against-all approach for multi-class SVM:I'm new in the world of Tensorflow and I'm working on the simple example of mnist dataset classification. I would like to know how can I obtain other metrics (e.g precision, recall etc) in addition to accuracy and loss (and possibly to show them). Here's my code: Since I get only accuracy and loss, how can i get other metrics?
Thank you in advance, I'm sorry if it is a simple question or If was already answered somewhere. I am adding another answer because this is the cleanest way in order to compute these metrics correctly on your test set (as of 22nd of March 2020). The first thing you need to do is to create a custom callback, in which you send your test data: Starting from TensorFlow 2.X, precision and recall are both available as built-in metrics. Therefore, you do not need to implement them by hand. In addition to this, they were removed before in Keras 2.X versions because they were misleading --- as they were being computed in a batch-wise manner, the global(true) values of precision and recall would be actually different. You can have a look here : https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall Now they have a built-in accumulator, which ensures the correct calculation of those metrics. There is a list of available metrics in the Keras documentation. It includes recall, precision, etc.  For instance, recall: I could not get Timbus' answer to work and I found a very interesting explanation here. It says:
The meaning of 'accuracy' depends on the loss function. The one that corresponds to sparse_categorical_crossentropy is tf.keras.metrics.SparseCategoricalAccuracy(), not tf.metrics.Accuracy().
Which makes a lot of sense. So what metrics you can use depend on the loss you chose. E.g. using the metric 'TruePositives' won't work in the case of SparseCategoricalAccuracy, because that loss means you're working with more than 1 class, which in turn means True Positives cannot be defined because it is only used in binary classification problems. A loss like tf.keras.metrics.CategoricalCrossentropy() will work because it is designed with multiple classes in mind! Example: In my case the other 2 answers gave me shape mismatches. For a list of supported metrics, see: tf.keras MetricsI have a provided standardize function for a machine learning course that wasn't well documented and I'm still new to MATLAB so I'm just trying to break down the function. Any explanation of the syntax or the general idea of standardizing would greatly help. We use this function to standardize a set of training data provided in a large matrix. A break down of most of the lines of the code snippet would help me greatly. Thank you so much.  This code accepts a data matrix of size M x N, where M is the dimensionality of one data sample from this matrix and N is the total number of samples.  Therefore, one column of this matrix is one data sample.  Data samples are all stacked horizontally and are columns.   Now, the true purpose of this code is to take all of the columns of your matrix and standardize / normalize the data so that each data sample exhibits zero mean and unit variance.  This means that after this transform, if you found the mean value of any column in this matrix, it would be 0 and the variance would be 1.  This is a very standard method for normalizing values in statistical analysis, machine learning, and computer vision. This actually comes from the z-score in statistical analysis.  Specifically, the equation for normalization is:  Given a set of data points, we subtract the value in question by the mean of these data points, then divide by the respective standard deviation.  How you'd call this code is the following.  Given this matrix, which we will call X, there are two ways you can call this code: The first method automatically infers the mean of each column of X  and the standard deviation of each column of X.  mean_X and std_X will both return 1 x N vectors that give you the mean and standard deviation of each column in the matrix X.  The second method allows you to manually specify a mean (mu) and standard deviation (sigma) for each column of X.  This is possibly for use in debugging, but you would specify both mu and sigma as 1 x N vectors in this case.  What is returned for mean_X and std_X is identical to mu and sigma. The code is a bit poorly written IMHO, because you can certainly achieve this vectorized, but the gist of the code is that it finds the mean of every column of the matrix X if we are are using Method #1, duplicates this vector so that it becomes a M x N matrix, then we subtract this matrix with X.  This will subtract each column by its respective mean.  We also compute the standard deviation of each column before the mean subtraction.   Once we do that, we then normalize our X by dividing each column by its respective standard deviation.  BTW, doing std_X(:, i) is superfluous as std_X is already a 1 x N vector.  std_X(:, i) means to grab all of the rows at the ith column.  If we already have a 1 x N vector, this can simply be replaced with std_X(i) - a bit overkill for my taste. Method #2 performs the same thing as Method #1, but we provide our own mean and standard deviation for each column of X. For the sake of documentation, this is how I would have commented the code: If I can suggest another way to write this code, I would use the mighty and powerful bsxfun function.  This avoids having to do any duplication of elements and we can do this under the hood.  I would rewrite this function so that it looks like this: I would argue that the new code above is much faster than using for and repmat.  In fact, it is known that bsxfun is faster than the former approach - especially for larger matrices.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I am finding it hard to understand the process of Naive Bayes, and I was wondering if someone could explain it with a simple step by step process in English. I understand it takes comparisons by times occurred as a probability, but I have no idea how the training data is related to the actual dataset. Please give me an explanation of what role the training set plays. I am giving a very simple example for fruits here, like banana for example The accepted answer has many elements of k-NN (k-nearest neighbors), a different algorithm. Both k-NN and NaiveBayes are classification algorithms. Conceptually, k-NN uses the idea of "nearness" to classify new entities. In k-NN 'nearness' is modeled with ideas such as Euclidean Distance or Cosine Distance. By contrast, in NaiveBayes, the concept of 'probability' is used to classify new entities. Since the question is about Naive Bayes, here's how I'd describe the ideas and steps to someone. I'll try to do it with as few equations and in plain English as much as possible. Before someone can understand and appreciate the nuances of Naive Bayes', they need to know a couple of related concepts first, namely, the idea of Conditional Probability, and Bayes' Rule. (If you are familiar with these concepts, skip to the section titled Getting to Naive Bayes') Conditional Probability in plain English: What is the probability that something will happen, given that something else has already happened. Let's say that there is some Outcome O. And some Evidence E. From the way these probabilities are defined: The Probability of having both the Outcome O and Evidence E is:
(Probability of O occurring) multiplied by the (Prob of E given that O happened) One Example to understand Conditional Probability: Let say we have a collection of US Senators. Senators could be Democrats or Republicans. They are also either male or female. If we select one senator completely randomly, what is the probability that this person is a female Democrat? Conditional Probability can help us answer that. Probability of (Democrat and Female Senator)= Prob(Senator is Democrat) multiplied by Conditional Probability of Being Female given that they are a Democrat. We could compute the exact same thing, the reverse way: Conceptually, this is a way to go from P(Evidence| Known Outcome) to P(Outcome|Known Evidence). Often, we know how frequently some particular evidence is observed, given a known outcome. We have to use this known fact to compute the reverse, to compute the chance of that outcome happening, given the evidence. P(Outcome given that we know some Evidence) = P(Evidence given that we know the Outcome) times Prob(Outcome), scaled by the P(Evidence) The classic example to understand Bayes' Rule: Now, all this was just preamble, to get to Naive Bayes. So far, we have talked only about one piece of evidence. In reality, we have to predict an outcome given multiple evidence. In that case, the math gets very complicated. To get around that complication, one approach is to 'uncouple' multiple pieces of evidence, and to treat each of piece of evidence as independent. This approach is why this is called naive Bayes. Many people choose to remember this as: Notice a few things about this equation: Just run the formula above for each possible outcome. Since we are trying to classify, each outcome is called a class and it has a class label. Our job is to look at the evidence, to consider how likely it is to be this class or that class, and assign a label to each entity.
Again, we take a very simple approach: The class that has the highest probability is declared the "winner" and that class label gets assigned to that combination of evidences. Let's try it out on an example to increase our understanding: The OP asked for a 'fruit' identification example. Let's say that we have data on 1000 pieces of fruit. They happen to be Banana, Orange or some Other Fruit.
We know 3 characteristics about each fruit: This is our 'training set.' We will use this to predict the type of any new fruit we encounter. We can pre-compute a lot of things about our fruit collection. The so-called "Prior" probabilities. (If we didn't know any of the fruit attributes, this would be our guess.) These are our base rates. Probability of "Evidence" Probability of "Likelihood" Let's say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit? We can simply run the numbers for each of the 3 outcomes, one by one. Then we choose the highest probability and 'classify' our unknown fruit as belonging to the class that had the highest probability based on our prior evidence (our 1000 fruit training set): By an overwhelming margin (0.252 >> 0.01875), we classify this Sweet/Long/Yellow fruit as likely to be a Banana. Look at what it eventually comes down to. Just some counting and multiplication. We can pre-compute all these terms, and so classifying becomes easy, quick and efficient. Let z = 1 / P(evidence). Now we quickly compute the following three quantities. Assign the class label of whichever is the highest number, and you are done. Despite the name, Naive Bayes turns out to be excellent in certain applications. Text classification is one area where it really shines. Your question as I understand it is divided in two parts, part one being you need a better understanding of the Naive Bayes classifier & part two being the confusion surrounding Training set.  In general all of Machine Learning Algorithms need to be trained for supervised learning tasks like classification, prediction etc. or for unsupervised learning tasks like clustering. During the training step, the algorithms are taught with a particular input dataset (training set) so that later on we may test them for unknown inputs (which they have never seen before) for which they may classify or predict etc (in case of supervised learning) based on their learning. This is what most of the Machine Learning techniques like Neural Networks, SVM, Bayesian etc. are based upon. So in a general Machine Learning project basically you have to divide your input set to a Development Set (Training Set + Dev-Test Set) & a Test Set (or Evaluation set). Remember your basic objective would be that your system learns and classifies new inputs which they have never seen before in either Dev set or test set. The test set typically has the same format as the training set. However, it is very important that the test set be distinct from the training corpus: if we simply
reused the training set as the test set, then a model that simply memorized its input, without learning how to generalize to new examples, would receive misleadingly high scores. In general, for an example, 70% of our data can be used as training set cases. Also remember to partition the original set into the training and test sets randomly. Now I come to your other question about Naive Bayes. To demonstrate the concept of Naïve Bayes Classification, consider the example given below:  As indicated, the objects can be classified as either GREEN or RED. Our task is to classify new cases as they arrive, i.e., decide to which class label they belong, based on the currently existing objects. Since there are twice as many GREEN objects as RED, it is reasonable to believe that a new case (which hasn't been observed yet) is twice as likely to have membership GREEN rather than RED. In the Bayesian analysis, this belief is known as the prior probability. Prior probabilities are based on previous experience, in this case the percentage of GREEN and RED objects, and often used to predict outcomes before they actually happen. Thus, we can write: Prior Probability of GREEN: number of GREEN objects / total number of objects Prior Probability of RED: number of RED objects / total number of objects Since there is a total of 60 objects, 40 of which are GREEN and 20 RED, our prior probabilities for class membership are: Prior Probability for GREEN: 40 / 60 Prior Probability for RED: 20 / 60 Having formulated our prior probability, we are now ready to classify a new object (WHITE circle in the diagram below). Since the objects are well clustered, it is reasonable to assume that the more GREEN (or RED) objects in the vicinity of X, the more likely that the new cases belong to that particular color. To measure this likelihood, we draw a circle around X which encompasses a number (to be chosen a priori) of points irrespective of their class labels. Then we calculate the number of points in the circle belonging to each class label. From this we calculate the likelihood:   From the illustration above, it is clear that Likelihood of X given GREEN is smaller than Likelihood of X given RED, since the circle encompasses 1 GREEN object and 3 RED ones. Thus:   Although the prior probabilities indicate that X may belong to GREEN (given that there are twice as many GREEN compared to RED) the likelihood indicates otherwise; that the class membership of X is RED (given that there are more RED objects in the vicinity of X than GREEN). In the Bayesian analysis, the final classification is produced by combining both sources of information, i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes' rule (named after Rev. Thomas Bayes 1702-1761).  Finally, we classify X as RED since its class membership achieves the largest posterior probability. Naive Bayes comes under supervising machine learning which used to make classifications of data sets.
It is used to predict things based on its prior knowledge and independence assumptions. They call it naive because it’s assumptions (it assumes that all of the features in the dataset are equally important and independent) are really optimistic and rarely true in most real-world applications. It is classification algorithm which makes the decision for the unknown data set. It is based on Bayes Theorem which describe the probability of an event based on its prior knowledge. Below diagram shows how naive Bayes works  Formula to predict NB:  How to use Naive Bayes Algorithm ? Let's take an example of how N.B woks Step 1: First we find out Likelihood of table which shows the probability of yes or no in below diagram.
Step 2: Find the posterior probability of each class.  For more reference refer these blog. Refer GitHub Repository Naive-Bayes-Examples Ram Narasimhan explained the concept very nicely here below is an alternative explanation through the code example of Naive Bayes in action
It uses an example problem from this book on page 351
This is the data set that we will be using 
In the above dataset if we give the hypothesis = {"Age":'<=30', "Income":"medium", "Student":'yes' , "Creadit_Rating":'fair'} then what is the probability that he will buy or will not buy a computer.
The code below exactly answers that question.
Just create a file called named new_dataset.csv and paste the following content. Here is the code the comments explains everything we are doing here! [python] output: I try to explain the Bayes rule with an example. What is the chance that a random person selected from the society is a smoker? You may reply 10%, and let's assume that's right. Now, what if I say that the random person is a man and is 15 years old? You may say 15 or 20%, but why?. In fact, we try to update our initial guess with new pieces of evidence ( P(smoker) vs. P(smoker | evidence) ). The Bayes rule is a way to relate these two probabilities. Each evidence may increase or decrease this chance. For example, this fact that he is a man may increase the chance provided that this percentage (being a man) among non-smokers is lower. In the other words, being a man must be an indicator of being a smoker rather than a non-smoker. Therefore, if an evidence is an indicator of something, it increases the chance. But how do we know that this is an indicator? For each feature, you can compare the commonness (probability) of that feature under the given conditions with its commonness alone. (P(f | x) vs. P(f)). For example, if we know that 90% of smokers are men, it's not still enough to say whether being a man is an indicator of being smoker or not. For example if the probability of being a man in the society is also 90%, then knowing that someone is a man doesn't help us ((90% / 90%) = 1. But if men contribute to 40% of the society, but 90% of the smokers, then knowing that someone is a man increases the chance of being a smoker (90% / 40%) = 2.25, so it increases the initial guess (10%) by 2.25 resulting 22.5%. However, if the probability of being a man was 95% in the society, then regardless of the fact that the percentage of men among smokers is high (90%)! the evidence that someone is a man decreases the chance of him being a smoker! (90% / 95%) = 0.95). So we have: Note that in this formula we assumed that being a man and being under 20 are independent features so we multiplied them, it means that knowing that someone is under 20 has no effect on guessing that he is man or woman. But it may not be true, for example maybe most adolescence  in a society are men... To use this formula in a classifier The classifier is given with some features (being a man and being under 20) and it must decide if he is an smoker or not (these are two classes). It uses the above formula to calculate the probability of each class under the evidence (features), and it assigns the class with the highest probability to the input. To provide the required probabilities (90%, 10%, 80%...) it uses the training set. For example, it counts the people in the training set that are smokers and find they contribute 10% of the sample. Then for smokers checks how many of them are men or women .... how many are above 20 or under 20....In the other words, it tries to build the probability distribution of the features for each class based on the training data.This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. I've been reading some things on neural networks and I understand the general principle of a single layer neural network. I understand the need for aditional layers, but why are nonlinear activation functions used? This question is followed by this one: What is a derivative of the activation function used for in backpropagation? The purpose of the activation function is to introduce non-linearity into the network in turn, this allows you to model a response variable (aka target variable, class label, or score) that varies non-linearly with its explanatory variables non-linear means that the output cannot be reproduced from a linear combination of the inputs (which is not the same as output that renders to a straight line--the word for this is affine). another way to think of it: without a non-linear activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function (see definition just above). A common activation function used in backprop (hyperbolic tangent) evaluated from -2 to 2:  A linear activation function can be used, however on very limited occasions. In fact to understand activation functions better it is important to look at the ordinary least-square or simply the linear regression. A linear regression aims at finding the optimal weights that result in minimal vertical effect between the explanatory and target variables, when combined with the input. In short, if the expected output reflects the linear regression as shown below then linear activation functions can be used: (Top Figure). But  as in the second figure below linear function will not produce the desired results:(Middle figure). However, a non-linear function as shown below would produce the desired results:   Activation functions cannot be linear because neural networks with a linear activation function are effective only one layer deep, regardless of how complex their architecture is. Input to networks is usually linear transformation (input * weight), but real world and problems are non-linear. To make the incoming data nonlinear, we use nonlinear mapping called activation function. An activation function is a decision making function that determines the presence of a particular neural feature. It is mapped between 0 and 1, where zero means absence of the feature, while one means its presence. Unfortunately, the small changes occurring in the weights cannot be reflected in the activation values because it can only take either 0 or 1. Therefore, nonlinear functions must be continuous and differentiable between this range.
A neural network must be able to take any input from -infinity to +infinite, but it should be able to map it to an output that ranges between {0,1} or between {-1,1} in some cases - thus the need for activation function. Non-linearity is needed in activation functions because its aim in a neural network is to produce a nonlinear decision boundary via non-linear combinations of the weight and inputs. If we only allow linear activation functions in a neural network, the output will just be a linear transformation of the input, which is not enough to form a universal function approximator. Such a network can just be represented as a matrix multiplication, and you would not be able to obtain very interesting behaviors from such a network. The same thing goes for the case where all neurons have affine activation functions (i.e. an activation function on the form f(x) = a*x + c, where a and c are constants, which is a generalization of linear activation functions), which will just result in an affine transformation from input to output, which is not very exciting either. A neural network may very well contain neurons with linear activation functions, such as in the output layer, but these require the company of neurons with a non-linear activation function in other parts of the network. Note: An interesting exception is DeepMind's synthetic gradients, for which they use a small neural network to predict the gradient in the backpropagation pass given the activation values, and they find that they can get away with using a neural network with no hidden layers and with only linear activations. A feed-forward neural network with linear activation and any number of hidden layers is equivalent to just a linear neural neural network with no hidden layer. For example lets consider the neural network in figure with two hidden layers and no activation
 We can do the last step because combination of several linear transformation can be replaced with one transformation and combination of several bias term is just a single bias. The outcome is same even if we add some linear activation. So we could replace this neural net with a single layer neural net.This can be extended to n layers. This indicates adding layers doesn't increase the approximation power of a linear neural net at all. We need non-linear activation functions to approximate non-linear functions and most real world problems are highly complex and non-linear. In fact when the activation function is non-linear, then a two-layer neural network with sufficiently large number of hidden units can be proven to be a universal function approximator. Several good answers are here. It will be good to point out the book "Pattern Recognition and Machine Learning" by Christopher M. Bishop. It is a book worth referring to for getting a deeper insight about several ML related concepts. Excerpt from page 229 (section 5.1): If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units. This follows from the fact that the composition of successive linear transformations is itself a linear transformation. However, if the number of hidden units is smaller than either the number of input or output units, then the transformations that the network can generate are not the most general possible linear transformations from inputs to outputs because information is lost in the dimensionality reduction at the hidden units. In Section 12.4.2, we show that networks of linear units give rise to principal component analysis. In general, however, there is little interest in multilayer networks of linear units. "The present paper makes use of the Stone-Weierstrass Theorem and the cosine squasher of Gallant and White to establish that standard multilayer feedforward network architectures using abritrary squashing functions can approximate virtually any function of interest to any desired degree of accuracy, provided sufficently many hidden units are available." (Hornik et al., 1989, Neural Networks) A squashing function is for example a nonlinear activation function that maps to [0,1] like the sigmoid activation function. There are times when a purely linear network can give useful results. Say we have a network of three layers with shapes (3,2,3). By limiting the middle layer to only two dimensions, we get a result that is the "plane of best fit" in the original three dimensional space.  But there are easier ways to find linear transformations of this form, such as NMF, PCA etc. However, this is a case where a multi-layered network does NOT behave the same way as a single layer perceptron. Neural Networks are used in pattern recognition. And pattern finding is a very non-linear technique. Suppose for the sake of argument we use a linear activation function y=wX+b for every single neuron and set something like if y>0 -> class 1 else class 0. Now we can compute our loss using square error loss and back propagate it so that the model learns well, correct? WRONG. For the last hidden layer, the updated value will be w{l} = w{l} - (alpha)*X. For the second last hidden layer, the updated value will be w{l-1} = w{l-1} - (alpha)*w{l}*X. For the ith last hidden layer, the updated value will be w{i} = w{i} - (alpha)*w{l}...*w{i+1}*X. This results in us multiplying all the weight matrices together hence resulting in the possibilities:
A)w{i} barely changes due to vanishing gradient
B)w{i} changes dramatically and inaccurately due to exploding gradient
C)w{i} changes well enough to give us a good fit score In case C happens that means that our classification/prediction problem was most probably a simple linear/logistic regressor based one and never required a neural network in the first place! No matter how robust or well hyper tuned your NN is, if you use a linear activation function, you will never be able to tackle non-linear requiring pattern recognition problems As I remember - sigmoid functions are used because their derivative that fits in BP algorithm is easy to calculate, something simple like f(x)(1-f(x)). I don't remember exactly the math. Actually any function with derivatives can be used.   To understand the logic behind non-linear activation functions first you should understand why activation functions are used. In general, real world problems requires non-linear solutions which are not trivial. So we need some functions to generate the non-linearity. Basically what an activation function does is to generate this non-linearity while mapping input values into a desired range.  However, linear activation functions could be used in very limited set of cases where you do not need hidden layers such as linear regression. Usually, it is pointless to generate a neural network for this kind of problems because independent from number of hidden layers, this network will generate a linear combination of inputs which can be done in just one step. In other words, it behaves like a single layer.  There are also a few more desirable properties for activation functions such as continuous differentiability. Since we are using backpropagation the function we generate must be differentiable at any point. I strongly advise you to check the wikipedia page for activation functions from here to have a better understanding of the topic. It is important to use the nonlinear activation function in neural networks, especially in deep NNs and backpropagation. According to the question posed in the topic, first I will say the reason for the need to use the nonlinear activation function for the backpropagation. Simply put: if a linear activation function is used, the derivative of the cost function is a constant with respect to (w.r.t) input, so the value of input (to neurons) does not affect the updating of weights. This means that we can not figure out which weights are most effective in creating a good result and therefore we are forced to change all weights equally. Deeper: In general, weights are updated as follows: This means that the new weight is equal to the old weight minus the derivative of the cost function. If the activation function is a linear function, then its derivative w.r.t input is a constant, and the input values ​​have no direct effect on the weight update. For example, we intend to update the weights of last layer neurons using backpropagation. We need to calculate the gradient of the weight function w.r.t weight. With chain rule we have:  h and y are (estimated) neuron output and actual output value, respectively. And x is the input of neurons. grad (f) is derived from the input w.r.t activation function. The value calculated above (by a factor) is subtracted from the current weight and a new weight is obtained. We can now compare these two types of activation functions more clearly. 1- If the activating function is a linear function, such as:
F(x) = 2 * x then:  the new weight will be:  As you can see, all the weights are updated equally and it does not matter what the input value is!! 2- But if we use a non-linear activation function like Tanh(x) then:  and:  and now we can see the direct effect of input in updating weights! different input value makes different weights changes. I think the above is enough to answer the question of the topic but it is useful to mention other benefits of using the non-linear activation function. As mentioned in other answers, non-linearity enables NNs to have more hidden layers and deeper NNs. A sequence of layers with a linear activator function can be merged as a layer (with a combination of previous functions) and is practically a neural network with a hidden layer, which does not take advantage of the benefits of deep NN. Non-linear activation function can also produce a normalized output. A layered NN of several neurons can be used to learn linearly inseparable problems. For example XOR function can be obtained with two layers with step activation function. It's not at all a requirement.  In fact, the rectified linear activation function is very useful in large neural networks. Computing the gradient is much faster, and it induces sparsity by setting a minimum bound at 0. See the following for more details: https://www.academia.edu/7826776/Mathematical_Intuition_for_Performance_of_Rectified_Linear_Unit_in_Deep_Neural_Networks Edit: There has been some discussion over whether the rectified linear activation function can be called a linear function.  Yes, it is technically a nonlinear function because it is not linear at the point x=0, however, it is still correct to say that it is linear at all other points, so I don't think it's that useful to nitpick here,  I could have chosen the identity function and it would still be true, but I chose ReLU as an example because of its recent popularity.I can't figure out how the sklearn.pipeline.Pipeline works exactly. There are a few explanation in the doc. For example what do they mean by: Pipeline of transforms with a final estimator. To make my question clearer, what are steps? How do they work? Edit Thanks to the answers I can make my question clearer: When I call pipeline and pass, as steps, two transformers and one estimator, e.g: What happens when I call this? I can't figure out how an estimator can be a transformer and how a transformer can be fitted. Transformer in scikit-learn - some class that have fit and transform method, or fit_transform method. Predictor - some class that has fit and predict methods, or fit_predict method. Pipeline is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator. Here is a good example of Pipeline usage.
Pipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside, and now you can do something like: With just: With pipelines you can easily perform a grid-search over set of parameters for each step of this meta-estimator. As described in the link above. All steps except last one must be transforms, last step can be transformer or predictor.
Answer to edit:
When you call pipln.fit() - each transformer inside pipeline will be fitted on outputs of previous transformer (First transformer is learned on raw dataset).  Last estimator may be transformer or predictor, you can call fit_transform() on pipeline only if your last estimator is transformer (that implements fit_transform, or transform and fit methods separately), you can call fit_predict() or predict() on pipeline only if your last estimator is predictor. So you just can't call fit_transform or transform on pipeline, last step of which is predictor. I think that M0rkHaV has the right idea. Scikit-learn's pipeline class is a useful tool for encapsulating multiple different transformers alongside an estimator into one object, so that you only have to call your important methods once (fit(), predict(), etc). Let's break down the two major components: Transformers are classes that implement both fit() and transform(). You might be familiar with some of the sklearn preprocessing tools, like TfidfVectorizer and Binarizer. If you look at the docs for these preprocessing tools, you'll see that they implement both of these methods. What I find pretty cool is that some estimators can also be used as transformation steps, e.g. LinearSVC! Estimators are classes that implement both fit() and predict(). You'll find that many of the classifiers and regression models implement both these methods, and as such you can readily test many different models. It is possible to use another transformer as the final estimator (i.e., it doesn't necessarily implement predict(), but definitely implements fit()). All this means is that you wouldn't be able to call predict(). As for your edit: let's go through a text-based example. Using LabelBinarizer, we want to turn a list of labels into a list of binary values.  Now, when the binarizer is fitted on some data, it will have a structure called classes_ that contains the unique classes that the transformer 'knows' about. Without calling fit() the binarizer has no idea what the data looks like, so calling transform() wouldn't make any sense. This is true if you print out the list of classes before trying to fit the data. I get the following error when trying this: But when you fit the binarizer on the vec list: and try again  I get the following: And now, after calling transform on the vec object, we get the following: As for estimators being used as transformers, let us use the DecisionTree classifier as an example of a feature-extractor. Decision Trees are great for a lot of reasons, but for our purposes, what's important is that they have the ability to rank features that the tree found useful for predicting. When you call transform() on a Decision Tree, it will take your input data and find what it thinks are the most important features. So you can think of it transforming your data matrix (n rows by m columns) into a smaller matrix (n rows by k columns), where the k columns are the k most important features that the Decision Tree found. ML algorithms typically process tabular data. You may want to do preprocessing and post-processing of this data before and after your ML algorithm. A pipeline is a way to chain those data processing steps. A pipeline is a series of steps in which data is transformed. It comes from the old "pipe and filter" design pattern (for instance, you could think of unix bash commands with pipes “|” or redirect operators “>”). However, pipelines are objects in the code. Thus, you may have a class for each filter (a.k.a. each pipeline step), and then another class to combine those steps into the final pipeline. Some pipelines may combine other pipelines in series or in parallel, have multiple inputs or outputs, and so on. We like to view Pipelining Machine Learning as: Pipelines (or steps in the pipeline) must have those two methods: It's also possible to call this method to chain both: Scikit-Learn’s “pipe and filter” design pattern is simply beautiful. But how to use it for Deep Learning, AutoML, and complex production-level pipelines? Scikit-Learn had its first release in 2007, which was a pre deep learning era. However, it’s one of the most known and adopted machine learning library, and is still growing. On top of all, it uses the Pipe and Filter design pattern as a software architectural style - it’s what makes Scikit-Learn so fabulous, added to the fact it provides algorithms ready for use. However, it has massive issues when it comes to do the following, which we should be able to do in 2020 already: For sure, Scikit-Learn is very convenient and well-built. However, it needs a refresh. Here are our solutions with Neuraxle to make Scikit-Learn fresh and useable within modern computing projects! Note: if a step of a pipeline doesn’t need to have one of the fit or transform methods, it could inherit from NonFittableMixin or NonTransformableMixin to be provided a default implementation of one of those methods to do nothing. As a starter, it is possible for pipelines or their steps to also optionally define those methods: The following methods are provided by default to allow for managing hyperparameters: For more info on our suggested solutions, read the entries in the big list with links above.My problem: I have a dataset which is a large JSON file. I read it and store it in the trainList variable. Next, I pre-process it - in order to be able to work with it. Once I have done that I start the classification: Finally, I would use this to put in HTML in order to show a chart with the TPs of each label. Code: The variables I have for the moment: Most part of the method: For the multi-class case, everything you need can be found from the confusion matrix. For example, if your confusion matrix looks like this:  Then what you're looking for, per class, can be found like this:  Using pandas/numpy, you can do this for all classes at once like so: If you have two lists that have the predicted and actual values; as it appears you do, you can pass them to a function that will calculate TP, FP, TN, FN with something like this: From here I think you will be able to calculate rates of interest to you, and other performance measure like specificity and sensitivity. According to scikit-learn documentation, http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix By definition a confusion matrix C is such that C[i, j] is equal to the number of observations known to be in group i but predicted to be in group j. Thus in binary classification, the count of true negatives is C[0,0], false negatives is C[1,0], true positives is C[1,1] and false positives is C[0,1]. You can obtain all of the parameters from the confusion matrix.
The structure of the confusion matrix(which is 2X2 matrix) is as follows (assuming the first index is related to the positive label, and the rows are related to the true labels): So  More details at https://en.wikipedia.org/wiki/Confusion_matrix The one liner to get true postives etc. out of the confusion matrix is to ravel it: One should set the labels parameter in case the data contains only a single case, e.g. only true positives. Setting labels correctly ensures that the confusion matrix has a 2x2 shape. In the scikit-learn 'metrics' library there is a confusion_matrix method which gives you the desired output. You can use any classifier that you want. Here I used the KNeighbors as example. The docs: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix I wrote a version that works using only numpy.
I hope it helps you. Just in case some is looking for the same in MULTI-CLASS Example you can try sklearn.metrics.classification_report as below: output: In scikit version 0.22, you can do it  like this this works fine Source - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html if you have more than one classes in your classifier, you might want to use pandas-ml at that part. Confusion Matrix of pandas-ml give more detailed information. check that  I think both of the answers are not fully correct. For example, suppose that we have the following arrays;
y_actual = [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0] y_predic = [1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0] If we compute the FP, FN, TP and TN values manually, they should be as follows: FP: 3
FN: 1
TP: 3
TN: 4 However, if we use the first answer, results are given as follows: FP: 1
FN: 3
TP: 3
TN: 4 They are not correct, because in the first answer, False Positive should be where actual is 0, but the predicted is 1, not the opposite. It is also same for False Negative. And, if we use the second answer, the results are computed as follows: FP: 3
FN: 1
TP: 4
TN: 3 True Positive and True Negative numbers are not correct, they should be opposite. Am I correct with my computations? Please let me know if I am missing something. #FalseNegatives None of the answers given so far worked for me as I sometimes ended up having a confusion matrix with a single entry only. The following code is able to mitigate this issue: Please note that "y" is the groundtruth and "y_hat" is the prediction. Although it does not relate to scikit-learn, what you could also do is I have tried some of the answers and found them not working. This works for me:  Here's a fix to invoketheshell's buggy code (which currently appears as the accepted answer):OpenAI's REINFORCE and actor-critic example for reinforcement learning has the following code: REINFORCE: actor-critic: One is using torch.cat, the other uses torch.stack, for similar use cases. As far as my understanding goes, the doc doesn't give any clear distinction between them. I would be happy to know the differences between the functions. stack Concatenates sequence of tensors along a new dimension. cat Concatenates the given sequence of seq tensors in the given dimension. So if A and B are of shape (3, 4): These functions are analogous to numpy.stack and numpy.concatenate. The original answer lacks a good example that is self-contained so here it goes: output: for reference here are the definitions: cat: Concatenates the given sequence of seq tensors in the given dimension. The consequence is that a specific dimension changes size e.g. dim=0 then you are adding elements to the row which increases the dimensionality of the column space. stack: Concatenates sequence of tensors along a new dimension. I like to think of this as the torch "append" operation since you can index/get your original tensor by "poping it" from the front. With no arguments, it appends tensors to the front of the tensor. Related: here is a few unit tests (I didn't write more tests but it worked with my real code so I trust it's fine. Feel free to help me by adding more tests if you want): If someone is looking into the performance aspects of this, I've done a small experiment. In my case, I needed to convert a list of scalar tensors into a single tensor. My conclusion is that even if you want to have the additional dimension of torch.stack, using torch.cat and then reshape is better. Note: this post is taken from the PyTorch forum (I am the author of the original post)I have an assignment to make an AI Agent that will learn to play a video game using ML. I want to create a new environment using OpenAI Gym because I don't want to use an existing environment. How can I create a new, custom Environment? Also, is there any other way I can start to develop making AI Agent to play a specific video game without the help of OpenAI Gym? See my banana-gym for an extremely small environment. See the main page of the repository: https://github.com/openai/gym/blob/master/docs/creating_environments.md The steps are: It should look like this For the contents of it, follow the link above. Details which are not mentioned there are especially how some functions in foo_env.py should look like. Looking at examples and at gym.openai.com/docs/ helps. Here is an example: Its definitely possible. They say so in the Documentation page, close to the end. https://gym.openai.com/docs As to how to do it, you should look at the source code of the existing environments for inspiration. Its available in github: https://github.com/openai/gym#installation Most of their environments they did not implement from scratch, but rather created a wrapper around existing environments and gave it all an interface that is convenient for reinforcement learning. If you want to make your own, you should probably go in this direction and try to adapt something that already exists to the gym interface. Although there is a good chance that this is very time consuming. There is another option that may be interesting for your purpose. It's OpenAI's Universe https://universe.openai.com/ It can integrate with websites so that you train your models on kongregate games, for example. But Universe is not as easy to use as Gym. If you are a beginner, my recommendation is that you start with a vanilla implementation on a standard environment. After you get passed the problems with the basics, go on to increment...This question does not appear to be about programming within the scope defined in the help center. Closed 1 year ago. The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved How do I calculate the output size in a convolution layer? For example, I have a 2D convolution layer that takes a 3x128x128 input and has 40 filters of size 5x5. you can use this formula [(W−K+2P)/S]+1. So, we input into the formula: NOTE: Stride defaults to 1 if not provided and the 40 in (124, 124, 40) is the number of filters provided by the user. You can find it in two ways:
simple method: input_size - (filter_size - 1) But the second method is the standard to find the output size. Let me start simple; since you have square matrices for both input and filter let me get one dimension. Then you can apply the same for other dimension(s). Imagine your are building fences between trees, if there are N trees, you have to build N-1 fences. Now apply that analogy to convolution layers. Your output size will be: input size - filter size + 1 Because your filter can only have n-1 steps as fences I mentioned. Let's calculate your output with that idea.
128 - 5 + 1 = 124
Same for other dimension too. So now you have a 124 x 124 image. That is for one filter. If you apply this 40 times you will have another dimension: 124 x 124 x 40 Here is a great guide if you want to know more about advanced convolution arithmetic: https://arxiv.org/pdf/1603.07285.pdf Formula : n[i]=(n[i-1]−f[i]+2p[i])/s[i]+1 where, so, n[i]=(128-5+0)/1+1 =124 so the size of the output layer is: 124x124x40
Where '40' is the number of filters (124*124*3)*40 = 1845120 width = 124 height = 124 depth = 3 no. of filters = 40 stride = 1 padding = 0I have a dataset containing grayscale images and I want to train a state-of-the-art CNN on them. I'd very much like to fine-tune a pre-trained model (like the ones here). The problem is that almost all models I can find the weights for have been trained on the ImageNet dataset, which contains RGB images. I can't use one of those models because their input layer expects a batch of shape (batch_size, height, width, 3) or (64, 224, 224, 3) in my case, but my images batches are (64, 224, 224). Is there any way that I can use one of those models? I've thought of dropping the input layer after I've loaded the weights and adding my own (like we do for the top layers). Is this approach correct? The model's architecture cannot be changed because the weights have been trained for a specific input configuration. Replacing the first layer with your own would pretty much render the rest of the weights useless.  -- Edit: elaboration suggested by Prune--
CNNs are built so that as they go deeper, they can extract high-level features derived from the lower-level features that the previous layers extracted. By removing the initial layers of a CNN, you are destroying that hierarchy of features because the subsequent layers won't receive the features that they are supposed to as their input. In your case the second layer has been trained to expect the features of the first layer. By replacing your first layer with random weights, you are essentially throwing away any training that has been done on the subsequent layers, as they would need to be retrained. I doubt that they could retain any of the knowledge learned during the initial training.
--- end edit --- There is an easy way, though, which you can make your model work with grayscale images. You just need to make the image to appear to be RGB. The easiest way to do so is to repeat the image array 3 times on a new dimension. Because you will have the same image over all 3 channels, the performance of the model should be the same as it was on RGB images. In numpy this can be easily done like this: The way this works is that it first creates a new dimension (to place the channels) and then it repeats the existing array 3 times on this new dimension. I'm also pretty sure that keras' ImageDataGenerator can load grayscale images as RGB. Converting grayscale images to RGB as per the currently accepted answer is one approach to this problem, but not the most efficient. You most certainly can modify the weights of the model's first convolutional layer and achieve the stated goal. The modified model will both work out of the box (with reduced accuracy) and be finetunable. Modifying the weights of the first layer does not render the rest of the weights useless as suggested by others. To do this, you'll have to add some code where the pretrained weights are loaded. In your framework of choice, you need to figure out how to grab the weights of the first convolutional layer in your network and modify them before assigning to your 1-channel model. The required modification is to sum the weight tensor over the dimension of the input channels. The way the weights tensor is organized varies from framework to framework. The PyTorch default is [out_channels, in_channels, kernel_height, kernel_width]. In Tensorflow I believe it is [kernel_height, kernel_width, in_channels, out_channels]. Using PyTorch as an example, in a ResNet50 model from Torchvision (https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py), the shape of the weights for conv1 is [64, 3, 7, 7]. Summing over dimension 1 results in a tensor of shape [64, 1, 7, 7]. At the bottom I've included a snippet of code that would work with the ResNet models in Torchvision assuming that an argument (inchans) was added to specify a different number of input channels for the model. To prove this works I did three runs of ImageNet validation on ResNet50 with pretrained weights. There is a slight difference in the numbers for run 2 & 3, but it's minimal and should be irrelevant once finetuned. A simple way to do this is to add a convolution layer before the base model and then feed the output to the base model. Like this: Why not try to convert a grayscale image to a fake "RGB" image? Dropping the input layer will not work out. This will cause that the all following layers will suffer. What you can do is Concatenate 3 black and white images together to expand your color dimension. I faced the same problem while working with VGG16 along with gray-scale images. I solved this problem like follows: Let's say our training images are in train_gray_images, each row containing the unrolled gray scale image intensities. So if we directly pass it to fit function it will create an error as the fit function is expecting a 3 channel (RGB) image data-set instead of gray-scale data set. So before passing to fit function do the following: Create a dummy RGB image data set just like the gray scale data set with the same shape (here dummy_RGB_image). The only difference is here we are using the number of the channel is 3. Therefore just copy the whole data-set 3 times to each of the channels of the "dummy_RGB_images". (Here the dimensions are [no_of_examples, height, width, channel]) Finally pass the dummy_RGB_images instead of the gray scale data-set, like: numpy's depth-stack function, np.dstack((img, img, img)) is a natural way to go. If you're already using scikit-image, you can get the desired result by using gray2RGB. I believe you can use a pretrained resnet with 1 channel gray scale images without repeating 3 times the image. What I have done is to replace the first layer (this is pythorch not keras, but the idea might be similar): With the following layer: And then copy the sum (in the channel axis) of the weights to the new layer, for example, the shape of the original weights was: So I did: And then check that the output of the new model is the same than the output with the gray scale image: input_image_1: one channel image input_image_3: 3 channel image (gray scale - all channels equal) model_resnet_1: modified model model_resnet_3: Original resnet model It's really easy !
example for 'resnet50':
before do it you should have : Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),
bias=False) Just do this ! the final step is to update state_dict. so if run as follow : results would be : Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3,
3), bias=False) As you see input channel is for the grayscale images. what I did is to just simply expand grayscales into RGB images by using the following transform stage: When you add the Resnet to model, you should input the input_shape in Resnet definition like   .As from the title I am wondering what is the difference between StratifiedKFold with the parameter shuffle=True and StratifiedShuffleSplit and what is the advantage of using StratifiedShuffleSplit In stratKFolds, each test set should not overlap, even when shuffle is included. With stratKFolds and shuffle=True, the data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest. In ShuffleSplit, the data is shuffled every time, and then split. This means the test sets may overlap between the splits. See this block for an example of the difference. Note the overlap of the elements in the test sets for ShuffleSplit. Output: As for when to use them, I tend to use stratKFolds for any cross validation, and I use ShuffleSplit with a split of 2 for my train/test set splits. But I'm sure there are other use cases for both. @Ken Syme already has a very good answer. I just want to add something. With  shuffle = True, the data is shuffled by your random_state. Otherwise, 
the data is shuffled by np.random (as default).
For example, with n_splits = 4, and your data has 3 classes (label) for y (dependent variable). 4 test sets cover all the data without any overlap.   So, the difference here is that StratifiedKFold just shuffles and splits once, therefore the test sets do not overlap, while StratifiedShuffleSplit shuffles each time before splitting, and it splits n_splits times, the test sets can overlap.  Output examples of KFold, StratifiedKFold, StratifiedShuffleSplit:
 The above pictorial output is an extension of @Ken Syme's code:I'm using the MinMaxScaler model in sklearn to normalize the features of a model. Now I want to use the same scaler to normalize the test set: But I don't want so use the scaler.fit() with the training data all the time. Is there a way to save the scaler and load it later from a different file?     Update: sklearn.externals.joblib is deprecated. Install and use the pure joblib instead. Please see Engineero's answer below, which is otherwise identical to mine. Even better than pickle (which creates much larger files than this method), you can use sklearn's built-in tool: So I'm actually not an expert with this but from a bit of research and a few helpful links, I think pickle and sklearn.externals.joblib are going to be your friends here. The package pickle lets you save models or "dump" models to a file.  I think this link is also helpful. It talks about creating a persistence model. Something that you're going to want to try is: Here is where you can learn more about the sklearn  externals. Let me know if that doesn't help or I'm not understanding something about your model. Note: sklearn.externals.joblib is deprecated. Install and use the pure joblib instead Just a note that sklearn.externals.joblib has been deprecated and is superseded by plain old joblib, which can be installed with pip install joblib: Note that file extensions can be anything, but if it is one of ['.z', '.gz', '.bz2', '.xz', '.lzma'] then the corresponding compression protocol will be used. Docs for joblib.dump() and joblib.load() methods. You can use pickle, to save the scaler: Load it back:I have a few thousand audio files and I want to classify them using Keras and Theano. So far, I generated a 28x28 spectrograms (bigger is probably better, but I am just trying to get the algorithm work at this point) of each audio file and read the image into a matrix. So in the end I get this big image matrix to feed into the network for image classification. In a tutorial I found this mnist classification code: This code runs, and I get the result as expected: Up to this point everything runs perfectly, however when I apply the above algorithm to my dataset, accuracy gets stuck. My code is as follows: AudioProcessing.py ImageTools.py So I run the above code and recieve: I tried changing the network, adding more epochs, but I always get the same result no matter what. I don't understand why I am getting the same result. Any help would be appreciated. Thank you. Edit:
I found a mistake where pixel values were not read correctly. I fixed the ImageTools.py below as: Now I actually get grayscale pixel values from 0 to 255, so now my dividing it by 255 makes sense. However, I still get the same result. The most likely reason is that the optimizer is not suited to your dataset. Here is a list of Keras optimizers from the documentation. I recommend you first try SGD with default parameter values. If it still doesn't work, divide the learning rate by 10. Do that a few times if necessary. If your learning rate reaches 1e-6 and it still doesn't work, then you have another problem. In summary, replace this line: with this: and change the learning rate a few times if it doesn't work. If it was the problem, you should see the loss getting lower after just a few epochs. Another solution that I do not see mentioned here, but caused a similar problem for me was the activiation function of the last neuron, especialy if it is relu and not something non linear like sigmoid. In other words, it might help you to use a non-linear activation function in the last layer Last layer: Output: Now I used a non linear activation function: Output: This is not directly a solution to the original answer, but as the answer is #1 on Google when searching for this problem, it might benefit someone. If the accuracy is not changing, it means the optimizer has found a local minimum for the loss. This may be an undesirable minimum. One common local minimum is to  always predict the class with the most number of data points. You should use weighting on the classes to avoid this minimum. After some examination, I found that the issue was the data itself. It was very dirty as in same input had 2 different outputs, hence creating confusion. After clearing up the data now my accuracy goes up to %69. Still not enough to be good, but at least I can now work my way up from here now that the data is clear. I used the below code to test: Check out this one Check out the  documentation  I had better results with MNIST  By mistake I had added a softmax at the end instead of sigmoid. Try doing the latter. It worked as expected when I did this. For one output layer, softmax always gives values of 1 and this is what had happened. I faced a similar issue. One-hot encoding the target variable using nputils in Keras, solved the issue of accuracy and validation loss being stuck. Using weights for balancing the target classes further improved performance. Solution : I've the same problem as you 
my solution was a loop instead of epochs    I got 13% Accuracy increment using this 'sigmoid' activation    Or you can also test the following, where 'relu' in first and hidden layer. As mentioned above, the problem mainly arises from the type of optimizers chosen. However, it can also be driven from the fact of topping 2 Dense layers with the same activation functions(softmax, for example).
In this case, NN finds a local minimum and is not able to descent more from that point, rolling around the same acc (val_acc) values.
Hope it helps out. I had similar problem. I had binary class which was labeled by 1 and 2. After testing different kinds of optimizer and activation functions I found that the root of the problem was my labeling to classes. In the other words I changed the labels to 0 and 1 instead of 1 and 2, then this problem solved! I faced same problem for multi-class, Try to changing optimizer by default it is Adam change it to sgd. you can also try different Activation functions eg. (relu, sigmoid, softmax, softplus, etc.) Some imp links Optimizers Activations As pointed out by others, the optimizer probably doesn't suit your data/model which stuck in local minima. A neural network should at least be able to overfit the data (training_acc close to 1).
I once had a similar problem. I solved by trying different optimizers (in my case from SGD to RMSprop) In my case, my problem was binary and I was using the 'softmax' activation function and it doesn't work. I changed to 'sigmoid' it works properly for me. I had the exactly same problem: validation loss and accuracy remaining the same through the epochs.  I increased the batch size 10x times, reduced learning rate by 100x times, etc.  It did not work. My last try, inspired by monolingual's and Ranjab's answers, worked. my solution was to add Batchnormalization AND arrange the order as below: Conv - DropOut - BatchNorm - Activation - Pool. as recommended in Ordering of batch normalization and dropout?. I know this is an old question but as of today (14/06/2021), the comment from @theTechGuy works well on tf 2.3. The code is: I tried playing a lot with the optimizers and activation functions, but the only thing that worked was Batchnormalization1. And I guess it is a good practice too.
You can import it as: and simply add it before each hidden layer: I had the same problem, but in my case, it was caused by a non-regularized column on my data. This column had huge value. Fixing that solved it for me. So, I just converted it to values around 0 and 1. I had the same problem. My solution was to change the last layer activation function from "softmax" to "sigmoid" since i was dealing with a binary classification problem.I am attempting to apply k-means on a set of high-dimensional data points (about 50 dimensions) and was wondering if there are any implementations that find the optimal number of clusters.  I remember reading somewhere that the way an algorithm generally does this is such that the inter-cluster distance is maximized and intra-cluster distance is minimized but I don't remember where I saw that. It would be great if someone can point me to any resources that discuss this. I am using SciPy for k-means currently but any related library would be fine as well. If there are alternate ways of achieving the same or a better algorithm, please let me know. One approach is cross-validation.  In essence, you pick a subset of your data and cluster it into k clusters, and you ask how well it clusters, compared with the rest of the data: Are you assigning data points to the same cluster memberships, or are they falling into different clusters?  If the memberships are roughly the same, the data fit well into k clusters. Otherwise, you try a different k. Also, you could do PCA (principal component analysis) to reduce your 50 dimensions to some more tractable number. If a PCA run suggests that most of your variance is coming from, say, 4 out of the 50 dimensions, then you can pick k on that basis, to explore how the four cluster memberships are assigned. Take a look at this wikipedia page on determining the number of clusters in a data set.  Also you might want to try Agglomerative hierarchical clustering out. This approach does not need to know the number of clusters, it will incrementally form clusters of cluster till only one exists. This technique also exists in SciPy (scipy.cluster.hierarchy).  One interesting approach is that of evidence accumulation by Fred and Jain. This is based on combining multiple runs of k-means with a large number of clusters, aggregating them into an overall solution. Nice aspects of the approach include that the number of clusters is determined in the process and that the final clusters don't have to be spherical. There are visualization that should hint good parameters. For k-means you could visualize several runs with different k using Graphgrams (see the WEKA graphgram package - best obtained by the package manager or here. An introduction and examples can also be found here. You should also make sure that each dimension is in fact independent. Many so called multi-dimensional datasets have multiple representations of the same thing. It is not wrong to have these in your data. It is wrong to use multiple versions of the same thing as support for a cluster argument. http://en.wikipedia.org/wiki/Cronbach's_alpha One way to do it is to run k-means with large k (much larger than what you think is the correct number), say 1000. then, running mean-shift algorithm on the these 1000 point (mean shift uses the whole data but you will only "move" these 1000 points). mean shift will find the amount of clusters then.
Running mean shift without the k-means before is a possibility but it is just too slow usually O(N^2*#steps), so running k-means before will speed things up: O(NK#steps) If the cluster number is unknow, why not use Hierarchical Clustering instead? At the begining, every isolated one is a cluster, then every two cluster will be merged if their distance is lower than a threshold, the algorithm will end when no more merger goes. The Hierarchical clustering algorithm can carry out a suitable "K" for your data.I am new to TensorFlow. While I am reading the existing documentation, I found the term tensor really confusing. Because of it, I need to clarify the following questions: TensorFlow doesn't have first-class Tensor objects, meaning that there are no notion of Tensor in the underlying graph that's executed by the runtime. Instead the graph consists of op nodes connected to each other, representing operations. An operation allocates memory for its outputs, which are available on endpoints :0, :1, etc, and you can think of each of these endpoints as a Tensor. If you have tensor corresponding to nodename:0 you can fetch its value as sess.run(tensor) or sess.run('nodename:0'). Execution granularity happens at operation level, so the run method will execute op which will compute all of the endpoints, not just the :0 endpoint. It's possible to have an Op node with no outputs (like tf.group) in which case there are no tensors associated with it. It is not possible to have tensors without an underlying Op node. You can examine what happens in underlying graph by doing something like this So with tf.constant you get a single operation node, and you can fetch it using sess.run("Const:0") or sess.run(value) Similarly, value=tf.placeholder(tf.int32) creates a regular node with name Placeholder, and you could feed it as feed_dict={"Placeholder:0":2} or feed_dict={value:2}. You can not feed and fetch a placeholder in the same session.run call, but you can see the result by attaching a tf.identity node on top and fetching that. For variable You'll see that it creates two nodes Variable and Variable/read, the :0 endpoint is a valid value to fetch on both of these nodes. However Variable:0 has a special ref type meaning it can be used as an input to mutating operations. The result of Python call tf.Variable is a Python Variable object and there's some Python magic to substitute Variable/read:0 or Variable:0 depending on whether mutation is necessary. Since most ops have only 1 endpoint, :0 is dropped. Another example is Queue -- close() method will create a new Close op node which connects to Queue op. To summarize -- operations on python objects like Variable and Queue map to different underlying TensorFlow op nodes depending on usage.  For ops like tf.split or tf.nn.top_k which create nodes with multiple endpoints, Python's session.run call automatically wraps output in tuple or collections.namedtuple of Tensor objects which can be fetched individually. From the glossary: A Tensor is a typed multi-dimensional array. For example, a 4-D array of floating point numbers representing a mini-batch of images with dimensions [batch, height, width, channel]. Basically, every data is a Tensor in TensorFlow (hence the name): However, in the graph, every node is an operation, which can have Tensors as inputs or outputs. As already mentioned by others, yes they are all tensors. The way I understood those is to first visualize and understand 1D, 2D, 3D, 4D, 5D, and 6D tensors as in the picture below. (source: knoldus)  Now, in the context of TensorFlow, you can imagine a computation graph like the one below,  Here, the Ops take two tensors a and b as input; multiplies the tensors with itself and then adds the result of these multiplications to produce the result tensor t3. And these multiplications and addition Ops happen at the nodes in the computation graph. And these tensors a and b can be constant tensors, Variable tensors, or placeholders. It doesn't matter, as long as they are of the same data type and compatible shapes(or broadcastable to it) to achieve the operations. Data is stored in matrices. A 28x28 pixel grayscale image fits into a
28x28 two-dimensional matrix. But for a color image, we need more
dimensions. There are 3 color values per pixel (Red, Green, Blue), so
a three-dimensional table will be needed with dimensions [28, 28, 3].
And to store a batch of 128 color images, a four-dimensional table is
needed with dimensions [128, 28, 28, 3]. These multi-dimensional tables are called "tensors" and the list of
their dimensions is their "shape". Source TensorFlow's central data type is the tensor. Tensors are the underlying components of computation and a fundamental data structure in TensorFlow. Without using complex mathematical interpretations, we can say a tensor (in TensorFlow) describes a multidimensional numerical array, with zero or n-dimensional collection of data, determined by rank, shape, and type.Read More: What is tensors in TensorFlow?I am new to machine learning and deep learning, and for learning purposes I tried to play with Resnet. I tried to overfit over small data (3 different images) and see if I can get almost 0 loss and 1.0 accuracy - and I did. The problem is that predictions on the training images (i.e. the same 3 images used for training) are not correct.. Training Images  
 Image labels [1,0,0], [0,1,0], [0,0,1] My python code The model does overfit the data: but predictions are: which means that all images got label=[0,1,0] why? and how can that happen? It's because of the batch normalization layers. In training phase, the batch is normalized w.r.t. its mean and variance. However, in testing phase, the batch is normalized w.r.t. the moving average of previously observed mean and variance. Now this is a problem when the number of observed batches is small (e.g., 5 in your example) because in the BatchNormalization layer, by default moving_mean is initialized to be 0 and moving_variance is initialized to be 1. Given also that the default momentum is 0.99, you'll need to update the moving averages quite a lot of times before they converge to the "real" mean and variance. That's why the prediction is wrong in the early stage, but is correct after 1000 epochs. You can verify it by forcing the BatchNormalization layers to operate in "training mode". During training, the accuracy is 1 and the loss is close to zero: Now if we evaluate the model, we'll observe high loss and low accuracy because after 5 updates, the moving averages are still pretty close to the initial values: However, if we manually specify the "learning phase" variable and let the BatchNormalization layers use the "real" batch mean and variance, the result becomes the same as what's observed in fit(). It's also possible to verify it by changing the momentum to a smaller value. For example, by adding momentum=0.01 to all the batch norm layers in ResNet50, the prediction after 20 epochs is: ResNet50V2 (the 2nd version) has the much higher accuracy than ResNet50in predicting a given image such as the classical Egyptian cat. Predicted: [[('n02124075', 'Egyptian_cat', 0.8233388), ('n02123159', 'tiger_cat', 0.103765756), ('n02123045', 'tabby', 0.07267675), ('n03958227', 'plastic_bag', 3.6531426e-05), ('n02127052', 'lynx', 3.647774e-05)]] Comparing with the EfficientNet(90% accuracy), the ResNet50/101/152 predicts quite a bad result(15~50% accuracy) while adopting the given weights provided by Francios Cholett. It is not related to the weights, but related to the inherent complexity of the above model. In other words, it is necessary to re-train the above model to predict an given image. But EfficientNet does not need such the training to predict an image. For instance, while given a classical cat image, it shows the final result as follows. 1. Adoption of the decode_predictions Predicted: [[('n01930112', 'nematode', 0.122968934), ('n03041632', 'cleaver', 0.04236396), ('n03838899', 'oboe', 0.03846453), ('n02783161', 'ballpoint', 0.027445247), ('n04270147', 'spatula', 0.024508419)]] 2. Adoption of the CV2 Predicted: [[('n04065272', 'recreational_vehicle', 0.46529356), ('n01819313', 'sulphur-crested_cockatoo', 0.31684962), ('n04074963', 'remote_control', 0.051597465), ('n02111889', 'Samoyed', 0.040776145), ('n04548362', 'wallet', 0.029898684)]] Therefore, ResNet50/101/152 models are not suitable to predict an image without training even provided with the weights. But users can feel its value after 100~1000 epochs training for prediction because it helps obtain a better moving average. If users want an easy prediction, EfficientNet is a good choice with the given weights. It seems that predicting with a batch of images will not work correctly in Keras. It is better to do prediction for each image individually and then calculate the accuracy manually.
As an example, in the following code, I don't use batch prediction, but use individual image prediction. What happens is basically that keras.fit() i.e your  is while having the best fit the precision is lost. As, the precision is lost the models fit gives problems and varied results.The keras.fit only has a good fit not the required precisionI thought mask_zero=True will output 0's when the input value is 0, so the following layers could skip computation or something. How does mask_zero works?  Example:  The actual output is: (the numbers are random) However, I thought the output will be: Actually, setting mask_zero=True for the Embedding layer does not result in returning a zero vector. Rather, the behavior of the Embedding layer would not change and it would return the embedding vector with index zero. You can confirm this by checking the Embedding layer weights (i.e. in the example you mentioned it would be m.layers[0].get_weights()). Instead, it would affect the behavior of the following layers such as RNN layers.  If you inspect the source code of Embedding layer you would see a method called compute_mask: This output mask will be passed, as the mask argument, to the following layers which support masking. This has been implemented in the __call__ method of base layer, Layer: And this makes the following layers to ignore (i.e. does not consider in their computations) this inputs steps. Here is a minimal example: As you can see the outputs of the LSTM layer for the second and forth timesteps are the same as the output of first and third timesteps, respectively. This means that  those timesteps have been masked. Update: The mask will also be considered when computing the loss since the loss functions are internally augmented to support masking using weighted_masked_objective: when compiling the model: You can verify this using the following example: The process of informing the Model that some part of the Data is actually Padding and should be ignored is called Masking. There are three ways to introduce input masks in Keras models: Given below is the code to introduce Input Masks using keras.layers.Embedding Output of the above code is shown below: For more information, refer this Tensorflow Tutorial.Is it possible to have two fit_generator? I'm creating a model with two inputs,
The model configuration is shown below.  Label Y uses the same labeling for X1 and X2 data. The following error will continue to occur. Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected
  to see 2 array(s), but instead got the following list of 1 arrays:
  [array([[[[0.75686276, 0.75686276, 0.75686276],
           [0.75686276, 0.75686276, 0.75686276],
           [0.75686276, 0.75686276, 0.75686276],
           ...,
           [0.65882355, 0.65882355, 0.65882355... My code looks like this: Try this generator: Generator for 3 inputs: EDIT (add generator, output image and numpy array, and target) I have an implementation for multiple inputs for TimeseriesGenerator that I have adapted it (I have not been able to test it unfortunately) to meet this example with ImageDataGenerator. My approach was to build a wrapper class for the multiple generators from keras.utils.Sequence and then implement the base methods of it: __len__ and __getitem__: You can use this generator with model.fit_generator() once the generator has been instanced.If using a library like scikit-learn, how do I assign more weight on certain features in the input to a classifier like SVM? Is this something people do or not? First of all - you should probably not do it. The whole concept of machine learning is to use statistical analysis to assign optimal weights. You are interfering here with the whole concept, thus you need really strong evidence that this is crucial to the process you are trying to model, and for some reason your model is currently missing it. That being said - there is no general answer. This is purely model specific, some of which will allow you to weight features - in random forest you could bias distribution from which you sample features to analyse towards the ones that you are interested in; in SVM it should be enough to just multiply given feature by a constant - remember when you were told to normalize your features in SVM? This is why - you can use the scale of features to 'steer' your classifier towards given features. The ones with high values will be preffered. This will actually work for most linear weight norm-regularized models (regularized logistic regression, ridge regression, lasso etc.). The best way to do this is:
Assume you have f[1,2,..N] and weight of particular feature is w_f[0.12,0.14...N].
First of all, you need to normalize features by any feature scaling methods and then you need to also normalize the weights of features w_f to [0-1] range and then multiply the normalized weight by f[1,2,..N] with the new transformed features.
Remember you need to transform this in test data as well. Now you can check the performance of both models: without introducing the feature and with introducing the feature. As already mentioned, I wouldn't suggest using index weights as that is the job of ML. However, a ranking of weights in my opinion will have to be done in the original data source (database table, .txt, etc.) by updating an additional field and always with the range from 0 to 1 i.e., 0.1, 0.2. ... 0.7 ...), and certainly always in absolute correlation with the corresponding features(parameters).I'm trying to run a linear regression in PySpark and I want to create a table containing summary statistics such as coefficients, P-values and t-values for each column in my dataset. However, in order to train a linear regression model I had to create a feature vector using Spark's VectorAssembler, and now for each row I have a single feature vector and the target column.
When I try to access Spark's in-built regression summary statistics, they give me a very raw list of numbers for each of these statistics, and there's no way to know which attribute corresponds to which value, which is really difficult to figure out manually with a large number of columns.
How do I map these values back to the column names? For example, I have my current output as something like this: Coefficients: [-187.807832407,-187.058926726,85.1716641376,10595.3352802,-127.258892837,-39.2827730493,-1206.47228704,33.7078197705,99.9956812528] P-Value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18589731365614548, 0.275173571416679, 0.0] t-statistic: [-23.348593508995318, -44.72813283953004, 19.836508234714472, 144.49248881747755, -16.547272230754242, -9.560681351483941, -19.563547400189073, 1.3228378389036228, 1.0912415361190977, 20.383256127350474] Coefficient Standard Errors: [8.043646497811427, 4.182131353367049, 4.293682291754585, 73.32793120907755, 7.690626652102948, 4.108783841348964, 61.669402913526625, 25.481445101737247, 91.63478289909655, 609.7007361468519] These numbers mean nothing unless I know which attribute they correspond to. But in my DataFrame I only have one column called "features" which contains rows of sparse Vectors. This is an ever bigger problem when I have one-hot encoded features, because if I have one variable with an encoding of length n, I will get n corresponding coefficients/p-values/t-values etc. As of today Spark doesn't provide any method that can do it for you, so if you have to create your own. Let's say your data looks like this: and is processed using following pipeline: Get the LinearRegressionModel: Transform the data: Extract and flatten ML attributes: and map to the output: You can see the actual order of the columns here there will be two classes usually, ["binary] & ["numeric"] Should give the exact order of all the columns Here's the one line answer: Thanks to @pratiklodha for the core of this.How do you calculate a best fit line in python, and then plot it on a scatterplot in matplotlib?  I was I calculate the linear best-fit line using Ordinary Least Squares Regression as follows: This is multivariate (there are many x-values for each case). So, X is a list of lists, and y is a single list. 
For example:  But how do I do this with higher order polynomial functions. For example, not just linear (x to the power of M=1), but binomial (x to the power of M=2), quadratics (x to the power of M=4), and so on. For example, how to I get the best fit curves from the following? Extracted from Christopher Bishops's "Pattern Recognition and Machine Learning", p.7:  The accepted answer to this question
provides a small multi poly fit library which will do exactly what you need using numpy, and you can plug the result into the plotting as I've outlined below. You would just pass in your arrays of x and y points and the degree(order) of fit you require into multipolyfit. This returns the coefficients which you can then use for plotting using numpy's polyval. Note: The code below has been amended to do multivariate fitting, but the plot image was part of the earlier, non-multivariate answer.  Note: This was part of the answer earlier on, it is still relevant if you don't have multivariate data. Instead of coeffs = mpf(..., use coeffs = numpy.polyfit(x,y,3) For non-multivariate data sets, the easiest way to do this is probably with numpy's polyfit: numpy.polyfit(x, y, deg, rcond=None, full=False, w=None, cov=False) Least squares polynomial fit. Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). Returns a vector of coefficients p that minimises the squared error. Slightly out of context because the resulting function is not a polynomial, but still interesting perhaps. One major problem with polynomial fitting is Runge's phenomenon: The higher the degree, the more dramatic oscillations will occur. This isn't just constructed either but it will come back to bite you. As a remedy, I created smoothfit a while ago. It solves an appropriate least-squares problem and gives nice results, e.g.:I am implementing logistic regression using batch gradient descent. There are two classes into which the input samples are to be classified. The classes are 1 and 0. While training the data, I am using the following sigmoid function: where And I am using the following cost function to calculate cost, to determine when to stop training.  I am getting the cost at each step to be NaN as the values of htheta are either 1 or zero in most cases. What should I do to determine the cost value at each iteration?  This is the gradient descent code for logistic regression: There are two possible reasons why this may be happening to you.  This is because when you apply the sigmoid / logit function to your hypothesis, the output probabilities are almost all approximately 0s or all 1s and with your cost function, log(1 - 1) or log(0) will produce -Inf.  The accumulation of all of these individual terms in your cost function will eventually lead to NaN.   Specifically, if y = 0 for a training example and if the output of your hypothesis is log(x) where x is a very small number which is close to 0, examining the first part of the cost function would give us 0*log(x) and will in fact produce NaN.  Similarly, if y = 1 for a training example and if the output of your hypothesis is also log(x) where x is a very small number, this again would give us 0*log(x) and will produce NaN.  Simply put, the output of your hypothesis is either very close to 0 or very close to 1.   This is most likely due to the fact that the dynamic range of each feature is widely different and so a part of your hypothesis, specifically the weighted sum of x*theta for each training example you have will give you either very large negative or positive values, and if you apply the sigmoid function to these values, you'll get very close to 0 or 1. One way to combat this is to normalize the data in your matrix before performing training using gradient descent.  A typical approach is to normalize with zero-mean and unit variance.  Given an input feature x_k where k = 1, 2, ... n where you have n features, the new normalized feature x_k^{new} can be found by:  m_k is the mean of the feature k and s_k is the standard deviation of the feature k.  This is also known as standardizing data.  You can read up on more details about this on another answer I gave here: How does this code for standardizing data work? Because you are using the linear algebra approach to gradient descent, I'm assuming you have prepended your data matrix with a column of all ones.  Knowing this, we can normalize your data like so:  The mean and standard deviations of each feature are stored in mX and sX respectively.  You can learn how this code works by reading the post I linked to you above. I won't repeat that stuff here because that isn't the scope of this post. To ensure proper normalization, I've made the mean and standard deviation of the first column to be 0 and 1 respectively.  xnew contains the new normalized data matrix.  Use xnew with your gradient descent algorithm instead.  Now once you find the parameters, to perform any predictions you must normalize any new test instances with the mean and standard deviation from the training set.  Because the parameters learned are with respect to the statistics of the training set, you must also apply the same transformations to any test data you want to submit to the prediction model.  Assuming you have new data points stored in a matrix called xx, you would do normalize then perform the predictions: Now that you have this, you can perform your predictions: You can change the threshold of 0.5 to be whatever you believe is best that determines whether examples belong in the positive or negative class. As you mentioned in the comments, once you normalize the data the costs appear to be finite but then suddenly go to NaN after a few iterations. Normalization can only get you so far. If your learning rate or alpha is too large, each iteration will overshoot in the direction towards the minimum and would thus make the cost at each iteration oscillate or even diverge which is what is appearing to be happening. In your case, the cost is diverging or increasing at each iteration to the point where it is so large that it can't be represented using floating point precision.  As such, one other option is to decrease your learning rate alpha until you see that the cost function is decreasing at each iteration. A popular method to determine what the best learning rate would be is to perform gradient descent on a range of logarithmically spaced values of alpha and seeing what the final cost function value is and choosing the learning rate that resulted in the smallest cost.  Using the two facts above together should allow gradient descent to converge quite nicely, assuming that the cost function is convex. In this case for logistic regression, it most certainly is.  Let's assume you have an observation where:  Then your cost function will get a value of NaN because you're adding 0 * log(0), which is undefined. Hence: As @rayryeng pointed out, 0 * log(0) produces a NaN because 0 * Inf isn't kosher. This is actually a huge problem: if your algorithm believes it can predict a value perfectly, it incorrectly assigns a cost of NaN.  Instead of: You can avoid multiplying 0 by infinity by instead writing your cost function in Matlab as: The idea is if y_i is 1, we add -log(htheta_i) to the cost, but if y_i is 0, we add -log(1 - htheta_i) to the cost. This is mathematically equivalent to -y_i * log(htheta_i) - (1 - y_i) * log(1- htheta_i) but without running into numerical problems that essentially stem from htheta_i being equal to 0 or 1 within the limits of double precision floating point. It happened to me because an indetermination of the type: This can happen when one of the predicted values Y equals either 0 or 1.
In my case the solution was to add an if statement to the python code as follows: This way, when the actual value (y) and the predicted one (Y) are equal, no cost needs to be computed, which is the expected behavior.   (Notice that when a given Y is converging to 0 the left addend is canceled (because of y=0) and the right addend tends toward 0. The same happens when Y converges to 1, but with the opposite addend.) (There is also a very rare scenario, which you probably won't need to worry about, where y=0 and Y=1 or viceversa, but if your dataset is standarized and the weights are properly initialized it won't be an issue.)I know that Cross validation is used for selecting good parameters. After finding them, i need to re-train the whole data without the -v option. But the problem i face is that after i train with -v option, i get the cross-validation accuracy( e.g 85%). There is no model and i can't see the values of C and gamma. In that case how do i retrain? Btw i applying 10 fold cross validation.
e.g  Need some help on it.. To get the best C and gamma, i use this code that is available in the LIBSVM FAQ Another question : Is that cross-validation accuracy after using -v option similar to that we get when we train without -v option and use that model to predict? are the two accuracy similar? Another question : Cross-validation basically improves the accuracy of the model by avoiding the overfitting. So, it needs to have a model in place before it can improve. Am i right? Besides that, if i have a different model, then the cross-validation accuracy will be different? Am i right? One more question: In the cross-validation accuracy, what is the value of C and gamma then? The graph is something like this 
 Then the values of C are 2 and gamma = 0.0078125. But when i retrain the model with the new parameters. The value is not the same as 99.63%. Could there be any reason?
Thanks in advance... The -v option here is really meant to be used as a way to avoid the overfitting problem (instead of using the whole data for training, perform an N-fold cross-validation training on N-1 folds and testing on the remaining fold, one at-a-time, then report the average accuracy). Thus it only returns the cross-validation accuracy (assuming you have a classification problem, otherwise mean-squared error for regression) as a scalar number instead of an actual SVM model. If you want to perform model selection, you have to implement a grid search using cross-validation (similar to the grid.py helper python script), to find the best values of C and gamma. This shouldn't be hard to implement: create a grid of values using MESHGRID, iterate overall all pairs (C,gamma) training an SVM model with say 5-fold cross-validation, and choosing the values with the best CV-accuracy... Example:  If you use your entire dataset to determine your parameters, then train on that dataset, you are going to overfit your data.  Ideally, you would divide the dataset, do the parameter search on a portion (with CV), then use the other portion to train and test with CV.  Will you get better results if you use the whole dataset for both? Of course, but your model is likely to not generalize well.  If you want determine true performance of your model, you need to do parameter selection separately.I have written an RNN language model using TensorFlow. The model is implemented as an RNN class. The graph structure is built in the constructor, while RNN.train and RNN.test methods run it. I want to be able to reset the RNN state when I move to a new document in the training set, or when I want to run a validation set during training. I do this by managing the state inside the training loop, passing it into the graph via a feed dictionary. In the constructor I define the the RNN like so The training loop looks like this x and y are batches of training data in a document. The idea is that I pass the latest state along after each batch, except when I start a new document, when I zero out the state by running self.reset_state. This all works.  Now I want to change my RNN to use the recommended state_is_tuple=True. However, I don't know how to pass the more complicated LSTM state object via a feed dictionary. Also I don't know what arguments to pass to the self.state = tf.placeholder(...) line in my constructor. What is the correct strategy here? There still isn't much example code or documentation for dynamic_rnn available. TensorFlow issues 2695 and 2838 appear relevant. A blog post on WILDML addresses these issues but doesn't directly spell out the answer. See also TensorFlow: Remember LSTM state for next batch (stateful LSTM). One problem with a Tensorflow placeholder is that you can only feed it with a Python list or Numpy array (I think). So you can't save the state between runs in tuples of LSTMStateTuple.  I solved this by saving the state in a tensor like this initial_state = np.zeros((num_layers, 2, batch_size, state_size)) You have two components in an LSTM layer, the cell state and hidden state, thats what the "2" comes from. (this article is great: https://arxiv.org/pdf/1506.00019.pdf) When building the graph you unpack and create the tuple state like this: Then you get the new state the usual way It shouldn't be like this... perhaps they are working on a solution. A simple way to feed in an RNN state is to simply feed in both components of the state tuple individually.Given a predefined Keras model, I am trying to first load in pre-trained weights, then remove one to three of the models internal (non-last few) layers, and then replace it with another layer. I can't seem to find any documentation on keras.io about to do such a thing or remove layers from a predefined model at all. The model I am using is a good ole VGG-16 network which is instantiated in a function as shown below: So as an example, I'd like to take the two Conv layers in Block 1 and replace them with just one Conv layer, after loading the original weights into all of the other layers. Any ideas?  Assuming that you have a model vgg16_model, initialized either by your function above or by keras.applications.VGG16(weights='imagenet'). Now, you need to insert a new layer in the middle in such a way that the weights of other layers will be saved. The idea is to disassemble the whole network to separate layers, then assemble it back. Here is the code specifically for your task: And the output of the above code is: Another way to do this is by building a Sequential model.
See the following example where I swap ReLU layers for PReLU.
You would need to simply not add the layers you don't want, and add a new layer.As described in figure 1, I have 3 models which each apply to a particular domain. The 3 models are trained separately with different datasets.
 And inference is sequential :  I tried to parallelize the call of these 3 models thanks to the Multiprocess library of python but it is very unstable and it is not advised. Here's the idea I got to make sure to do this all at once: As the 3 models share a common pretrained-model, I want to make a single model that has multiple inputs and multiple outputs. As the following drawing shows:
 Like that during the inference, I will call a single model which will do all 3 operations at the same time.  I saw that with The Functional API of KERAS, it is possible but I have no idea how to do that.
The inputs of the datasets have the same dimension. These are pictures of (200,200,3). If anyone has an example of a Multi-Input Multi-output model that shares a common structure, I'm all ok. Here is the example of my code but it returns an error because of the layers. concatenate (...) line which propagates a shape that is not taken into account by the EfficientNet model. We can do that easily in tf. keras using its awesome Functional API. Here we will walk you through how to build multi-out with a different type (classification and regression) using Functional API. According to your last diagram, you need one input model and three outputs of different types. To demonstrate, we will use MNIST which is a handwritten dataset. It's normally a 10 class classification problem data set. From it, we will create an additionally 2 class classifier (whether a digit is even or odd) and also a 1 regression part (which is to predict the square of a digit, i.e for image input of 9, it should give approximately it's square). Data Set So, our training pairs will be xtrain and [y_out_a, y_out_b, y_out_c], the same as your last diagram. Model Building Let's build the model accordingly using the Functional API of tf. keras. See the model definition below. The MNIST samples are a 28 x 28 grayscale image. So our input is set in that way. I'm guessing your data set is probably RGB, so change the input dimension accordingly.  One thing to note, while defining out_a, out_b, and  out_c during model definition we set their name variable which is very important. Their names are set '10cls', '2cls', and '1rg' respectively. You can also see this from the above diagram (last 3 tails). Compile and Run Now, we can see why that name variable is important. In order to run the model, we need to compile it first with the proper loss function, metrics, and optimizer. Now, if you know that, for the classification and regression problem, the optimizer can be the same but for the loss function and metrics should be changed. And in our model, which has a multi-type output model (2 classifications and 1 regression), we need to set proper loss and metrics for each of these types. Please, see below how it's done. See, each last output of our above model, which is here represented by their name variables. And we set proper compilation to them. Hope you understand this part. Now, time to train the model. That's how each of the outputs of the last layer optimizes by their concern loss function. FYI, one thing to mention, there is an essential parameter while .compile the model which you might need: loss_weights - to weight the loss contributions of different model outputs. See my other answer here on this. Prediction / Inference Let's see some output. We now hope this model will predict 3 things: (1) is what the digit is, (2) is it even or odd, and (3) its square value.  If we like to quickly check the output layers of our model Passing this xtrain[0] (which we know 5) to the model to do predictions. Based on your comment, we can extend the above model to take multi-input too. We need to change things. To demonstrate, we will use train and test samples of the mnist data set to the model as a multi-input. Next, we need to modify some parts of the above model to take multi-input. And next if you now plot, you will see the new graph.  Now, we can train the model as follows Now, we can test the multi-input model and get multi-out from it.I want to select the top N=10,000 principal components from a matrix. After the pca is completed, MATLAB should return a pxp matrix, but it doesn't! It should be coefs:153600 x 153600? and scores:400 X 153600? When I use the below code it gives me an Out of Memory error:: I don't understand why MATLAB returns a lesser dimensional matrix. It
should return an error with pca: 153600*153600*8 bytes=188 GB Error with eigs: I think you are falling prey to the XY problem, since trying to find 153.600 dimensions in your data is completely non-physical, please ask about the problem (X) and not your proposed solution (Y) in order to get a meaningful answer. I will use this post only to tell you why PCA is not  a good fit in this case. I cannot tell you what will solve your problem, since you have not told us what that is. This is a mathematically unsound problem, as I will try to explain here. PCA is, as user3149915 said, a way to reduce dimensions. This means that somewhere in your problem you have one-hundred-fifty-three-thousand-six-hundred dimensions floating around. That's a lot. A heck of a lot. Explaining a physical reason for the existence of all of them might be a bigger problem than trying to solve the mathematical problem. Trying to fit that many dimensions to only 400 observations will not work, since even if all observations are linear independent vectors in your feature space, you can still extract only 399 dimensions, since the rest simply cannot be found since there are no observations. You can at most fit N-1 unique dimensions through N points, the other dimensions have an infinite number of possibilities of location. Like trying to fit a plane through two points: there's a line you can fit through those and the third dimension will be perpendicular to that line, but undefined in the rotational direction. Hence, you are left with an infinite number of possible planes that fit through those two points. After the first 400 components, there's no more dimensions left. You are fitting a void after that. You used all your data to get the dimensions and cannot create more dimensions. Impossible. All you can do is get more observations, some 1.5M, and do the PCA again. Why do you need more observations than dimensions? you might ask. Easy, you cannot fit a unique line through a point, nor a unique plane through two points, nor a unique 153.600 dimensional hyperplane through 400 points. Sadly, no. If you have two points and fit a line through it you get a 100% fit. No error, jay! Done for the day, let's go home and watch TV! Sadly, your boss will call you in the next morning since your fit is rubbish. Why? Well, if you'd have for instance 20 points scattered around, the fit would not be without errors, but at least closer to representing your actual data, since the first two could be outliers, see this very illustrative figure, where the red points would be your first two observations:  If you were to extract the first 10.000 components, that'd be 399 exact fits and 9601 zero dimensions. Might as well not even attempt to calculate beyond the 399th dimension, and stick that into a zero array with 10.000 entries. TL;DR You cannot use PCA and we cannot help you solve your problem as long as you do not tell us what your problem is. PCA is a dimension reduction algorithm, as such it tries to reduce the number of features to principal components (PC) that each represents some linear combination of the total features. All of this is done in order to reduce the dimensions of the feature space, i.e. transform the large feature space to one that is more manageable but still retains most if not all of the information.  Now for your problem, you are trying to explain the variance across your 400 observations using 153600 features, however, we don't need that much information 399 PC's will explain 100% of the variance across your sample (I will be very surprised if that is not the case). The reason for that is basicly overfitting, your algorithm finds noise that explain every observation in your sample.  So what the rayryeng was telling you is correct, if you want to reduce your feature space to 10,000 PC's you will need 100,000 observations for the PC's to mean anything (that is a rule of thumb but a rather stable one). And the reason that matlab was giving you 399 PC's because it was able to correctly extract 399 linear combinations that explained some #% of the variance across your sample.  If on the other hand what you are after are the most relevant features than you are not looking for dimensional reduction flows, but rather feature elimination processes. These will keep only the most relevant feature while nulling the irrelevant ones.   So just to make clear, if your feature space is rubbish and there isn't any information there just noise, the variance explained will be irrelevant and will indeed be less than 100% for example see the following    Again if you want to reduce your feature space there are ways to that even with a small m, but PCA is not one of them.  Good Luck     Matlab tries to not waste too much resources computing it.
But you still can do what you want, just use:I convert my image data to caffe db format (leveldb, lmdb) using C++ as example I use this code for imagenet. Is data need to be shuffled, can I write to db all my positives and then all my negatives like 00000000111111111, or data need to be shuffled and labels should look like 010101010110101011010? How caffe sample data from DB, is it true that it use random subset of all data with size = batch_size? Should you shuffle the samples? Think about the learning process if you don't shuffle; caffe sees only 0 samples - what do you expect the algorithm to deduce? simply predict 0 all the time and everything is cool. If you have plenty of 0 before you hit the first 1 caffe will be very confident in predicting always 0. It will be very difficult to move the model from this point.
On the other hand, if it constantly sees a mix of 0 and 1 it learns from the beginning meaningful features for separating the examples.
Bottom line: it is very advantageous to shuffle the training samples, especially when using SGD-based approaches.  AFAIK, caffe does not randomly sample batch_size samples, but rather goes sequentially over the input DB batch_size after batch_size samples. TL;DR
shuffle.I am very new to matplotlib and am working on simple projects to get acquainted with it. I was wondering how I might plot the decision boundary which is the weight vector of the form [w1,w2], which basically separates the two classes lets say C1 and C2, using matplotlib. Is it as simple as plotting a line from (0,0) to the point (w1,w2) (since W is the weight "vector") if so, how do I extend this like in both directions if I need to? Right now all I am doing is :  Thanks in advance. Decision boundary is generally much more complex then just a line, and so (in 2d dimensional case) it is better to use the code for generic case, which will also work well with linear classifiers. The simplest idea is to plot contour plot of the decision function some examples from sklearn documentationI am trying to convert my model in Tensorflow (.pb) format to Keras (.h5) format to view post hoc attention visualisation. 
I have tried below code. Can anyone help me with this? Is this even possible? In the Latest Tensorflow Version (2.2), when we Save the Model using tf.keras.models.save_model, the Model will be Saved in not just a pb file but it will be Saved in a Folder, which comprises Variables Folder and Assets Folder, in addition to the saved_model.pb file, as shown in the screenshot below:  For example, if the Model is Saved with the Name, "Model", we have to Load using the Name of the Folder, "Model", instead of saved_model.pb, as shown below: instead of One more change you can do is to replace with Complete working Code to convert a Model from Tensorflow Saved Model Format (pb) to Keras Saved Model Format (h5) is shown below: Output of the New_Model.summary command is: Continuing the code: Output of the command, print(loaded_model_from_h5.summary()) is shown below: ​
As can be seen from the Summary of both the Models above, both the Models are same.I'm trying to do image classification with two classes. I have 1000 images with balanced classes. When I train the model, I get a low constant validation accuracy but a decreasing validation loss. Is this a sign of overfitting or underfitting? I should also note that I'm attempting to retrain the Inception V3 model with new classes and a different dataset. Overfitting ( or underfitting) occurs when a model is too specific (or not specific enough) to the training data, and doesn't extrapolate well to the true domain. I'll just say overfitting from now on to save my poor typing fingers [*] I think the wikipedia image is good:  Clearly, the green line, a decision boundary trying to separate the red class from the blue, is "overfit", because although it will do well on the training data, it lacks the "regularized" form we like to see when generalizing [**]. These CMU slides on overfitting/cross validation also make the problem clear:  And here's some more intuition for good measure Overfitting is observed numerically when the testing error does not  reflect the training error Obviously, the testing error will always (in expectation) be worse than the training error, but at a certain number of iterations, the loss in testing will start to increase, even as the loss in training continues to decline. Overfitting can be observed by plotting the decision boundary (as in
  the wikipedia image above) when dimensionality allows, or by looking
  at testing loss in addition to training loss during the fit procedure You don't give us enough points to make these graphs, but here's an example (from someone asking a similar question) showing what those loss graphs would look like:
 While loss curves are sometimes more pretty and logarthmic, note the trend here that training error is still decreasing but testing error is on the rise. That's a big red flag for overfitting. SO discusses loss curves here The slightly cleaner and more real-life example is from this CMU lecture on ovefitting ANN's:  The top graph is overfitting, as before. The bottom graph is not. When does this occur? When a model has too many parameters, it is susceptible to overfitting (like a n-degree polynomial to n-1 points). Likewise, a model with not enough parameters can be underfit. Certain regularization techniques like dropout or batch normalization, or traditionally l-1 regularization combat this. I believe this is beyond the scope of your question. Footnotes [*] There's no reason to keep writing "overfitting/underfitting", since the reasoning is the same for both, but the indicators are flipped, obviously (a decision boundary that hasn't latched onto the true border enough, as opposed to being too tightly wrapped against individual points). In general, overfitting is the more common to avoid, since "more iterations/more parameters" is the current theme. If you have lots of data and not lot of parameters, maybe you really are worried about underfitting, but I doubt it.  [**] One way to formalize the idea that the black line is preferable than the green one in the first image from wikipedia is to penalize the number of parameters required by your model during model selectionI'm using Pydantic model (Basemodel) with FastAPI and converting the input into a dictionary, and then converting it into a Pandas DataFrame to assign it into model.predict() function for Machine Learning prediction, as shown below : It works fine, I'm just not quite sure if it's optimized or the right way to do it, since I convert the input two times to get the predictions. Also, I'm not sure if it is going to work fast in the case of having a huge number of inputs. Any improvements for this? If there's a way (even other than using Pydantic models, where I can work directly and avoid going through conversions and the loop. First, you should use more descriptive names for your variables/objects. For example: You cannot pass the Pydantic model directly to the predict() function, as it accepts a data array, not a Pydantic model. Available options are listed below. You could use: If you don't wish to use a Pandas DataFrame, as shown in your question, i.e., then, you could use the __dict__ method to get the values of all attributes in the model and convert it to a list: or, preferably, use the Pydantic's .dict() method: You could avoid looping over individual items and calling the predict() function multiple times, by using, instead, the below: or (in case you don't wish using Pandas DataFrame):I have a numpy array like this: I transform it like this to reduce the memory demand: resulting in this: However, when I do this: I get: Any ideas why? Ultimately, the numpy array contains the labels for a binary classification problem. So far, I have used it as float32 as is in a Keras ANN and it worked fine and I achieved pretty good performance. So is it actually necessary to run to_categorical? You don't need to use to_categorical since I guess you are doing multi-label classification. To avoid any confusion once and for all(!), let me explain this. If you are doing binary classification, meaning each sample may belong to only one 
of two classes e.g. cat vs dog or happy vs sad or positive review vs negative review, then: If you are doing multi-class classification, meaning each sample may belong to only one of many classes e.g. cat vs dog vs lion or happy vs neutral vs sad or positive review vs neutral review vs negative review, then: If you are doing multi-label classification, meaning each sample may belong to zero, one or more than one classes e.g. an image may contain both cat and dog, then: Ignoring the fact that the application of to_categorical is pointless in my scenario. The following solves the memory issue:Below is my code. I know why the error is occurring during transform. It is because of the feature list mismatch during fit and transform.
How can i solve this? How can i get 0 for all the rest features? After this i want to use this for partial fit of SGD classifier.  Instead of using pd.get_dummies() you need LabelEncoder + OneHotEncoder which can store the original values and then use them on the new data. Changing your code like below will give you required results. You encoder is fitted on refreshed_df which contains 10 columns and your refreshed_df1 contains only 4, literally what it is reported in the error. You have either to delete the columns not appearing on your refreshed_df1 or just fit your encoder to a new version of refreshed_df that only contains the 4 columns appearing in refreshed_df1 .This question does not appear to be about programming within the scope defined in the help center. Closed 2 years ago. When we have to predict the value of a categorical (or discrete) outcome we use logistic regression. I believe we use linear regression to also predict the value of an outcome given the input values. Then, what is the difference between the two methodologies? Linear regression output as probabilities It's tempting to use the linear regression output as probabilities but it's a mistake because the output can be negative, and greater than 1 whereas probability can not. As regression might actually
produce probabilities that could be less than 0, or even bigger than
1, logistic regression was introduced.  Source: http://gerardnico.com/wiki/data_mining/simple_logistic_regression  Outcome In linear regression, the outcome (dependent variable) is continuous.
It can have any one of an infinite number of possible values.  In logistic regression, the outcome (dependent variable) has only a limited number of possible values.  The dependent variable Logistic regression is used when the response variable is categorical in nature. For instance, yes/no, true/false, red/green/blue,
1st/2nd/3rd/4th, etc.   Linear regression is used when your response variable is continuous. For instance, weight, height, number of hours,  etc. Equation Linear regression gives an equation which is of the form Y = mX + C,
means equation with degree 1.  However, logistic regression gives an equation which is of the form 
Y = eX + e-X Coefficient interpretation In linear regression, the coefficient interpretation of independent variables are quite straightforward (i.e. holding all other variables constant, with a unit increase in this variable, the dependent variable is expected to increase/decrease by xxx).  However, in logistic regression, depends on the family (binomial, Poisson,
etc.) and link (log, logit, inverse-log, etc.) you use, the interpretation is different.  Error minimization technique Linear regression uses ordinary least squares method to minimise the
errors and arrive at a best possible fit, while logistic regression
uses maximum likelihood method to arrive at the solution. Linear regression is usually solved by minimizing the least squares error of the model to the data, therefore large errors are penalized quadratically.  Logistic regression is just the opposite. Using the logistic loss function causes large errors to be penalized to an asymptotically constant. Consider linear regression on categorical {0, 1} outcomes to see why this is a problem. If your model predicts the outcome is 38, when the truth is 1, you've lost nothing. Linear regression would try to reduce that 38, logistic wouldn't (as much)2. In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values. For instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be any, there are so many possible values that a linear regression model would be chosen. If, instead, you wanted to predict, based on size, whether a house would sell for more than $200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than $200K, or No, the house will not. Just to add on the previous answers. Linear regression Is meant to resolve the problem of predicting/estimating the output value for a given element X (say f(x)). The result of the prediction is a continuous function where the values may be positive or negative. In this case you normally have an input dataset with lots of examples and the output value for each one of them. The goal is to be able to fit a model to this data set so you are able to predict that output for new different/never seen elements. Following is the classical example of fitting a line to set of points, but in general linear regression could be used to fit more complex models (using higher polynomial degrees):  Resolving the problem Linear regression can be solved in two different ways: Logistic regression Is meant to resolve classification problems where given an element you have to classify the same in N categories. Typical examples are, for example, given a mail to classify it as spam or not, or given a vehicle find to which category it belongs (car, truck, van, etc ..). That's basically the output is a finite set of discrete values. Resolving the problem Logistic regression problems could be resolved only by using Gradient descent. The formulation in general is very similar to linear regression the only difference is the usage of different hypothesis function. In linear regression the hypothesis has the form: where theta is the model we are trying to fit and [1, x_1, x_2, ..] is the input vector. In logistic regression the hypothesis function is different:  This function has a nice property, basically it maps any value to the range [0,1] which is appropiate to handle propababilities during the classificatin. For example in case of a binary classification g(X) could be interpreted as the probability to belong to the positive class. In this case normally you have different classes that are separated with a decision boundary which basically a curve that decides the separation between the different classes. Following is an example of dataset separated in two classes.  You can also use the below code to generate the linear regression
curve
q_df = details_df
# q_df = pd.get_dummies(q_df) lmod = sm.OLS(train_y, train_x).fit() lmod.summary() lmod.predict()[:10] lmod.get_prediction().summary_frame()[:10] sm.qqplot(lmod.resid,line="q") plt.title("Q-Q plot of Standardized
Residuals") plt.show() Simply put, linear regression is a regression algorithm, which outpus a possible continous and infinite value; logistic regression is considered as a binary classifier algorithm, which outputs the 'probability' of the input belonging to a label (0 or 1). The basic difference : Linear regression is basically a regression model which means its will give a non discreet/continuous output of a function. So this approach gives the value. For example : given x what is f(x) For example given a training set of different factors and the price of a property after training we can provide the required factors to determine what will be the property price. Logistic regression is basically a binary classification algorithm which means that here there will be discreet valued output for the function . For example : for a given x if f(x)>threshold classify it to be 1 else classify it to be 0. For example given a set of brain tumour size as training data we can use the size as input to determine whether its a benine or malignant tumour. Therefore here the output is discreet either 0 or 1. *here the function is basically the hypothesis function They are both quite similar in solving for the solution, but as others have said, one (Logistic Regression) is for predicting a category "fit" (Y/N or 1/0), and the other (Linear Regression) is for predicting a value. So if you want to predict if you have cancer Y/N (or a probability) - use logistic.  If you want to know how many years you will live to - use Linear Regression ! Regression means continuous variable, Linear means there is linear relation between y and x. 
Ex= You are trying to predict salary from no of years of experience. So here salary is independent variable(y) and yrs of experience is dependent variable(x).
y=b0+ b1*x1

We are trying to find optimum value of constant b0 and b1 which will give us best fitting line for your observation data.
It is a equation of line which gives continuous value from x=0 to very large value.
This line is called Linear regression model. Logistic regression is type of classification technique. Dnt be misled by term regression. Here we predict whether y=0 or 1. Here we first need to find p(y=1) (wprobability of y=1) given x from formuale below.  Probaibility p is related to y by below formuale  Ex=we can make classification of tumour having more than 50% chance of having cancer  as 1 and tumour having less than 50% chance of having cancer as 0.
 Here red point will be predicted as 0 whereas green point will be predicted as 1. Cannot agree more with the above comments. 
Above that, there are some more differences like In Linear Regression, residuals are assumed to be normally distributed. 
In Logistic Regression, residuals need to be independent but not normally distributed.  Linear Regression assumes that a constant change in the value of the explanatory variable results in constant change in the response variable. 
This assumption does not hold if the value of the response variable represents a probability (in Logistic Regression) GLM(Generalized linear models) does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model. In short:
Linear Regression gives continuous output. i.e. any value between a range of values.
Logistic Regression gives discrete output. i.e. Yes/No, 0/1 kind of outputs. To put it simply, if in linear regression model more test cases arrive which are far away from the threshold(say =0.5)for a prediction of y=1 and y=0. Then in that case the hypothesis will change and become worse.Therefore linear regression model is not used for classification problem. Another Problem is that if the classification is y=0 and y=1, h(x) can be > 1 or < 0.So we use Logistic regression were 0<=h(x)<=1. Logistic Regression is used in predicting categorical outputs like Yes/No, Low/Medium/High etc. You have basically 2 types of logistic regression Binary Logistic Regression (Yes/No, Approved/Disapproved) or Multi-class Logistic regression (Low/Medium/High, digits from 0-9 etc) On the other hand, linear regression is if your dependent variable (y) is continuous. 
y = mx + c is a simple linear regression equation (m = slope and c is the y-intercept). Multilinear regression has more than 1 independent variable (x1,x2,x3 ... etc)  In linear regression the outcome is continuous whereas in logistic regression, the outcome has only a limited number of possible values(discrete). example:
In a scenario,the given value of x is size of a plot in square feet then predicting y ie rate of the plot comes under linear regression.  If, instead, you wanted to predict, based on size, whether the plot would sell for more than 300000 Rs, you would use logistic regression. The possible outputs are either Yes, the plot will sell for more than 300000 Rs, or No. In case of Linear Regression the outcome is continuous while in case of Logistic Regression outcome is discrete (not continuous) To perform Linear regression we require a linear relationship between the dependent and independent variables. But to perform Logistic regression we do not require a linear relationship between the dependent and independent variables. Linear Regression is all about fitting a straight line in the data while Logistic Regression is about fitting a curve to the data. Linear Regression is a regression algorithm for Machine Learning while Logistic Regression is a classification Algorithm for machine learning. Linear regression assumes gaussian (or normal) distribution of dependent variable. Logistic regression assumes binomial distribution of dependent variable. The basic difference between Linear Regression and Logistic Regression is :
Linear Regression is used to predict a continuous or numerical value but when we are looking for predicting a value that is categorical Logistic Regression come into picture. Logistic Regression is used for binary classification.I am unable to understand the page of the StandardScaler in the documentation of sklearn. Can anyone explain this to me in simple terms? I assume that you have a matrix X where each row/line is a sample/observation and each column is a variable/feature (this is the expected input for any sklearn ML function by the way -- X.shape should be [number_of_samples, number_of_features]). The main idea is to normalize/standardize i.e. μ = 0 and σ = 1 your features/variables/columns of X, individually,  before applying any machine learning model. StandardScaler() will normalize the features i.e. each
column of X, INDIVIDUALLY, so that each column/feature/variable will have μ = 0 and σ = 1. P.S: I find the most upvoted answer on this page, wrong.
I am quoting "each value in the dataset will have the sample mean value subtracted" -- This is neither true nor correct. See also: How and why to Standardize your data: A python tutorial Verify that the mean of each feature (column) is 0: Verify that the std of each feature (column) is 1:  UPDATE 08/2020: Concerning the input parameters with_mean and with_std to False/True, I have provided an answer here: StandardScaler difference between “with_std=False or True” and “with_mean=False or True” The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.
In case of multivariate data, this is done feature-wise (in other words independently for each column of the data).
Given the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case). StandardScaler performs the task of Standardization. Usually a dataset contains variables that are different in scale. For e.g. an Employee dataset will contain AGE column with values on scale 20-70 and SALARY column with values on scale 10000-80000. As these two columns are different in scale, they are Standardized to have common scale while building machine learning model. How to calculate it:  You can read more here: This is useful when you want to compare data that correspond to different units. In that case, you want to remove the units. To do that in a consistent way of all the data, you transform the data in a way that the variance is unitary and that the mean of the series is 0. Following is a simple working example to explain how standarization calculation works. The theory part is already well explained in other answers. Calculation As you can see in the output, mean is [6. , 2.5] and std deviation is [1.41421356, 0.8660254 ] Data is (0,1) position is 2
Standardization = (2 - 2.5)/0.8660254 = -0.57735027 Data in (1,0) position is 4
Standardization = (4-6)/1.41421356 = -1.414 Result After Standardization  Check Mean and Std Deviation After Standardization  Note: -2.77555756e-17 is very close to 0. References Compare the effect of different scalers on data with outliers What's the difference between Normalization and Standardization? Mean of data scaled with sklearn StandardScaler is not zero The answers above are great, but I needed a simple example to alleviate some concerns that I have had in the past. I wanted to make sure it was indeed treating each column separately. I am now reassured and can't find what example had caused me concern. All columns ARE scaled separately as described by those above. The scipy.stats module is correctly reporting the "sample" variance, which uses (n - 1) in the denominator. The "population" variance would use n in the denominator for the calculation of variance. To understand better, please see the code below that uses scaled data from the first column of the data set above: After applying StandardScaler(), each column in X will have mean of 0 and standard deviation of 1. Formulas are listed by others on this page. Rationale: some algorithms require data to look like this (see sklearn docs). We apply StandardScalar() on a row basis. So, for each row in a column (I am assuming that you are working with a Pandas DataFrame): Few points - It is called Standard Scalar as we are dividing it by the standard deviation of the distribution (distr. of the feature). Similarly, you can guess for MinMaxScalar(). The original distribution remains the same after applying StandardScalar(). It is a common misconception that the distribution gets changed to a Normal Distribution. We are just squashing the range into [0, 1].This: Gives the error: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same You get this error because your model is on the GPU, but your data is on the CPU. So, you need to send your input tensors to the GPU. Or like this, to stay consistent with the rest of your code: The same error will be raised if your input tensors are on the GPU but your model weights aren't. In this case, you need to send your model weights to the GPU. See the documentation for cuda(), and its opposite, cpu(). The new API is to use .to() method. The advantage is obvious and important.
Your device may tomorrow be something other than "cuda": So try to avoid model.cuda()
It is not wrong to check for the device or to hardcode it: same as: In general you can use this code: As already mentioned in the previous answers, the issue can be that your model is trained on the GPU, but it's tested on the CPU. If that's the case then you need to port your model's weights and the data from the GPU to the CPU like this: NOTE: Here we still check if the configuration arguments are set to GPU or CPU, so that this piece of code can be used for both training (on the GPU) and testing (on the CPU). When loading a model, both weights and inputs have to be in the same device, we can do this by using the .to(device) as pointed by others. However it might be the case that also the datatype of the saved weights and the input tensors are different. If this is the case then we must also change the datatype of both model weights and inputs: I have same problem,My CNN model: I put for Conv2d.to(device) its work for me. Notice that (from pytorch documentation): If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device. That is, you might need to do: Instead of just: With the first approach you'll be in the safe side. First check cuda is available or not: In case you want to load some model do this: Now you probably get this error: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same It is needed to convert the type of input data from torch.tensor to torch.cuda.tensor by : and then convert the result from torch.cuda.tensor to torch.tensor: Works, perfectly fine...I'm learning different methods to convert categorical variables to numeric for machine-learning classifiers.  I came across the pd.get_dummies method and sklearn.preprocessing.OneHotEncoder() and I wanted to see how they differed in terms of performance and usage.  I found a tutorial on how to use OneHotEncoder() on https://xgdgsc.wordpress.com/2015/03/20/note-on-using-onehotencoder-in-scikit-learn-to-work-on-categorical-features/ since the sklearn documentation wasn't too helpful on this feature. I have a feeling I'm not doing it correctly...but Can some explain the pros and cons of using pd.dummies over sklearn.preprocessing.OneHotEncoder() and vice versa? I know that OneHotEncoder() gives you a sparse matrix but other than that I'm not sure how it is used and what the benefits are over the pandas method.  Am I using it inefficiently?  For machine learning, you almost definitely want to use sklearn.OneHotEncoder. For other tasks like simple analyses, you might be able to use pd.get_dummies, which is a bit more convenient. Note that sklearn.OneHotEncoder has been updated in the latest version so that it does accept strings for categorical variables, as well as integers. The crux of it is that the sklearn encoder creates a function which persists and can then be applied to new data sets which use the same categorical variables, with consistent results. Note how we apply the same encoder we created via X_train to the new data set X_test. Consider what happens if X_test contains different levels than X_train for one of its variables. For example, let's say X_train["color"] contains only "red" and "green", but in addition to those, X_test["color"] sometimes contains "blue". If we use pd.get_dummies, X_test will end up with an additional "color_blue" column which X_train doesn't have, and the inconsistency will probably break our code later on, especially if we are feeding X_test to an sklearn model which we trained on X_train. And if we want to process the data like this in production, where we're receiving a single example at a time, pd.get_dummies won't be of use. With sklearn.OneHotEncoder on the other hand, once we've created the encoder, we can reuse it to produce the same output every time, with columns only for "red" and "green". And we can explicitly control what happens when it encounters the new level "blue": if we think that's impossible, then we can tell it to throw an error with handle_unknown="error"; otherwise we can tell it to continue and simply set the red and green columns to 0, with handle_unknown="ignore". OneHotEncoder cannot process string values directly. If your nominal features are strings, then you need to first map them into integers. pandas.get_dummies is kind of the opposite. By default, it only converts string columns into one-hot representation, unless columns are specified.  I really like Carl's answer and upvoted it.  I will just expand Carl's example a bit so that more people hopefully will appreciate that pd.get_dummies can handle unknown.  The two examples below shows that pd.get_dummies can accomplish the same thing in handling unknown as OHE .  why wouldn't you just cache or save the columns as variable col_list from the resulting get_dummies then use pd.reindex to align the train vs test datasets....   example:update: this question is related to Google Colab's "Notebook settings: Hardware accelerator: GPU". This question was written before the "TPU" option was added. Reading multiple excited announcements about Google Colaboratory providing free Tesla K80 GPU, I tried to run fast.ai lesson on it for it to never complete - quickly running out of memory. I started investigating of why. The bottom line is that “free Tesla K80” is not "free" for all - for some only a small slice of it is "free".  I connect to Google Colab from West Coast Canada and I get only 0.5GB of what supposed to be a 24GB GPU RAM. Other users get access to 11GB of GPU RAM. Clearly 0.5GB GPU RAM is insufficient for most ML/DL work. If you're not sure what you get, here is little debug function I scraped together (only works with the GPU setting of the notebook): Executing it in a jupyter notebook before running any other code gives me: The lucky users who get access to the full card will see: Do you see any flaw in my calculation of the GPU RAM availability, borrowed from GPUtil? Can you confirm that you get similar results if you run this code on Google Colab notebook? If my calculations are correct, is there any way to get more of that GPU RAM on the free box? update: I'm not sure why some of us get 1/20th of what other users get. e.g. the person who helped me to debug this is from India and he gets the whole thing! note: please don't send any more suggestions on how to kill the potentially stuck/runaway/parallel notebooks that might be consuming parts of the GPU. No matter how you slice it, if you are in the same boat as I and were to run the debug code you'd see that you still get a total of 5% of GPU RAM (as of this update still). So to prevent another dozen of answers suggesting invalid in the context of this thread suggestion to !kill -9 -1, let's close this thread: The answer is simple: As of this writing Google simply gives only 5% of GPU to some of us, whereas 100% to the others. Period. dec-2019 update: The problem still exists - this question's upvotes continue still. mar-2019 update: A year later a Google employee @AmiF commented on the state of things, stating that the problem doesn't exist, and anybody who seems to have this problem needs to simply reset their runtime to recover memory. Yet, the upvotes continue, which to me this tells that the problem still exists, despite @AmiF's suggestion to the contrary. dec-2018 update: I have a theory that Google may have a blacklist of certain accounts, or perhaps browser fingerprints, when its robots detect a non-standard behavior. It could be a total coincidence, but for quite some time I had an issue with Google Re-captcha on any website that happened to require it, where I'd have to go through dozens of puzzles before I'd be allowed through, often taking me 10+ min to accomplish. This lasted for many months. All of a sudden as of this month I get no puzzles at all and any google re-captcha gets resolved with just a single mouse click, as it used to be almost a year ago.  And why I'm telling this story? Well, because at the same time I was given 100% of the GPU RAM on Colab. That's why my suspicion is that if you are on a theoretical Google black list then you aren't being trusted to be given a lot of resources for free. I wonder if any of you find the same correlation between the limited GPU access and the Re-captcha nightmare. As I said, it could be totally a coincidence as well. Last night I ran your snippet and got exactly what you got: but today: I think the most probable reason is the GPUs are shared among VMs, so each time you restart the runtime you have chance to switch the GPU, and there is also probability you switch to one that is being used by other users. UPDATED:
It turns out that I can use GPU normally even when the GPU RAM Free is 504 MB, which I thought as the cause of ResourceExhaustedError I got last night.  If you execute a cell that just has
!kill -9 -1
in it, that'll cause all of your runtime's state (including memory, filesystem, and GPU) to be wiped clean and restarted.  Wait 30-60s and press the CONNECT button at the top-right to reconnect. Restart Jupyter IPython Kernel: Find the Python3 pid and kill the pid. Please see the below image Note: kill only python3(pid=130) not jupyter python(122). just give a heavy task to google colab, it will ask us to change to 25 gb of ram.   example run this code twice: then click on get more ram :)

  Im not sure if this blacklisting is true! Its rather possible, that the cores are shared among users. I ran also the test, and my results are the following: It seems im getting also full core. However i ran it a few times, and i got the same result. Maybe i will repeat this check a few times during the day to see if there is any change. I believe if we have multiple notebooks open. Just closing it doesn't actually stop the process. I haven't figured out how to stop it. But I used top to find PID of the python3 that was running longest and using most of the memory and I killed it. Everything back to normal now. Google Colab resource allocation is dynamic, based on users past usage. Suppose if a user has been using more resources recently and a new user who is less frequently uses Colab, he will be given relatively more preference in resource allocation. Hence to get the max out of Colab , close all your Colab tabs and all other active sessions, reset the runtime of the one you want to use. You'll definitely get better GPU allocation.I've been trying to use tensorflow for two days now installing and reinstalling it over and over again in python2.7 and 3.4.  No matter what I do, I get this error message when trying to use tensorflow.placeholder() It's very boilerplate code: No matter what I do I always get the trace back: Anyone know how I can fix this? If you have this error after an upgrade to TensorFlow 2.0, you can still use 1.X API by replacing: by Solution: Do not use "tensorflow" as your filename. Notice that you use tensorflow.py as your filename. And I guess you write code like: Then you are actually importing the script file "tensorflow.py" that is under your current working directory, rather than the "real" tensorflow module from Google. Here is the order in which a module will be searched when importing: The directory containing the input script (or the current directory when no file is specified).  PYTHONPATH (a list of directory names,
  with the same syntax as the shell variable PATH).  The installation-dependent default. It happened to me too. I had tensorflow and it was working pretty well, but when I install tensorflow-gpu along side the previous tensorflow this error arose then I did these 3 steps and it started working with no problem: conda remove tensorflow-gpu tensorflow tensorflow-base conda install tensorflow Instead of tf.placeholder(shape=[None, 2], dtype=tf.float32) use something like
tf.compat.v1.placeholder(shape=[None, 2], dtype=tf.float32) if you don't want to disable v2 completely. works.
I am using Python 3.7 and tensorflow 2.0. It appears that .placeholder() , .reset_default_graph() , and others were removed with version 2.  I ran into this issue using Docker image: tensorflow/tensorflow:latest-gpu-py3 which automatically pulls the latest version.  I was working in 1.13.1 and was 'upgraded to 2' automatically and started getting the error messages.  I fixed this by being more specific with my image: tensorflow/tensorflow:1.13.1-gpu-py3. More info can be found here:  https://www.tensorflow.org/alpha/guide/effective_tf2 Avoid using the below striked out statement in tensorflow=2.0 i̶m̶p̶o̶r̶t̶ ̶t̶e̶n̶s̶o̶r̶f̶l̶o̶w̶ ̶a̶s̶ ̶t̶f̶ ̶x̶ ̶=̶ ̶t̶f̶.̶p̶l̶a̶c̶e̶h̶o̶l̶d̶e̶r̶(̶s̶h̶a̶p̶e̶=̶[̶N̶o̶n̶e̶,̶ ̶2̶]̶,̶ ̶d̶t̶y̶p̶e̶=̶t̶f̶.̶f̶l̶o̶a̶t̶3̶2̶)̶ You can disable the v2 behavior by using the following code  This one is perfectly working for me. I also got the same error. May be because of the version of tensorflow. 
After installing tensorflow 1.4.0, I got relief from the error. If you are using TensorFlow 2.0, then some code developed for tf 1.x may not code work. Either you can follow the link : https://www.tensorflow.org/guide/migrate or you can install a previous version of tf by
pip3 install tensorflow==version Import the old version of tensorflow instead of the new version [https://inneka.com/ml/tf/tensorflow-module-object-has-no-attribute-placeholder/][1] import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() You need to use the keras model with tensorflow 2, as here Recent version 2.0 does not support placeholder. 
I uninstalled 2.0 using command: conda remove tensorflow.
then I installed 1.15.0 using command: conda install -c conda-forge tensorflow=1.15.0.
1.15 is latest in version 1 series. You can change as per you wish and requirement.
For seeing all version, use command: conda search tensorflow.
It worked for Anaconda3 in Windows. Try this: or this (if you have GPU): Please take a look at the Migrate your TensorFlow 1 code to TensorFlow 2. These codes: need to be migrated in TensorFlow 2 as below: If you get this on tensorflow 2.0.0+, it's very likely because the code isn't compatible with the newer version of tensorflow. To fix this, run the tf_upgrade_v2 script. Faced same issue on Ubuntu 16LTS when tensor flow was installed over existing python installation. Workaround:
1.)Uninstall tensorflow from pip and pip3 2.)Uninstall python & python3  3.)Install only a single version of  python(I used python 3) 4.)Install tensorflow to python3 for non GPU tensorflow, run this command  for GPU tensorflow, run below command Suggest not to install GPU and vanilla version of tensorflow The error shows up because we are using tensorflow version 2 and the command is from version 1. So if we use: It'll work Because you cant use placeholder in tensflow2.0version, so you need to use tensflow1*, or you need to change your code to fix tensflow2.0 I had the same problem before after tried to upgrade tensorflow, I solved it by reinstalling Tensorflow and Keras.  pip uninstall tensorflow  pip uninstall keras  Then: pip install tensorflow  pip install keras The problem is with TensorFlow version; the one you are running is 2.0 or something above 1.5, while placeholder can only work with 1.4. So simply uninstall TensorFlow, then install it again with version 1.4 and everything will work.  It may be the typo if you incorrectly wrote the placeholder word.
In my case I misspelled it as placehoder and got the error like this:
AttributeError: 'module' object has no attribute 'placehoder'I'm working on a classification problem with unbalanced classes (5% 1's). I want to predict the class, not the probability. In a binary classification problem, is scikit's classifier.predict() using 0.5 by default?
If it doesn't, what's the default method? If it does, how do I change it? In scikit some classifiers have the class_weight='auto' option, but not all do. With class_weight='auto', would .predict() use the actual population proportion as a threshold? What would be the way to do this in a classifier like MultinomialNB that doesn't support class_weight? Other than using predict_proba() and then calculation the classes myself. The threshold can be set using clf.predict_proba() for example: The threshold in scikit learn is 0.5 for binary classification and whichever class has the greatest probability for multiclass classification. In many problems a much better result may be obtained by adjusting the threshold. However, this must be done with care and NOT on the holdout test data but by cross validation on the training data. If you do any adjustment of the threshold on your test data you are just overfitting the test data. Most methods of adjusting the threshold is based on the receiver operating characteristics (ROC) and Youden's J statistic but it can also be done by other methods such as a search with a genetic algorithm. Here is a peer review journal article describing doing this in medicine: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2515362/  So far as I know there is no package for doing it in Python but it is relatively simple (but inefficient) to find it with a brute force search in Python. This is some R code that does it.  is scikit's classifier.predict() using 0.5 by default? In probabilistic classifiers, yes. It's the only sensible threshold from a mathematical viewpoint, as others have explained. What would be the way to do this in a classifier like MultinomialNB that doesn't support class_weight? You can set the class_prior, which is the prior probability P(y) per class y. That effectively shifts the decision boundary. E.g. You seem to be confusing concepts here. Threshold is not a concept for a "generic classifier" - the most basic approaches are based on some tunable threshold, but most of the existing methods create complex rules for classification which cannot (or at least shouldn't) be seen as a thresholding. So first - one cannot answer your question for scikit's classifier default threshold because there is no such thing. Second - class weighting is not about threshold, is about classifier ability to deal with imbalanced classes, and it is something dependent on a particular classifier. For example - in SVM case it is the way of weighting the slack variables in the optimization problem, or if you prefer - the upper bounds for the lagrange multipliers values connected with particular classes. Setting this to 'auto' means using some default heuristic, but once again - it cannot be simply translated into some thresholding. Naive Bayes on the other hand directly estimates the classes probability from the training set. It is called "class prior" and you can set it in the constructor with "class_prior" variable. From the documentation: Prior probabilities of the classes. If specified the priors are not adjusted according to the data. In case someone visits this thread hoping for ready-to-use function (python 2.7). In this example cutoff is designed to reflect ratio of events to non-events in original dataset df, while y_prob could be the result of .predict_proba method (assuming stratified train/test split). Feel free to criticize/modify. Hope it helps in rare cases when class balancing is out of the question and the dataset itself is highly imbalanced.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 4 years ago. By processing a time series graph, I Would like to detect patterns that look similar to this:  Using a sample time series as an example, I would like to be able to detect the patterns as marked here:  What kind of AI algorithm (I am assuming marchine learning techniques) do I need to use to achieve this? Is there any library (in C/C++) out there that I can use? Here is a sample result from a small project I did to partition ecg data.  My approach was a "switching autoregressive HMM" (google this if you haven't heard of it) where each datapoint is predicted from the previous datapoint using a Bayesian regression model. I created 81 hidden states: a junk state to capture data between each beat, and 80 separate hidden states corresponding to different positions within the heartbeat pattern. The pattern 80 states were constructed directly from a subsampled single beat pattern and had two transitions - a self transition and a transition to the next state in the pattern. The final state in the pattern transitioned to either itself or the junk state. I trained the model with Viterbi training, updating only the regression parameters. Results were adequate in most cases. A similarly structure Conditional Random Field would probably perform better, but training a CRF would require manually labeling patterns in the dataset if you don't already have labelled data. Edit: Here's some example python code - it is not perfect, but it gives the general approach. It implements EM rather than Viterbi training, which may be slightly more stable.
The ecg dataset is from http://www.cs.ucr.edu/~eamonn/discords/ECG_data.zip Why not using a simple matched filter? Or its general statistical counterpart called cross correlation. Given a known pattern x(t) and a noisy compound time series containing your pattern shifted in a,b,...,z like y(t) = x(t-a) + x(t-b) +...+ x(t-z) + n(t). The cross correlation function between x and y should give peaks in a,b, ...,z Weka is a powerful collection of machine-learning software, and supports some time-series analysis tools, but I do not know enough about the field to recommend a best method. However, it is Java-based; and you can call Java code from C/C++ without great fuss. Packages for time-series manipulation are mostly directed at the stock-market. I suggested Cronos in the comments; I have no idea how to do pattern recognition with it, beyond the obvious: any good model of a length of your series should be able to predict that, after small bumps at a certain distance to the last small bump, big bumps follow. That is, your series exhibits self-similarity, and the models used in Cronos are designed to model it. If you don't mind C#, you should request a version of TimeSearcher2 from the folks at HCIL - pattern recognition is, for this system, drawing what a pattern looks like, and then checking whether your model is general enough to capture most instances with a low false-positive rate. Probably the most user-friendly approach you will find; all others require quite a background in statistics or pattern recognition strategies. I'm not sure what package would work best for this. I did something similar at one point in college where I tried to automatically detect certain similar shapes on an x-y axis for a bunch of different graphs. You could do something like the following. Class labels like: Features like: I am using deep learning if it's an option for you.  It's done in Java, Deeplearning4j. I am experimenting with LSTM. I tried 1 hidden layer and 2 hidden layers to process time series.  Found a few things:I'd like to reset (randomize) the weights of all layers in my Keras (deep learning) model. The reason is that I want to be able to train the model several times with different data splits without having to do the (slow) model recompilation every time. Inspired by this discussion, I'm trying the following code: However, it only partly works. Partly, becuase I've inspected some layer.get_weights() values, and they seem to change. But when I restart the training, the cost values are much lower than the initial cost values on the first run. It's almost like I've succeeded resetting some of the weights, but not all of them. Save the initial weights right after compiling the model but before training it: and then after training, "reset" the model by reloading the initial weights: This gives you an apples to apples model to compare different data sets and should be quicker than recompiling the entire model. Reset all layers by checking for initializers: Update: kernel_initializer is kernel.initializer now. If you want to truly re-randomize the weights, and not merely restore the initial weights, you can do the following. The code is slightly different depending on whether you're using TensorFlow or Theano. I have found the clone_model function that creates a cloned network with the same architecture but new model weights. Example of use: Comparing the weights: If you execute this code several times, you will notice that the cloned model receives new weights each time. Tensorflow 2 answer: Original weights: New weights: Try set_weights. for example: build a model with say, two convolutional layers then define your weights (i'm using a simple w, but you could use np.random.uniform or anything like that if you want) Take a peek at what are the layers inside a model Set each weight for each convolutional layer (you'll see that the first layer is actually input and you don't want to change that, that's why the range starts from 1 not zero). Generate some input for your test and predict the output from your model You could change it again if you want and check again for the output: Sample output: From your peek at .layers you can see that the first layer is input and the others your convolutional layers. For tf2 the simplest way to actually reset weights would be: clone_model() as mentioned by @danielsaromo returns new model with trainable params initialized from scratch, we use its weights to reinitialize our model thus no model compilation (knowledge about its loss or optimizer) is needed. There are two caveats though, first is mentioned in clone_model()'s documentation: clone_model will not preserve the uniqueness of shared objects within the model (e.g. a single variable attached to two distinct layers will be restored as two separate variables). Another caveat is that for large models cloning might fail due to memory limit. To "random" re-initialize weights of a compiled untrained model in TF 2.0 (tf.keras): Note the "if wdim > 1 else w". You don't want to re-initialize the biases (they stay 0 or 1). use keras.backend.clear_session()I have a Java app which needs to perform partial least squares regression. It would appear there are no Java implementations of PLSR out there. Weka might have had something like it at some point, but it is no longer in the API. On the other hand, I have found a good R implementation, which has an added bonus to it. It was used by the people whose result I want to replicate, which means there is less chance that things will go wrong because of differences in the way PLSR is implemented. The question is: is there a good enough (and simple to use) package that enable Java to call R, pass in some parameters to a function and read back the results? My other option is to have Java spawn R in a Process and then monitor it. Data would be read and written to disk. Which of the two would you recommend? Am I missing the obvious third option? I have successfully used two alternatives in the past. JRI RServe Other alternatives I have never used : RCaller There has been work by Duncan Temple Lang: http://rss.acs.unt.edu/Rdoc/library/SJava/Docs/RFromJava.pdf .  My guess as to the most robust solution would be JGR. The developers of JGR have a mailing list, Stats-Rosuda and the mailing list Archive indicates the list remains active as of 2013. There is also code that has been put up at Googlecode, with an example here:
http://stdioe.blogspot.com/2011/07/rcaller-20-calling-r-from-java.html This is an old question.. but for anyone browsing through here that is still interested: I wrote a blog article that provides a detailed example of how to use JRI/rjava (a JNI based bridge) to do this type of thing (the how-to is focused on Linux dev environments).  I also compare and contrast alternative approaches for doing 'mathy' stuff by calling out to R and similar frameworks. URL > http://buildlackey.com/integrating-r-and-java-with-jrirjava-a-jni-based-bridge/ Renjin is an alternative that allows not only the integration of many packages of R also a easy going communication between Java and R through objects: http://www.renjin.org/ JRI has both low level and High level interface to Call R from Java. There is an eclipse plugin that helps in setting up the R Java environment at http://www.studytrails.com/RJava-Eclipse-Plugin/.  This seems to be an old question. However Rserve and rJava are two good packages to integrate R with Java. Following blogs explain usage of both these libraries. For rJava: http://www.codophile.com/how-to-integrate-r-with-java-using-rjava/ For Rserve: http://www.codophile.com/how-to-integrate-r-with-java-using-rserve/ I hope this will help. I had similar need a while back and tested a few of the interfaces to R.  The one I found to be the best for my needs (windows, c#) was Rserve which I believe is written in Java.  My only gripe with it is that it wasn't 64-bit.  I used a simple client written in c# and it worked very well.  I'm sure the Java client is a lot better. FastR is a GraalVM based implementation of R. Embedding it in a JVM application is as simple as: More details in this article: https://medium.com/graalvm/faster-r-with-fastr-4b8db0e0dcebCan anyone tell me why we set random state to zero in splitting train and test set. I have seen situations like this where random state is set to 1! What is the consequence of this random state in cross validation as well? It doesn't matter if the random_state is 0 or 1 or any other integer. What matters is that it should be set the same value, if you want to validate your processing over multiple runs of the code. By the way I have seen random_state=42 used in many official examples of scikit as well as elsewhere also. random_state as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case. In the documentation, it is stated that: If random_state is None or np.random, then a randomly-initialized RandomState object is returned. If random_state is an integer, then it is used to seed a new RandomState object. If random_state is a RandomState object, then it is passed through. This is to check and validate the data when running the code multiple times. Setting random_state a fixed value will guarantee that same sequence of random numbers are generated each time you run the code. And unless there is some other randomness present in the process, the results produced will be same as always. This helps in verifying the output. when random_state set to an integer, train_test_split will return same results for each execution. when random_state set to an None, train_test_split will return different results for each execution. see below example: Output: [2, 8, 4] [2, 8, 4] [2, 8, 4] [2, 8, 4] [2, 8, 4] [4, 7, 6] [4, 3, 7] [8, 1, 4] [9, 5, 8] [6, 4, 5] If you don't mention the random_state in the code, then whenever you execute your code a new random value is generated and the train and test datasets would have different values each time. However, if you use a particular value for random_state(random_state = 1 or any other value) everytime the result will be same,i.e, same values in train and test datasets. The random_state splits a randomly selected data but with a twist. And the twist is the order of the data will be same for a particular value of random_state.You need to understand that it's not a bool accpeted value. starting from 0 to any integer no, if you pass as random_state,it'll be a permanent order for it. Ex: the order you will get in random_state=0 remain same. After that if you execuit random_state=5 and again come back to random_state=0 you'll get the same order. And like 0 for all integer will go same.
How ever random_state=None splits randomly each time. If still having doubt watch this  If you don't specify the random_state in your code, then every time you run(execute) your code a new random value is generated and the train and test datasets would have different values each time. However, if a fixed value is assigned like random_state = 0 or 1 or 42 then no matter how many times you execute your code the result would be the same .i.e, same values in train and test datasets. random_state is None by default which means every time when you run your program you will get different output because of splitting between train and test varies within. random_state = any int value means every time when you run your program you will get tehe same output because of splitting between train and test does not varies within. The random_state is an integer value which implies the selection of a random combination of train and test. When you set the test_size as 1/4 the there is a set generated of permutation and combination of train and test and each combination has one state.
Suppose you have a dataset---> [1,2,3,4] We need it because while param tuning of model same state will considered again and again.
So that there won't be any inference with the accuracy. But in case of Random forest there is also similar story but in a different way w.r.t the variables. We used the random_state parameter for reproducibility of the initial shuffling of training datasets after each epoch. For multiple times of execution of our model, random state  make sure that data values  will be same for training and testing data sets. It fixes the order of data for train_test_split Lets say our dataset is having one feature and 10data points. X=[0,1,2,3,4,5,6,7,8,9]
and lets say 0.3(30% is testset) is  specified as test data percentage then we are going to have 10C3=120 different combinations of data.[Refer picture in link for tabular explanation]: https://i.stack.imgur.com/FZm4a.png Based  on the random number specified system will pick random state and assigns train and test data In addition to what already said, different values of random state may produce different results during the training phase. Internally, the train_test_split() function uses a seed that allows you to pseudorandomly separate the data into two groups: training and test set. The number is pseudorandom because the same data subdivision corresponds to the same seed value. This aspect is very useful to ensure the reproducibility of the experiments. Unfortunately, the use of one seed rather than another could lead to totally different datasets, and even modify the performance of the chosen Machine Learning model that receives the training set as input. You can read the following article to deepen this aspect:
https://towardsdatascience.com/why-you-should-not-trust-the-train-test-split-function-47cb9d353ad2 The article also shows a practical example. You can also find other considerations in this article:
https://towardsdatascience.com/is-a-small-dataset-risky-b664b8569a21I have a dataset and I want to train my model on that data. After training, I need to know the features that are major contributors in the classification for a SVM classifier.  There is something called feature importance for forest algorithms, is there anything similar? Yes, there is attribute coef_ for SVM classifier but it only works for SVM with linear kernel. For other kernels it is not possible because data are transformed by kernel method to another space, which is not related to input space, check the explanation. And the output of the function looks like this:
 If you're using rbf (Radial basis function) kernal, you can use sklearn.inspection.permutation_importance as follows to get feature importance. [doc]  In only one line of code: fit an SVM model: and implement the plot as follows: The resuit will be: the most contributing features of the SVM model in absolute values I created a solution which also works for Python 3 and is based on Jakub Macina's code snippet.Can the Keras deal with input images with different size? For example, in the fully convolutional neural network, the input images can have any size. However, we need to specify the input shape when we create a network by Keras. Therefore, how can we use Keras to deal with different input size without resizing the input images to the same size? Thanks for any help. Yes.
Just change your input shape to shape=(n_channels, None, None).
Where n_channels is the number of channels in your input image. I'm using Theano backend though, so if you are using tensorflow you might have to change it to (None,None,n_channels) You should use: input_shape=(1, None, None) None in a shape denotes a variable dimension. Note that not all layers
  will work with such variable dimensions, since some layers require
  shape information (such as Flatten).
  https://github.com/fchollet/keras/issues/1920 For example, using keras's functional API your input layer would be: For a RGB dataset For a Gray dataset Implementing arbitrarily sized input arrays with the same computational kernels can pose many challenges - e.g. on a GPU, you need to know how big buffers to reserve, and more weakly how much to unroll your loops, etc.  This is the main reason that Keras requires constant input shapes, variable-sized inputs are too painful to deal with. This more commonly occurs when processing variable-length sequences like sentences in NLP. The common approach is to establish an upper bound on the size (and crop longer sequences), and then pad the sequences with zeros up to this size. (You could also include masking on zero values to skip computations on the padded areas, except that the convolutional layers in Keras might still not support masked inputs...) I'm not sure if for 3D data structures, the overhead of padding is not prohibitive - if you start getting memory errors, the easiest workaround is to reduce the batch size.  Let us know about your experience with applying this trick on images! Just use None while specifying input shape. But I still do not know how to pass different-shaped images into fit function.We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 2 years ago. It seems like R is really designed to handle datasets that it can pull entirely into memory. What R packages are recommended for signal processing and machine learning on very large datasets that can not be pulled into memory?  If R is simply the wrong way to do this, I am open to other robust free suggestions (e.g. scipy if there is some nice way to handle very large datasets) Have a look at the "Large memory and out-of-memory data" subsection of the high performance computing task view on CRAN. bigmemory and ff are two popular packages. For bigmemory (and the related biganalytics, and bigtabulate), the bigmemory website has a few very good presentations, vignettes, and overviews from Jay Emerson. For ff, I recommend reading Adler Oehlschlägel and colleagues' excellent slide presentations on the ff website.  Also, consider storing data in a database and reading in smaller batches for analysis. There are likely any number of approaches to consider. To get started, consdier looking through some of the examples in the biglm package, as well as this presentation from Thomas Lumley. And do investigate the other packages on the high-performance computing task view and mentioned in the other answers. The packages I mention above are simply the ones I've happened to have more experience with. I think the amount of data you can process is more limited by ones programming skills than anything else. Although a lot of standard functionality is focused on in memory analysis, cutting your data into chunks already helps a lot. Ofcourse, this takes more time to program than picking up standard R code, but often times it is quite possible.  Cutting up data can for exale be done using read.table or readBin which support only reading a subset of the data. Alternatively, you can take a look at the high performance computing task view for packages which deliver out of the box out of memory functionality. You could also put your data in a database. For spatial raster data, the excellent raster package provides out of memory analysis. For machine learning tasks I can recommend using biglm package, used to do "Regression for data too large to fit in memory". For using R with really big data, one can use Hadoop as a backend and then use package rmr to perform statistical (or other) analysis via MapReduce on a Hadoop cluster. It all depends on algorithms you need. If they may be translated into incremental form (when only small part of data is needed at any given moment, e.g. for Naive Bayes you can hold in memory only the model itself and current observation being processed), then the best suggestion is to perform machine learning incrementally, reading new batches of data from disk.  However, many algorithms and especially their implementations really require the whole dataset. If size of the dataset fits you disk (and file system limitations), you can use mmap package that allows to map file on disk to memory and use it in the program. Note however, that read-writes to disk are expensive, and R sometimes likes to move data back and forth frequently. So be careful.  If your data can't be stored even on you hard drive, you will need to use distributed machine learning systems. One such R-based system is Revolution R which is designed to handle really large datasets. Unfortunately, it is not open source and costs quite a lot of money, but you may try to get free academic license. As alternative, you may be interested in Java-based Apache Mahout - not so elegant, but very efficient solution, based on Hadoop and including many important algorithms.  If the memory is not sufficient enough, one solution is push data to disk and using distributed computing. I think RHadoop(R+Hadoop) may be one of the solution to tackle with large amount dataset.I need to cluster a simple univariate data set into a preset number of clusters. Technically it would be closer to binning or sorting the data since it is only 1D, but my boss is calling it clustering, so I'm going to stick to that name. 
The current method used by the system I'm on is K-means, but that seems like overkill. Is there a better way of performing this task? Answers to some other posts are mentioning KDE (Kernel Density Estimation), but that is a density estimation method, how would that work?  I see how KDE returns a density, but how do I tell it to split the data into bins?  How do I have a fixed number of bins independent of the data (that's one of my requirements) ?  More specifically, how would one pull this off using scikit learn?  My input file looks like:  I want to group the sls number into clusters or bins, such that: And my output file will look like:  Write code yourself. Then it fits your problem best! Boilerplate: Never assume code you download from the net to be correct or optimal... make sure to fully understand it before using it.  Your clusters therefore are and visually, we did this split:  We cut at the red markers. The green markers are our best estimates for the cluster centers. There is a little error in the accepted answer by @Has QUIT--Anony-Mousse (I can't comment nor suggest an edit due my reputation). The line: Should be edited into: That's because mi and ma is an index, where s[mi] and s[ma] is the value. If you use mi[0] as the limit, you risk and error splitting if your upper and lower linspace >> your upper and lower data. For example, run this code and see the difference in split result: result: Further improving the responses above by @yasirroni, to dynamically print all clusters (not just 3 from the above) the line: can be changed into: This would ensure that all the clusters are taken into account.How can you write a python script to read Tensorboard log files, extracting the loss and accuracy and other numerical data, without launching the GUI tensorboard --logdir=...? You can use TensorBoard's Python classes or script to extract the data: How can I export data from TensorBoard? If you'd like to export data to visualize elsewhere (e.g. iPython Notebook), that's possible too. You can directly depend on the underlying classes that TensorBoard uses for loading data: python/summary/event_accumulator.py (for loading data from a single run) or python/summary/event_multiplexer.py (for loading data from multiple runs, and keeping it organized). These classes load groups of event files, discard data that was "orphaned" by TensorFlow crashes, and organize the data by tag. As another option, there is a script (tensorboard/scripts/serialize_tensorboard.py) which will load a logdir just like TensorBoard does, but write all of the data out to disk as json instead of starting a server. This script is setup to make "fake TensorBoard backends" for testing, so it is a bit rough around the edges. Using EventAccumulator: size_guidance: To finish user1501961's answer, you can then just export the list of scalars to a csv file easily with pandas pd.DataFrame(ea.Scalars('Loss)).to_csv('Loss.csv') For anyone interested, I've adapted user1501961's answer into a function for parsing tensorboard scalars into a dictionary of pandas dataframes: Try this: bat is optimal.I'm building a model that converts a string to another string using recurrent layers (GRUs). I have tried both a Dense and a TimeDistributed(Dense) layer as the last-but-one layer, but I don't understand the difference between the two when using return_sequences=True, especially as they seem to have the same number of parameters. My simplified model is the following: The summary of the network is: This makes sense to me as my understanding of TimeDistributed is that it applies the same layer at all timepoints, and so the Dense layer has 16*15+15=255 parameters (weights+biases). However, if I switch to a simple Dense layer: I still only have 255 parameters: I wonder if this is because Dense() will only use the last dimension in the shape, and effectively treat everything else as a batch-like dimension. But then I'm no longer sure what the difference is between Dense and TimeDistributed(Dense). Update Looking at https://github.com/fchollet/keras/blob/master/keras/layers/core.py it does seem that Dense uses the last dimension only to size itself: It also uses keras.dot to apply the weights: The docs of keras.dot imply that it works fine on n-dimensional tensors. I wonder if its exact behavior means that Dense() will in effect be called at every time step. If so, the question still remains what TimeDistributed() achieves in this case. TimeDistributedDense applies a same dense to every time step during GRU/LSTM Cell unrolling. So the error function will be between predicted label sequence and the actual label sequence. (Which is normally the requirement for sequence to sequence labeling problems). However, with return_sequences=False, Dense layer is applied only once at the last cell. This is normally the case when RNNs are used for classification problem. If return_sequences=True then Dense layer is applied to every timestep just like TimeDistributedDense. So for as per your models both are same, but if you change your second model to return_sequences=False, then Dense will be applied only at the last cell. Try changing it and the model will throw as error because then the Y will be of size [Batch_size, InputSize], it is no more a sequence to sequence but a full sequence to label problem. In the above example architecture of model1 and model2 are sample (sequence to sequence models) and model3 is a full sequence to label model. Here is a piece of code that verifies TimeDistirbuted(Dense(X)) is identical to Dense(X): (2, 4, 3) (3, 5) (2, 4, 5) (2, ?, 5) And the difference is:I'm looking into clustering points on a map (latitude/longitude). Are there any recommendations as to a suitable algorithm that is fast and scalable? More specifically, I have a series of latitude/longitude coordinates and a map viewport. I'm trying to cluster the points that are close together in order to remove clutter. I already have a solution to the problem (see here), only I am wondering if there is any formal algorithm that solves the problem efficiently. For a virtual earth application I've used the clustering described 
here. It's lightning fast and easily extensible. Google Maps Hacks has a hack, "Hack 69. Cluster Markers at High Zoom Levels", on that. Also, see Wikipedia on clustering algorithms. You could look at indexing all your points using a QuadTile scheme, and then based upon the scale the further down the quad-splits you go. All similarly located points will then be near each other in your index, allowing the clustering to happen efficiently. QuadTiles are an example of Morton Codes, and there is a python example linked from that wikipedia article that may help. I looked at various libraries and found them so complex couldn't understand a word so I decided to make my own clustering algorithm Here goes my code in Java // This calculates the pixel distance between tow lat long points at a particular zoom level  // The main function which actually calculates the clusters
1. ArrayList of lat long points is iterated to length .
2. inner loop a copy of the same arraylist is iterated from i+1 position ie leaving the top loop's index
3. 0th element is taken as the centre of centroid and all other points are compared if their pixel distance is very less add it into cluster
4. remove all  elements from top arraylist and copy arraylist which have formed cluster
5 restart the process by reinitializing the index from 0;
6 if the centroid selected has no clusters then that element is not deletedIs it possible to use GridSearchCV without cross validation? I am trying to optimize the number of clusters in KMeans clustering via grid search, and thus I don't need or want cross validation.  The documentation is also confusing me because under the fit() method, it has an option for unsupervised learning (says to use None for unsupervised learning). But if you want to do unsupervised learning, you need to do it without cross validation and there appears to be no option to get rid of cross validation. After much searching, I was able to find this thread. It appears that you can get rid of cross validation in GridSearchCV if you use: cv=[(slice(None), slice(None))] I have tested this against my own coded version of grid search without cross validation and I get the same results from both methods. I am posting this answer to my own question in case others have the same issue. Edit: to answer jjrr's question in the comments, here is an example use case: I'm going to answer your question since it seems like it has been unanswered still. Using the parallelism method with the for loop, you can use the multiprocessing module. I think that using cv=ShuffleSplit(test_size=0.20, n_splits=1) with n_splits=1 is a better solution like this post suggested I recently came out with the following custom cross-validator, based on this answer. I passed it to GridSearchCV and it properly disabled the cross-validation for me: I hope it can help.I am training a simple model in keras for NLP task with following code. Variable names are self explanatory for train, test and validation set. This dataset has 19 classes so final layer of the network has 19 outputs. Labels are also one-hot encoded. After first epoch, this gives me these outputs. Then I evaluate my model on testing dataset and this also shows me accuracy around 0.98. However, the labels are one-hot encoded, so I need prediction vector of classes so that I can generate confusion matrix etc. So I use, This shows that total predicted classes were 83% accurate however model1.evaluate shows 98% accuracy!! What am I doing wrong here? Is my loss function okay with categorical class labels? Is my choice of sigmoid activation function for prediction layer okay? or there is difference in the way keras evaluates a model? Please suggest on what can be wrong. This is my first try to make a deep model so I don't have much understanding of what's wrong here. I have found the problem. metrics=['accuracy'] calculates accuracy automatically from cost function. So using binary_crossentropy shows binary accuracy, not categorical accuracy. Using categorical_crossentropy automatically switches to categorical accuracy and now it is the same as calculated manually using model1.predict(). Yu-Yang was right to point out the cost function and activation function for multi-class problem. P.S: One can get both categorical and binary accuracy by using metrics=['binary_accuracy', 'categorical_accuracy']I'm getting this error 'ValueError: Tensor Tensor("Placeholder:0", shape=(1, 1), dtype=int32)
  is not an element of this graph.' The code is running perfectly fine without with tf.Graph(). as_default():. However I need to call M.sample(...) multiple times and each time the memory won't be free after session.close(). Probably there is a memory leak but not sure where is it. I want to restore a pre-trained neural network, set it as default graph, and testing it multiple times (like 10000) over the default graph without making it larger each time. The code is: And the model is: and the output is: Try first:  Then, when you need to use predict: When you create a Model, the session hasn't been restored yet. All placeholders, variables and ops that are defined in Model.__init__ are placed in a new graph, which makes itself a default graph inside with block. This is the key line:  This means that this instance of tf.Graph() equals to tf.get_default_graph() instance inside with block, but not before or after it. From this moment on, there exist two different graphs. When you later create a session and restore a graph into it, you can't access the previous instance of tf.Graph() in that session. Here's a short example: The best way to deal with this is give names to all nodes, e.g. 'input', 'target', etc, save the model and then look up the nodes in the restored graph by name, something like this: This method guarantees that all nodes will be from the graph in session. If you are calling the python function that calls Tensorflow from an external module, make sure that you the model isn't being loaded as a global variable or else it may not be loaded in time for usage. This happened to me calling a Tensorflow model from the Flask server. Use this line before making models: This will make a new graph to use in new models. For me, this issue was resolved by using Keras' APIs to save and load model. I had more than one models being trained in my code and I had to use the particular model for prediction under a condition. So I saved the entire model to a HDF5 file after model training and then recreate/reload the saved model at the time of prediction This helped me get rid of error. Inside
def LoadPredictor(save):
Just after loading the model, add model._make_predict_function()
So the function becomes:
 I had this issue when trying to make a model using another class that uses keras to create a model. I got this issue corrected by doing the followingI'm trying to understand GMM by reading the sources available online. I have achieved clustering using K-Means and was seeing how GMM would compare to K-means. Here is what I have understood, please let me know if my concept is wrong: GMM is like KNN, in the sense that clustering is achieved in both cases. But in GMM each cluster has their own independent mean and covariance. Furthermore k-means performs hard assignments of data points to clusters whereas in GMM we get a collection of independant gaussian distributions, and for each data point we have a probability that it belongs to one of the distributions. To understand it better I have used MatLab to code it and achieve the desired clustering. I have used SIFT features for the purpose of feature extraction. And have used k-means clustering to initialize the values. (This is from the VLFeat documentation) Based on the above I have means, covariances and priors. My main question is, What now? I am kind of lost now. Also the means, covariances vectors are each of the size 128 x 50. I was expecting them to be 1 x 50 since each column is a cluster, wont each cluster have only one mean and covariance? (I know 128 are the SIFT features but I was expecting means and covariances). In k-means I used the the MatLab command knnsearch(X,Y) which basically finds the nearest neighbour in X for each point in Y.  So how to achieve this in GMM, I know its a collection of probabilities, and ofcourse the nearest match from that probability will be our winning cluster. And this is where I am confused. 
All tutorials online have taught how to achieve the means, covariances values, but do not say much in how to actually use them in terms of clustering. Thank you I think it would help if you first look at what a GMM model represents. I'll be using functions from the Statistics Toolbox, but you should be able to do the same using VLFeat. Let's start with the case of a mixture of two 1-dimensional normal distributions. Each Gaussian is represented by a pair of mean and variance. The mixture assign a weight to each component (prior). For example, lets mix two normal distributions with equal weights (p = [0.5; 0.5]), the first centered at 0 and the second at 5 (mu = [0; 5]), and the variances equal 1 and 2 respectively for the first and second distributions (sigma = cat(3, 1, 2)). As you can see below, the mean effectively shifts the distribution, while the variance determines how wide/narrow and flat/pointy it is. The prior sets the mixing proportions to get the final combined model.  The idea of EM clustering is that each distribution represents a cluster. So in the example above with one dimensional data, if you were given an instance x = 0.5, we would assign it as belonging to the first cluster/mode with 99.5% probability you can see how the instance falls well under the first bell-curve. Whereas if you take a point in the middle, the answer would be more ambiguous (point assigned to class=2 but with much less certainty): The same concepts extend to higher dimension with multivariate normal distributions. In more than one dimension, the covariance matrix is a generalization of variance, in order to account for inter-dependencies between features. Here is an example again with a mixture of two MVN distributions in 2-dimensions:  There is some intuition behind how the the covariance matrix affects the shape of the joint density function. For instance in 2D, if the matrix is diagonal it implies that the two dimensions don't co-vary. In that case the PDF would look like an axis-aligned ellipse stretched out either horizontally or vertically according to which dimension has the bigger variance. If they are equal, then the shape is a perfect circle (distribution spread out in both dimensions at an equal rate). Finally if the covariance matrix is arbitrary (non-diagonal but still symmetric by definition), then it will probably look like a stretched ellipse rotated at some angle. So in the previous figure, you should be able to tell the two "bumps" apart and what individual distribution each represent. When you go 3D and higher dimensions, think of the it as representing (hyper-)ellipsoids in N-dims.  Now when you're performing clustering using GMM, the goal is to find the model parameters (mean and covariance of each distribution as well as the priors) so that the resulting model best fits the data. The best-fit estimation translates into maximizing the likelihood of the data given the GMM model (meaning you choose model that maximizes Pr(data|model)). As other have explained, this is solved iteratively using the EM algorithm; EM starts with an initial estimate or guess of the parameters of the mixture model. It iteratively re-scores the data instances against the mixture density produced by the parameters. The re-scored instances are then used to update the parameter estimates. This is repeated until the algorithm converges. Unfortunately the EM algorithm is very sensitive to the initialization of the model, so it might take a long time to converge if you set poor initial values, or even get stuck in local optima. A better way to initial the GMM parameters is to use K-means as a first step (like you've shown in your code), and using the mean/cov of those clusters to initialize EM. As with other cluster analysis techniques, we first need to decide on the number of clusters to use. Cross-validation is a robust way to find a good estimate of the number of clusters. EM clustering suffers from the fact that there a lot parameters to fit, and usually requires lots of data and many iterations to get good results. An unconstrained model with M-mixtures and D-dimensional data involves fitting D*D*M + D*M + M parameters (M covariance matrices each of size DxD, plus M mean vectors of length D, plus a vector of priors of length M). That could be a problem for datasets with large number of dimensions. So it is customary to impose restrictions and assumption to simplify the problem (a sort of regularization to avoid overfitting problems). For instance you could fix the covariance matrix to be only diagonal or even have the covariance matrices shared across all Gaussians. Finally once you've fitted the mixture model, you can explore the clusters by computing the posterior probability of data instances using each mixture component (like I've showed with the 1D example). GMM assigns each instance to a cluster according to this "membership" likelihood. Here is a more complete example of clustering data using Gaussian mixture models:  You are right, there is the same insight behind clustering with K-Means or GMM.
But as you mentionned Gaussian Mixtures take data covariances into account.
To find the maximum likelihood parameters (or maximum a posteriori MAP) of the GMM statistical model, you need to use an iterative process called the EM algorithm. Each iteration is composed of a E-step (Expectation) and a M-step (Maximization) and repeat until convergence.
After convergence you can easily estimate the membership probabilities of each data vectors for each cluster model. Covariance tells you how the data varies in the space, if a distribution has large covariance, that means data is more spread and vice versa. When you have the PDF of a gaussian distribution (mean and covariance params), you can check the membership confidence of a test point under that distribution.  However GMM also suffers from the weakness of K-Means, that you have to pick the parameter K which is the number of clusters. This requires a good understanding of your data's multimodality.While tuning the hyperparameters to get my model to perform better, I noticed that the score I get (and hence the model that is created) is different every time I run the code despite fixing all the seeds for random operations. This problem does not happen if I run on CPU. I googled and found out that this is a common issue when using a GPU to train. Here is a very good/detailed example with short code snippets to verify the existence of that problem. They pinpointed the non-determinism to "tf.reduce_sum" function. However, that is not the case for me. it could be because I'm using different hardware (1080 TI) or a different version of CUDA libraries or Tensorflow. It seems like there are many different parts of the CUDA libraries that are non-deterministic and it doesn't seem easy to figure out exactly which part and how to get rid of it. Also, this must have been by design, so it's likely that there is a sufficient efficiency increase in exchange for non-determinism. So, my question is: Since GPUs are popular for training NNs, people in this field must have a way to deal with non-determinism, because I can't see how else you'd be able to reliably tune the hyperparameters. What is the standard way to handle non-determinism when using a GPU? TL;DR That, but much longer When you see neural network operations as mathematical operations, you would expect everything to be deterministic. Convolutions, activations, cross-entropy – everything here are mathematical equations and should be deterministic. Even pseudo-random operations such as shuffling, drop-out, noise and the likes, are entirely determined by a seed. When you see those operations from their computational implementation, on the other hand, you see them as massively parallelized computations, which can be source of randomness unless you are very careful. The heart of the problem is that, when you run operations on several parallel threads, you typically do not know which thread will end first. It is not important when threads operate on their own data, so for example, applying an activation function to a tensor should be deterministic. But when those threads need to synchronize, such as when you compute a sum, then the result may depend on the order of the summation, and in turn, on the order in which thread ended first. From there, you have broadly speaking two options: Keep non-determinism associated with simpler implementations. Take extra care in the design of your parallel algorithm to reduce or remove non-determinism in your computation. The added constraint usually results in slower algorithms Which route takes CuDNN? Well, mostly the deterministic one. In recent releases, deterministic operations are the norm rather than the exception. But it used to offer many non-deterministic operations, and more importantly, it used to not offer some operations such as reduction, that people needed to implement themselves in CUDA with a variable degree of consideration to determinism. Some libraries such as theano were more ahead of this topic, by exposing early on a deterministic flag that the user could turn on or off – but as you can see from its description, it is far from offering any guarantee. If more, sometimes we will select some implementations that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementation, e.g. when we do not have a GPU implementation that is deterministic. Also, see the dnn.conv.algo* flags to cover more cases. In TensorFlow, the realization of the need for determinism has been rather late, but it's slowly getting there – helped by the advance of CuDNN on that front also. For a long time, reductions have been non-deterministic, but now they seem to be deterministic. The fact that CuDNN introduced deterministic reductions in version 6.0 may have helped of course. It seems that currently, the main obstacle for TensorFlow towards determinism is the backward pass of the convolution. It is indeed one of the few operations for which CuDNN proposes a non-deterministic algorithm, labeled CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0. This algorithm is still in the list of possible choices for the backward filter in TensorFlow. And since the choice of the filter seems to be based on performance, it could indeed be picked if it is more efficient. (I am not so familiar with TensorFlow's C++ code so take this with a grain of salt.) Is this important? If you are debugging an issue, determinism is not merely important: it is mandatory. You need to reproduce the steps that led to a problem. This is currently a real issue with toolkits like TensorFlow. To mitigate this problem, your only option is to debug live, adding checks and breakpoints at the correct locations – not great. Deployment is another aspect of things, where it is often desirable to have a deterministic behavior, in part for human acceptance. While nobody would reasonably expect a medical diagnosis algorithm to never fail, it would be awkward that a computer could give the same patient a different diagnosis depending on the run. (Although doctors themselves are not immune to this kind of variability.) Those reasons are rightful motivations to fix non-determinism in neural networks. For all other aspects, I would say that we need to accept, if not embrace, the non-deterministic nature of neural net training. For all purposes, training is stochastic. We use stochastic gradient descent, shuffle data, use random initialization and dropout – and more importantly, training data is itself but a random sample of data. From that standpoint, the fact that computers can only generate pseudo-random numbers with a seed is an artifact. When you train, your loss is a value that also comes with a confidence interval due to this stochastic nature. Comparing those values to optimize hyper-parameters while ignoring those confidence intervals does not make much sense – therefore it is vain, in my opinion, to spend too much effort fixing non-determinism in that, and many other, cases. Starting from TF 2.9 (TF >= 2.9), if you want your TF models to run deterministically, the following lines need to be added at the beginning of the program. Important note: The first line sets the random seed for the following : Python, NumPy and TensorFlow. The second line makes each TensorFlow operation deterministic. To get a MNIST network (https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) to train deterministically on my GPU (1050Ti): Or: Note that the resulting loss is repeatable with either method for selecting deterministic algorithms from TF, but the two methods result in different losses. Also, the solution above doesn't make a more complicated model I'm using repeatable. Check out https://github.com/NVIDIA/framework-determinism for a more current answer. A side note: For cuda  cuDNN 8.0.1, non deterministic algorithms exist for: (from https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html)When trying to create a neural network and optimize it using Pytorch, I am getting ValueError: optimizer got an empty parameter list Here is the code. and then the call gives the very informative error ValueError: optimizer got an empty parameter list I find it hard to understand what exactly in the network's definition makes the network have parameters. I am following and expanding the example I found in Pytorch's tutorial code. I can't really tell the difference between my code and theirs that makes mine think it has no parameters to optimize. How to make my network have parameters like the linked example? Your NetActor does not directly store any nn.Parameter. Moreover, all other layers it eventually uses in forward are stored as a simple list in self.nn_layers.
If you want self.actor_nn.parameters() to know that the items stored in the list self.nn_layers may contain trainable parameters, you should work with containers.
Specifically, making self.nn_layers to be a nn.ModuleList instead of a simple list should solve your problem:I'm currently working on classifying images with different image-descriptors. Since they have their own metrics, I am using precomputed kernels. So given these NxN kernel-matrices (for a total of N images) i want to train and test a SVM. I'm not very experienced using SVMs though.  What confuses me though is how to enter the input for training. Using a subset of the kernel MxM (M being the number of training images), trains the SVM with M features. However, if I understood it correctly this limits me to use test-data with similar amounts of features. Trying to use sub-kernel of size MxN, causes infinite loops during training, consequently, using more features when testing gives poor results. This results in using equal sized training and test-sets giving reasonable results. But if i only would want to classify, say one image, or train with a given amount of images for each class and test with the rest, this doesn't work at all. How can i remove the dependency between number of training images and features, so i can test with any number of images? I'm using libsvm for MATLAB, the kernels are distance-matrices ranging between [0,1]. You seem to already have figured out the problem... According to the README file included in the MATLAB package: To use precomputed kernel, you must include sample serial number as
  the first column of the training and testing data. Let me illustrate with an example: The output:I m doing an assignment where I am trying to build a collaborative filtering model for the Netflix prize data. The data that I am using is in a CSV file which I easily imported into a data frame. Now what I need to do is create a sparse matrix consisting of the Users as the rows and Movies as the columns and each cell is filled up by the corresponding rating value. When I try to map out the values in the data frame I need to run a loop for each row in the data frame, which is taking a lot of time in R, please can anyone suggest a better approach. Here is the sample code and data: Sample of data in the dataframe from which the sparse matrix is being created: So in the end I want something like this:
The columns are the movie IDs and the rows are the user IDs So the interpretation is something like this: user 2 rated movie 1 as 3 star, user 3 rated the movie 2 as 3 star and so on for the other users and movies. There are about 8500000 rows in my data frame for which my code takes just about 30-45 mins to create this user item matrix, i would like to get any suggestions  The Matrix package has a constructor made especially for your type of data: Otherwise, you might like knowing about that cool feature of the [ function known as matrix indexing. Your could have tried: (but I would definitely recommend the sparseMatrix approach over this.) This will probably be faster than a loop. If you use data.tables, it will be a lot faster: And as I'm sure someone will point out, you can use this instead This converts df to a data.table in place (without making a copy). if your data set is enormous, that can make a difference...To get to grips with PyTorch (and deep learning in general) I started by working through some basic classification examples. One such example was classifying a non-linear dataset created using sklearn (full code available as notebook here)  This is then accurately classified using a pretty basic neural net As I have an interest in health data I then decided to try and use the same network structure to classify some a basic real-world dataset. I took heart rate data for one patient from here, and altered it so all values > 91 would be labelled as anomalies (e.g. a 1 and everything <= 91 labelled a 0). This is completely arbitrary, but I just wanted to see how the classification would work. The complete notebook for this example is here.  What is not intuitive to me is why the first example reaches a loss of 0.0016 after 1,000 epochs, whereas the second example only reaches a loss of 0.4296 after 10,000 epochs   Perhaps I am being naive in thinking that the heart rate example would be much easier to classify. Any insights to help me understand why this is not what I am seeing would be great! Your input data is not normalized. You'll get
 convergence in only 1000 iterations. The key difference between the two examples you have is that the data x in the first example is centered around (0, 0) and has very low variance.
On the other hand, the data in the second example is centered around 92 and has relatively large variance. This initial bias in the data is not taken into account when you randomly initialize the weights which is done based on the assumption that the inputs are  roughly normally distributed around zero.
It is almost impossible for the optimization process to compensate for this gross deviation - thus the model gets stuck in a sub-optimal solution. Once you normalize the inputs, by subtracting the mean and dividing by the std, the optimization process becomes stable again and rapidly converges to a good solution. For more details about input normalization and weights initialization, you can read section 2.2 in He et al Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (ICCV 2015). If, for some reason, you cannot compute mean and std data in advance, you can still use nn.BatchNorm1d to estimate and normalize the data as part of the training process.  For example This modification without any change to the input data, yields similar convergance after only 1000 epochs:
 For numerical stability, it is better to use nn.BCEWithLogitsLoss instead of nn.BCELoss. For this end, you need to remove the torch.sigmoid from the forward() output, the sigmoid will be computed inside the loss.
See, for example, this thread regarding the related sigmoid + cross entropy loss for binary predictions. Let's start first by understanding how neural networks work, neural networks observe patterns, hence the necessity for large datasets. In the case of the example, two what pattern you intend to find is when if HR < 91: label = 0, this if-condition can be represented by the formula, sigmoid((HR-91) * 1) , if you plug various values into the formula you can see you that all values < 91, label 0 and others label 1. I have inferred this formula and it could be anything as long as it gives the correct values. Basically, we apply the formula wx+b, where x in our input data and we learn the values for w and b. Now initially the values are all random, so getting the b value from 1030131190 (a random value), to maybe 98 is fast, since the loss is great, the learning rate allows the values to jump fast. But once you reach 98, your loss is decreasing, and when you apply the learning rate, it takes it more time to reach closer to 91, hence the slow decrease in loss. As the values get closer, the steps taken are even slower.  This can be confirmed via the loss values, they are constantly decreasing, initially, the deceleration is higher, but then it becomes smaller. Your network is still learning but slowly. Hence in deep learning, you use this method called stepped learning rate, wherewith the increase in epochs you decrease your learning rate so that your learning is fasterI trained GoogLeNet model from scratch. But it didn't give me the promising results.
As an alternative, I would like to do fine tuning of GoogLeNet model on my dataset. Does anyone know what are the steps should I follow?  Assuming you are trying to do image classification. These should be the steps for finetuning a model: The original classification layer "loss3/classifier" outputs predictions for 1000 classes (it's mum_output is set to 1000). You'll need to replace it with a new layer with appropriate num_output. Replacing the classification layer: You need to make a new training dataset with the new labels you want to fine tune to. See, for example, this post on how to make an lmdb dataset. When finetuning a model, you can train ALL model's weights or choose to fix some weights (usually filters of the lower/deeper layers) and train only the weights of the top-most layers. This choice is up to you and it ususally depends on the amount of training data available (the more examples you have the more weights you can afford to finetune).
Each layer (that holds trainable parameters) has param { lr_mult: XX }. This coefficient determines how susceptible these weights to SGD updates. Setting param { lr_mult: 0 } means you FIX the weights of this layer and they will not be changed during the training process.
Edit your train_val.prototxt accordingly. Run caffe train but supply it with caffemodel weights as an initial weights: Fine-tuning is a very useful trick to achieve a promising accuracy compared to past manual feature. @Shai already posted a good tutorial for fine-tuning the Googlenet using Caffe, so I just want to give some recommends and tricks for fine-tuning for general cases. In most of time, we face a task classification problem that new dataset (e.g. Oxford 102 flower dataset or Cat&Dog) has following four common situations CS231n: In practice, most of time we do not have enough data to train the network from scratch, but may be enough for pre-trained model. Whatever which cases I mentions above only thing we must care about is that do we have enough data to train the CNN? If yes, we can train the CNN from scratch. However, in practice it is still beneficial to initialize the weight from pre-trained model. If no, we need to check whether data is very different from original datasets? If it is very similar, we can just fine-tune the fully connected neural network or fine-tune with SVM. However, If it is very different from original dataset, we may need to fine-tune the convolutional neural network to improve the generalization.since in TensorFlow 2.0 they plan on unifying all high-level APIs under keras (which I'm not much familiar with) and removing Sessions altogether, I was wondering: How can I create a custom keras layer that has a custom gradient? I've seen the (quite limited) guide on creating custom layers in keras but it doesn't describe what we should do if we want our operation to have a custom gradient. First of all, the "unification" of the APIs (as you call it) under keras doesn't prevent you from doing things like you did in TensorFlow 1.x. Sessions might be gone but you can still define your model like any python function and train it eagerly without keras (i.e. through tf.GradientTape) Now, if you want to build a keras model with a custom layer that performs a custom operation and has a custom gradient, you should do the following: a) Write a function that performs your custom operation and define your custom gradient. More info on how to do this here. Note that in the function you should treat x and dy as Tensors and not numpy arrays (i.e. perform tensor operations) b) Create a custom keras layer that performs your custom_op. For this example I'll assume that your layer doesn't have any trainable parameters or change the shape of its input, but it doesn't make much difference if it does. For that you can refer to the guide that you posted check this one. Now you can use this layer in a keras model and it will work. For example:I am trying to train a very large model. Therefore, I can only fit a very small batch size into GPU memory. Working with small batch sizes results with very noisy gradient estimations.
What can I do to avoid this problem? You can change the iter_size in the solver parameters.
Caffe accumulates gradients over iter_size x batch_size instances in each stochastic gradient descent step.
So increasing iter_size can also get more stable gradient when you cannot use large batch_size due to the limited memory. As stated in this post, the batch size is not a problem in theory (the efficiency of stochastic gradient descent has been proven with a batch of size 1). Make sure you implement your batch correctly (the samples should be randomly picked over your data).I want to do a 10-fold cross-validation in my one-against-all support vector machine classification in MATLAB. I tried to somehow mix these two related answers: But as I'm new to MATLAB and its syntax, I didn't manage to make it work till now. On the other hand, I saw just the following few lines about cross validation in the LibSVM README files and I couldn't find any related example there: option -v randomly splits the data into n parts and calculates cross
  validation accuracy/mean squared error on them. See libsvm FAQ for the meaning of outputs. Could anyone provide me an example of 10-fold cross-validation and one-against-all classification? Mainly there are two reasons we do cross-validation: For the first case which we are interested in, the process involves training k models for each fold, and then training one final model over the entire training set.
We report the average accuracy over the k-folds. Now since we are using one-vs-all approach to handle the multi-class problem, each model consists of N support vector machines (one for each class). The following are wrapper functions implementing the one-vs-all approach: And here are functions to support cross-validation: Finally, here is simple demo to illustrate the usage: Compare that against the one-vs-one approach which is used by default by libsvm: It may be confusing you that one of the two questions is not about LIBSVM. You should try to adjust this answer and ignore the other. You should select the folds, and do the rest exactly as the linked question. Assume the data has been loaded into data and the labels into labels:I am working on one deep learning model where I am trying to combine two different model's output : The overall structure is like this :  So the first model takes one matrix, for example [ 10 x 30 ] Now the second model takes two input matrix : I want to make these two matrices trainable like in TensorFlow I was able to do this by : I am not getting any clue how to make those matrix_a and matrix_b trainable and how to merge the output of both networks then give input. I went through this  question But couldn't find an answer because their problem statement is different from mine. What I have tried so far is : Overview of the model :  Update: Model b model a I am merging like this: Is it right way to matmul two keras model? I don't know if I am merging the output correctly and the model is correct. I would greatly appreciate it if anyone kindly gives me some advice on how should I make that matrix trainable and how to merge the model's output correctly then give input. Thanks in advance! Ok. Since you are going to have custom trainable weights, the way to do this in Keras is creating a custom layer. Now, since your custom layer has no inputs, we will need a hack that will be explained later. So, this is the layer definition for the custom weights: Now, this layer should be used like this: Having the layer defined, we can start modeling.
First, let's see the model_a side: For this, we are going to use our TrainableWeights layer.
But first, let's simulate a New_model() as mentioned. Now the entire branch: Finally, we can join the branches in a whole model.
Notice how I didn't have to use model_a or model_s here. You can do it if you want, but those submodels are not needed, unless you want later to get them individually for other usages. (Even if you created them, you don't need to change the code below to use them, they're already part of the same graph) Now train it: Since the output is 2D now, there is no problem about the 'categorical_crossentropy', my comment was because of doubts on the output shape.This is my train.prototxt. And this is my deploy.prototxt. When I want to load my deploy file I get this error: So, I removed the data layer: Than, I removed bottom: "data" from conv1 layer. After it, I got this error: I removed bottom: "label" from loss layer. And I got this error: What should I do to fix it and create my deploy file? There are two main differences between a "train" prototxt and a "deploy" one: 1. Inputs: While for training data is fixed to a pre-processed training dataset (lmdb/HDF5 etc.), deploying the net require it to process other inputs in a more "random" fashion.
Therefore, the first change is to remove the input layers (layers that push "data" and "labels" during TRAIN and TEST phases). To replace the input layers you need to add the following declaration: This declaration does not provide the actual data for the net, but it tells the net what shape to expect, allowing caffe to pre-allocate necessary resources. 2. Loss: the top most layers in a training prototxt define the loss function for the training. This usually involve the ground truth labels. When deploying the net, you no longer have access to these labels. Thus loss layers should be converted to "prediction" outputs. For example, a "SoftmaxWithLoss" layer should be converted to a simple "Softmax" layer that outputs class probability instead of log-likelihood loss. Some other loss layers already have predictions as inputs, thus it is sufficient just to remove them. Update: see this tutorial for more information. Besides advices from @Shai, you may also want to disable the dropout layers. Although Jia Yangqing, author of Caffe once said that dropout layers have negligible impact on the testing results (google group conversation, 2014), other Deeplearning tools suggest to disable dropout in the deploy phase (for example, lasange).I'm trying to understand how to identify statistical outliers in groups of dataframe. I will need to group the rows by the conditions and then reduce those groups into a single row and later find the outliers in all reduced rows.  Using a dataset like this I would like to group by different conditions such as: At this step, I am reducing each data frame into a single row, for that, I have a few ideas, a straightforward way is to take the mean of each dataframe but the problem is some of the columns are categorical and some of them are continuous, to take the mean of the entire data frame, I am converting the categorical columns into freq count columns : which looks like this for each df group:  Now I can take the mean and reduce the data frames into a single row : concatinating all reduced rows in single dataframe : The final reduced rows data frame looks like this, where each row represents reduced data frame group:  I want to find the outliers in this reduced dataset, I tried to find outliers using zscore such as : But it doesn't seem to work.
I feel like there has to be a way to do this without too much complexity but I've been stuck on how to proceed. How can I reduce the groups into single rows and find the outliers in the reduced dataset? get the mean and std.
We need to loop over each column, get the mean and std, then set the max and min value we accept for this column. not knowing what the data represents makes it harder, so I had to try and explore. I used groupby with mean to reduce rows  I imagined the binary columns are input and decimal are output, so i pivot to make them into one row making new column named input:  pivoting the needed columns and adjusting the names:  reducing the columns with PCA  df7 seems the outlier. thanks for reading :) in practice for Gaussian distribution: data_cases that falls outside mean+/-3*st.dev. are considered to be outliers (as is outside the 99.7% range of distribution)... Thus, for Gaussian & Gaussian-like distributions I would better rewrite the previous answer to: with data - being taken in for-loop for each column_range that you need
___ P.S.  sometimes even 4 st.dev. can be used (covering 99.9% of distribution), but 1 st.dev. is only 68%, 2 st.dev - 95%... So, be sure what you really needIn the paper Girshick, R Fast-RCNN (ICCV 2015), section "3.1 Truncated SVD for faster detection", the author proposes to use SVD trick to reduce the size and computation time of a fully connected layer.   Given a trained model (deploy.prototxt and weights.caffemodel), how can I use this trick to replace a fully connected layer with a truncated one? Some linear-algebra background
Singular Value Decomposition (SVD) is a decomposition of any matrix W into three matrices: Where U and V are ortho-normal matrices, and S is diagonal with elements in decreasing magnitude on the diagonal. 
One of the interesting properties of SVD is that it allows to easily approximate W with a lower rank matrix: Suppose you truncate S to have only its k leading elements (instead of all elements on the diagonal) then is a rank k approximation of W. Using SVD to approximate a fully connected layer
Suppose we have a model deploy_full.prototxt with a fully connected layer Furthermore, we have trained_weights_full.caffemodel - trained parameters for deploy_full.prototxt model. Copy deploy_full.protoxt to deploy_svd.protoxt and open it in editor of your choice. Replace the fully connected layer with these two layers: In python, a little net surgery: Now we have deploy_svd.prototxt with trained_weights_svd.caffemodel that approximate the original net with far less multiplications, and weights. Actually, Ross Girshick's py-faster-rcnn repo includes an implementation for the SVD step: compress_net.py. BTW, you usually need to fine-tune the compressed model to recover the accuracy (or to compress in a more sophisticated way, see for example "Accelerating Very Deep Convolutional Networks for Classification and Detection", Zhang et al). Also, for me scipy.linalg.svd worked faster than numpy's svd.I just made a Adaboost Classifier with these parameters, 1.n_estimators = 50 2.base_estimator = svc (support vector classifier) 3.learning_rate = 1 here is my code: Dataset has 18 independent variables and 1 categorical dependent variable dataset has 10480 datapoints whenever i run this it will take so much time but no any result. Is there any way to check execution time? Or any better way to do this? In practice, we never use SVMs as base classifiers for Adaboost. Adaboost (and similar ensemble methods) were conceived using decision trees as base classifiers (more specifically, decision stumps, i.e. DTs with a depth of only 1); there is good reason why still today, if you don't specify explicitly the base_classifier argument, it assumes a value of DecisionTreeClassifier(max_depth=1). DTs are suitable for such ensembling because they are essentially unstable classifiers, which is not the case with SVMs, hence the latter are not expected to offer much when used as base classifiers. On top of this, SVMs are computationally much more expensive than decision trees (let alone decision stumps), which is the reason for the long processing times you have observed. Unless you have a very good reason to stick to SVMs as base classifiers (and I highly doubt that you do), remove the base_estimator = svc in order to revert to the default setting, and most probably you will be fine. I had a similar experience recently. In my case though, I realised I wasn't scaling the X before using SVM as the base estimator. Just make sure you scale the data from 0 to 1 (you can use StandardScaler() from sklearn) which is always required prior to using SVM.I'm trying to make a XOR gate by using 2 perceptron network but for some reason the network is not learning, when I plot the change of error in a graph the error comes to a static level and oscillates in that region.  I did not add any bias to the network at the moment. This is the error changing by the number of learning rounds. Is this correct? The red color line is the line I was expecting how the error should change.  Anything wrong I'm doing in the code? As I can't seem to figure out what's causing the error. Help much appreciated.  Thanks in advance Here is a one hidden layer network with backpropagation which can be customized to run experiments with relu, sigmoid and other activations. After several experiments it was concluded that with relu the network performed better and reached convergence sooner, while with sigmoid the loss value fluctuated. This happens because, "the gradient of sigmoids becomes increasingly small as the absolute value of x increases". End Result:  The weights obtained after training were: nn.w1 nn.w2 I found the following youtube series extremely helpful for understanding neural nets: Neural networks demystified There is only little which I know and also that can be explained in this answer. If you want an even better understanding of neural nets, then I would suggest you to go through the following link: cs231n: Modelling one neuron The error calculated in each epoch should be a sum total of all sum squared errors (i.e. error for every target)I am following the tutorial over here : https://www.rpubs.com/loveb/som . This tutorial shows how to use the Kohonen Network (also called SOM, a type of machine learning algorithm) on the iris data. I ran this code from the tutorial: The above code fits a Kohonen Network on the iris data. Each observation from the data set is assigned to each one of the "colorful circles" (also called "neurons") in the below pictures. My question: In these plots, how would you identify which observations were assigned to which circles? Suppose I wanted to know which observations belong in the circles outlined in with the black triangles below: 
 Is it possible to do this? Right now, I am trying to use iris.som$classif to somehow trace which points are in which circle. Is there a better way to do this? UPDATE: @Jonny Phelps showed me how to identify observations within a triangular form (see answer below). But i am still not sure if it possible to identify irregular shaped forms. E.g.  In a previous post (Labelling Points on a Plot (R Language)), a user showed me how to assign arbitrary numbers to each circle on the grid:  Based on the above plot, how could you use the "som$classif" statement to find out which observations were in circles 92,91,82,81,72 and 71? Thanks EDIT: Now with Shiny App! A plotly solution is also possible, where you can mouse over individual neurons to display the associated iris rownames (called id here). Based on your iris.som data and Jonny Phelps' grid approach, you can just assign the row numbers as concatenated strings to the individual neurons and have these shown upon mouseover:  Here is a full Shiny app that allows lasso selection and shows a table with the data: From what I can see, using iris.som$unit.classif & iris.som$grid is the way to go in isolating circles within the plotting grid. I have made an assumption that the classifier value matches the row index of iris.som$grid so this will need some more validation. Let me know if this helps your problem :) Output data: Validation plotting on the grid:
 I elaborated the example in my post, however, not on the iris data set but I suppose it is no problem: R, SOM, Kohonen Package, Outlier Detection and also added code snippets you might need. They show I think this answers your questions. It would also be nice to compare the performance of SOM with t-SNE. I have only used SOM as an experiment on the data I generated and on the real wine data set. It would also be nice to prepare heat maps if you have more than 2 variables. All the best to you analysis!Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 2 years ago. In terms of artificial intelligence and machine learning, what is the difference between supervised and unsupervised learning?
Can you provide a basic, easy explanation with an example?  Since you ask this very basic question, it looks like it's worth specifying what Machine Learning itself is. Machine Learning is a class of algorithms which is data-driven, i.e. unlike "normal" algorithms it is the data that "tells" what the "good answer" is. Example: a hypothetical non-machine learning algorithm for face detection in images would try to define what a face is (round skin-like-colored disk, with dark area where you expect the eyes etc). A machine learning algorithm would not have such coded definition, but would "learn-by-examples": you'll show several images of faces and not-faces and a good algorithm will eventually learn and be able to predict whether or not an unseen image is a face. This particular example of face detection is supervised, which means that your examples must be labeled, or explicitly say which ones are faces and which ones aren't. In an unsupervised algorithm your examples are not labeled, i.e. you don't say anything. Of course, in such a case the algorithm itself cannot "invent" what a face is, but it can try to cluster the data into different groups, e.g. it can distinguish that faces are very different from landscapes, which are very different from horses. Since another answer mentions it (though, in an incorrect way): there are "intermediate" forms of supervision, i.e. semi-supervised and active learning. Technically, these are supervised methods in which there is some "smart" way to avoid a large number of labeled examples. In active learning, the algorithm itself decides which thing you should label (e.g. it can be pretty sure about a landscape and a horse, but it might ask you to confirm if a gorilla is indeed the picture of a face). In semi-supervised learning, there are two different algorithms which start with the labeled examples, and then "tell" each other the way they think about some large number of unlabeled data. From this "discussion" they learn. Supervised learning is when the data you feed your algorithm with is "tagged" or "labelled", to help your logic make decisions. Example: Bayes spam filtering, where you have to flag an item as spam to refine the results. Unsupervised learning are types of algorithms that try to find correlations without any external inputs other than the raw data. Example: data mining clustering algorithms. Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering Pattern Recognition and Machine Learning (Bishop, 2006) In supervised learning, the input x is provided with the expected outcome y (i.e., the output the model is supposed to produce when the input is x), which is often called the "class" (or "label") of the corresponding input x. In unsupervised learning, the "class" of an example x is not provided. So, unsupervised learning can be thought of as finding "hidden structure" in unlabelled data set.  Approaches to supervised learning include: Classification (1R, Naive Bayes, decision tree learning algorithm, such
as ID3 CART, and so on) Numeric Value Prediction Approaches to unsupervised learning include: Clustering (K-means, hierarchical clustering) Association Rule Learning I can tell you an example. Suppose you need to recognize which vehicle is a car and which one is a motorcycle. In the supervised learning case, your input (training) dataset needs to be labelled, that is, for each input element in your input (training) dataset, you should specify if it represents a car or a motorcycle. In the unsupervised learning case, you do not label the inputs. The unsupervised model clusters the input into clusters based e.g. on similar features/properties. So, in this case, there is are no labels like "car". For instance, very often training a neural network is supervised learning: you're telling the network to which class corresponds the feature vector you're feeding. Clustering is unsupervised learning: you let the algorithm decide how to group samples into classes that share common properties. Another example of unsupervised learning is Kohonen's self organizing maps. I have always found the distinction between unsupervised and supervised learning to be arbitrary and a little confusing. There is no real distinction between the two cases, instead there is a range of situations in which an algorithm can have more or less 'supervision'. The existence of semi-supervised learning is an obvious examples where the line is blurred. I tend to think of supervision as giving feedback to the algorithm about what solutions should be preferred. For a traditional supervised setting, such as spam detection, you tell the algorithm "don't make any mistakes on the training set"; for a traditional unsupervised setting, such as clustering, you tell the algorithm "points that are close to each other should be in the same cluster". It just so happens that, the first form of feedback is a lot more specific than the  latter. In short, when someone says 'supervised', think classification, when they say 'unsupervised' think clustering and try not to worry too much about it beyond that. Supervised Learning Supervised learning is based on training a data sample
from data source with correct classification already assigned.
Such techniques are utilized in feedforward or MultiLayer
Perceptron (MLP) models. These MLP has three distinctive
characteristics: These characteristics along with learning through training
solve difficult and diverse problems. Learning through
training in a supervised ANN model also called as error backpropagation algorithm. The error correction-learning
algorithm trains the network based on the input-output
samples and finds error signal, which is the difference of the
output calculated and the desired output and adjusts the
synaptic weights of the neurons that is proportional to the
product of the error signal and the input instance of the
synaptic weight. Based on this principle, error back
propagation learning occurs in two passes: Forward Pass:  Here, input vector is presented to the network. This input signal propagates forward, neuron by neuron through the network and emerges at the output end of
the network as output signal: y(n) = φ(v(n)) where v(n) is the induced local field of a neuron defined by v(n) =Σ w(n)y(n). The output that is calculated at the output layer o(n) is compared with the desired response d(n) and finds the error e(n) for that neuron. The synaptic weights of the network during this pass are remains same. Backward Pass:  The error signal that is originated at the output neuron of that layer is propagated backward through network. This calculates the local gradient for each neuron in each layer and allows the synaptic weights of the network to undergo changes in accordance with the delta rule as: This recursive computation is continued, with forward pass followed by the backward pass for each input pattern till the network is converged. Supervised learning paradigm of an ANN is efficient and finds solutions to several linear and non-linear problems such as classification, plant control, forecasting, prediction, robotics etc. Unsupervised Learning Self-Organizing neural networks learn using unsupervised learning algorithm to identify hidden patterns in unlabelled input data. This unsupervised refers to the ability to learn and organize information without providing an error signal to evaluate the potential solution. The lack of direction for the learning algorithm in unsupervised learning can sometime be advantageous, since it lets the algorithm to look back for patterns that have not been previously considered. The main characteristics of Self-Organizing Maps (SOM) are: The computational layer is also called as competitive layer since the neurons in the layer compete with each other to become active. Hence, this learning algorithm is called competitive algorithm. Unsupervised algorithm in SOM
works in three phases: Competition phase: for each input pattern x, presented to the network, inner product with synaptic weight w is calculated and the neurons in the competitive layer finds a discriminant function that induce competition among the neurons and the synaptic weight vector that is close to the input vector in the Euclidean distance is announced as winner in the competition. That neuron is called best matching neuron, Cooperative phase:  the winning neuron determines the center of a topological neighborhood h of cooperating neurons. This is performed by the lateral interaction d among the
cooperative neurons. This topological neighborhood reduces its size over a time period. Adaptive phase:  enables the winning neuron and its neighborhood neurons to increase their individual values of the discriminant function in relation to the input pattern
through suitable synaptic weight adjustments,  Upon repeated presentation of the training patterns, the synaptic weight vectors tend to follow the distribution of the input patterns due to the neighborhood updating and thus ANN learns without supervisor. Self-Organizing Model naturally represents the neuro-biological behavior, and hence is used in many real world applications such as clustering, speech recognition, texture segmentation, vector coding etc. Reference. There are many answers already which explain the differences in detail. I found these gifs on codeacademy and they often help me explain the differences effectively. 
Notice that the training images have labels here and that the model is learning the names of the images. 
Notice that what's being done here is just grouping(clustering) and that the model doesn't know anything about any image. Machine learning: 
It explores the study and construction of algorithms that can learn from and make predictions on data.Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions expressed as outputs,rather than following strictly static program instructions. Supervised learning: 
It  is the machine learning task of inferring a function from labeled training data.The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.Specifically, a supervised learning algorithm takes a known set of input data and known responses to the data (output), and trains a model to generate reasonable predictions for the response to new data. Unsupervised learning: 
It is learning without a teacher. One basic
thing that you might want to do with data is to visualize it. It is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning. Unsupervised learning uses procedures that attempt to find natural partitions
of patterns.  With unsupervised learning there is no feedback based on the prediction results, i.e., there is no teacher to correct you.Under the Unsupervised learning methods no labeled examples are provided and there is no notion of the output during the learning process. As a result, it is up to the learning scheme/model to find patterns or discover the groups of the input data You should use unsupervised learning methods when you need a large
  amount of data to train your models, and the willingness and ability
  to experiment and explore, and of course a challenge that isn’t well
  solved via more-established methods.With unsupervised learning it is
  possible to learn larger and more complex models than with supervised
  learning.Here is a good example on it . Supervised Learning: You give variously labelled example data as input, along with the correct answers. This algorithm will learn from it, and start predicting correct results based on the inputs thereafter. Example: Email Spam filter  Unsupervised Learning: You just give data and don't tell anything - like labels or correct answers. Algorithm automatically analyses patterns in the data. Example: Google News Supervised learning:
say a kid goes to kinder-garden. here teacher shows him 3 toys-house,ball and car. now teacher gives him 10 toys. 
he will classify them  in 3 box of house,ball and car based on his previous experience.
so kid was first supervised by teachers for getting right answers for few sets. then he was tested on unknown toys.
 Unsupervised learning:
again kindergarten example.A child is given 10 toys. he is told to segment similar ones.
so based on features like shape,size,color,function etc he will try to make 3 groups say A,B,C and group them.
 The word Supervise means you are giving supervision/instruction to machine to help it find answers. Once it learns instructions, it can easily predict for new case. Unsupervised  means there is no supervision or instruction how to find answers/labels and machine will use its intelligence to find some pattern in our data. Here it will not make prediction, it will just try to find clusters which has similar data. Supervised learning, given the data with an answer. Given email labeled as spam/not spam, learn a spam filter. Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabetes or not. Unsupervised learning, given the data without an answer, let the pc to group things. Given a set of news articles found on the web, group the into set of articles about the same story. Given a database of custom data, automatically discover market segments and group customers into different market segments. Reference Supervised Learning In this, every input pattern that is used to train the network is
  associated with an output pattern, which is the target or the desired
  pattern. A teacher is assumed to be present during the learning
  process, when a comparison is made between the network's computed
  output and the correct expected output, to determine the error. The
  error can then be used to change network parameters, which result in
  an improvement in performance. Unsupervised Learning In this learning method, the target output is not presented to the
  network. It is as if there is no teacher to present the desired
  pattern and hence, the system learns of its own by discovering and
  adapting to structural features in the input patterns. I'll try to keep it simple. Supervised Learning: In this technique of learning, we are given a data set and the system already knows the correct output of the data set. So here, our system learns by predicting a value of its own. Then, it does an accuracy check by using a cost function to check how close its prediction was to the actual output. Unsupervised Learning: In this approach, we have little or no knowledge of what our result would be. So instead, we derive structure from the data where we don't know effect of variable.
We make structure by clustering the data based on relationship among the variable in data.
Here, we don't have a feedback based on our prediction.   You have input x and a target output t. So you train the algorithm to generalize to the missing parts. It is supervised because the target is given. You are the supervisor telling the algorithm: For the example x, you should output t! Although segmentation, clustering and compression are usually counted in this direction, I have a hard time to come up with a good definition for it. Let's take auto-encoders for compression as an example. While you only have the input x given, it is the human engineer how tells the algorithm that the target is also x. So in some sense, this is not different from supervised learning. And for clustering and segmentation, I'm not too sure if it really fits the definition of machine learning (see other question). Supervised Learning: You have labeled data and have to learn from that. e.g house data along with price and then learn to predict price Unsupervised learning: you have to find the trend and then predict, no prior labels given.
e.g different people in the class and then a new person comes so what group does this new student belong to. In Supervised Learning we know what the input and output should be. For example , given a set of cars. We have to find out which ones red and which ones blue.  Whereas, Unsupervised learning is where we have to find out the answer with a very little or without any idea about how the output should be. For example, a learner might be able to build a model that detects when people are smiling based on correlation of facial patterns and words such as "what are you smiling about?".  Supervised learning can label a new item into one of the trained labels based on learning during training. You need to provide large numbers of training data set, validation data set and test data set. If you provide say pixel image vectors of digits along with training data with labels, then it can identify the numbers.   Unsupervised learning does not require training data-sets. In unsupervised learning it can group items into different clusters based on the difference in the input vectors. If you provide pixel image vectors of digits and ask it to classify into 10 categories, it may do that. But it does know how to labels it as you have not provided training labels. Supervised Learning is basically where you have input variables(x) and output variable(y) and use algorithm to learn the mapping function from input to the output. The reason why we called this as supervised is because algorithm learns from the training dataset, the algorithm iteratively makes predictions on the training data.
Supervised have two types-Classification and Regression.
Classification is when the output variable is category like yes/no, true/false.
Regression is when the output is real values like height of person, Temperature etc. UN supervised learning is where we have only input data(X) and no output variables.
This is called an unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data. Types of unsupervised learning are clustering and Association. Supervised Learning is basically a technique in which the training data from which the machine learns is already labelled that is suppose a simple even odd number classifier where you have already classified the data during training . Therefore it uses "LABELLED" data. Unsupervised learning on the contrary is a technique in which the machine by itself labels the data . Or you can say its the case when the machine learns by itself from scratch. In Simple 
     Supervised learning is  type of machine learning problem in which we have some labels and by using that labels we implement algorithm such as regression and classification .Classification is applied where our output is like in the form of 
0 or 1 ,true/false,yes/no. and regression is applied where out put a real value such a house of price  Unsupervised Learning is a type of machine learning problem in which we don't have any labels means we have some data only ,unstructured data and we have to cluster the data (grouping of data)using various unsupervised algorithm Supervised Machine Learning "The process of  an algorithm learning from  training dataset and
  predict the  output. " Accuracy of  predicted output  directly proportional   to the  training data (length) Supervised learning is where you have input variables (x) (training dataset) and an output variable (Y) (testing dataset) and you use an algorithm to learn the mapping function from the input to the output. Major types:  Algorithms:  Classification Algorithms: Predictive Algorithms: Application  areas:  Voice  Recognition  Predict  the HR select particular candidate or  not  Predict the stock market price Supervised learning: A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. Categories of problem: Regression:  Predict results within a continuous output => map input variables to some continuous function. Example: Given a picture of a person, predict his age Classification: Predict results in a discrete output =>  map input variables into discrete categories Example: Is this tumer cancerous?  Unsupervised learning: Unsupervised learning learns from test data that has not been labeled, classified or categorized. Unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. We can derive this structure by clustering the data based on relationships among the variables in the data. There is no feedback based on the prediction results. Categories of problem: Clustering: is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters) Example: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.  Popular use cases are listed here. Difference between classification and clustering in data mining? References: Supervised_learning Unsupervised_learning machine-learning from coursera towardsdatascience Supervised Learning  Unsupervised Learning  Example: Supervised Learning: One bag with orange => build model One mixed bag of apple and orange. => Please classify  Unsupervised Learning:  One mixed bag of apple and orange. => build model  Another mixed bag => Please classify In simple words.. :) It's my understanding, feel free to correct.
Supervised learning is, we know what we are predicting on the basis of provided data. So we have a column in the dataset which needs to be predicated.
Unsupervised learning is, we try to extract meaning out of the provided dataset. We don't have clarity on what to be predicted. So question is why we do this?.. :) Answer is - the outcome of Unsupervised learning is groups/clusters(similar data together). So if we receive any new data then we associate that with the identified cluster/group and understand it's features. I hope it will help you. supervised learning supervised learning is where we know the output of the raw input, i.e the data is labelled so that during the training of machine learning model it will understand what it need to detect in the give output, and it will guide the system during the training to detect the pre-labelled objects on that basis it will detect the similar objects which we have provided in training. Here the algorithms will know what's the structure and pattern of data. Supervised learning is used for classification  As an example, we can have a different objects whose shapes are square, circle, trianle our task is to arrange the same types of shapes 
the labelled dataset have all the shapes labelled, and we will train the machine learning model on that dataset, on the based of training dateset it will start detecting the shapes.   Un-supervised learning Unsupervised learning is a unguided learning where the end result is not known, it will cluster the dataset and based on similar properties of the object it will divide the objects on different bunches and detect the objects. Here algorithms will search for the different pattern in the raw data, and based on that it will cluster the data. Un-supervised learning is used for clustering. As an example, we can have different objects of multiple shapes square, circle, triangle, so it will make the bunches based on the object properties, if a object has four sides it will consider it square, and if it have three sides triangle and if no sides than circle, here the the data is not labelled, it will learn itself to detect the various shapes Machine learning is a field where you are trying to make machine to mimic the human behavior. You train machine just like a baby.The way humans learn, identify features, recognize patterns and train himself, same way you train machine by feeding data with various features. Machine algorithm identify the pattern within the data and classify it into particular category. Machine learning broadly divided into two category, supervised and unsupervised learning. Supervised learning is the concept where you have input vector / data with corresponding target value (output).On the other hand unsupervised learning is the concept where you only have input vectors / data without any corresponding target value. An example of supervised learning is handwritten digits recognition where you have image of digits with corresponding digit [0-9], and an example of unsupervised learning is grouping customers by purchasing behavior.Want to improve this question? Update the question so it can be answered with facts and citations by editing this post. Closed 4 years ago. Suppose I'm working on some classification problem. (Fraud detection and comment spam are two problems I'm working on right now, but I'm curious about any classification task in general.) How do I know which classifier I should use?  In which cases is one of these the "natural" first choice, and what are the principles for choosing that one? Examples of the type of answers I'm looking for (from Manning et al.'s Introduction to Information Retrieval book): a. If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes). I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data. b. If you have a ton of data, then the classifier doesn't really matter so much, so you should probably just choose a classifier with good scalability. What are other guidelines? Even answers like "if you'll have to explain your model to some upper management person, then maybe you should use a decision tree, since the decision rules are fairly transparent" are good. I care less about implementation/library issues, though. Also, for a somewhat separate question, besides standard Bayesian classifiers, are there 'standard state-of-the-art' methods for comment spam detection (as opposed to email spam)?  First of all, you need to identify your problem. It depends upon what kind of data you have and what your desired task is. If you are Predicting Category : If you are Predicting Quantity : Otherwise There are different algorithms within each approach mentioned above. The choice of a particular algorithm depends upon the size of the dataset. Source: http://scikit-learn.org/stable/tutorial/machine_learning_map/ Model selection using cross validation may be what you need. What you do is simply to split your dataset into k non-overlapping subsets (folds), train a model using k-1 folds and predict its performance using the fold you left out. This you do for each possible combination of folds (first leave 1st fold out, then 2nd, ... , then kth, and train with the remaining folds). After finishing, you estimate the mean performance of all folds (maybe also the variance/standard deviation of the performance). How to choose the parameter k depends on the time you have. Usual values for k are 3, 5, 10 or even N, where N is the size of your data (that's the same as leave-one-out cross validation). I prefer 5 or 10. Let's say you have 5 methods (ANN, SVM, KNN, etc) and 10 parameter combinations for each method (depending on the method). You simply have to run cross validation for each method and parameter combination (5 * 10 = 50) and select the best model, method and parameters. Then you re-train with the best method and parameters on all your data and you have your final model. There are some more things to say. If, for example, you use a lot of methods and parameter combinations for each, it's very likely you will overfit. In cases like these, you have to use nested cross validation. In nested cross validation, you perform cross validation on the model selection algorithm. Again, you first split your data into k folds. After each step, you choose k-1 as your training data and the remaining one as your test data. Then you run model selection (the procedure I explained above) for each possible combination of those k folds. After finishing this, you will have k models, one for each combination of folds. After that, you test each model with the remaining test data and choose the best one. Again, after having the last model you train a new one with the same method and parameters on all the data you have. That's your final model. Of course, there are many variations of these methods and other things I didn't mention. If you need more information about these look for some publications about these topics. The book "OpenCV" has a great two pages on this on pages 462-463. Searching the Amazon preview for the word "discriminative" (probably google books also) will let you see the pages in question. These two pages are the greatest gem I have found in this book. In short: Boosting - often effective when a large amount of training data is available. Random trees - often very effective and can also perform regression. K-nearest neighbors - simplest thing you can do, often effective but slow and requires lots of memory. Neural networks - Slow to train but very fast to run, still optimal performer for letter recognition. SVM - Among the best with limited data, but losing against boosting or random trees only when large data sets are available. Things you might consider in choosing which algorithm to use would include: Do you need to train incrementally (as opposed to batched)? If you need to update your classifier with new data frequently (or you have tons of data), you'll probably want to use Bayesian. Neural nets and SVM need to work on the training data in one go. Is your data composed of categorical only, or numeric only, or both? I think Bayesian works best with categorical/binomial data. Decision trees can't predict numerical values. Does you or your audience need to understand how the classifier works? Use Bayesian or decision trees, since these can be easily explained to most people. Neural networks and SVM are "black boxes" in the sense that you can't really see how they are classifying data. How much classification speed do you need? SVM's are fast when it comes to classifying since they only need to determine which side of the "line" your data is on.  Decision trees can be slow especially when they're complex (e.g. lots of branches). Complexity. Neural nets and SVMs can handle complex non-linear classification. As Prof Andrew Ng often states: always begin by implementing a rough, dirty algorithm, and then iteratively refine it. For classification, Naive Bayes is a good starter, as it has good performances, is highly scalable and can adapt to almost any kind of classification task. Also 1NN (K-Nearest Neighbours with only 1 neighbour) is a no-hassle best fit algorithm (because the data will be the model, and thus you don't have to care about the dimensionality fit of your decision boundary), the only issue is the computation cost (quadratic because you need to compute the distance matrix, so it may not be a good fit for high dimensional data). Another good starter algorithm is the Random Forests (composed of decision trees), this is highly scalable to any number of dimensions and has generally quite acceptable performances. Then finally, there are genetic algorithms, which scale admirably well to any dimension and any data with minimal knowledge of the data itself, with the most minimal and simplest implementation being the microbial genetic algorithm (only one line of C code! by Inman Harvey in 1996), and one of the most complex being CMA-ES and MOGA/e-MOEA. And remember that, often, you can't really know what will work best on your data before you try the algorithms for real. As a side-note, if you want a theoretical framework to test your hypothesis and algorithms theoretical performances for a given problem, you can use the PAC (Probably approximately correct) learning framework (beware: it's very abstract and complex!), but to summary, the gist of PAC learning says that you should use the less complex, but complex enough (complexity being the maximum dimensionality that the algo can fit) algorithm that can fit your data. In other words, use the Occam's razor. Sam Roweis used to say that you should try naive Bayes, logistic regression, k-nearest neighbour and Fisher's linear discriminant before anything else. My take on it is that you always run the basic classifiers first to get some sense of your data. More often than not (in my experience at least) they've been good enough. So, if you have supervised data, train a Naive Bayes classifier. If you have unsupervised data, you can try k-means clustering. Another resource is one of the lecture videos of the series of videos Stanford Machine Learning, which I watched a while back. In video 4 or 5, I think, the lecturer discusses some generally accepted conventions when training classifiers, advantages/tradeoffs, etc. You should always keep into account the inference vs prediction trade-off.  If you want to understand the complex relationship that is occurring in your data then you should go with a rich inference algorithm (e.g. linear regression or lasso). On the other hand, if you are only interested in the result you can go with high dimensional and more complex (but less interpretable) algorithms, like neural networks. Selection of Algorithm is depending upon the scenario and the type and size of data set.
There are many other factors. This is a brief cheat sheet for basic machine learning.I am trying to understand the role of the Flatten function in Keras. Below is my code, which is a simple two-layer network. It takes in 2-dimensional data of shape (3, 2), and outputs 1-dimensional data of shape (1, 4): This prints out that y has shape (1, 4). However, if I remove the Flatten line, then it prints out that y has shape (1, 3, 4). I don't understand this. From my understanding of neural networks, the model.add(Dense(16, input_shape=(3, 2))) function is creating a hidden fully-connected layer, with 16 nodes. Each of these nodes is connected to each of the 3x2 input elements. Therefore, the 16 nodes at the output of this first layer are already "flat". So, the output shape of the first layer should be (1, 16). Then, the second layer takes this as an input, and outputs data of shape (1, 4). So if the output of the first layer is already "flat" and of shape (1, 16), why do I need to further flatten it? If you read the Keras documentation entry for Dense, you will see that this call: would result in a Dense network with 3 inputs and 16 outputs which would be applied independently for each of 5 steps. So, if D(x) transforms 3 dimensional vector to 16-d vector, what you'll get as output from your layer would be a sequence of vectors: [D(x[0,:]), D(x[1,:]),..., D(x[4,:])] with shape (5, 16). In order to have the behavior you specify you may first Flatten your input to a 15-d vector and then apply Dense: EDIT:
As some people struggled to understand - here you have an explaining image:  
This is how Flatten works converting Matrix to single array. short read: Flattening a tensor means to remove all of the dimensions except for one. This is exactly what the Flatten layer does. long read: If we take the original model (with the Flatten layer) created in consideration we can get the following model summary: For this summary the next image will hopefully provide little more sense on the input and output sizes for each layer. The output shape for the Flatten layer as you can read is (None, 48). Here is the tip. You should read it (1, 48) or (2, 48) or ... or (16, 48) ... or (32, 48), ... In fact, None on that position means any batch size. For the inputs to recall, the first dimension means the batch size and the second means the number of input features. The role of the Flatten layer in Keras is super simple: A flatten operation on a tensor reshapes the tensor to have the shape that is equal to the number of elements contained in tensor non including the batch dimension.  Note: I used the model.summary() method to provide the output shape and parameter details. I came across this recently, it certainly helped me understand: https://www.cs.ryerson.ca/~aharley/vis/conv/ So there's an input, a Conv2D, MaxPooling2D etc, the Flatten layers are at the end and show exactly how they are formed and how they go on to define the final classifications (0-9). It is rule of thumb that the first layer in your network should be the same shape as your data. For example our data is 28x28 images, and 28 layers of 28 neurons would be infeasible, so it makes more sense to 'flatten' that 28,28 into a 784x1. Instead of wriitng all the code to handle that ourselves, we add the Flatten() layer at the begining, and when the arrays are loaded into the model later, they'll automatically be flattened for us. Flatten make explicit how you serialize a multidimensional tensor (tipically the input one). This allows the mapping between the (flattened) input tensor and the first hidden layer. If the first hidden layer is "dense" each element of the (serialized) input tensor will be connected with each element of the hidden array.
If you do not use Flatten, the way the input tensor is mapped onto the first hidden layer would be ambiguous. Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. In some architectures, e.g. CNN an image is better processed by a neural network if it is in 1D form rather than 2D.  Here I would like to present another alternative to Flatten function. This may help to understand what is going on internally. The alternative method adds three more code lines.
Instead of using we can use In the second case, we first create a tensor (using a placeholder)
and then create an Input layer. After, we reshape the tensor to flat form. So basically, Flatten is a convenient function, doing all this automatically. Of course both ways has its specific use cases. Keras provides enough flexibility to manipulate the way you want to create a model. Keras flatten class is very important when you have to deal with multi-dimensional inputs such as image datasets. Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension, so you can model your input layer and build your neural network model, then pass those data into every single neuron of the model effectively. You can understand this easily with the fashion MNIST dataset. The images in this dataset are 28 * 28 pixels. Hence if you print the first image in python you can see a multi-dimensional array, which we really can't feed into the input layer of our Deep Neural Network. first image of fashion MNIST To tackle this problem we can flatten the image data when feeding it into a neural network. We can do this by turning this multidimensional tensor into a one-dimensional array. In this flattened array now we have 784 elements (28 * 28). Then we can create out input layer with 784 neurons to handle each element of the incoming data. We can do this all by using a single line of code, sort of... As the name suggests it just flattens out the input Tensor. A very good visual to understand this is given below.
Please let me know if there is any confusion.
Flatten Input TensorI'm working in a sentiment analysis problem the data looks like this: So my data is unbalanced since 1190 instances are labeled with 5. For the classification Im using scikit's SVC. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches: First: Second: Third: However, Im getting warnings like this: How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics? I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;). The weights from the class_weight parameter are used to train the classifier.
They are not used in the calculation of any of the metrics you are using: with different class weights, the numbers will be different simply because the classifier is different. Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.
How they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it. Once you have a classifier, you want to know how well it is performing.
Here you can use the metrics you mentioned: accuracy, recall_score, f1_score... Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class. I will not detail all these metrics but note that, with the exception of accuracy, they are naturally applied at the class level: as you can see in this print of a classification report they are defined for each class. They rely on concepts such as true positives or false negative that require defining which class is the positive one. You get this warning because you are using the f1-score, recall and precision without defining how they should be computed!
The question could be rephrased: from the above classification report, how do you output one global number for the f1-score?
You could: These are 3 of the options in scikit-learn, the warning is there to say you have to pick one. So you have to specify an average argument for the score method. Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5. The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it. Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen.
This is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant. Here's a way to do it using StratifiedShuffleSplit, which gives you a random splits of your data (after shuffling) that preserve the label distribution. Lot of very detailed answers here but I don't think you are answering the right questions. As I understand the question, there are two concerns: You can use most of the scoring functions in scikit-learn with both multiclass problem as with single class problems. Ex.: This way you end up with tangible and interpretable numbers for each of the classes. Then... ... you can tell if the unbalanced data is even a problem. If the scoring for the less represented classes (class 1 and 2) are lower than for the classes with more training samples (class 4 and 5) then you know that the unbalanced data is in fact a problem, and you can act accordingly, as described in some of the other answers in this thread.
However, if the same class distribution is present in the data you want to predict on, your unbalanced training data is a good representative of the data, and hence, the unbalance is a good thing. Posed question Responding to the question 'what metric should be used for multi-class classification with imbalanced data': Macro-F1-measure. 
Macro Precision and Macro Recall can be also used, but they are not so easily interpretable as for binary classificaion, they are already incorporated into F-measure, and excess metrics complicate methods comparison, parameters tuning, and so on.  Micro averaging are sensitive to class imbalance: if your method, for example, works good for the most common labels and totally messes others, micro-averaged metrics show good results. Weighting averaging isn't well suited for imbalanced data, because it weights by counts of labels. Moreover, it is too hardly interpretable and unpopular: for instance, there is no mention of such an averaging in the following very detailed survey I strongly recommend to look through: Sokolova, Marina, and Guy Lapalme. "A systematic analysis of
  performance measures for classification tasks." Information Processing
  & Management 45.4 (2009): 427-437. Application-specific question However, returning to your task, I'd research 2 topics: Commonly used metrics.
As I can infer after looking through literature, there are 2 main evaluation metrics: Yu, April, and Daryl Chang. "Multiclass Sentiment Prediction using
  Yelp Business." (link) - note that the authors work with almost the same distribution of ratings, see Figure 5. Pang, Bo, and Lillian Lee. "Seeing stars: Exploiting class
  relationships for sentiment categorization with respect to rating
  scales." Proceedings of the 43rd Annual Meeting on Association for
  Computational Linguistics. Association for Computational Linguistics,
  2005. (link) Lee, Moontae, and R. Grafe. "Multiclass sentiment analysis with
  restaurant reviews." Final Projects from CS N 224 (2010). (link) - they explore both accuracy and MSE, considering the latter to be better Pappas, Nikolaos, Rue Marconi, and Andrei Popescu-Belis. "Explaining
  the Stars: Weighted Multiple-Instance Learning for Aspect-Based
  Sentiment Analysis." Proceedings of the 2014 Conference on Empirical
  Methods In Natural Language Processing. No. EPFL-CONF-200899. 2014. (link) - they utilize scikit-learn for evaluation and baseline approaches and state that their code is available; however, I can't find it, so if you need it, write a letter to the authors, the work is pretty new and seems to be written in Python. Cost of different errors.
If you care more about avoiding gross blunders, e.g. assinging 1-star to 5-star review or something like that, look at MSE; 
if difference matters, but not so much, try MAE, since it doesn't square diff; 
otherwise stay with Accuracy. About approaches, not metrics Try regression approaches, e.g. SVR, since they generally outperforms Multiclass classifiers like SVC or OVA SVM. First of all it's a little bit harder using just counting analysis to tell if your data is unbalanced or not. For example: 1 in 1000 positive observation is just a noise, error or a breakthrough in science? You never know.
So it's always better to use all your available knowledge and choice its status with all wise. Okay, what if it's really unbalanced?
Once again — look to your data. Sometimes you can find one or two observation multiplied by hundred times. Sometimes it's useful to create this fake one-class-observations.
If all the data is clean next step is to use class weights in prediction model. So what about multiclass metrics?
In my experience none of your metrics is usually used. There are two main reasons.
First: it's always better to work with probabilities than with solid prediction (because how else could you separate models with 0.9 and 0.6 prediction if they both give you the same class?)
And second: it's much easier to compare your prediction models and build new ones depending on only one good metric.
From my experience I could recommend logloss or MSE (or just mean squared error). How to fix sklearn warnings?
Just simply (as yangjie noticed) overwrite average parameter with one of these 
values: 'micro' (calculate metrics globally), 'macro' (calculate metrics for each label) or 'weighted' (same as macro but with auto weights). All your Warnings came after calling metrics functions with default average value 'binary' which is inappropriate for multiclass prediction.
Good luck and have fun with machine learning! Edit:
I found another answerer recommendation to switch to regression approaches (e.g. SVR) with which I cannot agree. As far as I remember there is no even such a thing as multiclass regression. Yes there is multilabel regression which is far different and yes it's possible in some cases switch between regression and classification (if classes somehow sorted) but it pretty rare. What I would recommend (in scope of scikit-learn) is to try another very powerful classification tools: gradient boosting, random forest (my favorite), KNeighbors and many more. After that you can calculate arithmetic or geometric mean between predictions and most of the time you'll get even better result.In the iOS email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. It not only works for emails in English, but in other languages also. I love this feature and would like to understand how they do it.  The naive way to do this would be to have many regular expressions and run them all. However I  this is not going to scale very well and will work for only a specific language or date format, etc. I think that Apple must be using some concept of machine learning to extract entities (8:00PM, 8PM, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.). Any idea how Apple is able to extract entities so quickly in its email client? What machine learning algorithm would you to apply accomplish such task?  They likely use Information Extraction techniques for this. Here is a demo of Stanford's SUTime tool: http://nlp.stanford.edu:8080/sutime/process You would extract attributes about n-grams (consecutive words) in a document: And then use a classification algorithm, and feed it positive and negative examples: You might get away with 50 examples of each, but the more the merrier. Then, the algorithm learns based on those examples, and can apply to future examples that it hasn't seen before. It might learn rules such as  Here is a decent video by a Google engineer on the subject That's a technology Apple actually developed a very long time ago called Apple Data Detectors. You can read more about it here: http://www.miramontes.com/writing/add-cacm/ Essentially it parses the text and detects patterns that represent specific pieces of data, then applies OS-contextual actions to it. It's neat. This is called temporal expression identification and parsing.  Here are some Google searches to get you started:  https://www.google.com/#hl=en&safe=off&sclient=psy-ab&q=timebank+timeml+timex https://www.google.com/#hl=en&safe=off&sclient=psy-ab&q=temporal+expression+tagger One part of the puzzle could be the NSDataDetector class. Its used to recognize some standard types like phone numbers. I once wrote a parser to do this, using pyparsing. It's really very simple, you just need to get all the different ways right, but there aren't that many. It only took a few hours and was pretty fast. Apple has a patent on how they did it System and method for performing an action on a structure in computer data, and here's a story on this patent apples-patent-on-nsdatadetectorIn Keras, we can return the output of model.fit to a history as follows: Now, how to save the history attribute of the history object to a file for further uses (e.g. draw plots of acc or loss against epochs)? What I use is the following: In this way I save the history as a dictionary in case I want to plot the loss or accuracy later on. Later, when you want to load the history again, you can use: The comment under this answer accurately states: [Storing the history as json] does not work anymore in tensorflow keras. I had issues with: TypeError: Object of type 'float32' is not JSON serializable. There are ways to tell json how to encode numpy objects, which you can learn about from this other question, so there's nothing wrong with using json in this case, it's just more complicated than simply dumping to a pickle file. As history.history is a dict, you can convert it as well to a pandas DataFrame object, which can then be saved to suit your needs. Step by step: The easiest way: Saving: Loading: Then history is a dictionary and you can retrieve all desirable values using the keys. The model history can be saved into a file as follows A history objects has a history field is a dictionary which helds different training metrics spanned across every training epoch. So e.g. history.history['loss'][99] will return a loss of your model in a 100th epoch of training. In order to save that you could pickle this dictionary or simple save different lists from this dictionary to appropriate file. I came across the problem that the values inside of the list in keras are not json seriazable. Therefore I wrote this two handy functions for my use cause. where saveHist just needs to get the path to where the json file should be saved, and the history object returned from the keras fit or fit_generator method. I'm sure there are many ways to do this, but I fiddled around and came up with a version of my own. First, a custom callback enables grabbing and updating the history at the end of every epoch. In there I also have a callback to save the model. Both of these are handy because if you crash, or shutdown, you can pick up training at the last completed epoch. Second, here are some 'helper' functions to do exactly the things that they say they do. These are all called from the LossHistory() callback. After that, all you need is to set history_filename to something like data/model-history.json, as well as set model_filename to something like data/model.h5. One final tweak to make sure not to mess up your history at the end of training, assuming you stop and start, as well as stick in the callbacks, is to do this: Whenever you want, history = loadHist(history_filename) gets your history back. The funkiness comes from the json and the lists but I wasn't able to get it to work without converting it by iterating. Anyway, I know that this works because I've been cranking on it for days now. The pickle.dump answer at https://stackoverflow.com/a/44674337/852795 might be better, but I don't know what that is. If I missed anything here or you can't get it to work, let me know. You can save History attribute of tf.keras.callbacks.History in .txt form The above answers are useful when saving history at the end of the training process. If you want to save the history during the training, the CSVLogger callback will be helpful.  Below code saves the model weight and history training in form of a datasheet file log.csv. Here is a callback that pickles the logs into a file.  Provide the model file path when instantiating the callback obj; this will create an associated file - given model path '/home/user/model.h5', the pickled path '/home/user/model_history_pickle'.  Upon reloading the model, the callback will continue from the epoch that it left off at.I am trying to grasp what TimeDistributed wrapper does in Keras. I get that TimeDistributed "applies a layer to every temporal slice of an input." But I did some experiment and got the results that I cannot understand. In short, in connection to LSTM layer, TimeDistributed and just Dense layer bear same results. For both models, I got output shape of (None, 10, 1). Can anyone explain the difference between TimeDistributed and Dense layer after an RNN layer? In keras - while building a sequential model - usually the second dimension (one after sample dimension) - is related to a time dimension. This means that if for example, your data is 5-dim with (sample, time, width, length, channel) you could apply a convolutional layer using TimeDistributed (which is applicable to 4-dim with (sample, width, length, channel)) along a time dimension (applying the same layer to each time slice) in order to obtain 5-d output. The case with Dense is that in keras from version 2.0 Dense is by default applied to only last dimension (e.g. if you apply Dense(10) to input with shape (n, m, o, p) you'll get output with shape (n, m, o, 10)) so in your case Dense and TimeDistributed(Dense) are equivalent.I want to make a simple neural network which uses the ReLU function. Can someone give me a clue of how can I implement the function using numpy. There are a couple of ways. If timing the results with the following code: We get: So the multiplication seems to be the fastest. You can do it in much easier way: I'm completely revising my original answer because of points raised in the other questions and comments. Here is the new benchmark script: It takes care to use a different ndarray for each implementation and iteration. Here are the results: EDIT As  jirassimok has mentioned below my function will change the data in place, after that it runs a lot faster in timeit. This causes the good results. It's some kind of cheating. Sorry for your inconvenience. I found a faster method for ReLU with numpy. You can use the fancy index feature of numpy as well. fancy index: 20.3 ms ± 272 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) Here is my benchmark: Richard Möhn's comparison  is not fair.
As Andrea Di Biagio's comment, the in-place method np.maximum(x, 0, x) will modify x at the first loop.
So here is my benchmark:   Timing it:   Get the results:   In-place maximum method is only a bit faster than the maximum method, and it may because it omits the variable assignment for 'out'. And it's still slower than the multiplication method.
And since you're implementing the ReLU func. You may have to save the 'x' for backprop through relu. E.g.:   So i recommend you to use multiplication method. numpy didn't have the function of relu, but you define it by yourself as follow: for example: If we have 3 parameters (t0, a0, a1) for Relu, that is we want to implement We can use the following code: X there is a matrix. ReLU(x) also is equal to (x+abs(x))/2   For a single neuron Where net is the net activity at the neuron's input(net=dot(w,x)), where dot() is the dot product of w and x (weight vector and input vector respectively). dot() is a function defined in numpy package in Python. For neurons in a layer with net vector This is more precise implementation:Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 2 years ago. The community reviewed whether to reopen this question 9 months ago and left it closed: Original close reason(s) were not resolved From the XGBoost guide: After training, the model can be saved. The model and its feature map can also be dumped to a text file. A saved model can be loaded as follows: My questions are following. Here is how I solved the problem: Both functions save_model and dump_model save the model, the difference is that in dump_model you can save feature name and save tree in text format. The load_model will work with model from save_model. The model from dump_model can be used  for example with xgbfi. During loading the model, you need to specify the path where your models is saved. In the example bst.load_model("model.bin") model is loaded from file model.bin - it is just a name of file with model. Good luck! EDIT: From Xgboost documentation (for version 1.3.3), the dump_model() should be used for saving the model for further interpretation. For saving and loading the model the save_model() and load_model() should be used. Please check the docs for more details. There is also a difference between Learning API and Scikit-Learn API of Xgboost. The latter saves the best_ntree_limit variable which is set during the training with early stopping. You can read details in my article How to save and load Xgboost in Python? The save_model() method recognize the format of the file name, if *.json is specified, then model is saved in JSON, otherwise it is text file. Don't use pickle or joblib as that may introduces dependencies on xgboost version. The canonical way to save and restore models is by load_model and save_model. If you’d like to store or archive your model for long-term storage, use save_model (Python) and xgb.save (R). This is the relevant documentation for the latest versions of XGBoost. It also explains the difference between dump_model and save_model. Note that you can serialize/de-serialize your models as json by specifying json as the extension when using bst.save_model. If the speed of saving and restoring the model is not important for you, this is very convenient, as it allows you to do proper version control of the model since it's a simple text file. An easy way of saving and loading a xgboost model is with joblib library. If you are using the sklearn api you can use the following: If you used the above booster method for loading, you will get the xgboost booster within the python api not the sklearn booster in the sklearn api. So yeah, this seems to be the most pythonic way to load in a saved xgboost model data if you are using the sklearn api.I am trying to do a transfer learning; for that purpose I want to remove the last two layers of the neural network and add another two layers. This is an example code which also output the same error. I removed the layer using pop() but when I tried to add its outputting this error I know the most probable reason for the error is improper use of model.add(). what other syntax should I use? EDIT: I tried to remove/add layers in keras but its not  allowing it to be added after loading external weights. its showing this error You can take the output of the last model and create a new model. The lower layers remains the same. Check How to use models from keras.applications for transfer learnig? Update on Edit: The new error is because you are trying to create the new model on global in_img which is actually not used in the previous model creation.. there you are actually defining a local in_img. So the global in_img is obviously not connected to the upper layers in the symbolic graph. And it has nothing to do with loading weights. To better resolve this problem you should instead use model.input to reference to the input. Another way to do it As of Keras 2.3.1 and TensorFlow 2.0, model.layers.pop() is not working as intended (see issue here). They suggested two options to do this. One option is to recreate the model and copy the layers. For instance, if you want to remove the last layer and add another one, you can do: Another option is to use the functional model: model.layers[-1].output means the last layer's output which is the final output, so in your code, you actually didn't remove any layers, you added another head/path. An alternative to Wesam Na's answer, if you don't know the layer names you can simply cut off the last layer via:I'm using scikit-learn in Python to develop a classification algorithm to predict the gender of certain customers. Amongst others, I want to use the Naive Bayes classifier but my problem is that I have a mix of categorical data (ex: "Registered online", "Accepts email notifications" etc) and continuous data (ex: "Age", "Length of membership" etc). I haven't used scikit much before but I suppose that that Gaussian Naive Bayes is suitable for continuous data and that Bernoulli Naive Bayes can be used for categorical data. However, since I want to have both categorical and continuous data in my model, I don't really know how to handle this. Any ideas would be much appreciated! You have at least two options: Transform all your data into a categorical representation by computing percentiles for each continuous variables and then binning the continuous variables using the percentiles as bin boundaries. For instance for the height of a person create the following bins: "very small", "small", "regular", "big", "very big" ensuring that each bin contains approximately 20% of the population of your training set. We don't have any utility to perform this automatically in scikit-learn but it should not be too complicated to do it yourself. Then fit a unique multinomial NB on those categorical representation of your data. Independently fit a gaussian NB model on the continuous part of the data and a multinomial NB model on the categorical part. Then transform all the dataset by taking the class assignment probabilities (with predict_proba method) as new features: np.hstack((multinomial_probas, gaussian_probas)) and then refit a new model (e.g. a new gaussian NB) on the new features. Hope I'm not too late. I recently wrote a library called Mixed Naive Bayes, written in NumPy. It can assume a mix of Gaussian and categorical (multinoulli) distributions on the training data features. https://github.com/remykarem/mixed-naive-bayes The  library is written such that the APIs are similar to scikit-learn's. In the example below, let's assume that the first 2 features are from a categorical distribution and the last 2 are Gaussian. In the fit() method, just specify categorical_features=[0,1], indicating that Columns 0 and 1 are to follow categorical distribution. Pip installable via pip install mixed-naive-bayes. More information on the usage in the README.md file. Pull requests are greatly appreciated :) The simple answer: multiply result!! it's the same. Naive Bayes based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features - meaning you calculate the Bayes probability dependent on a specific feature without holding the others - which means that the algorithm multiply each probability from one feature with the probability from the second feature (and we totally ignore the denominator - since it is just a normalizer). so the right answer is: @Yaron's approach needs an extra step (4. below): Step 4. is the normalization step. Take a look at @remykarem's mixed-naive-bayes as an example (lines 268-278): The probabilities of the Gaussian and Categorical models (t and p respectively) are multiplied together in line 269 (line 2 in extract above) and then normalized as in 4. in line 275 (fourth line from the bottom in extract above). For hybrid features, you can check this implementation. The author has presented mathematical justification in his Quora answer, you might want to check. You will need the following steps: It should be easy enough to see how you can add your own prior instead of using those learned from the data.Regression algorithms seem to be working on features represented as numbers. 
For example:  This data set doesn't contain categorical features/variables. It's quite clear how to do regression on this data and predict price. But now I want to do a regression analysis on data that contain categorical features:  There are 5 features: District, Condition, Material, Security, Type How can I do a regression on this data? Do I have to transform all the string/categorical data to numbers manually? I mean if I have to create some encoding rules and according to that rules transform all data to numeric values.  Is there any simple way to transform string data to numbers without having to create my own encoding rules manually? Maybe there are some libraries in Python that can be used for that? Are there some risks that the regression model will be somehow incorrect due to "bad encoding"? Yes, you will have to convert everything to numbers. That requires thinking about what these attributes represent. Usually there are three possibilities: You have to be carefull to not infuse information you do not have in the application case. If you have categorical data, you can create dummy variables with 0/1 values for each possible value. E. g. to This can easily be done with pandas: will result in: Create a mapping of your sortable categories,  e. g.
old < renovated < new → 0, 1, 2 This is also possible with pandas: Result: You could use the mean for each category over past (known events). Say you have a DataFrame with the last known mean prices for cities: Result: In linear regression with categorical variables you should be careful of the Dummy Variable Trap. The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others. This can produce singularity of a model, meaning your model just won't work. Read about it here Idea is to use dummy variable encoding with drop_first=True, this will omit one column from each category after converting categorical variable into dummy/indicator variables. You WILL NOT lose any relevant information by doing that simply because your all point in dataset can fully be explained by rest of the features.  Here is complete code on how you can do it for your housing dataset So you have categorical features:  And one numerical features that you are trying to predict: First you need to split your initial dataset on input variables and prediction, assuming its pandas dataframe it would look like this: Input variables: Prediction: Convert categorical variable into dummy/indicator variables and drop one in each category: So now if you check shape of X with drop_first=True you will see that it has 4 columns less - one for each of your categorical variables.  You can now continue to use them in your linear model. For scikit-learn implementation it could look like this: You can use "Dummy Coding" in this case.
There are Python libraries to do dummy coding, you have a few options: An example with pandas is below: One way to achieve regression with categorical  variables as independent variables  is as mentioned above - Using encoding. 
Another way of  doing is by using R like statistical formula using statmodels library. Here is a code  snippet Dataset Summary of regressionDoes tensorflow have something similar to scikit learn's one hot encoder for processing categorical data?  Would using a placeholder of tf.string behave as categorical data? I realize I can manually pre-process the data before sending it to tensorflow, but having it built in is very convenient. As of TensorFlow 0.8, there is now a native one-hot op, tf.one_hot that can convert a set of sparse labels to a dense one-hot representation.  This is in addition to tf.nn.sparse_softmax_cross_entropy_with_logits, which can in some cases let you compute the cross entropy directly on the sparse labels instead of converting them to one-hot. Previous answer, in case you want to do it the old way:
@Salvador's answer is correct - there (used to be) no native op to do it.  Instead of doing it in numpy, though, you can do it natively in tensorflow using the sparse-to-dense operators: The output, labels, is a one-hot matrix of batch_size x num_labels. Note also that as of 2016-02-12 (which I assume will eventually be part of a 0.7 release), TensorFlow also has the tf.nn.sparse_softmax_cross_entropy_with_logits op, which in some cases can let you do training without needing to convert to a one-hot encoding. Edited to add:  At the end, you may need to explicitly set the shape of labels.  The shape inference doesn't recognize the size of the num_labels component.  If you don't need a dynamic batch size with derived_size, this can be simplified. Edited 2016-02-12 to change the assignment of outshape per comment below. tf.one_hot() is available in TF and easy to use.  Lets assume you have 4 possible categories (cat, dog, bird, human) and 2 instances (cat, human). So your depth=4 and your indices=[0, 3] Keep in mind that if you provide index=-1 you will get all zeros in your one-hot vector. Old answer, when this function was not available. After looking though the python documentation, I have not found anything similar. One thing that strengthen my belief that it does not exist is that in their own example they write one_hot manually. You can also do this in scikitlearn. numpy does it! A simple and short way to one-hot encode any integer or list of intergers: Recent versions of TensorFlow (nightlies and maybe even 0.7.1) have an op called tf.one_hot that does what you want.  Check it out! On the other hand if you have a dense matrix and you want to look up and aggregate values in it, you would want to use the embedding_lookup function. Maybe it's due to changes to Tensorflow since Nov 2015, but @dga's answer produced errors. I did get it to work with the following modifications: Take a look at tf.nn.embedding_lookup. It maps from categorical IDs to their embeddings.  For an example of how it's used for input data, see here. You can use tf.sparse_to_dense: The sparse_indices argument indicates where the ones should go, output_shape should be set to the number of possible outputs (e.g. the number of labels), and sparse_values should be 1 with the desired type (it will determine the type of the output from the type of sparse_values). There's embedding_ops in Scikit Flow and examples that deal with categorical variables, etc.  If you just begin to learn TensorFlow, I would suggest you trying out examples in TensorFlow/skflow first and then once you are more familiar with TensorFlow it would be fairly easy for you to insert TensorFlow code to build a custom model you want (there are also examples for this).  Hope those examples for images and text understanding could get you started and let us know if you encounter any issues! (post issues or tag skflow in SO).  Current versions of tensorflow implement the following function for creating one-hot tensors: https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#one_hot As mentioned above by @dga, Tensorflow has tf.one_hot now: You need to specify depth, otherwise you'll get a pruned one-hot tensor. If you like to do it manually: Note arguments order in tf.concat() There are  a couple ways to do it. The other way to do it is. My version of @CFB and @dga example, shortened a bit to ease understanding. works on TF version 1.3.0. As of Sep 2017.  Tensorflow 2.0 Compatible Answer: You can do it efficiently using Tensorflow Transform.  Code for performing One-Hot Encoding using Tensorflow Transform is shown below: For more information, refer this Tutorial on TF_Transform.I have a binary prediction model trained by logistic regression algorithm. I want know which features(predictors) are more important for the decision of positive or negative class. I know there is coef_ parameter comes from the scikit-learn package, but I don't know whether it is enough to for the importance. Another thing is how I can evaluate the coef_ values in terms of the importance for negative and positive classes. I also read about standardized regression coefficients and I don't know what it is. Lets say there are features like size of tumor, weight of tumor, and etc to make a decision for a test case like malignant or not malignant. I want to know which of the features are more important for malignant and not malignant prediction. Does it make sort of sense? One of the simplest options to get a feeling for the "influence" of a given parameter in a linear classification model (logistic being one of those), is to consider the magnitude of its coefficient times the standard deviation of the corresponding parameter in the data. Consider this example: An alternative way to get a similar result is to examine the coefficients of the model fit on standardized parameters: Note that this is the most basic approach and a number of other techniques for finding feature importance or parameter influence exist (using p-values, bootstrap scores, various "discriminative indices", etc).  I am pretty sure you would get more interesting answers at https://stats.stackexchange.com/.I'm using TfidfVectorizer from scikit-learn to do some feature extraction from text data. I have a CSV file with a Score (can be +1 or -1) and a Review (text). I pulled this data into a DataFrame so I can run the Vectorizer. This is my code:  This is the traceback for the error I get:  I checked the CSV file and DataFrame for anything that's being read as NaN but I can't find anything. There are 18000 rows, none of which return isnan as True.  This is what df['Review'].head() looks like:  You need to convert the dtype object to unicode string as is clearly mentioned in the traceback. From the Doc page of TFIDF Vectorizer: fit_transform(raw_documents, y=None)  Parameters:     raw_documents : iterable 
an iterable which yields either str, unicode or file objects I find a more efficient way to solve this problem. Of course you can use df['Review'].values.astype('U') to convert the entire Series. But I found using this function will consume much more memory if the Series you want to convert is really big. (I test this with a Series with 800k rows of data, and doing this astype('U') will consume about 96GB of memory) Instead, if you use the lambda expression to only convert the data in the Series from str to numpy.str_, which the result will also be accepted by the fit_transform function, this will be faster and will not increase the memory usage. I'm not sure why this will work because in the Doc page of TFIDF Vectorizer: fit_transform(raw_documents, y=None) Parameters: raw_documents : iterable an iterable which yields either str, unicode or file objects But actually this iterable must yields np.str_ instead of str. I was getting MemoryError even after using .values.astype('U') for the reviews in my dataset.  So i tried .astype('U').values and it worked.  This is a answer from: Python: how to avoid MemoryError when transform text data into Unicode using astype('U')Cross entropy formula:  But why does the following give loss = 0.7437 instead of loss = 0 (since 1*log(1) = 0)? In your example you are treating output [0, 0, 0, 1] as probabilities as required by the mathematical definition of cross entropy.  But PyTorch treats them as outputs, that don’t need to sum to 1, and need to be first converted into probabilities for which it uses the softmax function. So H(p, q) becomes: Translating the output [0, 0, 0, 1] into probabilities: whence: Your understanding is correct but pytorch doesn't compute cross entropy in that way. Pytorch uses the following formula. Since, in your scenario, x = [0, 0, 0, 1] and class = 3, if you evaluate the above expression, you would get: Pytorch considers natural logarithm. I would like to add an important note, as this often leads to confusion. Softmax is not a loss function, nor is it really an activation function. It has a very specific task: It is used for multi-class classification to normalize the scores for the given classes. By doing so we get probabilities for each class that sum up to 1. Softmax is combined with Cross-Entropy-Loss to calculate the loss of a model. Unfortunately, because this combination is so common, it is often abbreviated. Some are using the term Softmax-Loss, whereas PyTorch calls it only Cross-Entropy-Loss. The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using
nn.CrossEntropyLoss. This terminology is a particularity of PyTorch, as the
nn.NLLoss [sic] computes, in fact, the cross entropy but with log probability predictions as inputs where nn.CrossEntropyLoss takes scores (sometimes called logits). Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs. PyTorch's CrossEntropyLoss expects unbounded scores (interpretable as logits / log-odds) as input, not probabilities (as the CE is traditionally defined).When you run a Keras neural network model you might see something like this in the console:  As time goes on the loss hopefully improves. I want to log these losses to a file over time so that I can learn from them. I have tried:  but this doesn't work. I am not sure what level of logging I need in this situation.  I have also tried using a callback like in:  but obviously this isn't writing to a file. Whatever the method, through a callback or the logging module or anything else, I would love to hear your solutions for logging loss of a keras neural network to a file. Thanks!  You can use CSVLogger callback. as example: Look at: Keras Callbacks There is a simple solution to your problem. Every time any of the fit methods are used - as a result the special callback called History Callback is returned. It has a field history which is a dictionary of all metrics registered after every epoch. So to get list of loss function values after every epoch you can easly do: It's easy to save such list to a file (e.g. by converting it to numpy array and using savetxt method). UPDATE: Try: UPDATE 2: The solution to the problem of recording a loss after every batch is written in Keras Callbacks Documentation in a Create a Callback paragraph. Old question, but here goes. Keras history output perfectly matches pandas DataSet input. If you want the entire history to csv in one line:

pandas.DataFrame(model.fit(...).history).to_csv("history.csv")
 Cheers You can redirect the sys.stdout object to a file before the model.fit method and reassign it to the standard console after model.fit method as follows: So In TensorFlow 2.0, it is quite easy to get Loss and Accuracy of each epoch because it returns a History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values  If you have validation Data If you don't have validation Data Then to save list data into text file use the below code Best is to create a LambdaCallback: Now,Just add it like this in the model.fit function:The problem is that my train data could not be placed into RAM due to train data size. So I need a method which first builds one tree on whole train data set, calculate residuals build another tree and so on (like gradient boosted tree do). Obviously if I call model = xgb.train(param, batch_dtrain, 2) in some loop - it will not help, because in such case it just rebuilds whole model for each batch. Try saving your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model. Here's a small experiment that I ran to convince myself that it works: First, split the boston dataset into training and testing sets.
Then split the training set into halves.
Fit a model with the first half and get a score that will serve as a benchmark.
Then fit two models with the second half; one model will have the additional parameter xgb_model. If passing in the extra parameter didn't make a difference, then we would expect their scores to be similar..
But, fortunately, the new model seems to perform much better than the first. reference: https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py There is now (version 0.6?) a process_update parameter that might help.  Here's an experiment with it: Output: I created a gist of jupyter notebook to demonstrate that xgboost model can be trained incrementally. I used boston dataset to train the model. I did 3 experiments - one shot learning, iterative one shot learning, iterative incremental learning. In incremental training, I passed the boston data to the model in batches of size 50. The gist of the gist is that you'll have to iterate over the data multiple times for the model to converge to the accuracy attained by one shot (all data) learning. Here is the corresponding code for doing iterative incremental learning with xgboost. XGBoost version: 0.6 looks like you don't need anything other than call your xgb.train(....) again but provide the model result from the previous batch: this is based on https://xgboost.readthedocs.io/en/latest/python/python_api.html
  If your problem is regarding the dataset size and you do not really need Incremental Learning (you are not dealing with an Streaming app, for instance), then you should check out Spark or Flink.  This two frameworks can train on very large datasets with a small RAM, leveraging disk memory. Both framework deal with memory issues internally. While Flink had it solved first, Spark has caught up in recent releases. Take a look at: To paulperry's code, If change one line from "train_split = round(len(train_idx) / 2)" to "train_split = len(train_idx) - 50". model 1+update2 will changed from 14.2816257268 to 45.60806270012028. And a lot of "leaf=0" result in dump file. Updated model is not good when update sample set is relative small.
For binary:logistic, updated model is unusable when update sample set has only one class. One possible solution that I have not tested is to used a dask dataframe which  should act the same as a pandas dataframe but (I assume) utilize disk and reads in and out of RAM. here are some helpful links.
this link mentions how to use it with xgboost also see
also see.
further there is an experimental options from XGBoost as well here but it is "not ready for production" I agree with @desertnaut in his solution. I have a dataset where I split it into 4 batches.  I have to do an initial fit without the xgb_model parameter first, then the next fits will have the  xgb_model parameter, like in this (I'm using the Sklearn API): It's not based on xgboost, but there is a C++ incremental decision tree.
see gaenari. Continuous chunking data can be inserted and updated, and rebuilds can be run if concept drift reduces accuracy.I'm trying to learn scikit-learn and Machine Learning by using the Boston Housing Data Set. Based on this new model clf_sgd, I am trying to predict the y based on the first instance of X_train. However, the result is quite odd for me (1.34032174, instead of 20-30, the range of the price of the houses) I guess that this 1.34032174 value should be scaled back, but I am trying to figure out how to do it with no success. Any tip is welcome. Thank you very much. You can use inverse_transform using your scalery object: Bit late to the game: 
Just don't scale your y. With scaling y you actually loose your units. The regression or loss optimization is actually determined by the relative differences between the features. BTW for house prices (or any other monetary value) it is common practice to take the logarithm. Then you obviously need to do an numpy.exp() to get back to the actual dollars/euros/yens...I am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools. Additionally, I will be grateful if you point any Python based solution / library for this. Thanks One way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document. To identify keywords like this, you could use the point-wise mutual information of the keyword and the document. This is given by PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection. To identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score.  If you want to extract multiword tags, see the StackOverflow question How to extract common / significant phrases from a series of text entries.  Borrowing from my answer to that question, the NLTK collocations how-to covers how to do 
extract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.: First, the key python library for computational linguistics is NLTK ("Natural Language Toolkit"). This is a stable, mature library created and maintained by professional computational linguists. It also has an extensive collection of tutorials, FAQs, etc. I recommend it highly. Below is a simple template, in python code, for the problem raised in your Question; although it's a template it runs--supply any text as a string (as i've done) and it will return a list of word frequencies as well as a ranked list of those words in order of 'importance' (or suitability as keywords) according to a very simple heuristic. Keywords for a given document are (obviously) chosen from among important words in a document--ie, those words that are likely to distinguish it from another document. If you had no a priori knowledge of the text's subject matter, a common technique is to infer the importance or weight of a given word/term from its frequency, or importance = 1/frequency. http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation tries to represent each document in a training corpus as mixture of topics, which in turn are distributions mapping words to probabilities.   I had used it once to dissect a corpus of product reviews into the latent ideas that were being spoken about across all the documents such as 'customer service', 'product usability', etc.. The basic model does not advocate a way to convert the topic models into a single word describing what a topic is about.. but people have come up with all kinds of heuristics to do that once their model is trained.   I recommend you try playing with http://mallet.cs.umass.edu/ and seeing if this model fits your needs..   LDA is a completely unsupervised algorithm meaning it doesn't require you to hand annotate anything which is great, but on the flip side, might not deliver you the topics you were expecting it to give. A very simple solution to the problem would be: I'm sure there are cleverer, stats based solutions though. If you need a solution to use in a larger project rather than for interests sake, Yahoo BOSS has a key term extraction method. Latent Dirichlet allocation or Hierarchical Dirichlet Process can be used to generate tags for individual texts within a greater corpus (body of texts) by extracting the most important words from the derived topics. A basic example would be if we were to run LDA over a corpus and define it to have two topics, and that we find further that a text in the corpus is 70% one topic, and 30% another. The top 70% of the words that define the first topic and 30% that define the second (without duplication) could then be considered as tags for the given text. This method provides strong results where tags generally represent the broader themes of the given texts. With a general reference for preprocessing needed for these codes being found here, we can find tags through the following process using gensim. A heuristic way of deriving the optimal number of topics for LDA is found in this answer. Although HDP does not require the number of topics as an input, the standard in such cases is still to use LDA with a derived topic number, as HDP can be problematic. Assume here that the corpus is found to have 10 topics, and we want 5 tags per text: Assume further that we have a variable corpus, which is a preprocessed list of lists, with the subslist entries being word tokens. Initialize a Dirichlet dictionary and create a bag of words where texts are converted to their indexes for their component tokens (words): Create an LDA or HDP model: The following code produces ordered lists for the most important words per topic (note that here is where num_tags defines the desired tags per text): Then find the coherence of the topics across the texts: From here we have the percentage that each text coheres to a given topic, and the words associated with each topic, so we can combine them for tags with the following: corpus_tags will be a list of tags for each text based on how coherent the text is to the derived topics. See this answer for a similar version of this that generates tags for a whole text corpus.I'm slightly confused in regard to how I save a trained classifier. As in, re-training a classifier each time I want to use it is obviously really bad and slow, how do I save it and the load it again when I need it? Code is below, thanks in advance for your help. I'm using Python with NLTK Naive Bayes Classifier.  To save: To load later: I went thru the same problem, and you cannot save the object since is a ELEFreqDistr NLTK class. Anyhow NLTK is hell slow. Training took 45 mins on a decent set and I decided to implement my own version of the algorithm (run it with pypy or rename it .pyx and install cython). It takes about 3 minutes with the same set and it can simply save data as json (I'll implement pickle which is faster/better).  I started a simple github project, check out the code here To Retrain the Pickled Classifer :I have a dataset from sklearn and I plotted the distribution of the load_diabetes.target data (i.e. the values of the regression that the load_diabetes.data are used to predict).  I used this because it has the fewest number of variables/attributes of the regression sklearn.datasets. Using Python 3, How can I get the distribution-type and parameters of the distribution this most closely resembles?  All I know the target values are all positive and skewed (positve skew/right skew). . . Is there a way in Python to provide a few distributions and then get the best fit for the target data/vector? OR, to actually suggest a fit based on the data that's given? That would be realllllly useful for people who have theoretical statistical knowledge but little experience with applying it to "real data".  Bonus
Would it make sense to use this type of approach to figure out what your posterior distribution would be with "real data" ? If no, why not?  Use this approach To the best of my knowledge, there is no automatic way of obtaining the distribution type and parameters of a sample (as inferring the distribution of a sample is a statistical problem by itself). In my opinion, the best you can do is: (for each attribute) Try to fit each attribute to a reasonably large list of possible distributions 
(e.g. see Fitting empirical distribution to theoretical ones with Scipy (Python)? for an example with Scipy) Evaluate all your fits and pick the best one. This can be done by performing a Kolmogorov-Smirnov test between your sample and each of the distributions of the fit (you have an implementation in Scipy, again), and picking the one that minimises D, the test statistic (a.k.a. the difference between the sample and the fit). Bonus: It would make sense - as you'll be building a model on each of the variables as you pick a fit for each one - although the goodness of your prediction would depend on the quality of your data and the distributions you are using for fitting. You are building a model, after all. You can use that code to fit (according to the maximum likelihood) different distributions with your datas:  You can see a sample snippet about how to use the parameters obtained here: Fitting empirical distribution to theoretical ones with Scipy (Python)? Then, you can pick the distribution with the best log likelihood (there are also other criteria to match the "best" distribution, such as Bayesian posterior probability, AIC, BIC or BICc values, ...).  For your bonus question, there's I think no generic answer. If your set of data is significant and obtained under the same conditions as the real word datas, you can do it.  This code also works: fitter provides an iteration process over possible fitting distributions.
It also outputs a plot and summary table with statistic values. fitter package provides a simple class to identify the distribution
from which a data samples is generated from. It uses 80 distributions
from Scipy and allows you to plot the results to check what is the
most probable distribution and the best parameters. So basically the same iterative fit test procedure as described in other answers, but conveniently executed by the module.  Result for your SR_y series:  Code: Parameters of those fitted distributions as a dict: To get a list of all available distributions: Testing for all of them takes a long time, so it's best to use the implemened get_common_distributions() and potentially extend them with likely distribution as done in the code above. Make sure to have the current (1.4.1 or newer) fitter version installed: I had logging errors with a previous one and for my conda environment I needed: On a similar question (see here) you may be interrested in @Michel_Baudin answer explaining. His code assesses around 40 different distributions available OpenTURNS library and chooses the best one according to the BIC criterion. Looks something like that:I was using the Decision Tree and this error was raised. The same situation appeared when I used Back Propagation. How can I solve it? Traceback (most recent call last):
  File "<ipython-input-40-4359c06ae1f0>", line 1, in <module>
    runfile('C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py', wdir='C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib')
  File "C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py", line 710, in runfile
    execfile(filename, namespace)
  File "C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File "C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py", line 9, in <module>
    from numpy.testing.nosetester import import_nose ModuleNotFoundError: No module named 'numpy.testing.nosetester' This is happening due to a version incompatibility between numpy and scipy. numpy in its latest versions have deprecated numpy.testing.nosetester. and Triggers the error. Upgrade your scipy to a higher version. But not limited to this. By upgrading the above libraries to the latest stable, you should be able to get rid of this error. I needed to upgrade scipy pip3 install -U scipy I was facing the same error while using lexnlp package 
Got it fixed by installing: (Only install lexnlp if know you're explicitly using it in your project and you know what you're doing) I solved this by: and using: I also faced same issue while loading the model and fixed by upgrading below libraries try installing numpy version 1.17.0 using pip or pip3 
(assuming you already installed pip3) If you are using Jetson TX2 or any other aarch64 based device. You can solve the issue by installing latest numpy and scipy libraries. This also works for x86 based systems. (You can skip libatlas-base-dev and gfortran for x86 based systems) For me it solved by this link, apparently an open issue.
Downgrade to numpy==1.16.4I've trained a Linear Regression model with R caret. I'm now trying to generate a confusion matrix and keep getting the following error: Error in confusionMatrix.default(pred, testing$Final) : 
the data and reference factors must have the same number of levels The error occurs when generating the confusion matrix. The levels are the same on both objects. I cant figure out what the problem is. Their structure and levels are given below. They should be the same. Any help would be greatly appreciated as its making me cracked!! I had the same issue. 
I guess it happened because data argument was not casted as factor as I expected.
Try:  hope it helps Do table(pred) and table(testing$Final). You will see that there is at least one number in the testing set that is never predicted (i.e. never present in pred). This is what is meant why "different number of levels". There is an example of a custom made function to get around this problem here. However, I found that this trick works fine: It should give you exactly the same confusion matrix as with the function.  Whenever you try to build a confusion matrix, make sure that both the true values and prediction values are of factor datatype.  Here both pred and testing$Final must be of type factor. Instead of check for levels, check the type of both the variables and convert them to factor if they are not. Here testing$final is of type int. conver it to factor and then build the confusion matrix. Something like the follows seem to work for me. The idea is similar to that of @nayriz: The key is to make sure the factor levels match. On a similar error, I forced the GLM predictions to have the same class as the dependent variable. For example, a GLM will predict a "numeric" class. But with the target variable being a "factor" class, I ran into an error. erroneous code: Result: corrected code: Result: I had this problem due to NAs for the target variable in the dataset. If you're using the tidyverse, you can use the drop_na function to remove rows that contain NAs. Like this: For base R, it might look something like: We get this error when creating the confusion matrix. When creating a confusion matrix, we need to make sure that the predicted value and the actual value of the data type are "factors". If there are other data types, we must convert them to "factor" data factors before generating a confusion matrix. After this conversion, start compiling the confusion matrix. Your are using regression and trying to generate a confusion matrix. I believe confusion matrix is used for classification task. Generally people use R^2 and RMSE metrics.I would like to know if there is a way to implement the different score function from the scikit learn package like this one : into a tensorflow model to get the different score. Will i have to run the session again to get the prediction ? You do not really need sklearn to calculate precision/recall/f1 score. You can easily express them in TF-ish way by looking at the formulas:  Now if you have your actual and predicted values as vectors of 0/1, you can calculate TP, TN, FP, FN using tf.count_nonzero: Now your metrics are easy to calculate: Maybe this example will speak to you :     Previous answers do not specify how to handle the multi-label case so here is such a version implementing three types of multi-label f1 score in tensorflow: micro, macro and weighted (as per scikit-learn) Update (06/06/18): I wrote a blog post about how to compute the streaming multilabel f1 score in case it helps anyone (it's a longer process, don't want to overload this answer) outputs: Since i have not enough reputation to add a comment to Salvador Dalis answer this is the way to go: tf.count_nonzero casts your values into an tf.int64 unless specified otherwise. Using: is a realy good idea. Use the metrics APIs provided in tf.contrib.metrics, for example:I have tried many examples with F1 micro and Accuracy in scikit-learn and in all of them, I see that F1 micro is the same as Accuracy. Is this always true? Script Output F1 micro = Accuracy In classification tasks for which every test case is guaranteed to be assigned to exactly one class, micro-F is equivalent to accuracy. It won't be the case in multi-label classification. This is because we are dealing with a multi class classification , where every test data should belong to only 1 class and not multi label , in such case where there is no TN , we can call True Negatives as True Positives. Formula wise ,  correction : F1 score is 2* precision* recall / (precision + recall)  Micoaverage precision, recall, f1 and accuracy are all equal for cases in which every instance must be classified into one (and only one) class. A simple way to see this is by looking at the formulas precision=TP/(TP+FP) and recall=TP/(TP+FN). The numerators are the same, and every FN for one class is another classes's FP, which makes the denominators the same as well. If precision = recall, then f1 will also be equal. For any inputs should should be able to show that: I had the same issue so I investigated and came up with this: Just thinking about the theory, it is impossible that accuracy and the f1-score are the very same for every single dataset. The reason for this is that the f1-score is independent from the true-negatives while accuracy is not. By taking a dataset where f1 = acc and adding true negatives to it, you get f1 != acc.I just try to find out how I can use Caffe. To do so, I just took a look at the different .prototxt files in the examples folder. There is one option I don't understand: Possible values seem to be: Could somebody please explain those options? It is a common practice to decrease the learning rate (lr) as the optimization/learning process progresses. However, it is not clear how exactly the learning rate should be decreased as a function of the iteration number. If you use DIGITS as an interface to Caffe, you will be able to visually see how the different choices affect the learning rate. fixed: the learning rate is kept fixed throughout the learning process. inv: the learning rate is decaying as ~1/T
 step: the learning rate is piecewise constant, dropping every X iterations
  multistep: piecewise constant at arbitrary intervals
 You can see exactly how the learning rate is computed in the function SGDSolver<Dtype>::GetLearningRate (solvers/sgd_solver.cpp line ~30). Recently, I came across an interesting and unconventional approach to learning-rate tuning: Leslie N. Smith's work "No More Pesky Learning Rate Guessing Games". In his report, Leslie suggests to use lr_policy that alternates between decreasing and increasing the learning rate. His work also suggests how to implement this policy in Caffe. If you look inside the /caffe-master/src/caffe/proto/caffe.proto file (you can find it online here) you will see the following descriptions:I'm dealing with an imbalanced dataset and want to do a grid search to tune my model's parameters using scikit's gridsearchcv. To oversample the data, I want to use SMOTE, and I know I can include that as a stage of a pipeline and pass it to gridsearchcv.
My concern is that I think smote will be applied to both train and validation folds, which is not what you are supposed to do. The validation set should not be oversampled.
Am I right that the whole pipeline will be applied to both dataset splits? And if yes, how can I turn around this?
Thanks a lot in advance Yes, it can be done, but with imblearn Pipeline. You see, imblearn has its own Pipeline to handle the samplers correctly. I described this in a similar question here. When called predict() on a imblearn.Pipeline object, it will skip the sampling method and leave the data as it is to be passed to next transformer.
You can confirm that by looking at the source code here: So for this to work correctly, you need the following: Fill the details as necessary, and the pipeline will take care of the rest.I am fine-tuning a MobileNet with 14 new classes. When I add new layers by: I get the error: Also using: I get the error: What does lower mean? I saw other fine-tuning scripts and there were no other arguments other than the name of the model which is x in this case. The tensor must be passed to the layer when you are calling it, and not as an argument. Therefore it must be like this: To make it more clear, it is equivalent to this:Confused about random_state parameter, not sure why decision tree training needs some randomness. My thoughts http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html This is explained in the documentation The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement. So, basically, a sub-optimal greedy algorithm is repeated a number of times using random selections of features and samples (a similar technique used in random forests). The random_state parameter allows controlling these random choices. The interface documentation specifically states: If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. So, the random algorithm will be used in any case. Passing any value (whether a specific int, e.g., 0, or a RandomState instance), will not change that. The only rationale for passing in an int value (0 or otherwise) is to make the outcome consistent across calls: if you call this with random_state=0 (or any other value), then each and every time, you'll get the same result. The above cited part of the documentation is misleading, the underlying problem is not greediness of the algorithm. The CART algorithm is deterministic (see e.g. here) and finds a global minimum of the weighted Gini indices. Repeated runs of the decision tree can give different results because it is sometimes possible to split the data using different features and still achieve the same Gini index. This is described here:
https://github.com/scikit-learn/scikit-learn/issues/8443. Setting the random state simply assures that the CART implementation works through the same randomized list of features when looking for the minimum. Decision trees use heuristics process. Decision tree do not guarantee the same solution globally. There will be variations in the tree structure each time you build a model. Passing a specific seed to random_state ensures the same result is generated each time you build the model. The random_state parameter present for decision trees in scikit-learn determines which feature to select for a split if (and only if) there are two splits that are equally good (i.e. two features yield the exact same improvement in the selected splitting criteria (e.g. gini)). If this is not the case, the random_state parameter has no effect. The issue linked in teatrader's answer discusses this in more detail and as a result of that discussion the following section was added to the docs (emphasis added): random_state int, RandomState instance or None, default=None Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to "best". When max_features < n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See Glossary for details. To illustrate, let's consider the following example with the iris sample data set and a shallow decision tree containing just a single split: The output of this code will alternate between the two following trees based on which random_state is used.   The reason for this is that splitting on either petal length <= 2.45 or petal width <= 0.8 will both perfectly separate out the setosa class from the other two classes (we can see that the leftmost setosa node contains all 50 of the setosa observations). If we change just one observation of the data so that one of the previous two  splitting criteria no longer produces a perfect separation, the random_state will have no effect and we will always end up with the same result, for example:  The first split will now always be petal length <= 2.45 since the split petal width <= 0.8 can only separate out 49 of the 50 setosa classes (in other words a lesser decreases in the gini score). For a random forest (which consists of many decision trees), we would create each individual tree with a random selections of features and samples (see https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters for details), so there is a bigger role for the random_state parameter, but this is not the case when training just a single decision tree (this is true with the default parameters, but it is worth noting that some parameters could be affected by randomness if they are changed from the default value, most notably setting splitter="random"). A couple of related issues: Many machine learning models allow some randomness in model training. Specifying a number for random_state ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won't depend meaningfully on exactly what value you choose.Suppose I want to have the general neural network architecture: Input1 is image data, input2 is non-image data. I have implemented this architecture in Tensorflow. All pytorch examples I have found are one input go through each layer. How can I define forward func to process 2 inputs separately then combine them in a middle layer?  By "combine them" I assume you mean to concatenate the two inputs.
Assuming you concat along the second dimension: Note that when you define the number of inputs to self.fc2 you need to take into account both out_channels of self.conv as well as the output spatial dimensions of c.I wish to implement early stopping with Keras and sklean's GridSearchCV. The working code example below is modified from How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras. The data set may be downloaded from here. The modification adds the Keras EarlyStopping callback class to prevent over-fitting. For this to be effective it requires the monitor='val_acc' argument for monitoring validation accuracy. For val_acc to be available KerasClassifier requires the validation_split=0.1 to generate validation accuracy, else EarlyStopping raises RuntimeWarning: Early stopping requires val_acc available!. Note the FIXME: code comment! Note we could replace val_acc by val_loss!  Question: How can I use the cross-validation data set generated by the GridSearchCV k-fold algorithm instead of wasting 10% of the training data for an early stopping validation set?  [Answer after the question was edited & clarified:] Before rushing into implementation issues, it is always a good practice to take some time to think about the methodology and the task itself; arguably, intermingling early stopping with the cross validation procedure is not a good idea. Let's make up an example to highlight the argument. Suppose that you indeed use early stopping with 100 epochs, and 5-fold cross validation (CV) for hyperparameter selection. Suppose also that you end up with a hyperparameter set X giving best performance, say 89.3% binary classification accuracy. Now suppose that your second-best hyperparameter set, Y, gives 89.2% accuracy. Examining closely the individual CV folds, you see that, for your best case X, 3 out of the 5 CV folds exhausted the max 100 epochs, while in the other 2 early stopping kicked in, say in 95 and 93 epochs respectively. Now imagine that, examining your second-best set Y, you see that again 3 out of the 5 CV folds exhausted the 100 epochs, while the other 2 both stopped early enough at ~ 80 epochs. What would be your conclusion from such an experiment? Arguably, you would have found yourself in an inconclusive situation; further experiments might reveal which is actually the best hyperparameter set, provided of course that you would have thought to look into these details of the results in the first place. And needless to say, if all this was automated through a callback, you might have missed your best model despite the fact that you would have actually tried it. The whole CV idea is implicitly based on the "all other being equal" argument (which of course is never true in practice, only approximated in the best possible way). If you feel that the number of epochs should be a hyperparameter, just include it explicitly in your CV as such, rather than inserting it through the back door of early stopping, thus possibly compromising the whole process (not to mention that early stopping has itself a hyperparameter, patience). Not intermingling these two techniques doesn't mean of course that you cannot use them sequentially: once you have obtained your best hyperparameters through CV, you can always employ early stopping when fitting the model in your whole training set (provided of course that you do have a separate validation set). The field of deep neural nets is still (very) young, and it is true that it has yet to establish its "best practice" guidelines; add the fact that, thanks to an amazing community, there are all sort of tools available in open source implementations, and you can easily find yourself into the (admittedly tempting) position of mixing things up just because they happen to be available. I am not necessarily saying that this is what you are attempting to do here - I am just urging for more caution when combining ideas that may have not been designed to work along together... [Old answer, before the question was edited & clarified - see updated & accepted answer above] I am not sure I have understood your exact issue (your question is quite unclear, and you include many unrelated details, which is never good when asking a SO question - see here). You don't have to (and actually should not) include any arguments about validation data in your model = KerasClassifier() function call (it is interesting why you don't feel the same need for training data here, too). Your grid.fit() will take care of both the training and validation folds. So provided that you want to keep the hyperparameter values as included in your example, this function call should be simply You can see some clear and well-explained examples regarding the use of GridSearchCV with Keras here. Here is how to do it with only a single split. If you want more splits, you can use 'cl__validation_split' with a fixed ratio and construct splits that meet that criteria. It might be too paranoid, but I don't use the early stopping data set as a validation data set since it was indirectly used to create the model. I also think if you are using early stopping with your final model, then it should also be done when you are doing hyper-parameter search.If I want to train a model with train_generator, is there a significant difference between choosing and Currently I am training for 10 epochs, because each epoch takes a long time, but any graph showing improvement looks very "jumpy" because I only have 10 datapoints. I figure I can get a smoother graph if I use 100 Epochs, but I want to know first if there is any downside to this Based on what you said it sounds like you need a larger batch_size, and of course there are implications with that which could impact the steps_per_epoch and number of epochs. To solve for jumping-around Implications of a larger batch-size When to reduce epochs When to adjust steps-per-epoch Steps per epoch does not connect to epochs. Naturally what you want if to 1 epoch your generator pass through all of your training data one time. To achieve this you should provide steps per epoch equal to number of batches like this: as from above equation the largest the batch_size, the lower the steps_per_epoch.  Next you will choose epoch based on chosen validation. (choose what you think best) steps_per_epoch tells the network how many batches to include in an epoch. By definition, an epoch is considered complete when the dataset has been run through the model once in its entirety. With other words, it means that all training samples have been run through the model. (For further discussion, let us assume that the size of the training examples is 'm'). Also by definition, we know that `batch size' is between [1, m]. Below is what TensorFlow page says about steps_per_epoch If you want to run training only on a specific number of batches from this Dataset, you can pass the steps_per_epoch argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch. Now suppose that your training_size, m = 128 and batch_size, b = 16, which means that your data is grouped into 8 batches. According to the above quote, the maximum value you can assign to steps_per_epoch is 8, as computed in one of the answers by @Ioannis Nasios. However, it is not necessary that you set the value to 8 only (as in our example). You can choose any value between 1 and 8. You just need to be aware that the training will be performed only with this number of batches. The reason for the jumpy error values could be the size of your batch, as correctly mentioned in this answer by @Chris Farr. Training & evaluation from tf.data Datasets If you do this, the dataset is not reset at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset). The advantage of a low value for steps_per_epoch is that different epochs are trained with different data sets (a kind of regularization). However, if you have a limited training size, using only a subset of stacks would not be what we want. It is a decision one has to make. The Steps per epoch denote the number of batches to be selected for one epoch.
If 500 steps are selected then the network will train for 500 batches to complete one epoch.
If we select the large number of epochs it can be computationalI'm building a model in Keras using some tensorflow function (reduce_sum and l2_normalize) in the last layer while encountered this problem. I have searched for a solution but all of it related to "Keras tensor". Here is my code: and then the error:  ValueError: Output tensors to a Model must be the output of a
  TensorFlow Layer (thus holding past layer metadata). Found:
  Tensor("l2_normalize_3:0", shape=(?, 3), dtype=float32) I noticed that without passing fc2 layer to these functions, the model works fine: Can someone please explain to me this problem and some suggestion on how to fix it? I have found a way to work around to solve the problem.
For anyone who encounters the same issue, you can use the Lambda layer to wrap your tensorflow operations, this is what I did: I had this issue because I was adding 2 tensors as x1+x2 somewhere in my model instead of using Add()([x1,x2]).  That solved the problem.I'm trying to convert a string array of categorical variables to an integer array of categorical variables. Ex. I realize this can be done with a loop but I imagine there is an easier way. Thanks. np.unique has some optional returns return_inverse gives the integer encoding, which I use very often it can be used to recreate the original array from uniques ... years later....  For completeness (because this isn't mentioned in the answers) and personal reasons (I always have pandas imported in my modules but not necessarily sklearn), this is also quite straightforward with pandas.get_dummies() One way is to use the categorical function from scikits.statsmodels.  For example: The return value from categorical (b) is actually a design matrix, hence the call to argmax above to get it close to your desired format. Another option is to use a categorical pandas Series: Another way is to use sklearn.preprocessing.LabelEncoder It can convert hashable labels like strings to numerical values ranging between 0 and n_classes-1. It is done like this: If you insist on having the values start from 1 in the resulting array you could simply do c + 1 afterwards. It might not be worth it to bring in sklearn as a dependency for a project only to do this, but it is a good option if you have sklearn already imported.  Another approach is to use Pandas factorize to map items to a number: Well, this is a hack... but does it help? ...some more years pass... Thought I would provide a pure python solution for completeness: You can also try something like this: It would be better if you know what's in there and wish to set specific index for each values. If there's only two categories, next code will work like a charm:LSTM/RNN can be used for text generation.
This shows way to use pre-trained GloVe word embeddings for Keras model. Sample approach tried: Sample code / psuedocode to train LSTM and predict will be appreciated.  I've created a gist with a simple generator that builds on top of your initial idea: it's an LSTM network wired to the pre-trained word2vec embeddings, trained to predict the next word in a sentence. The data is the list of abstracts from arXiv website. I'll highlight the most important parts here. Your code is fine, except for the number of iterations to train it. The default iter=5 seems rather low. Besides, it's definitely not the bottleneck -- LSTM training takes much longer. iter=100 looks better. The result embedding matrix is saved into pretrained_weights array which has a shape (vocab_size, emdedding_size). Your code is almost correct, except for the loss function. Since the model predicts the next word, it's a classification task, hence the loss should be categorical_crossentropy or sparse_categorical_crossentropy. I've chosen the latter for efficiency reasons: this way it avoids one-hot encoding, which is pretty expensive for a big vocabulary. Note passing the pre-trained weights to weights. In order to work with sparse_categorical_crossentropy loss, both sentences and labels must be word indices. Short sentences must be padded with zeros to the common length. This is pretty straight-forward: the model outputs the vector of probabilities, of which the next word is sampled and appended to the input. Note that the generated text would be better and more diverse if the next word is sampled, rather than picked as argmax. The temperature based random sampling I've used is described here. Doesn't make too much sense, but is able to produce sentences that look at least grammatically sound (sometimes). The link to the complete runnable script.I'm learning keras API in tensorflow(2.3). In this guide on tensorflow website, I found an example of custom loss funciton: The reduce_mean function in this custom loss function will return an scalar. Is it right to define loss function like this? As far as I know, the first dimension of the shapes of y_true and y_pred is the batch size. I think the loss function should return loss values for every sample in the batch. So the loss function shoud give an array of shape (batch_size,). But the above function gives a single value for the whole batch. Maybe the above example is wrong? Could anyone give me some help on this problem? p.s. Why do I think the loss function should return an array rather than a single value? I read the source code of Model class. When you provide a loss function (please note it's a function, not a loss class) to Model.compile() method, ths loss function is used to construct a LossesContainer object, which is stored in Model.compiled_loss. This loss function passed to the constructor of LossesContainer class is used once again to construct a LossFunctionWrapper object, which is stored in LossesContainer._losses. According to the source code of LossFunctionWrapper class, the overall loss value for a training batch is calculated by the LossFunctionWrapper.__call__() method (inherited from Loss class), i.e. it returns a single loss value for the whole batch. But the LossFunctionWrapper.__call__() first calls the LossFunctionWrapper.call() method to obtain an array of losses for every sample in the training batch. Then these losses are fianlly averaged to get the single loss value for the whole batch. It's in the LossFunctionWrapper.call() method that the loss function provided to the Model.compile() method is called. That's why I think the custom loss funciton should return an array of losses, insead of a single scalar value. Besides, if we write a custom Loss class for the Model.compile() method, the call() method of our custom Loss class should also return an array, rather than a signal value. I opened an issue on github. It's confirmed that custom loss function is required to return one loss value per sample. The example will need to be updated to reflect this. Actually, as far as I know, the shape of return value of the loss function is not important, i.e. it could be a scalar tensor or a tensor of one or multiple values per sample. The important thing is how it should be reduced to a scalar value so that it could be used in optimization process or shown to the user. For that, you can check the reduction types in Reduction documentation. Further, here is what the compile method documentation says about the loss argument, partially addressing this point: loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses. An objective function is any callable with the signature loss = fn(y_true,y_pred), where y_true = ground truth values with shape = [batch_size, d0, .. dN], except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1]. y_pred = predicted values with shape = [batch_size, d0, .. dN]. It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. In addition, it's worth noting that most of the built-in loss functions in TF/Keras are usually reduced over the last dimension (i.e. axis=-1). For those who doubt that a custom loss function which returns a scalar value would work: you can run the following snippet and you will see that the model would train and converge properly. I opened an issue on github. It's confirmed that custom loss function is required to return one loss value per sample. The example will need to be updated to reflect this. I think the question posted by @Gödel is totally legit and is correct. The custom loss function should return a loss value per sample. And, an explanation provided by @today is also correct. In the end, it all depends on the kind of reduction used. So if one uses class API to create a loss function, then, reduction parameter is automatically inherited in the custom class. Its default value "sum_over_batch_size" is used (which is simply averaging of all the loss values in a given batch). Other options are "sum", which computes a sum instead of averaging and the last option is "none", where an array of loss values are returned. It is also mentioned in the Keras documentation that these differences in reduction are irreverent when one is using model.fit() because reduction is then automatically handled by TF/Keras. And, lastly, it is also mentioned that when a custom loss function is created, then, an array of losses (individual sample losses) should be returned. Their reduction is handled by the framework. Links: The tf.math.reduce_mean takes the average for the batch and returns it. That's why it is a scalar. In machine learning, the loss we use is sum of losses of individual training examples, so it should be a scalar value. (Since for all the examples, we are using a single network, thus we need a single loss value to update the parameters.) When using parallel computation, making container is a simpler and feasible way to keep track of indices of losses computed as we are using batches to train and not the whole training set. The tensorflow documentation missed it, but this is clearly stated and clarified on the Keras documentation. It says: Note that this is an important difference between loss functions like
tf.keras.losses.mean_squared_error and default loss class instances
like tf.keras.losses.MeanSquaredError: the function version does not
perform reduction, but by default the class instance does. And it also states: By default, loss functions return one scalar loss value per input
sample. The dimensionality can be increased because of multiple channels...however, each channel should only have a scalar value for loss.I have some question about pytorch's backward function I don't think I'm getting the right output : the output is maybe it's 2*a*a but i think the output suppose to be 2*a. cause d(x^2)/dx=2x Please read carefully the documentation on backward() to better understand it. By default, pytorch expects backward() to be called for the last output of the network - the loss function. The loss function always outputs a scalar and therefore, the gradients of the scalar loss w.r.t all other variables/parameters is well defined (using the chain rule). Thus, by default, backward() is called on a scalar tensor and expects no arguments. For example: yields As expected: d(a^2)/da = 2a. However, when you call backward on the 2-by-3 out tensor (no longer a scalar function) - what do you expects a.grad to be? You'll actually need a 2-by-3-by-2-by-3 output: d out[i,j] / d a[k,l](!) Pytorch does not support this non-scalar function derivatives.  Instead, pytorch assumes out is only an intermediate tensor and somewhere "upstream" there is a scalar loss function, that through chain rule provides d loss/ d out[i,j]. This "upstream" gradient is of size 2-by-3 and this is actually the argument you provide backward in this case: out.backward(g) where g_ij = d loss/ d out_ij. The gradients are then calculated by chain rule d loss / d a[i,j] = (d loss/d out[i,j]) * (d out[i,j] / d a[i,j]) Since you provided a as the "upstream" gradients you got If you were to provide the "upstream" gradients to be all ones yields As expected. It's all in the chain rule.I need to transform the independent field from string to arithmetical notation. I am using OneHotEncoder for the transformation. My dataset has many independent columns of which some are as: I have to encode the Country column like I succeed to get the desire transformation via using OneHotEncoder as Now I'm getting the depreciation message to use categories='auto'. If I do so the transformation is being done for the all independent columns like country, age, salary etc. How to achieve the transformation on the dataset 0th column only? There is actually 2 warnings :  FutureWarning: The handling of integer data will change in version
  0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the
  unique values. If you want the future behaviour and silence this
  warning, you can specify "categories='auto'". In case you used a
  LabelEncoder before this OneHotEncoder to convert the categories to
  integers, then you can now use the OneHotEncoder directly. and the second : The 'categorical_features' keyword is deprecated in version 0.20 and
  will be removed in 0.22. You can use the ColumnTransformer instead.
  "use the ColumnTransformer instead.", DeprecationWarning) In the future, you should not define the columns in the OneHotEncoder directly, unless you want to use "categories='auto'". The first message also tells you to use OneHotEncoder directly, without the LabelEncoder first.
Finally, the second message tells you to use ColumnTransformer, which is like a Pipe for columns transformations. Here is the equivalent code for your case :  See also : ColumnTransformer documentation For the above example; Encoding Categorical data (Basically Changing Text to Numerical data i.e, Country Name) As of version 0.22, you can write the same code as below: As you can see, you don't need to use LabelEncoder anymore. Reminder will keep previous data while [0]th column will replace will be encoded Dont use the labelencoder and directly use OneHotEncoder. There is a way that you can do one hot encoding with pandas.
Python: Give names to the newly formed columns add it to your dataframe. Check the pandas documentation here. I had the same issue and the following worked for me: Hope this helps  Use the following code :-  This code should solve the error. When updating the code from this: To this: Note that I had to add dtype=np.float to fix the error message TypeError: can't convert np.ndarray of type numpy.object_. Where my colums were [0, 1, 4, 5, 6] and 'one_hot_encoder' is anything. My imports were:I'm currently working on a problem which compares three different machine learning algorithms performance on the same data-set. I divided the data-set into 70/30 training/testing sets and then performed grid search for the best parameters of each algorithm using GridSearchCV and X_train, y_train. First question, am I suppose to perform grid search on the training set or is it suppose to be on the whole data-set? Second question, I know that GridSearchCV uses K-fold in its' implementation, does it mean that I performed cross-validation if I used the same X_train, y_train for all three algorithms I compare in the GridSearchCV? Any answer would be appreciated, thank you. All estimators in scikit where name ends with CV perform cross-validation.
But you need to keep a separate test set for measuring the performance. So you need to split your whole data to train and test. Forget about this test data for a while.  And then pass this train data only to grid-search. GridSearch will split this train data further into train and test to tune the hyper-parameters passed to it. And finally fit the model on the whole train data with best found parameters. Now you need to test this model on the test data you kept aside in the beginning. This will give you the near real world performance of model.  If you use the whole data into GridSearchCV, then there would be leakage of test data into parameter tuning and then the final model may not perform that well on newer unseen data. You can look at my other answers which describe the GridSearch in more detail: Yes, GridSearchCV performs cross-validation. If I understand the concept correctly - you want to keep part of your data set unseen for the model in order to test it. So you train your models against train data set and test them on a testing data set. Here I was doing almost the same - you might want to check it...I basically have the same question as this guy.. The example in the NLTK book for the Naive Bayes classifier considers only whether a word occurs in a document as a feature.. it doesn't consider the frequency of the words as the feature to look at ("bag-of-words"). One of the answers seems to suggest this can't be done with the built in NLTK classifiers.  Is that the case?  How can I do frequency/bag-of-words NB classification with NLTK? scikit-learn has an implementation of multinomial naive Bayes, which is the right variant of naive Bayes in this situation. A support vector machine (SVM) would probably work better, though. As Ken pointed out in the comments, NLTK has a nice wrapper for scikit-learn classifiers. Modified from the docs, here's a somewhat complicated one that does TF-IDF weighting, chooses the 1000 best features based on a chi2 statistic, and then passes that into a multinomial naive Bayes classifier. (I bet this is somewhat clumsy, as I'm not super familiar with either NLTK or scikit-learn.) This printed for me: Not perfect, but decent, considering it's not a super easy problem and it's only trained on 100/100. The features in the NLTK bayes classifier are "nominal", not numeric. This means they can take a finite number of discrete values (labels), but they can't be treated as frequencies. So with the Bayes classifier, you cannot directly use word frequency as a feature-- you could do something like use the 50 more frequent words from each text as your feature set, but that's quite a different thing But maybe there are other classifiers in the NLTK that depend on frequency. I wouldn't know, but have you looked? I'd say it's worth checking out. If your sentence has the same word multiple times, it will just add the probs multiple times.  If the word appears multiple times in the same class, your training data should reflect that in the word count. For added accuracy, count all bi-grams, tri-grams, etc as separate features. It helps to manually write your own classifiers so that you understand exactly what is happening and what you need to do to imporve accuracy.  If you use a pre-packaged solution and it doesn't work well enough, there is not much you can do about it.I train a RandomForestRegressor model on 64bit python.
I pickle the object.
When trying to unpickle the object on 32bit python I get the following error: 'ValueError: Buffer dtype mismatch, expected 'SIZE_t' but got 'long long'' I really have no idea how to fix this, so any help would be hugely appreciated. Edit: more detail This occurs because the random forest code uses different types for indices on 32-bit and 64-bit machines. This can, unfortunately, only be fixed by overhauling the random forests code. Since several scikit-learn devs are working on that anyway, I put it on the todo list. For now, the training and testing machines need to have the same pointer size. For ease, please use python  64 bit version to decentralize your model. I faced the same issue recently. after taking that step it was resolved. So try running it on a 64 bit version. I hope this helps I fixed this problem with training the model in the same machine. I was training the model on Jupyter Notebook(Windows PC) and trying to load into Raspberry Pi but I got the error. Therefore, I trained the model in Raspberry Pi and maintained again then I fixed the problem. I had the same problem when I trained the model with python 3.7.0 32bit installed on my system. It got solved after installing the python 3.8.10 64bit version and training the model again.I need a help understanding the error in while executing the above code. Below is the error: "raise ValueError("x and y must be the same size")" I have .csv file with 1398 rows and 2 column. I have taken 40% as y_test set, as it is visible in the above code. Print X_train shape. What do you see? I'd bet X_train is 2d (matrix with a single column), while y_train 1d (vector). In turn you get different sizes.  I think using X_train[:,0] for plotting (which is from where the error originates) should solve the problem Slicing with [:, :-1] will give you a 2-dimensional array (including all rows and all columns excluding the last column). Slicing with [:, 1] will give you a 1-dimensional array (including all rows from the second column). To make this array also 2-dimensional use [:, 1:2] or [:, 1].reshape(-1, 1) or [:, 1][:, None] instead of [:, 1]. This will make x and y comparable. An alternative to making both arrays 2-dimensional is making them both one dimensional. For this one would do [:, 0] (instead of [:, :1]) for selecting the first column and [:, 1] for selecting the second column. Try this: It will make an evenly spaced array and your error will be gone permanently.This code attempts to utilize a custom implementation of dropout :  Custom dropout is implemented as :  It seems I've implemented the dropout function incorrectly ? : How to modify in order to correctly utilize dropout ? These posts were useful in getting to this point :  Hinton's Dropout in 3 Lines of Python : 
https://iamtrask.github.io/2015/07/28/dropout/ Making a Custom Dropout Function : https://discuss.pytorch.org/t/making-a-custom-dropout-function/14053/2 It seems I've implemented the dropout function incorrectly? In fact, the above implementation is known as Inverted Dropout. Inverted Dropout is how Dropout is implemented in practice in the various deep learning frameworks. What is inverted dropout? Before jump into the inverted dropout, it can be helpful to see how Dropout works for a single neuron:  Since during train phase a neuron is kept on with probability q (=1-p), during the testing phase we have to emulate the behavior of the ensemble of networks used in the training phase. To this end, the authors suggest scaling the activation function by a factor of q during the test phase in order to use the expected output produced in the training phase as the single output required in the test phase (Section 10, Multiplicative Gaussian Noise). Thus:  Inverted dropout is a bit different. This approach consists in the scaling of the activations during the training phase, leaving the test phase untouched. The scale factor is the inverse of the keep probability 1/1-p = 1/q, thus:  Inverted dropout helps to define the model once and just change a parameter (the keep/drop probability) to run train and test on the same model. Direct Dropout, instead, force you to modify the network during the test phase because if you don’t multiply by q the output the neuron will produce values that are higher respect to the one expected by the successive neurons (thus the following neurons can saturate or explode): that’s why Inverted Dropout is the more common implementation. References: Dropout Regularization, coursera by Andrew NG What is inverted dropout? Dropout: scaling the activation versus inverting the dropout Analysis of Dropout How implement inverted dropout Pytorch? How to implement in Numpy? How to implement in Tensorflow? Implementation with Torch and bernoulli..I want the classifier to run faster and stop early if the patience reaches the number I set. In the following code it does 10 iterations of fitting the model. Here is the resulting error- I changed the cross_val_score in the following- and now I get this error- This code came from here. The code is by far the most accurate I've used so far. The problem is that there is no defined model.fit() anywhere in the code. It also takes forever to fit. The fit() operation occurs at the results = cross_val_score(...) and there's no parameters to throw a callback in there. How do I go about doing this?
Also, how do I run the model trained on a test set? I need to be able to save the trained model for later use... Reading from here, which is the source code of KerasClassifier, you can pass it the arguments of fit and they should be used.
I don't have your dataset so I cannot test it, but you can tell me if this works and if not I will try and adapt the solution. Change this line : A small explaination of what's happening : KerasClassifier is taking all the possibles arguments for fit, predict, score and uses them accordingly when each method is called. They made a function that filters the arguments that should go to each of the above functions that can be called in the pipeline. 
I guess there are several fit and predict calls inside the StratifiedKFold step to train on different splits everytime.  The reason why it takes forever to fit and it fits 10 times is because one fit is doing 300 epochs, as you asked. So the KFold is repeating this step over the different folds : EDIT : Ok, so I took the time to download the dataset and try your code... First of all you need to correct a "few" things in your network :  your input have a 60 features. You clearly show it in your data prep : so why would you have this : please change to :  About your targets/labels. You changed the objective from the original code (binary_crossentropy) to categorical_crossentropy. But you didn't change your Y array. So either do this in your data preparation : or change your objective back to binary_crossentropy. Now the network's output size : 122 on the last dense layer? your dataset obviously has 2 categories so why are you trying to output 122 classes? it won't match the target. Please change back your last layer to : if you choose to use categorical_crossentropy, or  if you go back to binary_crossentropy. So now that your network compiles, I could start to troubleshout. here is your solution So now I could get the real error message. It turns out that when you feed fit_params=whatever in the cross_val_score() function, you are feeding those parameters to a pipeline. In order to know to which part of the pipeline you want to send those parameters you have to specify it like this : Your error was saying that the process couldn't unpack 'callbacks'.split('__', 1) into 2 values. It was actually looking for the name of the pipeline's step to apply this to. It should be working now :) BUT, you should be aware of what's happening here... the cross validation actually calls the create_baseline() function to recreate the model from scratch 10 times an trains it 10 times on different parts of the dataset. So it's not doing epochs as you were saying, it's doing 300 epochs 10 times. 
What is also happening as a consequence of using this tool : since the models are always differents, it means the fit() method is applied 10 times on different models, therefore, the callbacks are also applied 10 different times and the files saved by ModelCheckpoint() get overriden and you find yourself only with the best model of the last run. This is intrinsec to the tools you use, I don't see any way around this. This comes as consequence to using different general tools that weren't especially thought to be used together with all the possible configurations. Try: where list_of_callbacks is a list of callbacks you want to apply. You could find details here. It's mentioned there that parameters fed to KerasClassifier could be legal fitting parameters. It's also worth to mention that if you are using multiple runs with GPUs there might be a problem due to several reported memory leaks especially when you are using theano. I also noticed that running multiple fits consequently may show results which seem to be not independent when using sklearn API.  Edit: Try also: Instead of putting callbacks list in a wrapper instantiation. This is what I have done and has worked so far Despite the TensorFlow, Keras & SciKeras documentation suggesting you can define training callbacks via the fit method, for my setup it turns out (like @NassimBen suggests) you should do it through the model constructor instead. Rather than this: Try this:I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops? Here are the original input variables: A is a 2x4 array.
B is a 3x4 array. We want to compute the Euclidean distance matrix operation in one entirely vectorized operation, where dist[i,j] contains the distance between the ith instance in A and jth instance in B. So dist is 2x3 in this example. The distance   could ostensibly be written with numpy as However, as shown above, the problem is that the element-wise subtraction operation A-B involves incompatible array sizes, specifically the 2 and 3 in the first dimension. In order to do element-wise subtraction, we have to pad either A or B to satisfy numpy's broadcast rules. I'll choose to pad A with an extra dimension so that it becomes 2 x 1 x 4, which allows the arrays' dimensions to line up for broadcasting. For more on numpy broadcasting, see the tutorial in the scipy manual and the final example in this tutorial. You can perform the padding with either np.newaxis value or with the np.reshape command. I show both below: As you can see, using either approach will allow the dimensions to line up. I'll use the first approach with np.newaxis. So now, this will work to create A-B, which is a 2x3x4 array: Now we can put that difference expression into the dist equation statement to get the final result: Note that the sum is over axis=2, which means take the sum over the 2x3x4 array's third axis (where the axis id starts with 0). If your arrays are small, then the above command will work just fine. However, if you have large arrays, then you may run into memory issues. Note that in the above example, numpy internally created a 2x3x4 array to perform the broadcasting. If we generalize A to have dimensions a x z and B to have dimensions b x z, then numpy will internally create an a x b x z array for broadcasting. We can avoid creating this intermediate array by doing some mathematical manipulation. Because you are computing the Euclidean distance as a sum-of-squared-differences, we can take advantage of the mathematical fact that sum-of-squared-differences can be rewritten.  Note that the middle term involves the sum over element-wise multiplication. This sum over multiplcations is better known as a dot product. Because A and B are each a matrix, then this operation is actually a matrix multiplication. We can thus rewrite the above as:  We can then write the following numpy code: Note that the answer above is exactly the same as the previous implementation. Again, the advantage here is the we do not need to create the intermediate 2x3x4 array for broadcasting. For completeness, let's double-check that the dimensions of each summand in threeSums allowed broadcasting. So, as expected, the final dist array has dimensions 2x3. This use of the dot product in lieu of sum of element-wise multiplication is also discussed in this tutorial. I had the same problem recently working with deep learning(stanford cs231n,Assignment1),but when I used There was a error That means I ran out of memory(In fact,that produced a array of 500*5000*1024 in the middle.It's so huge!) To prevent that error,we can use a formula to simplify:  code: Simply use np.newaxis at the right place: This functionality is already included in scipy's spatial module and I recommend using it as it will be vectorized and highly optimized under the hood. But, as evident by the other answer, there are ways you can do this yourself. Using numpy.linalg.norm also works well with broadcasting. Specifying an integer value for axis will use a vector norm, which defaults to Euclidean norm.Here's the code I'm working with (pulled from Kaggle mostly): I have 4 classes that are very imbalanced. Class A equals 70%, class B = 15%, class C = 10%, and class D = 5%. However, I care most about class D. So I did the following type of calculations: D_weight = A/D = 70/5 = 14 and so on for the weight for class B and A. (if there are better methods to select these weights, then feel free) In the last line, I'm trying to properly set class_weights and I'm doing it as so: class_weights = {0: 1.0, 1: 6, 2: 7, 3: 14}. However, when I do this, I get the following error.  class_weight not supported for 3+ dimensional targets. Is it possible that I add a dense layer after the last layer and just use it as a dummy layer so I can pass the class_weights and then only use the output of the last conv2d layer to do the prediction?  If this is not possible, how would I modify the loss function (I'm aware of this post, however, just passing in the weights in to the loss function won't cut it, because the loss function is called separately for each class) ? Currently, I'm using the following loss function:  But I don't see any way in which I can input class weights. If someone wants the full working code see this post. But remember to change the final conv2d layer's num classes to 4 instead of 1.  You can always apply the weights yourself. The originalLossFunc below you can import from keras.losses.
The weightsList is your list with the weights ordered by class.  For using this in compile: You can change the balance of the input samples too.  For instance, if you have 5 samples from class 1 and 10 samples from class 2, pass the samples for class 5 twice in the input arrays. .   Instead of working "by class", you can also work "by sample". Create an array of weights for each sample in your input array: len(x_train) == len(weights)  And fit passing this array to the sample_weight argument.
(If it's fit_generator, the generator will have to return the weights along with the train/true pairs: return/yield inputs, targets, weights)I am struggling to use Random Forest in Python with Scikit learn. My problem is that I use it for text classification (in 3 classes - positive/negative/neutral) and the features that I extract are mainly words/unigrams, so I need to convert these to numerical features. I found a way to do it with DictVectorizer's fit_transform: My problem is that the fit_transform method is working on the train dataset, which contains around 8000 instances, but when I try to convert my test set to numerical features too, which is around 80000 instances, I get a memory error saying that: What could possibly cause this and is there any workaround? Many thanks! You are not supposed to do fit_transform on your test data, but only transform. Otherwise, you will get different vectorization than the one used during training. For the memory issue, I recommend TfIdfVectorizer, which has numerous options of reducing the dimensionality (by removing rare unigrams etc.). UPDATE If the only problem is fitting test data, simply split it to small chunks. Instead of something like you can do and record results/stats and analyze them afterwards. in particularI have built a simple neural network, and I would get its weights by: but, in this way, I only get the weights matrices (5x20 , 1x20) without the biases. How can I get the biases values? Quite simple, its just the second element in the array returned by get_weights() (For Dense layers): Here's a complete working example (implemented with TensorFlow 2 and Keras). YOu can view and output biases and weights using the following code: if you're looking for weights and bias from the validation dataset, you need to do model.predict on each vector from the dataset.I am running elastic net regularization in caret using glmnet. I pass sequence of values to trainControl for alpha and lambda, then I perform repeatedcv to get the optimal tunings of alpha and lambda. Here is an example where the optimal tunings for alpha and lambda are 0.7 and 0.5 respectively: My question?  When I run as.matrix(coef(model.test$finalModel)) which I would assume give me the coefficients corresponding to the best model, I get 100 different sets of coefficients.  So how do I get the coefficients corresponding to the best tuning? I've seen this recommendation to get the best model coef(model.test$finalModel, model.test$bestTune$lambda) However, this returns NULL coefficients, and In any case, would only be returning the best tunings related to lambda, and not to alpha in addition. EDIT: After searching everywhere on the internet, all I can find now which points me in the direction of the correct answer is this blog post, which says that model.test$finalModel returns the model corresponding to the best alpha tuning, and coef(model.test$finalModel, model.caret$bestTune$lambda) returns the set of coefficients corresponding to the best values of lambda. If this is true then this is the answer to my question. However, as this is a single blog post, and I can't find anything else to back up this claim, I am still skeptical. Can anyone validate this claim that model.test$finalModel returns the model corresponding to the best alpha?? If so then this question would be solved. Thanks! After a bit of playing with your code I find it very odd that glmnet train chooses different lambda ranges depending on the seed. Here is an example: optimum lambda is: and this works: giving the coefficients at best alpha and lambda when using this model to predict some y are predicted as X1 and some as X2 now with the seed you used lambda values are 10 times smaller and this gives empty coefficients since lambdaOpt is not in the range of tested lambda: now when predicting upon this model only X0 is predicted (the first level): quite odd behavior, probably worth reportingI am getting the following error c50 code called exit with value 1 I am doing this on the titanic data available from Kaggle Output :- Then I tried using C5.0 dtree So running the above lines gives me this error I'm not able to figure out what's going wrong? I was using similar code on different dataset and it was working fine. Any ideas about how can I debug my code? -Thanks For anyone interested, the data can be found here: http://www.kaggle.com/c/titanic-gettingStarted/data. I think you need to be registered in order to download it. Regarding your problem, first of I think you meant to write Next, notice the structure of the Cabin and Embarked Columns. These two factors have an empty character as a level name (check with levels(train$Embarked)). This is the point where C50 falls over. If you modify your data such that your algorithm will now run without an error. Just in case. You can take a look to the error by Also this error occurs when there are a special characters in the name of a variable. For example, one will get this error if there is "я"(it's from Russian alphabet) character in the name of a variable. Here is what worked finally:- Got this idea after reading this post The intuition behind this is that in this way both the train and test data set will have consistent factor levels. I had the same error, but I was using a numeric dataset without missing values. After a long time, I discovered that my dataset had a predictive attribute called "outcome" and the C5.0Control use this name, and this was the error cause :'( My solution was changing the column name. Other way, would be create a  C5.0Control object and change the value of the label attribute and then pass this object as parameter for the C50 method. I also struggled some hours with the same Problem (Return code "1") when building a model as well as when predicting.
With the hint of answer of Marco I have written a small function to remove all factor levels equal to "" in a data frame or vector, see code below. However, since R does not allow for pass by reference to functions, you have to use the result of the function (it can not change the original dataframe): Call of the functions may look like this: However, it seems, that C50 has a similar Problem with character columns containing an empty cell, so you will have probably to extend this to handle also character attributes if you have some. I also got the same error, but it was because of some illegal characters in the factor levels of one the columns. I used make.names function and corrected the factor levels: Then the problem was resolved.I am  trying to build a classifier with LightGBM on a very imbalanced dataset. Imbalance is in the ratio 97:3, i.e.: Params I used and the code for training is as shown below. I ran CV to get the best model and best round. I got 0.994 AUC on CV and similar score in Validation set. But when I am predicting on the test set I am getting very bad results. I am sure that the train set is sampled perfectly. What parameters are needed to be tuned.? What is the reason for the problem.? Should I resample the dataset such that the highest class is reduced.? The issue is that, despite the extreme class imbalance in your dataset, you are still using the "default" threshold of 0.5 when deciding the final hard classification in This should not be the case here. This is a rather big topic, and I strongly suggest you do your own research (try googling for threshold or cut off probability imbalanced data), but here are some pointers to get you started... From a relevant answer at Cross Validated (emphasis added): Don't forget that you should be thresholding intelligently to make predictions. It is not always best to predict 1 when the model probability is greater 0.5. Another threshold may be better. To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold. From a relevant academic paper, Finding the Best Classification Threshold in Imbalanced Classification: 2.2. How to set the classification threshold for the testing set Prediction
results
are
ultimately
determined
according
to
prediction
probabilities.
The
threshold
is
typically
set
to
0.5.
If
the
prediction
probability
exceeds
0.5,
the
sample
is
predicted
to
be
positive;
otherwise,
negative.
However,
0.5
is
not
ideal
for
some
cases,
particularly
for
imbalanced
datasets. The post Optimizing Probability Thresholds for Class Imbalances from the (highly recommended) Applied Predictive Modeling blog is also relevant. Take home lesson from all the above: AUC is seldom enough, but the ROC curve itself is often your best friend... On a more general level regarding the role of the threshold itself in the classification process (which, according to my experience at least, many practitioners get wrong), check also the Classification probability threshold thread (and the provided links) at Cross Validated; key point: the statistical component of your exercise ends when you output a probability for each class of your new sample. Choosing a threshold beyond which you classify a new observation as 1 vs. 0 is not part of the statistics any more. It is part of the decision component.I am trying to implement batch gradient descent on a data set with a single feature and multiple training examples (m). When I try using the normal equation, I get the right answer but the wrong one with this code below which performs batch gradient descent in MATLAB. y is the vector with target values, X is a matrix with the first column full of ones and second columns of values (variable). I have implemented this using vectorization, i.e  ... where delta is a 2 element column vector initialized to zeroes. The cost function J(Theta) is 1/(2m)*(sum from i=1 to m [(h(theta)-y)^2]). The error is very simple.  Your delta declaration should be inside the first for loop.  Every time you accumulate the weighted differences between the training sample and output, you should start accumulating from the beginning.   By not doing this, what you're doing is accumulating the errors from the previous iteration which takes the error of the the previous learned version of theta into account which isn't correct.  You must put this at the beginning of the first for loop. In addition, you seem to have an extraneous computeCost call.  I'm assuming this calculates the cost function at every iteration given the current parameters, and so I'm going to create a new output array called cost that shows you this at each iteration.  I'm also going to call this function and assign it to the corresponding elements in this array: FWIW, I don't consider this implementation completely vectorized.  You can eliminate the second for loop by using vectorized operations.  Before we do that, let me cover some theory so we're on the same page.  You are using gradient descent here in terms of linear regression.  We want to seek the best parameters theta that are our linear regression coefficients that seek to minimize this cost function:  m corresponds to the number of training samples we have available and x^{i} corresponds to the ith training example.  y^{i} corresponds to the ground truth value we have associated with the ith training sample.  h is our hypothesis, and it is given as:  Note that in the context of linear regression in 2D, we only have two values in theta we want to compute - the intercept term and the slope. We can minimize the cost function J to determine the best regression coefficients that can give us the best predictions that minimize the error of the training set.  Specifically, starting with some initial theta parameters... usually a vector of zeroes, we iterate over iterations from 1 up to as many as we see fit, and at each iteration, we update our theta parameters by this relationship:  For each parameter we want to update, you need to determine the gradient of the cost function with respect to each variable and evaluate what that is at the current state of theta.  If you work this out using Calculus, we get:  If you're unclear with how this derivation happened, then I refer you to this nice Mathematics Stack Exchange post that talks about it:   https://math.stackexchange.com/questions/70728/partial-derivative-in-gradient-descent-for-two-variables Now... how can we apply this to our current problem?  Specifically, you can calculate the entries of delta quite easily analyzing all of the samples together in one go.  What I mean is that you can just do this: The operations on delta(1) and delta(2) can completely be vectorized in a single statement for both.  What you are doing theta^{T}*X^{i} for each sample i from 1, 2, ..., m.  You can conveniently place this into a single sum statement. We can go even further and replace this with purely matrix operations.  First off, what you can do is compute theta^{T}*X^{i} for each input sample X^{i} very quickly using matrix multiplication.  Suppose if:  Here, X is our data matrix which composes of m rows corresponding to m training samples and n columns corresponding to n features.  Similarly, theta is our learned weight vector from gradient descent with n+1 features accounting for the intercept term. If we compute X*theta, we get:  As you can see here, we have computed the hypothesis for each sample and have placed each into a vector.  Each element of this vector is the hypothesis for the ith training sample.  Now, recall what the gradient term of each parameter is in gradient descent:  We want to implement this all in one go for all of the parameters in your learned vector, and so putting this into a vector gives us:  Finally:  Therefore, we know that y is already a vector of length m, and so we can very compactly compute gradient descent at each iteration by: .... so your code is now just:I have read the CNN Tutorial on the TensorFlow and I am trying to use the same model for my project. 
The problem is now in data reading. I have around 25000 images for training and around 5000 for testing and validation each. The files are in png format and I can read them and convert them into the numpy.ndarray.  The CNN example in the tutorials use a queue to fetch the records from the file list provided. I tried to create my own such binary file by reshaping my images into 1-D array and attaching a label value in the front of it. So my data looks like this  The single row of the above array is of length 22501 size where the first element is the label. I dumped the file to using pickle and the tried to read from the file using the 
tf.FixedLengthRecordReader to read from the file as demonstrated in example I am doing the same things as given in the cifar10_input.py to read the binary file and putting them into the record object. Now when I read from the files the labels and the image values are different. I can understand the reason for this to be that pickle dumps the extra information of braces and brackets also in the binary file and they change the fixed length record size.  The above example uses the filenames and pass it to a queue to fetch the files and then the queue to read a single record from the file.  I want to know if I can pass the numpy array as defined above instead of the filenames to some reader and it can fetch records one by one from that array instead of the files. Probably the easiest way to make your data work with the CNN example code is to make a modified version of read_cifar10() and use it instead: Write out a binary file containing the contents of your numpy array. This file is similar to the format used in CIFAR10 datafiles. You might want to generate multiple files in order to get read parallelism. Note that ndarray.tofile() writes binary data in row-major order with no other metadata; pickling the array will add Python-specific metadata that TensorFlow's parsing routines do not understand. Write a modified version of read_cifar10() that handles your record format. Modify distorted_inputs() to use your new dataset: This is intended to be a minimal set of steps, given your starting point. It may be more efficient to do the PNG decoding using TensorFlow ops, but that would be a larger change. In your question, you specifically asked: I want to know if I can pass the numpy array as defined above instead of the filenames to some reader and it can fetch records one by one from that array instead of the files. You can feed the numpy array to a queue directly, but it will be a more invasive change to the cifar10_input.py code than my other answer suggests. As before, let's assume you have the following array from your question: You can then define a queue that contains the entire data as follows: ...then call sess.run(enqueue_op) to populate the queue. Another—more efficient—approach would be to feed records to the queue, which you could do from a parallel thread (see this answer for more details on how this would work): Alternatively, to enqueue a batch at a time, which will be more efficient: I want to know if I can pass the numpy array as defined above instead
  of the filenames to some reader and it can fetch records one by one
  from that array instead of the files. tf.py_func, that wraps a python function and uses it as a TensorFlow operator, might help. Here's an example.  However, since you've mentioned that your images are stored in png files, I think the simplest solution would be to replace this: with this:My model is a simple fully connected network like this: So, after saving the model I want to give input to layer 3. What I am doing right now is this: But it isn't working, i.e. I am getting errors like incompatible input, inputs should be tuple etc. The error message is: Is there any way I can pass my own inputs in middle of network and get the output instead of giving an input at the start and getting output from the end? Any help will be highly appreciated. First you must learn that in Keras when you apply a layer on an input, a new node is created inside this layer which connects the input and output tensors. Each layer may have multiple nodes connecting different input tensors to their corresponding output tensors. To build a model, these nodes are traversed and a new graph of the model is created which consists all the nodes needed to reach output tensors from input tensors (i.e. which you specify when creating a model: model = Model(inputs=[...], outputs=[...]). Now you would like to feed an intermediate layer of a model and get the output of the model. Since this is a new data-flow path, we need to create new nodes for each layer corresponding to this new computational graph. We can do it like this: Fortunately, your model consists of one-branch and we could simply use a for loop to construct the new model. However, for more complex models it may not be easy to do so and you may need to write more codes to construct the new model. Here is another method for achieving the same result. Initially create a new input layer and then connect it to the lower layers(with weights). For this purpose, first re-initialize these layers(with same name) and reload the corresponding weights from the parent model using  new_model.load_weights("parent_model.hdf5", by_name=True) This will load the required weights from the parent model.Just make sure you name your layers properly beforehand. This method will work for complex models with multiple inputs or branches.You just need to copy the same code for required layers, connect the new inputs and finally load the corresponding weights. You can easily use keras.backend.function for this purpose: Sorry for ugly function naming - do it best) I was having the same problem and the proposed solutions worked for me but I was looking for something more explicit, so here it is for future reference:  Reference:
https://keras.io/guides/functional_api/#shared-layersNLTK package provides a method show_most_informative_features() to find the most important features for both class, with output like:  As answered in this question How to get most informative features for scikit-learn classifiers? , this can also work in scikit-learn. However, for binary classifier, the answer in that question only outputs the best feature itself.  So my question is, how can I identify the feature's associated class, like the example above (outstanding is most informative in pos class, and seagal is most informative in negative class)? EDIT: actually what I want is a list of most informative words for each class. How can I do that? Thanks! In the case of binary classification, it seems like the coefficient array has been flatten. Let's try to relabel our data with only two labels: [out]: So let's do some diagnostics: [out]: Seems like the features are counted and then when vectorized it was flattened to save memory, so let's try: [out]: Now we see some patterns... Seems like the higher coefficient favors a class and the other tail favors the other, so you can simply do this: [out]: Actually if you've read @larsmans comment carefully, he gave the hint on the binary classes' coefficient in How to get most informative features for scikit-learn classifiers? Basically you need: classifier.classes_ accesses the index of the class labels you have in the classifier vectorizer.get_feature_names() is self-explanatory sorted(zip(classifier.coef_[labelid], feature_names))[-n:] retrieves the coefficient of the classifier for a given class label and then sorts it in ascending order.  I'm going to use a simple example from https://github.com/alvations/bayesline Input file train.txt: Code: [out]: You can get the same with two classes on the left and right side:I am trying to teach my SVM algorithm using data of clicks and conversion by people who see the banners. The main problem is that the clicks is around 0.2% of all data so it's big disproportion in it. When I use simple SVM in testing phase it always predict only "view" class and never "click" or "conversion". In average it gives 99.8% right answers (because of disproportion), but it gives 0% right prediction if you check "click" or "conversion" ones. How can you tune the SVM algorithm (or select another one) to take into consideration the disproportion? The most basic approach here is to use so called "class weighting scheme" - in classical SVM formulation there is a C parameter used to control the missclassification count. It can be changed into C1 and C2 parameters used for class 1 and 2 respectively. The most common choice of C1 and C2 for a given C is to put where n1 and n2 are sizes of class 1 and 2 respectively. So you "punish" SVM for missclassifing the less frequent class much harder then for missclassification the most common one. Many existing libraries (like libSVM) supports this mechanism with class_weight parameters. Example using python and sklearn In particular, in sklearn you can simply turn on the automatic weighting by setting class_weight='auto'.   This paper describes a variety of techniques. One simple (but very bad method for SVM) is just replicating the minority class(s) until you have a balance: http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdfI am running Ridge regression with the use of glmnet R package. I noticed that the coefficients I obtain from glmnet::glmnet function are different from those I get by computing coefficients by definition (with the use of the same lambda value). Could somebody explain me why? Data (both: response Y and design matrix X) are scaled.   If you read ?glmnet, you will see that the penalized objective function of Gaussian response is: In case the ridge penalty 1/2 * ||beta_j||_2^2 is used, we have which is proportional to This is different to what we usually see in textbook regarding ridge regression: The formula you write: is for the textbook result; for glmnet we should expect: So, the textbook uses penalized least squares, but glmnet uses penalized mean squared error. Note I did not use your original code with t(), "%*%" and solve(A) %*% b; using crossprod and solve(A, b) is more efficient! See Follow-up section in the end. Now let's make a new comparison:  Note that I have set intercept = FALSE when I call cv.glmnet (or glmnet). This has more conceptual meaning than what it will affect in practice. Conceptually, our textbook computation has no intercept, so we want to drop intercept when using glmnet. But practically, since your X and Y are standardized, the theoretical estimate of intercept is 0. Even with intercepte = TRUE (glment default), you can check that the estimate of intercept is ~e-17 (numerically 0), hence estimate of other coefficients is not notably affected. The other answer is just showing this. Follow-up As for the using crossprod and solve(A, b) - interesting! Do you by chance have any reference to simulation comparison for that?  t(X) %*% Y will first take transpose X1 <- t(X), then do X1 %*% Y, while crossprod(X, Y) will not do the transpose. "%*%" is a wrapper for DGEMM for case op(A) = A, op(B) = B, while crossprod is a wrapper for op(A) = A', op(B) = B. Similarly tcrossprod for op(A) = A, op(B) = B'. A major use of crossprod(X) is for t(X) %*% X; similarly the tcrossprod(X) for X %*% t(X), in which case DSYRK instead of DGEMM is called. You can read the first section of Why the built-in lm function is so slow in R? for reason and a benchmark.  Be aware that if X is not a square matrix, crossprod(X) and tcrossprod(X) are not equally fast as they involve different amount of floating point operations, for which you may read the side notice of Any faster R function than “tcrossprod” for symmetric dense matrix multiplication? Regarding solvel(A, b) and solve(A) %*% b, please read the first section of How to compute diag(X %% solve(A) %% t(X)) efficiently without taking matrix inverse? Adding on top of Zheyuan's interesting post, did some more experiments to see that we can get the same results with intercept as well, as follows:I have the following Python test code (the arguments to ALS.train are defined elsewhere): Which works, because it has a count of 1 against the predictions variable and outputs: However, when I try and use an RDD I created myself using the following code, it doesn't appear to work anymore: Which outputs: As you can see, predictAllcomes back empty when passed the mapped RDD. The values going in are both of the same format. The only noticeable difference that I can see is that the first example uses parallelize and produces a ParallelCollectionRDDwhereas the second example just uses a map which produces a PythonRDD. Does predictAll only work if passed a certain type of RDD? If so, is it possible to convert between RDD types? I'm not sure how to get this working.  There are two basic conditions under which MatrixFactorizationMode.predictAll may return a RDD with lower number of items than the input: You can easily reproduce this behavior and check that it is is not dependent on the way how RDD has been created. First lets use example data to build a model: Next lets see which products and users are present in the training data: Now lets create test data and check predictions: So far so good. Next lets map it using the same logic as in your code: Still fine. Next lets create invalid data and repeat experiment: As expected there are no predictions for invalid input. Finally you can confirm this is really the case by using ML model which is completely independent in training / prediction from Python code: As you can see no corresponding user / item in the training data means no prediction.As part of the Enron project, built the attached model, Below is the summary of the steps, Used Kbest to find out the scores and sorted the features and trying a combination of higher and lower scores. Used SVM with a GridSearch using a StratifiedShuffle  Used the best_estimator_  to predict and calculate the precision and recall.  The problem is estimator is spitting out perfect scores, in some case 1  But when I refit the best classifier on training data then run the test it gives reasonable scores.  My doubt/question was what exactly GridSearch does with the test data after the split using the Shuffle split object we send in to it. I assumed it would not fit anything on Test data, if that was true then when I predict using the same test data, it should not give this high scores right.? since i used random_state value, the shufflesplit should have created the same copy for the Grid fit and also for the predict.  So, is using the same Shufflesplit for two wrong?  GridSearchCV as @Gauthier Feuillen said is used to search best parameters of an estimator for given data.
Description of GridSearchCV:- Because of last step, you are getting different scores in first and second approach. Because in the first approach, all data is used for training and you are predicting for that data only. Second approach has prediction on previously unseen data. Basically the grid search will: So your second case is the good one. Otherwise you are actually predicting data that you trained with (which is not the case in the second option, there you only keep the best parameters from your gridsearch)We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 3 years ago. I have worked recently with the DARPA network traffic packets and the derived version of it used in KDD99 for intrusion detection evaluation.  Excuse my limited domain knowledge in computer networks, I could only derive 9 features from the DARPA packet headers. and Not the 41 features used in KDD99.  I am intending to continue my work on the UNB ISCX Intrusion Detection Evaluation DataSet. However, I want to derive from the pcap files the 41 features used in the KDD99 and save it in a CSV format. Is there a fast/easy way to achieve this? Be careful with this data set. http://www.kdnuggets.com/news/2007/n18/4i.html Some excerpts: the artificial data was generated using a closed network, some proprietary network traffic generators, and hand-injected attacks Among the issues raised, the most important seemed to be that no validation was ever performed to show that the DARPA dataset actually looked like real network traffic. In 2003, Mahoney and Chan built a trivial intrusion detection system and ran it against the DARPA tcpdump data. They found numerous irregularities, including that -- due to the way the data was generated -- all the malicious packets had a TTL of 126 or 253 whereas almost all the benign packets had a TTL of 127 or 254. the DARPA dataset (and by extension, the KDD Cup '99 dataset) was fundamentally broken, and one could not draw any conclusions from any experiments run using them we strongly recommend that (1) all researchers stop using the KDD Cup '99 dataset As for the feature extraction used. IIRC the majority of features simply were attributes of the parsed IP/TCP/UDP headers. Such as, port number, last octet of IP, and some packet flags. As such, these findings no longer reflect realistic attacks anymore anyway. Todays TCP/IP stacks are much more robust than at the time the data set was created, where a "ping of death" would instantly lock up a windows host. Every developer of a TCP/IP stack should by now be aware of the risk of such malformed packets and stress-test the stack against such things. With this, these features have become pretty much meaningless. Incorrectly set SYN flags etc. are no longer used in network attacks; these are much more sophisticated; and most likely no longer attacking the TCP/IP stack, but the services running on the next layer. So I would not bother finding out which low level packet flags were used in that '99 flawed simulation using attacks that worked in the early '90s...My input is a array of 64 integers. I have 10,000 of these arrays in my training set.  And I supposed to be specifying this in order for conv1D to work? I am getting the dreaded error and I really don't understand what I need to do. Don't let the name confuse you. The layer tf.keras.layers.Conv1D needs the following shape: (time_steps, features). If your dataset is made of 10,000 samples with each sample having 64 values, then your data has the shape (10000, 64), which is not directly applicable to the tf.keras.layers.Conv1D layer. You are missing the time_steps dimension. What you can do is use the tf.keras.layers.RepeatVector, which repeats your array input n times, in the example 5. This way your Conv1D layer gets an input of the shape (5, 64). Check out the documentation for more information: As a side note, you should ask yourself if using a tf.keras.layers.Conv1D layer is the right option for your use case. This layer is usually used for NLP and other time series tasks. For example, in sentence classification, each word in a sentence is usually mapped to a high-dimensional word vector representation, as seen in the image. This results in data with the shape (time_steps, features).                                            If you want to use character one hot encoded embeddings it would look something like this:                                            This is a simple example of one single sample with the shape (10, 10) --> 10 characters along the time series dimension and 10 features. It  should help you understand the tutorial I mentioned a bit better. The Conv1D layer does temporal convolution, that is, along the first dimension (not the batch dimension of course), so you should put something like this: You will need to slice your data into time_steps temporal slices to feed the network. However, if your arrays don't have a temporal structure, then conv1D is not the layer you are looking for.What I want to do: I wish to compute a cross_val_score using roc_auc on a multiclass problem What I tried to do: Here is a reproducible example made with iris data set. I one hot encode my target  I use a decision tree classifier Finaly I perform cross val What is failing: This last line throw the following error My env: python==3.7.2 sklearn==0.19.2 My question: Is it a bug, or I'm making a miss-use? An unnecessary annoyance with the cross-validation functionality of scikit-learn is that, by default, the data are not shuffled; it would arguably be a good idea to make shuffling the default choice - of course, this would pre-suppose that a shuffling argument would be available for cross_val_score in the first place, but unfortunately it is not (docs). So, here is what is happening; the 150 samples of the iris dataset are stratified: Now, a 3-fold CV procedure with 150 samples stratified as shown above and an error message saying: should hopefully start making sense: in each one of your 3 validation folds only one label is present, so no ROC calculation is possible (let alone the fact that in each validation fold the model sees labels unseen in the respective training folds). So, just shuffle your data before: and you should be fine.I have a dataframe as below:
 I want to get the name of the column if column of a particular row if it contains 1 in the that column. Use DataFrame.dot: If there is multiple 1 per row: Your question is very ambiguous and I recommend reading this link in @sammywemmy's comment. If I understand your problem correctly... we'll talk about this mask first: What's happening? Lets work our way outward starting from within df.columns[**HERE**] : "Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent". This gives us a handy Series to mask the column names with. We will use this example to automate for your solution below Automate to get an output of (<row index> ,[<col name>, <col name>,..]) where there is 1 in the row values. Although this will be slower on large datasets, it should do the trick: Next step is a for loop that iterates the contents of each df in df_dict, checks them with the mask we created earlier, and prints the intended results: You see how I generated sample data that can be easily reproduced? In the future, please try to ask questions with posted sample data that can be reproduced. This way it helps you understand your problem better and it is easier for us to answer it for you. Getting column name are dividing in 2 sections. If you want in a new column name then condition should be unique because it will only give 1 col name for each row. If you were looking for min or maximum 2nd case, If your condition is satisfied in multiple columns for example you are looking for columns that contain 1 and you are looking for list because its not possible to adjust in same dataframe. Or you are looking for numerical condition columns contains value more than 1 Happy learningIf I am using two method (NN and KNN) with caret and then I want to provide significance test, how can I do wilcoxon test. I provided sample of my data as follows How to perform wilcox.test() here. One way to deal with your problem is to generate several performance values for knn and NN which you can compare using a statistical test. This can be achieved using Nested resampling. In nested resampling you are performing train/test splits multiple times and evaluating the model on each test set. Lets for instance use BostonHousing data: lets just select numerical columns for the example to make it simple: As far as I know there is no way to perform nested CV in caret out of the box so a simple wrapper is needed: generate outer folds for nested CV: Lets use bootstrap resampling as the inner resample loop to tune the hyper parameters: now loop over the outer folds and perform hyper parameter optimization using the train set and predict on the test set: extract just MAE from the results: Do the same for glmnet learner for instance: now compare the two using wilcox.test. Since the performance for both learners was generated using the same data splits a paired test is appropriate: If comparing more than two algorithms one can use friedman.test Does this work for you?I would like to build a GBM model with H2O. My data set is imbalanced, so I am using the balance_classes parameter. For grid search (parameter tuning) I would like to use 5-fold cross validation. I am wondering how H2O deals with class balancing in that case. Will only the training folds be rebalanced? I want to be sure the test-fold is not rebalanced. In class imbalance settings, artificially balancing the test/validation set does not make any sense: these sets must remain realistic, i.e. you want to test your classifier performance in the real world setting, where, say, the negative class will include the 99% of the samples, in order to see how well your model will do in predicting the 1% positive class of interest without too many false positives. Artificially inflating the minority class or reducing the majority one will lead to performance metrics that are unrealistic, bearing no real relation to the real world problem you are trying to solve. For corroboration, here is Max Kuhn, creator of the caret R package and co-author of the (highly recommended) Applied Predictive Modelling textbook, in Chapter 11: Subsampling For Class Imbalances of the caret ebook: You would never want to artificially balance the test set; its class frequencies should be in-line with what one would see “in the wild”. Re-balancing makes sense only in the training set, so as to prevent the classifier from simply and naively classifying all instances as negative for a perceived accuracy of 99%. Hence, you can rest assured that in the setting you describe the rebalancing takes action only for the training set/folds. A way to force balancing is using a weight columns to use different weights for different classes, in H2O weights_column