{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'https://stackoverflow.com'\n",
    "TAG = 'machine-learning'\n",
    "PAGES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hrefs(page: int) -> list[str]:\n",
    "    res = requests.get(f'{BASE}/questions/tagged/{TAG}?pagesize=50&page={page}')\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    return [i.get('href') for i in soup.select('a[class=s-link]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(href: str) -> str:\n",
    "    res = requests.get(f'{BASE}{href}')\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    posts = soup.select('div.s-prose.js-post-body')\n",
    "\n",
    "    content = []\n",
    "    for post in posts:\n",
    "        content += post.find_all('p')\n",
    "    \n",
    "    return ''.join(f'{i.text} ' for i in content).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = get_hrefs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_content(hrefs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am trying to implement skip gram algorithm in plain numpy (not pytorch) which requires doing calculation (possibly avoidable details at the end): So, this is a huge array 15*100000*2*50000 elements of type numpy.float64. There are two issues here: This takes huge memory (at least 3 GBs) as explained above. In fact, I was never able to complete this calculation because of second issue mentioned below. But it easily filled all my laptop RAM (total  16 GB). This takes huge time (at least couple of hours), may be because of first issue above. I also tried to pre-generate x with all zeroes as follows: But the moment I step over this second line in my debugger, my RAM fills up. How can deal with this? Avoidable details I am trying to implement skip gram algorithm, in which we have to prepare list of skip grams [target-word, context-words]. Each target-word and context-words are represented as one hot vector of size equal to input vocabulary size (50000 above). 100000 above is number of sentences in data. 15 is average number of words per sentence. PS I have to implement this in plain python + numpy. That is not using any ML library like pytorch or tensorflow 100000 sentences * 15 words per sentence * 50000 vocabulary size * 2 * 8 bytes per float 64 = 1200000000000 bytes = 1200 GiB. You cannot initialize this object in memory on your 16 GiB laptop.\\nMy guess is that you do not want to store all of the vectors of every sentence in memory but instead run the algorithm sentence by sentence? In this case it would take 12 MiB of memory per sentence.'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e96b7987e83db2bb51693718935e9a4c90615288c60e4ac5255cd193dc13706d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
